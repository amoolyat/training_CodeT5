Input function with masked if condition,Prediction correct (true/false),Expected if condition,Predicted if condition,CodeBLEU prediction score (0-100),BLEU-4 prediction score (0-100)
"def read(self, count=True, timeout=None, ignore_non_errors=True, ignore_timeouts=True): try: return self._read(count, timeout) except usb.USBError as e: if DEBUG_COMM: log.info( ""read: e.errno=%s e.strerror=%s e.message=%s repr=%s"" % (e.errno, e.strerror, e.message, repr(e)) ) if ignore_timeouts and is_timeout(e): return [] if ignore_non_errors and is_noerr(e): return [] raise",false,if ignore_timeouts and is_timeout ( e ) :,return []ifignore_non_errors andis_noerr(e):if:)if DEBUG_,62.5,13.08
"def _cache_mem(curr_out, prev_mem, mem_len, reuse_len=None): """"""cache hidden states into memory."""""" if mem_len is None or mem_len == 0: return None else: if reuse_len is not None and reuse_len > 0: curr_out = curr_out[:reuse_len] if prev_mem is None: new_mem = curr_out[-mem_len:] else: new_mem = tf.concat([prev_mem, curr_out], 0)[-mem_len:] new_mem.stop_gradient = True return new_mem",false,if prev_mem is None :,"= tf.concat([prev_mem,new_mem,= tf.concat([prev_mem,new_mem,",20.0,4.97
def filtered(gen): for example in gen: example_len = length_fn(example) # Checking max length boundary. if max_length is not None: if example_len > max_length: continue # Checking min length boundary. if min_length is not None: if example_len < min_length: continue # Within bounds. yield example,false,if example_len > max_length :,example_len:,40.0,18.31
"def search(self, query): # ""Search.ashx?query="" + query + filterVal if not query: logger.debug(""Empty search query"") return [] logger.debug('Searching TuneIn for ""%s""' % query) args = ""&query="" + query search_results = self._tunein(""Search.ashx"", args) results = [] for item in self._flatten(search_results): if item.get(""type"", """") == ""audio"": # Only return stations self._stations[item[""guide_id""]] = item results.append(item) return results",false,"if item . get ( ""type"" , """" ) == ""audio"" :","self._stations[item[""guide_id""]] =self._stations[item[""guide_id""]]self._stations[",38.89,1.77
"def _check_script(self, script, directive): for var in compile_script(script): if var.must_contain(""/""): # Skip variable checks return False if var.can_contain("".""): # Yay! Our variable can contain any symbols! reason = ( 'At least variable ""${var}"" can contain untrusted user input'.format( var=var.name ) ) self.add_issue(directive=[directive] + var.providers, reason=reason) return True return False",false,"if var . must_contain ( ""/"" ) :",var.providers:var.providers:,30.0,7.41
"def getAllDataLinkIDs(): linkDataIDs = set() dataType = _forestData.dataTypeBySocket for socketID, linkedIDs in _forestData.linkedSockets.items(): for linkedID in linkedIDs: if socketID[1]: # check which one is origin/target linkDataIDs.add( (socketID, linkedID, dataType[socketID], dataType[linkedID]) ) else: linkDataIDs.add( (linkedID, socketID, dataType[linkedID], dataType[socketID]) ) return linkDataIDs",false,if socketID [ 1 ] :,=_forestData.linkedSockets.items():=_forestData.linkedSockets.items():=,16.67,1.82
"def _stderr_supports_color(): try: if hasattr(sys.stderr, ""isatty"") and sys.stderr.isatty(): if curses: curses.setupterm() if curses.tigetnum(""colors"") > 0: return True elif colorama: if sys.stderr is getattr( colorama.initialise, ""wrapped_stderr"", object() ): return True except Exception: # Very broad exception handling because it's always better to # fall back to non-colored logs than to break at startup. pass return False",false,"if curses . tigetnum ( ""colors"" ) > 0 :",exceptexcept Exception:,8.33,0.0
"def offsets(self): offsets = {} offset_so_far = 0 for name, ty in self.fields.items(): if isinstance(ty, SimTypeBottom): l.warning( ""Found a bottom field in struct %s. Ignore and increment the offset using the default "" ""element size."", self.name, ) continue if not self._pack: align = ty.alignment if offset_so_far % align != 0: offset_so_far += align - offset_so_far % align offsets[name] = offset_so_far offset_so_far += ty.size // self._arch.byte_width return offsets",false,if offset_so_far % align != 0 :,offset_so_far=offset_so_far % align= ty.sizeoffset_so_far % align,50.0,25.92
"def Restore(self): picker, obj = self._window, self._pObject value = obj.RestoreValue(PERSIST_FILEDIRPICKER_PATH) if value is not None: if issubclass(picker.__class__, wx.FileDialog): if type(value) == list: value = value[-1] picker.SetPath(value) return True return False",false,if type ( value ) == list :,:,11.11,0.0
"def dt_s_tup_to_string(dt_s_tup): dt_string = dt_s_tup[0] # string for identifying the file to parse. if dt_s_tup[1] > 0: # if there are seasons in the model if ""co"" in dt_string or ""ci"" in dt_string or ""nc"" in dt_string: dt_string = dt_string[:2] + ""s"" + dt_string[2:] else: dt_string = ""s"" + dt_string return dt_string",false,"if ""co"" in dt_string or ""ci"" in dt_string or ""nc"" in dt_string :",=:=::::::::::::,5.26,1.35
"def writer(stream, items): sep = """" for item in items: stream.write(sep) sep = "" "" if not isinstance(item, str): item = str(item) if not PY3K: if not isinstance(item, unicode): item = str(item) stream.write(item) stream.write(""\n"")",false,"if not isinstance ( item , str ) :",=stream.write(sep))stream.write(sep),22.22,3.38
"def _get_result_keys(self, config): result_key = config.get(""result_key"") if result_key is not None: if not isinstance(result_key, list): result_key = [result_key] result_key = [jmespath.compile(rk) for rk in result_key] return result_key",false,"if not isinstance ( result_key , list ) :",config,0.0,0.0
"def _download_build_artifacts(self, build: Dict[str, Any]) -> None: arch = build[""arch_tag""] snap_build = self._lp_load_url(build[""self_link""]) urls = snap_build.getFileUrls() if not urls: logger.error(f""Snap file not available for arch {arch!r}."") return for url in urls: file_name = _get_url_basename(url) self._download_file(url=url, dst=file_name) if file_name.endswith("".snap""): logger.info(f""Snapped {file_name}"") else: logger.info(f""Fetched {file_name}"")",false,"if file_name . endswith ( "".snap"" ) :","if.snap""):self._download_file(url=url, dst=file_name)if..",81.82,19.59
"def _add_custom_statement(self, custom_statements): if custom_statements is None: return self.resource_policy[""Version""] = ""2012-10-17"" if self.resource_policy.get(""Statement"") is None: self.resource_policy[""Statement""] = custom_statements else: if not isinstance(custom_statements, list): custom_statements = [custom_statements] statement = self.resource_policy[""Statement""] if not isinstance(statement, list): statement = [statement] for s in custom_statements: if s not in statement: statement.append(s) self.resource_policy[""Statement""] = statement",false,if s not in statement :,"self.resource_policy[""Statement""] =self.resource_policy[""Statement""] =self.resource_policy[""Statement""] =",0.0,0.0
"def display_failures_for_single_test(result: TestResult) -> None: """"""Display a failure for a single method / endpoint."""""" display_subsection(result) checks = _get_unique_failures(result.checks) for idx, check in enumerate(checks, 1): message: Optional[str] if check.message: message = f""{idx}. {check.message}"" else: message = None example = cast(Case, check.example) # filtered in `_get_unique_failures` display_example(example, check.name, message, result.seed) # Display every time except the last check if idx != len(checks): click.echo(""\n"")",false,if check . message :,# filtered in `_get_unique_failures`# filtered in `_get_unique_failures`# filtered in `_,0.0,0.0
"def build(opt): dpath = os.path.join(opt[""datapath""], ""qangaroo"") version = ""v1.1"" if not build_data.built(dpath, version_string=version): print(""[building data: "" + dpath + ""]"") if build_data.built(dpath): # An older version exists, so remove these outdated files. build_data.remove_dir(dpath) build_data.make_dir(dpath) # Download the data. for downloadable_file in RESOURCES: downloadable_file.download_file(dpath) # Mark the data as built. build_data.mark_done(dpath, version_string=version)",false,if build_data . built ( dpath ) :,downloadable_file.download() #thethethethethethethethethethe data,37.5,6.27
"def call(self, step_input, states): new_states = [] for i in range(self.num_layers): out, new_state = self.lstm_cells[i](step_input, states[i]) step_input = ( layers.dropout( out, self.dropout_prob, dropout_implementation=""upscale_in_train"" ) if self.dropout_prob > 0.0 else out ) new_states.append(new_state) return step_input, new_states",false,if self . dropout_prob > 0.0,=====,0.0,0.0
"def jupyter_progress_bar(min=0, max=1.0): """"""Returns an ipywidget progress bar or None if we can't import it"""""" widgets = wandb.util.get_module(""ipywidgets"") try: if widgets is None: # TODO: this currently works in iPython but it's deprecated since 4.0 from IPython.html import widgets # type: ignore assert hasattr(widgets, ""VBox"") assert hasattr(widgets, ""Label"") assert hasattr(widgets, ""FloatProgress"") return ProgressWidget(widgets, min=min, max=max) except (ImportError, AssertionError): return None",false,if widgets is None :,importimport#############,0.0,0.0
"def _record_event(self, path, fsevent_handle, filename, events, error): with self.lock: self.events[path].append(events) if events | pyuv.fs.UV_RENAME: if not os.path.exists(path): self.watches.pop(path).close()",false,if events | pyuv . fs . UV_RENAME :,path:,11.11,0.0
"def _get_v1_id_from_tags(self, tags_obj, tag): """"""Get image id from array of tags"""""" if isinstance(tags_obj, dict): try: return tags_obj[tag] except KeyError: pass elif isinstance(tags_obj, []): try: for tag_dict in tags_obj: if tag_dict[""name""] == tag: return tag_dict[""layer""] except KeyError: pass return """"",false,"if tag_dict [ ""name"" ] == tag :",tag:,18.18,0.0
"def query_lister(domain, query="""", max_items=None, attr_names=None): more_results = True num_results = 0 next_token = None while more_results: rs = domain.connection.query_with_attributes( domain, query, attr_names, next_token=next_token ) for item in rs: if max_items: if num_results == max_items: raise StopIteration yield item num_results += 1 next_token = rs.next_token more_results = next_token != None",false,if max_items :,next_token =next_tokennext_tokennext_tokennext_token = next_tokennext_token =,0.0,1.91
"def filter(this, args): array = to_object(this, args.space) callbackfn = get_arg(args, 0) arr_len = js_arr_length(array) if not is_callable(callbackfn): raise MakeError(""TypeError"", ""callbackfn must be a function"") _this = get_arg(args, 1) k = 0 res = [] while k < arr_len: if array.has_property(unicode(k)): kValue = array.get(unicode(k)) if to_boolean(callbackfn.call(_this, (kValue, float(k), array))): res.append(kValue) k += 1 return args.space.ConstructArray(res)",false,if array . has_property ( unicode ( k ) ) :,===============,0.0,0.0
"def every_one_is(self, dst): msg = ""all members of %r should be %r, but the %dth is %r"" for index, item in enumerate(self._src): if self._range: if index < self._range[0] or index > self._range[1]: continue error = msg % (self._src, dst, index, item) if item != dst: raise AssertionError(error) return True",false,if item != dst :,self._range[0]self._range[1]self._range[1]self._range[0]self,0.0,0.0
"def schedule_logger(job_id=None, delete=False): if not job_id: return getLogger(""fate_flow_schedule"") else: if delete: with LoggerFactory.lock: try: for key in LoggerFactory.schedule_logger_dict.keys(): if job_id in key: del LoggerFactory.schedule_logger_dict[key] except: pass return True key = job_id + ""schedule"" if key in LoggerFactory.schedule_logger_dict: return LoggerFactory.schedule_logger_dict[key] return LoggerFactory.get_schedule_logger(job_id)",false,if job_id in key :,return),0.0,0.0
"def Tokenize(s): # type: (str) -> Iterator[Token] for item in TOKEN_RE.findall(s): # The type checker can't know the true type of item! item = cast(TupleStr4, item) if item[0]: typ = ""number"" val = item[0] elif item[1]: typ = ""name"" val = item[1] elif item[2]: typ = item[2] val = item[2] elif item[3]: typ = item[3] val = item[3] yield Token(typ, val)",false,elif item [ 2 ] :,:typ =============,16.67,2.63
"def _read_data_from_all_categories(self, directory, config, categories): lines = [] for category in categories: data_file = os.path.join(directory, _DATASET_VERSION, category, config) if os.path.exists(data_file): with open(data_file) as f: ls = f.read().split(""\n"") for l in ls[::-1]: if not l: ls.remove(l) lines.extend(ls) return lines",false,if os . path . exists ( data_file ) :,lines,0.0,0.0
"def find_handlers(self, forms): handlers = {} for form in forms.itervalues(): for action_name, _action_label in form.actions: if action_name not in handlers: handlers[action_name] = form else: raise HandlerError( ""More than one form defines the handler %s"" % action_name ) return handlers",false,if action_name not in handlers :,handlers,16.67,0.0
"def get_story_task_completed_body(payload: Dict[str, Any]) -> Optional[str]: action = get_action_with_primary_id(payload) kwargs = { ""task_description"": action[""description""], } story_id = action[""story_id""] for ref in payload[""references""]: if ref[""id""] == story_id: kwargs[""name_template""] = STORY_NAME_TEMPLATE.format( name=ref[""name""], app_url=ref[""app_url""], ) if action[""changes""][""complete""][""new""]: return STORY_TASK_COMPLETED_TEMPLATE.format(**kwargs) else: return None",false,"if ref [ ""id"" ] == story_id :","[ ""name_template""] =""name_template""], app_url=ref[""app_url""],""changes""",63.64,8.07
"def _create_valid_graph(graph): nodes = graph.nodes() for i in range(len(nodes)): for j in range(len(nodes)): if i == j: continue edge = (nodes[i], nodes[j]) if graph.has_edge(edge): graph.del_edge(edge) graph.add_edge(edge, 1)",false,if i == j :,=graph.add_edge(edge),16.67,4.77
"def _post_order(op): if isinstance(op, tvm.tir.Allocate): lift_stmt[-1].append(op) return op.body if isinstance(op, tvm.tir.AttrStmt): if op.attr_key == ""storage_scope"": lift_stmt[-1].append(op) return op.body if op.attr_key == ""virtual_thread"": return _merge_block(lift_stmt.pop() + [op], op.body) return op if isinstance(op, tvm.tir.For): return _merge_block(lift_stmt.pop() + [op], op.body) raise RuntimeError(""not reached"")",false,"if op . attr_key == ""storage_scope"" :",lift_stmt[1].push(op) returnop.bodylift_stmt[1].pop()op.,20.0,3.58
"def format_lazy_import(names): """"""Formats lazy import lines"""""" lines = """" for _, name, asname in names: pkg, _, _ = name.partition(""."") if asname is None: line = ""{pkg} = _LazyModule.load({pkg!r}, {mod!r})\n"" else: line = ""{asname} = _LazyModule.load({pkg!r}, {mod!r}, {asname!r})\n"" lines += line.format(pkg=pkg, mod=name, asname=asname) return lines",false,if asname is None :,lines,0.0,0.0
"def evaluateWord(self, argument): wildcard_count = argument[0].count(""*"") if wildcard_count > 0: if wildcard_count == 1 and argument[0].startswith(""*""): return self.GetWordWildcard(argument[0][1:], method=""endswith"") if wildcard_count == 1 and argument[0].endswith(""*""): return self.GetWordWildcard(argument[0][:-1], method=""startswith"") else: _regex = argument[0].replace(""*"", "".+"") matched = False for w in self.words: matched = bool(re.search(_regex, w)) if matched: break return matched return self.GetWord(argument[0])",false,if matched :,"w.match(""*"", "".+"") matched =matched =w.match(""*"", "".+"") matched =matched =",33.33,1.09
"def setup(self, ir: ""IR"", aconf: Config) -> bool: if self.kind == ""ConsulResolver"": self.resolve_with = ""consul"" if not self.get(""datacenter""): self.post_error(""ConsulResolver is required to have a datacenter"") return False elif self.kind == ""KubernetesServiceResolver"": self.resolve_with = ""k8s"" elif self.kind == ""KubernetesEndpointResolver"": self.resolve_with = ""k8s"" else: self.post_error(f""Resolver kind {self.kind} unknown"") return False return True",false,"if not self . get ( ""datacenter"" ) :","{self.kind}{self.get(""datacenter""))self.get(""datacenter""): self.post_error(f",81.82,25.16
"def get_success_url(self): """"""Continue to the flow index or redirect according `?back` parameter."""""" if ""back"" in self.request.GET: back_url = self.request.GET[""back""] if not is_safe_url(url=back_url, allowed_hosts={self.request.get_host()}): back_url = ""/"" return back_url return reverse(self.success_url)",false,"if not is_safe_url ( url = back_url , allowed_hosts = { self . request . get_host ( ) } ) :",back_url,4.76,0.0
"def download_main( download, download_playlist, urls, playlist, output_dir, merge, info_only ): for url in urls: if url.startswith(""https://""): url = url[8:] if not url.startswith(""http://""): url = ""http://"" + url if playlist: download_playlist( url, output_dir=output_dir, merge=merge, info_only=info_only ) else: download(url, output_dir=output_dir, merge=merge, info_only=info_only)",false,"if not url . startswith ( ""http://"" ) :","url,url,merge=merge, info_only=info_only)url,url,url,url",14.29,1.96
"def __str__(self): buf = [""""] if self.fileName: buf.append(self.fileName + "":"") if self.line != -1: if not self.fileName: buf.append(""line "") buf.append(str(self.line)) if self.column != -1: buf.append("":"" + str(self.column)) buf.append("":"") buf.append("" "") return str("""").join(buf)",false,if not self . fileName :,"buf.append("""")",16.67,6.57
"def parse_bash_set_output(output): """"""Parse Bash-like 'set' output"""""" if not sys.platform.startswith(""win""): # Replace ""\""-continued lines in *Linux* environment dumps. # Cannot do this on Windows because a ""\"" at the end of the # line does not imply a continuation. output = output.replace(""\\\n"", """") environ = {} for line in output.splitlines(0): line = line.rstrip() if not line: continue # skip black lines item = _ParseBashEnvStr(line) if item: environ[item[0]] = item[1] return environ",false,if item :,=== _ParseBashEnvStr(item) if= _ParseBashEnvStr(item) if=,66.67,2.72
"def remove_selected(self): """"""Removes selected items from list."""""" to_delete = [] for i in range(len(self)): if self[i].selected: to_delete.append(i) to_delete.reverse() for i in to_delete: self.pop(i) if len(to_delete) > 0: first_to_delete = to_delete[-1] if first_to_delete == 0 and len(self) > 0: self[0].selected = True elif first_to_delete > 0: self[first_to_delete - 1].selected = True",false,if first_to_delete == 0 and len ( self ) > 0 :,=first_to_delete========== 0,30.77,39.17
"def update(self, update_tracks=True): self.enable_update_metadata_images(False) old_album_title = self.metadata[""album""] self.metadata[""album""] = config.setting[""nat_name""] for track in self.tracks: if old_album_title == track.metadata[""album""]: track.metadata[""album""] = self.metadata[""album""] for file in track.linked_files: track.update_file_metadata(file) self.enable_update_metadata_images(True) super().update(update_tracks)",false,"if old_album_title == track . metadata [ ""album"" ] :",update_tracks),0.0,0.62
"def on_input(self, target, message): if message.strip() == """": self.panel(""No commit message provided"") return if target: command = [""git"", ""add""] if target == ""*"": command.append(""--all"") else: command.extend((""--"", target)) self.run_command(command, functools.partial(self.add_done, message)) else: self.add_done(message, """")",false,"if target == ""*"" :","self.panel(""Added"")",25.0,6.57
"def go_to_last_edit_location(self): if self.last_edit_cursor_pos is not None: filename, position = self.last_edit_cursor_pos if not osp.isfile(filename): self.last_edit_cursor_pos = None return else: self.load(filename) editor = self.get_current_editor() if position < editor.document().characterCount(): editor.set_cursor_position(position)",false,if not osp . isfile ( filename ) :,=,0.0,0.0
"def returnByType(self, results): new_results = {} for r in results: type_name = r.get(""type"", ""movie"") + ""s"" if type_name not in new_results: new_results[type_name] = [] new_results[type_name].append(r) # Combine movies, needs a cleaner way.. if ""movies"" in new_results: new_results[""movies""] = self.combineOnIMDB(new_results[""movies""]) return new_results",false,if type_name not in new_results :,"self.combineOnIMDB(new_results[""movies""]):",33.33,11.36
"def cache_sns_topics_across_accounts() -> bool: function: str = f""{__name__}.{sys._getframe().f_code.co_name}"" # First, get list of accounts accounts_d: list = async_to_sync(get_account_id_to_name_mapping)() for account_id in accounts_d.keys(): if config.get(""environment"") == ""prod"": cache_sns_topics_for_account.delay(account_id) else: if account_id in config.get(""celery.test_account_ids"", []): cache_sns_topics_for_account.delay(account_id) stats.count(f""{function}.success"") return True",false,"if account_id in config . get ( ""celery.test_account_ids"" , [ ] ) :",() :( account_id )) : return( account_id )) : return True( ) ),23.53,9.31
"def get(self, subject, topic): """"""Handles GET requests."""""" if subject in feconf.AVAILABLE_LANDING_PAGES: if topic in feconf.AVAILABLE_LANDING_PAGES[subject]: self.render_template(""topic-landing-page.mainpage.html"") else: raise self.PageNotFoundException else: raise self.PageNotFoundException",false,if topic in feconf . AVAILABLE_LANDING_PAGES [ subject ] :,"self.render_template(""topic-landing-page.mainpage.html"")",20.0,3.38
"def callback(compiled): if destpath is None: logger.show_tabulated( ""Compiled"", showpath(codepath), ""without writing to file."" ) else: with univ_open(destpath, ""w"") as opened: writefile(opened, compiled) logger.show_tabulated(""Compiled to"", showpath(destpath), ""."") if self.show: print(compiled) if run: if destpath is None: self.execute(compiled, path=codepath, allow_show=False) else: self.execute_file(destpath)",false,if destpath is None :,"ifself.show_tabulated(self.show_tabulated( ""Compiled"",self.show_tabulated( """,0.0,0.0
"def _find_start_index(self, string, start, end): while True: index = string.find(""{"", start, end) - 1 if index < 0: return -1 if self._start_index_is_ok(string, index): return index start = index + 2",false,"if self . _start_index_is_ok ( string , index ) :",string,10.0,0.0
"def _get_nlu_target_format(export_path: Text) -> Text: guessed_format = loading.guess_format(export_path) if guessed_format not in {MARKDOWN, RASA, RASA_YAML}: if rasa.shared.data.is_likely_json_file(export_path): guessed_format = RASA elif rasa.shared.data.is_likely_markdown_file(export_path): guessed_format = MARKDOWN elif rasa.shared.data.is_likely_yaml_file(export_path): guessed_format = RASA_YAML return guessed_format",false,elif rasa . shared . data . is_likely_yaml_file ( export_path ) :,guessed_format = RASA_YAML elifguessed_format = RASA_YAML elifguessed,0.0,2.66
"def moveToThreadNext(self): """"""Move a position to threadNext position."""""" p = self if p.v: if p.v.children: p.moveToFirstChild() elif p.hasNext(): p.moveToNext() else: p.moveToParent() while p: if p.hasNext(): p.moveToNext() break # found p.moveToParent() # not found. return p",false,if p . hasNext ( ) :,p.v:,42.86,16.7
"def copy_attributes(info_add, obj, name_fmt, attributes, formatter=None): for attr in attributes: value = getattr(obj, attr, None) if value is None: continue name = name_fmt % attr if formatter is not None: value = formatter(attr, value) info_add(name, value)",false,if value is None :,=name_fmt,0.0,0.0
"def getElement(self, aboutUri, namespace, name): for desc in self.rdfRoot.getElementsByTagNameNS(RDF_NAMESPACE, ""Description""): if desc.getAttributeNS(RDF_NAMESPACE, ""about"") == aboutUri: attr = desc.getAttributeNodeNS(namespace, name) if attr != None: yield attr for element in desc.getElementsByTagNameNS(namespace, name): yield element",false,"if desc . getAttributeNS ( RDF_NAMESPACE , ""about"" ) == aboutUri :",:,6.67,0.0
def run(self): while not self.completed: if self.block: time.sleep(self.period) else: self._completed.wait(self.period) self.counter += 1 try: self.callback(self.counter) except Exception: self.stop() if self.timeout is not None: dt = time.time() - self._start_time if dt > self.timeout: self.stop() if self.counter == self.count: self.stop(),false,if self . timeout is not None :,self.callback(self.counter)self.callback(self.counter),25.0,4.89
"def _parse_fixits(message, titer, line): """"""Parses fixit messages."""""" while ( OutputParser.message_line_re.match(line) is None and OutputParser.note_line_re.match(line) is None ): message_text = line.strip() if message_text != """": message.fixits.append( Note( message.path, message.line, line.find(message_text) + 1, message_text, ) ) line = next(titer) return line",false,"if message_text != """" :",line:line:,14.29,4.58
"def _connect_db(self, force_reconnect=False): thread_id = thread.get_ident() if force_reconnect and thread_id in ENGINES: del ENGINES[thread_id] conn = None try: engine = ENGINES[thread_id] conn = engine.connect() _test = conn.execute(""SELECT 1"") _test.fetchall() except (KeyError, MySQLdb.OperationalError): if conn: conn.close() engine = sqla.create_engine(self.db_url, pool_recycle=3600) ENGINES[thread_id] = engine conn = engine.connect() return conn",false,if conn :,===============,0.0,0.0
"def read(self, n): if self.current_frame: data = self.current_frame.read(n) if not data and n != 0: self.current_frame = None return self.file_read(n) if len(data) < n: raise UnpicklingError(""pickle exhausted before end of frame"") return data else: return self.file_read(n)",false,if not data and n != 0 :,data,11.11,0.0
"def __setLoadCmd(self): base = self.__rawLoadCmd for _ in range(self.__machHeader.ncmds): command = LOAD_COMMAND.from_buffer_copy(base) if command.cmd == MACHOFlags.LC_SEGMENT: segment = SEGMENT_COMMAND.from_buffer_copy(base) self.__setSections(segment, base[56:], 32) elif command.cmd == MACHOFlags.LC_SEGMENT_64: segment = SEGMENT_COMMAND64.from_buffer_copy(base) self.__setSections(segment, base[72:], 64) base = base[command.cmdsize :]",false,if command . cmd == MACHOFlags . LC_SEGMENT :,"base[56:], 32) elif command.cmd== MACHOFlags.LC_SEGMENT_32:= SEGMENT_",80.0,36.76
"def emit_post_sync_signal(created_models, verbosity, interactive, db): # Emit the post_sync signal for every application. for app in models.get_apps(): app_name = app.__name__.split(""."")[-2] if verbosity >= 2: print(""Running post-sync handlers for application %s"" % app_name) models.signals.post_syncdb.send( sender=app, app=app, created_models=created_models, verbosity=verbosity, interactive=interactive, db=db, )",false,if verbosity >= 2 :,app_name:,16.67,9.69
"def git_pull(args): if len(args) <= 1: repo = _get_repo() _confirm_dangerous() url = args[0] if len(args) == 1 else repo.remotes.get(""origin"", """") if url in repo.remotes: origin = url url = repo.remotes.get(origin) if url: repo.pull(origin_uri=url) else: print(""No pull URL."") else: print(command_help[""git pull""])",false,if url :,"=repo.remotes.get(""origin_uri"")========",0.0,0.0
"def version(self): try: return self._version except AttributeError: for line in self._get_metadata(self.PKG_INFO): if line.lower().startswith(""version:""): self._version = safe_version(line.split("":"", 1)[1].strip()) return self._version else: tmpl = ""Missing 'Version:' header and/or %s file"" raise ValueError(tmpl % self.PKG_INFO, self)",false,"if line . lower ( ) . startswith ( ""version:"" ) :",line,6.67,0.0
"def increment(self, metric, labels, delta): """"""Increment a value by |delta|."""""" with self._lock: key = self._get_key(metric.name, labels) if key in self._store: start_time = self._store[key].start_time value = self._store[key].value + delta else: start_time = time.time() value = metric.default_value + delta self._store[key] = _StoreValue(metric, labels, start_time, value)",false,if key in self . _store :,=self._store[key],57.14,36.56
"def get_current_connections(session): """"""Retrieves open connections using the the given session"""""" # Use Show process list to count the open sesions. res = session.sql(""SHOW PROCESSLIST"").execute() rows = res.fetch_all() connections = {} for row in rows: if row.get_string(""User"") not in connections: connections[row.get_string(""User"")] = [row.get_string(""Host"")] else: connections[row.get_string(""User"")].append(row.get_string(""Host"")) return connections",false,"if row . get_string ( ""User"" ) not in connections :","session.sql(""SHOWSELECT""User""))",46.15,19.4
"def asset(*paths): for path in paths: fspath = www_root + ""/assets/"" + path etag = """" try: if env.cache_static: etag = asset_etag(fspath) else: os.stat(fspath) except FileNotFoundError as e: if path == paths[-1]: if not os.path.exists(fspath + "".spt""): tell_sentry(e, {}) else: continue except Exception as e: tell_sentry(e, {}) return asset_url + path + (etag and ""?etag="" + etag)",false,if env . cache_static :,=path=============,0.0,0.0
def thread_loop(self) -> None: while not self.stop_event.is_set(): time.sleep(1) new_trials = self.study.trials with self.lock: need_to_add_callback = self.new_trials is None self.new_trials = new_trials if need_to_add_callback: self.doc.add_next_tick_callback(self.update_callback),false,if need_to_add_callback :,self.doc.add_next_tick_callback(self.update_callback),0.0,6.42
"def _cache_db_tables_iterator(tables, cache_alias, db_alias): no_tables = not tables cache_aliases = settings.CACHES if cache_alias is None else (cache_alias,) db_aliases = settings.DATABASES if db_alias is None else (db_alias,) for db_alias in db_aliases: if no_tables: tables = connections[db_alias].introspection.table_names() if tables: for cache_alias in cache_aliases: yield cache_alias, db_alias, tables",false,if tables :,"cache_alias, db_alias, tables)cache_alias, db_alias, tables)cache_alias, db_alias",33.33,1.39
"def remove_subscriber(self, topic, subscriber): if subscriber in self.subscribers[topic]: if hasattr(subscriber, ""_pyroRelease""): subscriber._pyroRelease() if hasattr(subscriber, ""_pyroUri""): try: proxy = self.proxy_cache[subscriber._pyroUri] proxy._pyroRelease() del self.proxy_cache[subscriber._pyroUri] except KeyError: pass self.subscribers[topic].discard(subscriber)",false,"if hasattr ( subscriber , ""_pyroUri"" ) :",subscriber,10.0,0.0
"def test_constructor(job_id): with patch(""apscheduler.job.Job._modify"") as _modify: scheduler_mock = MagicMock(BaseScheduler) job = Job(scheduler_mock, id=job_id) assert job._scheduler is scheduler_mock assert job._jobstore_alias is None modify_kwargs = _modify.call_args[1] if job_id is None: assert len(modify_kwargs[""id""]) == 32 else: assert modify_kwargs[""id""] == job_id",false,if job_id is None :,job_id==job_id=========,20.0,8.51
"def get_connection(self): if self.config.proxy_host != """": return httplib.HTTPConnection(self.config.proxy_host, self.config.proxy_port) else: if self.config.use_https: return httplib.HTTPSConnection(self.config.simpledb_host) else: return httplib.HTTPConnection(self.config.simpledb_host)",false,if self . config . use_https :,self.config.simpledb_port,57.14,32.67
"def notify_login(self, ipaddress=""""): if app.NOTIFY_ON_LOGIN: update_text = common.notifyStrings[common.NOTIFY_LOGIN_TEXT] title = common.notifyStrings[common.NOTIFY_LOGIN] if update_text and title and ipaddress: self._notify_pht(title, update_text.format(ipaddress))",false,if update_text and title and ipaddress :,self._notify_pht(title),14.29,5.67
"def _getItemHeight(self, item, ctrl=None): """"""Returns the full height of the item to be inserted in the form"""""" if type(ctrl) == psychopy.visual.TextBox2: return ctrl.size[1] if type(ctrl) == psychopy.visual.Slider: # Set radio button layout if item[""layout""] == ""horiz"": return 0.03 + ctrl.labelHeight * 3 elif item[""layout""] == ""vert"": # for vertical take into account the nOptions return ctrl.labelHeight * len(item[""options""])",false,"elif item [ ""layout"" ] == ""vert"" :","item[""options""]=item[""options""]=item[""options""]=item[""options""]=item",69.23,8.91
"def _get_errors_lines(self): """"""Return the number of lines that contains errors to highlight."""""" errors_lines = [] block = self.document().begin() while block.isValid(): user_data = get_user_data(block) if user_data.error: errors_lines.append(block.blockNumber()) block = block.next() return errors_lines",false,if user_data . error :,self.document().end(),20.0,4.77
"def set_pbar_fraction(self, frac, progress, stage=None): gtk.gdk.threads_enter() try: self.is_pulsing = False self.set_stage_text(stage or _(""Processing..."")) self.pbar.set_text(progress) if frac > 1: frac = 1.0 if frac < 0: frac = 0 self.pbar.set_fraction(frac) finally: gtk.gdk.threads_leave()",false,if frac < 0 :,=self.pbar.set_fraction(frac),20.0,3.75
"def list_files(basedir): """"""List files in the directory rooted at |basedir|."""""" if not os.path.isdir(basedir): raise NoSuchDirectory(basedir) directories = [""""] while directories: d = directories.pop() for basename in os.listdir(os.path.join(basedir, d)): filename = os.path.join(d, basename) if os.path.isdir(os.path.join(basedir, filename)): directories.append(filename) elif os.path.exists(os.path.join(basedir, filename)): yield filename",false,"elif os . path . exists ( os . path . join ( basedir , filename ) ) :","filename)filename)filename)filename)if os.path.exists(os.path.join(basedir, filename))",89.47,63.17
"def assistive(self): """"""Detects if item can be used as assistance"""""" # Make sure we cache results if self.__assistive is None: assistive = False # Go through all effects and find first assistive for effect in self.effects.values(): if effect.isAssistance is True: # If we find one, stop and mark item as assistive assistive = True break self.__assistive = assistive return self.__assistive",false,if effect . isAssistance is True :,assistiveTrue:self.__assistive =True:self.__assistive =self.__assistive,42.86,4.02
"def closest_unseen(self, row1, col1, filter=None): # find the closest unseen from this row/col min_dist = maxint closest_unseen = None for row in range(self.height): for col in range(self.width): if filter is None or (row, col) not in filter: if self.map[row][col] == UNSEEN: dist = self.distance(row1, col1, row, col) if dist < min_dist: min_dist = dist closest_unseen = (row, col) return closest_unseen",false,if dist < min_dist :,=min_dist= min_dist= min_dist= min_dist= min_dist= min_,20.0,6.59
"def _maybe_has_default_route(self): for route in self.iter_routes(): if self._is_default_route(route): return True for iface in self.iter_interfaces(): for subnet in iface.get(""subnets"", []): for route in subnet.get(""routes"", []): if self._is_default_route(route): return True return False",false,if self . _is_default_route ( route ) :,self):,37.5,0.0
"def data(self, data): if data is None: raise Exception(""Data cannot be None"") val = [] for d in data: if isinstance(d, str): val.append(bytes(d, ""utf-8"")) elif isinstance(d, bytes): val.append(d) else: raise Exception( ""Invalid type, data can only be an str or a bytes not {}: {}"".format( type(data), d ) ) self.__data = val",false,"elif isinstance ( d , bytes ) :",self.__data_type =self.__data_typeself.__data_typeself.__data_typeself.__data_,0.0,0.0
"def get_one_segment_function(data, context, echoerr): ext = data[""ext""] function_name = context[-2][1].get(""function"") if function_name: module, function_name = get_function_strings(function_name, context, ext) func = import_segment(function_name, data, context, echoerr, module=module) if func: yield func",false,if func :,"func=import_segment(function_name,context,echoerr):",66.67,3.13
"def generic_visit(self, node, parents=None): parents = (parents or []) + [node] for field, value in iter_fields(node): if isinstance(value, list): for item in value: if isinstance(item, AST): self.visit(item, parents) elif isinstance(value, AST): self.visit(value, parents)",false,"if isinstance ( item , AST ) :","=field,value):self.visit(field, parents)",50.0,6.75
"def find_scintilla_constants(f): lexers = [] states = [] for name in f.order: v = f.features[name] if v[""Category""] != ""Deprecated"": if v[""FeatureType""] == ""val"": if name.startswith(""SCE_""): states.append((name, v[""Value""])) elif name.startswith(""SCLEX_""): lexers.append((name, v[""Value""])) return (lexers, states)",false,"if v [ ""Category"" ] != ""Deprecated"" :",=f.order[name]=f.order[name]=f.order[name]=,23.08,2.27
"def things(self, query): limit = query.pop(""limit"", 100) offset = query.pop(""offset"", 0) keys = set(self.docs) for k, v in query.items(): if isinstance(v, dict): # query keys need to be flattened properly, # this corrects any nested keys that have been included # in values. flat = common.flatten_dict(v)[0] k += ""."" + web.rstrips(flat[0], "".key"") v = flat[1] keys = set(k for k in self.filter_index(self.index, k, v) if k in keys) keys = sorted(keys) return keys[offset : offset + limit]",false,"if isinstance ( v , dict ) :",.) fork in self.filter_index(self.filter_index(self.docs))k in self.filter,25.0,1.72
"def del_(self, key): initial_hash = hash_ = self.hash(key) while True: if self._keys[hash_] is self._empty: # That key was never assigned return None elif self._keys[hash_] == key: # key found, assign with deleted sentinel self._keys[hash_] = self._deleted self._values[hash_] = self._deleted self._len -= 1 return hash_ = self._rehash(hash_) if initial_hash == hash_: # table is full and wrapped around return None",false,elif self . _keys [ hash_ ] == key :,=self._values[hash_]=self._values[hash_]=self._values[hash_],63.64,17.9
"def test_204_invalid_content_length(self): # 204 status with non-zero content length is malformed with ExpectLog(gen_log, "".*Response with code 204 should not have body""): response = self.fetch(""/?error=1"") if not self.http1: self.skipTest(""requires HTTP/1.x"") if self.http_client.configured_class != SimpleAsyncHTTPClient: self.skipTest(""curl client accepts invalid headers"") self.assertEqual(response.code, 599)",false,if not self . http1 :,"self.skipTest(""requires HTTP/1.x"")",33.33,6.18
"def __str__(self) -> str: text = ""\n"" for k, r in self.result.items(): text += ""{}\n"".format(""#"" * 40) if r.failed: text += ""# {} (failed)\n"".format(k) else: text += ""# {} (succeeded)\n"".format(k) text += ""{}\n"".format(""#"" * 40) for sub_r in r: text += ""**** {}\n"".format(sub_r.name) text += ""{}\n"".format(sub_r) return text",false,if r . failed :,"text""****""****""****""****""****""****""****""****""****",0.0,0.0
"def DeleteTask(): oid = request.form.get(""oid"", """") if oid: result = Mongo.coll[""Task""].delete_one({""_id"": ObjectId(oid)}) if result.deleted_count > 0: result = Mongo.coll[""Result""].delete_many({""task_id"": ObjectId(oid)}) if result: return ""success"" return ""fail""",false,if result . deleted_count > 0 :,"""success""",0.0,0.0
"def _replace_vars(self, line, extracted, env_variables): for e in extracted: if e in env_variables: value = env_variables.get(e) if isinstance(value, dict) or isinstance(value, list): value = pprint.pformat(value) decorated = self._decorate_var(e) line = line.replace(decorated, str(value)) return line",false,if e in env_variables :,line,0.0,0.0
"def should_include(service): for f in filt: if f == ""status"": state = filt[f] containers = project.containers([service.name], stopped=True) if not has_container_with_state(containers, state): return False elif f == ""source"": source = filt[f] if source == ""image"" or source == ""build"": if source not in service.options: return False else: raise UserError(""Invalid value for source filter: %s"" % source) else: raise UserError(""Invalid filter: %s"" % f) return True",false,if source not in service . options :,:service.options[service.name]:::::::::,50.0,8.59
def state_callback_loop(): if usercallback: when = 1 while ( when and not self.future_removed.done() and not self.session.shutdownstarttime ): result = usercallback(self.get_state()) when = (await result) if iscoroutine(result) else result if when > 0.0 and not self.session.shutdownstarttime: await sleep(when),false,if when > 0.0 and not self . session . shutdownstarttime :,self.future_removed.done()whenself.future_removed.done()self.future_removed.done(),28.57,3.07
"def __get_new_timeout(self, timeout): """"""When using --timeout_multiplier=#.#"""""" self.__check_scope() try: timeout_multiplier = float(self.timeout_multiplier) if timeout_multiplier <= 0.5: timeout_multiplier = 0.5 timeout = int(math.ceil(timeout_multiplier * timeout)) return timeout except Exception: # Wrong data type for timeout_multiplier (expecting int or float) return timeout",false,if timeout_multiplier <= 0.5 :,timeout_multiplier=self.timeout_multiplier,37.5,19.07
"def readexactly(self, n): buf = b"""" while n: yield IORead(self.s) res = self.s.read(n) assert res is not None if not res: yield IOReadDone(self.s) break buf += res n -= len(res) return buf",false,if not res :,buf,0.0,0.0
"def contract_rendering_pane(event): """"""Expand the rendering pane."""""" c = event.get(""c"") if c: vr = c.frame.top.findChild(QtWidgets.QWidget, ""viewrendered_pane"") if vr: vr.contract() else: # Just open the pane. viewrendered(event)",false,if vr :,"event.get(""c"")",0.0,0.0
"def translate_headers(self, environ): """"""Translate CGI-environ header names to HTTP header names."""""" for cgiName in environ: # We assume all incoming header keys are uppercase already. if cgiName in self.headerNames: yield self.headerNames[cgiName], environ[cgiName] elif cgiName[:5] == ""HTTP_"": # Hackish attempt at recovering original header names. translatedHeader = cgiName[5:].replace(""_"", ""-"") yield translatedHeader, environ[cgiName]",false,if cgiName in self . headerNames :,=self.headerNames[cgiName],57.14,26.27
"def get_value_from_string(self, string_value): """"""Return internal representation starting from CFN/user-input value."""""" param_value = self.get_default_value() try: if string_value is not None: string_value = str(string_value).strip() if string_value != ""NONE"": param_value = int(string_value) except ValueError: self.pcluster_config.warn( ""Unable to convert the value '{0}' to an Integer. "" ""Using default value for parameter '{1}'"".format(string_value, self.key) ) return param_value",false,if string_value is not None :,"parameter '{0}'totheparameter '{1}'."".format(param_value, self.key)))",0.0,2.89
"def monitor_filter(self): """"""Return filtered service objects list"""""" services = self.client.services.list(filters={""label"": ""com.ouroboros.enable""}) monitored_services = [] for service in services: ouro_label = service.attrs[""Spec""][""Labels""].get(""com.ouroboros.enable"") if not self.config.label_enable or ouro_label.lower() in [""true"", ""yes""]: monitored_services.append(service) self.data_manager.monitored_containers[self.socket] = len(monitored_services) self.data_manager.set(self.socket) return monitored_services",false,"if not self . config . label_enable or ouro_label . lower ( ) in [ ""true"" , ""yes"" ] :","monitored_servicesself.socket] =self.socket.socket_name,self.socket.socket_name,",25.0,3.75
"def nextEditable(self): """"""Moves focus of the cursor to the next editable window"""""" if self.currentEditable is None: if len(self._editableChildren): self._currentEditableRef = self._editableChildren[0] else: for ref in weakref.getweakrefs(self.currentEditable): if ref in self._editableChildren: cei = self._editableChildren.index(ref) nei = cei + 1 if nei >= len(self._editableChildren): nei = 0 self._currentEditableRef = self._editableChildren[nei] return self.currentEditable",false,if nei >= len ( self . _editableChildren ) :,self.currentEditableRefself.currentEditableRefself.currentEditableRefself.currentEditableRefself.currentEditableRef,18.18,6.84
"def linkify_cm_by_tp(self, timeperiods): for rm in self: mtp_name = rm.modulation_period.strip() # The new member list, in id mtp = timeperiods.find_by_name(mtp_name) if mtp_name != """" and mtp is None: err = ( ""Error: the business impact modulation '%s' got an unknown "" ""modulation_period '%s'"" % (rm.get_name(), mtp_name) ) rm.configuration_errors.append(err) rm.modulation_period = mtp",false,"if mtp_name != """" and mtp is None :","=( rm.get_name(), mtp_name)=() rm .() rm .",18.18,6.59
def close_open_fds(keep=None): # noqa keep = [maybe_fileno(f) for f in (keep or []) if maybe_fileno(f) is not None] for fd in reversed(range(get_fdmax(default=2048))): if fd not in keep: try: os.close(fd) except OSError as exc: if exc.errno != errno.EBADF: raise,false,if fd not in keep :,except:raiseexcept:raise)========,16.67,2.84
"def _append_child_from_unparsed_xml(father_node, unparsed_xml): """"""Append child xml nodes to a node."""""" dom_tree = parseString(unparsed_xml) if dom_tree.hasChildNodes(): first_child = dom_tree.childNodes[0] if first_child.hasChildNodes(): child_nodes = first_child.childNodes for _ in range(len(child_nodes)): childNode = child_nodes.item(0) father_node.appendChild(childNode) return raise DistutilsInternalError( ""Could not Append append elements to "" ""the Windows msi descriptor."" )",false,if first_child . hasChildNodes ( ) :,"child nodes to a node."")father_node.appendChild(father_node) return father_node.appendChild",42.86,2.35
"def process_request(self, request): for old, new in self.names_name: request.uri = request.uri.replace(old, new) if is_text_payload(request) and request.body: body = six.ensure_str(request.body) if old in body: request.body = body.replace(old, new) return request",false,if is_text_payload ( request ) and request . body :,request.uri,20.0,0.0
"def __init__(self, **options): self.func_name_highlighting = get_bool_opt(options, ""func_name_highlighting"", True) self.disabled_modules = get_list_opt(options, ""disabled_modules"", []) self._functions = set() if self.func_name_highlighting: from pygments.lexers._luabuiltins import MODULES for mod, func in MODULES.iteritems(): if mod not in self.disabled_modules: self._functions.update(func) RegexLexer.__init__(self, **options)",false,if mod not in self . disabled_modules :,self.disabled_modules):self.disabled_modules.append(mod)self.disabled_modules.append(mod),62.5,14.13
"def GetBestSizeForParentSize(self, parentSize): """"""Finds the best width and height given the parent's width and height."""""" if len(self.GetChildren()) == 1: win = self.GetChildren()[0] if isinstance(win, RibbonControl): temp_dc = wx.ClientDC(self) childSize = win.GetBestSizeForParentSize(parentSize) clientParentSize = self._art.GetPanelClientSize( temp_dc, self, wx.Size(*parentSize), None ) overallSize = self._art.GetPanelSize( temp_dc, self, wx.Size(*clientParentSize), None ) return overallSize return self.GetSize()",false,"if isinstance ( win , RibbonControl ) :","parentSize, wx.Size(*parentSize), None )self._art.GetPanelClientSize(temp_dc, self, wx",37.5,1.9
"def pid_from_name(name): processes = [] for pid in os.listdir(""/proc""): try: pid = int(pid) pname, cmdline = SunProcess._name_args(pid) if name in pname: return pid if name in cmdline.split("" "", 1)[0]: return pid except: pass raise ProcessException(""No process with such name: %s"" % name)",false,if name in pname :,=pid,0.0,0.0
"def __get_file_by_num(self, num, file_list, idx=0): for element in file_list: if idx == num: return element if element[3] and element[4]: i = self.__get_file_by_num(num, element[3], idx + 1) if not isinstance(i, int): return i idx = i else: idx += 1 return idx",false,if element [ 3 ] and element [ 4 ] :,idx:,9.09,0.0
"def scan_block_scalar_indentation(self): # See the specification for details. chunks = [] max_indent = 0 end_mark = self.get_mark() while self.peek() in "" \r\n\x85\u2028\u2029"": if self.peek() != "" "": chunks.append(self.scan_line_break()) end_mark = self.get_mark() else: self.forward() if self.column > max_indent: max_indent = self.column return chunks, max_indent, end_mark",false,if self . column > max_indent :,=self.scan_line_break() end_mark =self.forward() end_mark =end_markend,28.57,3.2
"def ant_map(m): tmp = ""rows %s\ncols %s\n"" % (len(m), len(m[0])) players = {} for row in m: tmp += ""m "" for col in row: if col == LAND: tmp += ""."" elif col == BARRIER: tmp += ""%"" elif col == FOOD: tmp += ""*"" elif col == UNSEEN: tmp += ""?"" else: players[col] = True tmp += chr(col + 97) tmp += ""\n"" tmp = (""players %s\n"" % len(players)) + tmp return tmp",false,elif col == FOOD :,")=""""""""""""""""""""""""""",16.67,2.63
"def prepare_data(entry): branch_wise_entries = {} gross_pay = 0 for d in entry: gross_pay += d.gross_pay if branch_wise_entries.get(d.branch): branch_wise_entries[d.branch][d.mode_of_payment] = d.net_pay else: branch_wise_entries.setdefault(d.branch, {}).setdefault( d.mode_of_payment, d.net_pay ) return branch_wise_entries, gross_pay",false,if branch_wise_entries . get ( d . branch ) :,=,0.0,0.0
"def __init__(self, uuid=None, cluster_state=None, children=None, **kwargs): self.uuid = uuid self.cluster_state = cluster_state if self.cluster_state is not None: self.children = WeakSet( self.cluster_state.tasks.get(task_id) for task_id in children or () if task_id in self.cluster_state.tasks ) else: self.children = WeakSet() self._serializer_handlers = { ""children"": self._serializable_children, ""root"": self._serializable_root, ""parent"": self._serializable_parent, } if kwargs: self.__dict__.update(kwargs)",false,if task_id in self . cluster_state . tasks,"""children"" :""children"" : self .""children"" : self .""root"": self._serializable_root,",37.5,3.49
"def listdir(self, d): try: return [ p for p in os.listdir(d) if os.path.basename(p) != ""CVS"" and os.path.isdir(os.path.join(d, p)) ] except OSError: return []",false,"if os . path . basename ( p ) != ""CVS"" and os . path . isdir ( os . path . join ( d , p ) )",d,3.12,0.0
"def send_packed_command(self, command, check_health=True): if not self._sock: self.connect() try: if isinstance(command, str): command = [command] for item in command: self._sock.sendall(item) except socket.error as e: self.disconnect() if len(e.args) == 1: _errno, errmsg = ""UNKNOWN"", e.args[0] else: _errno, errmsg = e.args raise ConnectionError( ""Error %s while writing to socket. %s."" % (_errno, errmsg) ) except Exception: self.disconnect() raise",false,"if isinstance ( command , str ) :",": raise ConnectionError( ""Error%s while writing tosocket.errorsocket.error, errmsg)socket.error, errmsg",50.0,2.33
"def run(self): """"""Start the scanner"""""" logging.info(""Dirscanner starting up"") self.shutdown = False while not self.shutdown: # Wait to be woken up or triggered with self.loop_condition: self.loop_condition.wait(self.dirscan_speed) if self.dirscan_speed and not self.shutdown: self.scan()",false,if self . dirscan_speed and not self . shutdown :,self.loop_condition:,30.0,7.1
"def __aexit__( self, exc_type: type, exc_value: BaseException, tb: TracebackType ) -> None: if exc_type is not None: await self.close() await self._task while not self._receive_queue.empty(): data = await self._receive_queue.get() if isinstance(data, bytes): self.response_data.extend(data) elif not isinstance(data, HTTPDisconnect): raise data",false,"elif not isinstance ( data , HTTPDisconnect ) :",None:self._task.close(),33.33,5.52
"def f(msg): text = extractor(msg) for px in prefix: if text.startswith(px): chunks = text[len(px) :].split(separator) return chunks[0], (chunks[1:],) if pass_args else () return ((None,),) # to distinguish with `None`",false,if text . startswith ( px ) :,=,0.0,0.0
"def _flatten(*args): ahs = set() if len(args) > 0: for item in args: if type(item) is ActionHandle: ahs.add(item) elif type(item) in (list, tuple, dict, set): for ah in item: if type(ah) is not ActionHandle: # pragma:nocover raise ActionManagerError(""Bad argument type %s"" % str(ah)) ahs.add(ah) else: # pragma:nocover raise ActionManagerError(""Bad argument type %s"" % str(item)) return ahs",false,"elif type ( item ) in ( list , tuple , dict , set ) :",:)))))))))))))),18.75,3.24
"def find_class(self, module, name): # Subclasses may override this. sys.audit(""pickle.find_class"", module, name) if self.proto < 3 and self.fix_imports: if (module, name) in _compat_pickle.NAME_MAPPING: module, name = _compat_pickle.NAME_MAPPING[(module, name)] elif module in _compat_pickle.IMPORT_MAPPING: module = _compat_pickle.IMPORT_MAPPING[module] __import__(module, level=0) if self.proto >= 4: return _getattribute(sys.modules[module], name)[0] else: return getattr(sys.modules[module], name)",false,elif module in _compat_pickle . IMPORT_MAPPING :,name)name)name)name)name)namename)namename)name)name,0.0,0.0
"def _send_until_done(self, data): while True: try: return self.connection.send(data) except OpenSSL.SSL.WantWriteError: wr = util.wait_for_write(self.socket, self.socket.gettimeout()) if not wr: raise timeout() continue except OpenSSL.SSL.SysCallError as e: raise SocketError(str(e))",false,if not wr :,self.socket.gettimeout(),0.0,0.0
"def __new__(cls, *args, **kwargs): """"""Hack to ensure method defined as async are implemented as such."""""" coroutines = inspect.getmembers(BaseManager, predicate=inspect.iscoroutinefunction) for coroutine in coroutines: implemented_method = getattr(cls, coroutine[0]) if not inspect.iscoroutinefunction(implemented_method): raise RuntimeError(""The method %s must be a coroutine"" % implemented_method) return super().__new__(cls, *args, **kwargs)",false,if not inspect . iscoroutinefunction ( implemented_method ) :,coroutinecoroutine,0.0,0.0
"def add_directive(self, name, obj, content=None, arguments=None, **options): if isinstance(obj, clstypes) and issubclass(obj, Directive): if content or arguments or options: raise ExtensionError( ""when adding directive classes, no "" ""additional arguments may be given"" ) directives.register_directive(name, directive_dwim(obj)) else: obj.content = content obj.arguments = arguments obj.options = options directives.register_directive(name, obj)",false,if content or arguments or options :,=obj),0.0,0.0
"def create(self, w): if w.use_eventloop: # does not use dedicated timer thread. w.timer = _Timer(max_interval=10.0) else: if not w.timer_cls: # Default Timer is set by the pool, as for example, the # eventlet pool needs a custom timer implementation. w.timer_cls = w.pool_cls.Timer w.timer = self.instantiate( w.timer_cls, max_interval=w.timer_precision, on_error=self.on_timer_error, on_tick=self.on_timer_tick, )",false,if not w . timer_cls :,"w.timer_cls,= self.instantiate( w.timer_cls,w.timer_precision, on_tick",50.0,13.52
"def _config(_molecule_file, request): with open(_molecule_file) as f: d = util.safe_load(f) if hasattr(request, ""param""): if isinstance(request.getfixturevalue(request.param), str): d2 = util.safe_load(request.getfixturevalue(request.param)) else: d2 = request.getfixturevalue(request.param) # print(100, d) # print(200, d2) d = util.merge_dicts(d, d2) # print(300, d) return d",false,"if isinstance ( request . getfixturevalue ( request . param ) , str ) :","d2)= util.merge_dicts(d,d2) # print(200, d2)# print(",40.0,2.58
"def _instrument_model(self, model): for key, value in list( model.__dict__.items() ): # avoid ""dictionary keys changed during iteration"" if isinstance(value, tf.keras.layers.Layer): new_layer = self._instrument(value) if new_layer is not value: setattr(model, key, new_layer) elif isinstance(value, list): for i, item in enumerate(value): if isinstance(item, tf.keras.layers.Layer): value[i] = self._instrument(item) return model",false,"if isinstance ( value , tf . keras . layers . Layer ) :",model.__dict__.pop(),28.57,4.04
"def is_accepted_drag_event(self, event): if event.source() == self.table: return True mime = event.mimeData() if mime.hasUrls(): for url in mime.urls(): # Only support local files. if not url.isLocalFile(): break # And only allow supported extensions. filename = url.toLocalFile() extension = os.path.splitext(filename)[1].lower()[1:] if extension not in _dictionary_formats(): break else: return True return False",false,if not url . isLocalFile ( ) :,event:event:event:,12.5,5.82
"def explain(self, other, depth=0): exp = super(UnionType, self).explain(other, depth) for ndx, subtype in enumerate(self.params[""allowed_types""]): if ndx > 0: exp += ""\n{}and"".format("""".join([""\t""] * depth)) exp += ""\n"" + subtype.explain(other, depth=depth + 1) return exp",false,if ndx > 0 :,exp,0.0,0.0
"def test_k_is_stochastic_parameter(self): # k as stochastic parameter aug = iaa.MedianBlur(k=iap.Choice([3, 5])) seen = [False, False] for i in sm.xrange(100): observed = aug.augment_image(self.base_img) if np.array_equal(observed, self.blur3x3): seen[0] += True elif np.array_equal(observed, self.blur5x5): seen[1] += True else: raise Exception(""Unexpected result in MedianBlur@2"") if all(seen): break assert np.all(seen)",false,"elif np . array_equal ( observed , self . blur5x5 ) :",k==============,0.0,0.0
"def test_get_message(self): async with self.chat_client: await self._create_thread() async with self.chat_thread_client: message_id = await self._send_message() message = await self.chat_thread_client.get_message(message_id) assert message.id == message_id assert message.type == ChatMessageType.TEXT assert message.content.message == ""hello world"" # delete chat threads if not self.is_playback(): await self.chat_client.delete_chat_thread(self.thread_id)",false,if not self . is_playback ( ) :,async withself.thread_id)message_idmessage_idmessage_idmessage_idmessage_id,25.0,2.83
"def do_write_property(self, device, callback=None): try: iocb = ( device if isinstance(device, IOCB) else self.form_iocb(device, request_type=""writeProperty"") ) deferred(self.request_io, iocb) self.requests_in_progress.update({iocb: {""callback"": callback}}) iocb.add_callback(self.__general_cb) except Exception as error: log.exception(""exception: %r"", error)",false,"if isinstance ( device , IOCB )",self.request_io.add_callback(self.__general_cb),28.57,2.55
"def fit(self, dataset, force_retrain): if force_retrain: self.sub_unit_1[""fitted""] = True self.sub_unit_1[""calls""] += 1 self.sub_unit_2[""fitted""] = True self.sub_unit_2[""calls""] += 1 else: if not self.sub_unit_1[""fitted""]: self.sub_unit_1[""fitted""] = True self.sub_unit_1[""calls""] += 1 if not self.sub_unit_2[""fitted""]: self.sub_unit_2[""fitted""] = True self.sub_unit_2[""calls""] += 1 return self",false,"if not self . sub_unit_2 [ ""fitted"" ] :","self.sub_unit_2[""calls""] += 1self.sub_unit_2[""calls""] += 1self.sub",63.64,27.24
"def _insert_with_loop(self): id_list = [] last_id = None return_id_list = self._return_id_list for row in self._rows: last_id = InsertQuery(self.model_class, row).upsert(self._upsert).execute() if return_id_list: id_list.append(last_id) if return_id_list: return id_list else: return last_id",false,if return_id_list :,=,0.0,0.0
"def merge_block(self): """"""merges a block in the map"""""" for i in range(self.block.x): for j in range(self.block.x): c = self.block.get(i, j) if c: self.map[(i + self.block.pos.x, j + self.block.pos.y)] = c",false,if c :,=,0.0,0.0
"def configure_plex(config): core.PLEX_SSL = int(config[""Plex""][""plex_ssl""]) core.PLEX_HOST = config[""Plex""][""plex_host""] core.PLEX_PORT = config[""Plex""][""plex_port""] core.PLEX_TOKEN = config[""Plex""][""plex_token""] plex_section = config[""Plex""][""plex_sections""] or [] if plex_section: if isinstance(plex_section, list): plex_section = "","".join(plex_section) # fix in case this imported as list. plex_section = [tuple(item.split("","")) for item in plex_section.split(""|"")] core.PLEX_SECTION = plex_section",false,"if isinstance ( plex_section , list ) :",===============,0.0,0.0
"def select(self): e = xlib.XEvent() while xlib.XPending(self._display): xlib.XNextEvent(self._display, e) # Key events are filtered by the xlib window event # handler so they get a shot at the prefiltered event. if e.xany.type not in (xlib.KeyPress, xlib.KeyRelease): if xlib.XFilterEvent(e, e.xany.window): continue try: dispatch = self._window_map[e.xany.window] except KeyError: continue dispatch(e)",false,"if e . xany . type not in ( xlib . KeyPress , xlib . KeyRelease ) :",#event#event#event#event#event#event#event#,0.0,0.0
"def format_message(self): bits = [self.message] if self.possibilities: if len(self.possibilities) == 1: bits.append(""Did you mean %s?"" % self.possibilities[0]) else: possibilities = sorted(self.possibilities) bits.append(""(Possible options: %s)"" % "", "".join(possibilities)) return "" "".join(bits)",false,if len ( self . possibilities ) == 1 :,=,9.09,0.0
"def _collect_logs(model): page_token = None all_logs = [] while True: paginated_logs = model.lookup_logs(now, later, page_token=page_token) page_token = paginated_logs.next_page_token all_logs.extend(paginated_logs.logs) if page_token is None: break return all_logs",false,if page_token is None :,=,0.0,0.0
"def run(self): while True: context_id_list_tuple = self._inflated_addresses.get(block=True) if context_id_list_tuple is _SHUTDOWN_SENTINEL: break c_id, inflated_address_list = context_id_list_tuple inflated_value_map = dict(inflated_address_list) if c_id in self._contexts: self._contexts[c_id].set_from_tree(inflated_value_map)",false,if context_id_list_tuple is _SHUTDOWN_SENTINEL :,inflated_address_list,0.0,3.91
"def _setup_prefix(self): # we assume here that our metadata may be nested inside a ""basket"" # of multiple eggs; that's why we use module_path instead of .archive path = self.module_path old = None while path != old: if path.lower().endswith("".egg""): self.egg_name = os.path.basename(path) self.egg_info = os.path.join(path, ""EGG-INFO"") self.egg_root = path break old = path path, base = os.path.split(path)",false,"if path . lower ( ) . endswith ( "".egg"" ) :",============= os.path.,20.0,5.06
"def get_filename(self, prompt): okay = False val = """" while not okay: val = raw_input(""%s: %s"" % (prompt, val)) val = os.path.expanduser(val) if os.path.isfile(val): okay = True elif os.path.isdir(val): path = val val = self.choose_from_list(os.listdir(path)) if val: val = os.path.join(path, val) okay = True else: val = """" else: print(""Invalid value: %s"" % val) val = """" return val",false,elif os . path . isdir ( val ) :,=val =============,10.0,2.63
"def versions(self, sitename, data): # handle the query of type {""query"": '{""key"": ""/books/ia:foo00bar"", ...}} if ""query"" in data: q = json.loads(data[""query""]) itemid = self._get_itemid(q.get(""key"")) if itemid: key = q[""key""] return json.dumps([self.dummy_edit(key)]) # if not just go the default way return ConnectionMiddleware.versions(self, sitename, data)",false,if itemid :,data:,33.33,0.0
"def read_stanza(self): while True: try: stanza_end = self._buffer.index(b""\n"") stanza = self.decoder.decode(self._buffer[:stanza_end]) self._buffer = self._buffer[stanza_end + 1 :] colon = stanza.index("":"") return stanza[:colon], stanza[colon + 1 :] except ValueError: bytes = self.read_bytes() if not bytes: return None else: self._buffer += bytes",false,if not bytes :,=self._buffer[:stanza_end],25.0,3.75
def decodeattrs(attrs): names = [] for bit in range(16): mask = 1 << bit if attrs & mask: if attrnames.has_key(mask): names.append(attrnames[mask]) else: names.append(hex(mask)) return names,false,if attrs & mask :,=,0.0,0.0
"def _set_http_cookie(): if conf.cookie: if isinstance(conf.cookie, dict): conf.http_headers[HTTP_HEADER.COOKIE] = ""; "".join( map(lambda x: ""="".join(x), conf.cookie.items()) ) else: conf.http_headers[HTTP_HEADER.COOKIE] = conf.cookie",false,"if isinstance ( conf . cookie , dict ) :",=,0.0,0.0
"def __ne__(self, other): if isinstance(other, WeakMethod): if not self._alive or not other._alive: return self is not other return weakref.ref.__ne__(self, other) or self._func_ref != other._func_ref return True",false,if not self . _alive or not other . _alive :,self,9.09,0.0
"def update_unread(self, order_id, reset=False): conn = Database.connect_database(self.PATH) with conn: cursor = conn.cursor() if reset is False: cursor.execute( """"""UPDATE sales SET unread = unread + 1 WHERE id=?;"""""", (order_id,) ) else: cursor.execute(""""""UPDATE sales SET unread=0 WHERE id=?;"""""", (order_id,)) conn.commit() conn.close()",false,if reset is False :,update_unread()============,0.0,0.0
"def _get_field_value(self, test, key, match): if test.ver == ofproto_v1_0.OFP_VERSION: members = inspect.getmembers(match) for member in members: if member[0] == key: field_value = member[1] elif member[0] == ""wildcards"": wildcards = member[1] if key == ""nw_src"": field_value = test.nw_src_to_str(wildcards, field_value) elif key == ""nw_dst"": field_value = test.nw_dst_to_str(wildcards, field_value) else: field_value = match[key] return field_value",false,"elif key == ""nw_dst"" :","field_value = test.nw_dst_to_str(wildcards,else: field_value ==(",37.5,7.71
"def nested_filter(self, items, mask): keep_current = self.current_mask(mask) keep_nested_lookup = self.nested_masks(mask) for k, v in items: keep_nested = keep_nested_lookup.get(k) if k in keep_current: if keep_nested is not None: if isinstance(v, dict): yield k, dict(self.nested_filter(v.items(), keep_nested)) else: yield k, v",false,if k in keep_current :,"keep_currentkeep_nested_lookup:keep_nested_lookup:keep_nested_lookup:k, v)",40.0,3.74
"def goToPrevMarkedHeadline(self, event=None): """"""Select the next marked node."""""" c = self p = c.p if not p: return p.moveToThreadBack() wrapped = False while 1: if p and p.isMarked(): break elif p: p.moveToThreadBack() elif wrapped: break else: wrapped = True p = c.rootPosition() if not p: g.blue(""done"") c.treeSelectHelper(p) # Sets focus.",false,if p and p . isMarked ( ) :,p.focus()p.focus()p.focus()p.focus()p.focus()p.focus(),55.56,3.71
"def sample(self, **config): """"""Sample a configuration from this search space."""""" ret = {} ret.update(self.data) kwspaces = self.kwspaces kwspaces.update(config) striped_keys = [k.split(SPLITTER)[0] for k in config.keys()] for k, v in kwspaces.items(): if k in striped_keys: if isinstance(v, NestedSpace): sub_config = _strip_config_space(config, prefix=k) ret[k] = v.sample(**sub_config) else: ret[k] = v return ret",false,if k in striped_keys :,=k in striped_keys:= k.split(SPLITTER)[0]= k.split(SPLITTER)[,80.0,18.56
"def update_gradients_full(self, dL_dK, X, X2=None): if self.ARD: phi1 = self.phi(X) if X2 is None or X is X2: self.variance.gradient = np.einsum(""ij,iq,jq->q"", dL_dK, phi1, phi1) else: phi2 = self.phi(X2) self.variance.gradient = np.einsum(""ij,iq,jq->q"", dL_dK, phi1, phi2) else: self.variance.gradient = np.einsum(""ij,ij"", dL_dK, self._K(X, X2)) * self.beta",false,if X2 is None or X is X2 :,"=self.variance.gradient = np.einsum(""ij,iq,jq->q"", dL_dK,phi1",0.0,0.0
"def post(self): host_json = json.loads(request.data) host_os = host_json.get(""os"") if host_os: result = get_monkey_executable(host_os.get(""type""), host_os.get(""machine"")) if result: # change resulting from new base path executable_filename = result[""filename""] real_path = MonkeyDownload.get_executable_full_path(executable_filename) if os.path.isfile(real_path): result[""size""] = os.path.getsize(real_path) return result return {}",false,if os . path . isfile ( real_path ) :,===== os.path.abspath(real_path)= os.path.abspath(,70.0,27.09
"def _encode_data( self, data, content_type, ): if content_type is MULTIPART_CONTENT: return encode_multipart(BOUNDARY, data) else: # Encode the content so that the byte representation is correct. match = CONTENT_TYPE_RE.match(content_type) if match: charset = match.group(1) else: charset = settings.DEFAULT_CHARSET return force_bytes(data, encoding=charset)",false,if match :,=,0.0,0.0
"def _merge_scientific_float_tokens(tokens: Iterable[str]) -> List[str]: tokens = list(tokens) i = 0 while ""e"" in tokens[i + 1 :]: i = tokens.index(""e"", i + 1) s = i - 1 e = i + 1 if not re.match(""[0-9]"", str(tokens[s])): continue if re.match(""[+-]"", str(tokens[e])): e += 1 if re.match(""[0-9]"", str(tokens[e])): e += 1 tokens[s:e] = ["""".join(tokens[s:e])] i -= 1 return tokens",false,"if re . match ( ""[0-9]"" , str ( tokens [ e ] ) ) :","re.match(""[+-]"", str(tokens[s])):1 if re.match(""[+-]"",re.",86.36,39.56
"def convert_with_key(self, key, value, replace=True): result = self.configurator.convert(value) # If the converted value is different, save for next time if value is not result: if replace: self[key] = result if type(result) in (ConvertingDict, ConvertingList, ConvertingTuple): result.parent = self result.key = key return result",false,if replace :,result:self.configurator.convert(value),33.33,4.2
"def OnListEndLabelEdit(self, std, extra): item = extra[0] text = item[4] if text is None: return item_id = self.GetItem(item[0])[6] from bdb import Breakpoint for bplist in Breakpoint.bplist.itervalues(): for bp in bplist: if id(bp) == item_id: if text.strip().lower() == ""none"": text = None bp.cond = text break self.RespondDebuggerData()",false,if id ( bp ) == item_id :,===============,22.22,5.26
"def add(self, url: str, future_nzo: NzbObject, when: Optional[int] = None): """"""Add an URL to the URLGrabber queue, 'when' is seconds from now"""""" if future_nzo and when: # Always increase counter future_nzo.url_tries += 1 # Too many tries? Cancel if future_nzo.url_tries > cfg.max_url_retries(): self.fail_to_history(future_nzo, url, T(""Maximum retries"")) return future_nzo.url_wait = time.time() + when self.queue.put((url, future_nzo))",false,if future_nzo . url_tries > cfg . max_url_retries ( ) :,"url,when:url,when:url",9.09,2.07
def _is_datetime_string(series): if series.dtype == object: not_numeric = False try: pd.to_numeric(series) except Exception as e: not_numeric = True datetime_col = None if not_numeric: try: datetime_col = pd.to_datetime(series) except Exception as e: return False if datetime_col is not None: return True return False,false,if not_numeric :,True,0.0,0.0
"def _getEventAndObservers(self, event): if isinstance(event, xpath.XPathQuery): # Treat as xpath observers = self._xpathObservers else: if self.prefix == event[: len(self.prefix)]: # Treat as event observers = self._eventObservers else: # Treat as xpath event = xpath.internQuery(event) observers = self._xpathObservers return event, observers",false,if self . prefix == event [ : len ( self . prefix ) ] :,event,5.88,0.0
"def test_wildcard_import(): bonobo = __import__(""bonobo"") assert bonobo.__version__ for name in dir(bonobo): # ignore attributes starting by underscores if name.startswith(""_""): continue attr = getattr(bonobo, name) if inspect.ismodule(attr): continue assert name in bonobo.__all__",false,if inspect . ismodule ( attr ) :,name,0.0,0.0
"def relint_views(wid=None): windows = [sublime.Window(wid)] if wid else sublime.windows() for window in windows: for view in window.views(): if view.buffer_id() in persist.assigned_linters and view.is_primary(): hit(view, ""relint_views"")",false,if view . buffer_id ( ) in persist . assigned_linters and view . is_primary ( ) :,view,5.88,0.0
def _check_for_unknown_gender(self): if self.obj.get_gender() == Person.UNKNOWN: d = GenderDialog(parent=self.window) gender = d.run() d.destroy() if gender >= 0: self.obj.set_gender(gender),false,if gender >= 0 :,=,16.67,0.0
"def add_to_path(self, fnames): """"""Add fnames to path"""""" indexes = [] for path in fnames: project = self.get_source_project(path) if project.add_to_pythonpath(path): self.parent_widget.emit(SIGNAL(""pythonpath_changed()"")) indexes.append(self.get_index(path)) if indexes: self.reset_icon_provider() for index in indexes: self.update(index)",false,if project . add_to_pythonpath ( path ) :,"self.parent_widget.emit(SIGNAL(""pythonpath_changed()""))",37.5,3.17
"def validate(self, value): if value.grid_id is not None: if not isinstance(value, self.proxy_class): self.error(""FileField only accepts GridFSProxy values"") if not isinstance(value.grid_id, ObjectId): self.error(""Invalid GridFSProxy value"")",false,"if not isinstance ( value . grid_id , ObjectId ) :",value,9.09,0.0
"def shortcut(self, input, ch_out, stride, name, if_first=False): ch_in = input.shape[1] if ch_in != ch_out or stride != 1: if if_first: return self.conv_bn_layer(input, ch_out, 1, stride, name=name) else: return self.conv_bn_layer_new(input, ch_out, 1, stride, name=name) else: return input",false,if if_first :,=,0.0,0.0
"def convert_path(ctx, tpath): for points, code in tpath.iter_segments(): if code == Path.MOVETO: ctx.move_to(*points) elif code == Path.LINETO: ctx.line_to(*points) elif code == Path.CURVE3: ctx.curve_to( points[0], points[1], points[0], points[1], points[2], points[3] ) elif code == Path.CURVE4: ctx.curve_to(*points) elif code == Path.CLOSEPOLY: ctx.close_path()",false,elif code == Path . CURVE4 :,elif== Path.CLOSEPOLY:elif== Path.CLOSEPOLY:elif== Path.CLOSE,75.0,13.26
"def _get_build_status(self, job_name, build_number): try: build_info = self.server.get_build_info(job_name, build_number) if build_info[""building""]: return ""building"" else: return ""built"" except jenkins.NotFoundException: return ""not found""",false,"if build_info [ ""building"" ] :",job_name,0.0,0.0
"def _parse_param_value(name, datatype, default): if datatype == ""bool"": if default.lower() == ""true"": return True elif default.lower() == ""false"": return False else: _s = ""{}: Invalid default value '{}' for bool parameter {}"" raise SyntaxError(_s.format(self.name, default, p)) elif datatype == ""int"": if type(default) == int: return default else: return int(default, 0) elif datatype == ""real"": if type(default) == float: return default else: return float(default) else: return str(default)",false,if type ( default ) == float :,p:returnp):p)ppppppp,22.22,5.8
"def get_fills(self, exchange_order_id): async with aiohttp.ClientSession() as client: response: aiohttp.ClientResponse = await client.get( f""{BASE_URL}{FILLS_ROUTE}"", params={""orderId"": exchange_order_id, ""limit"": 100}, ) if response.status >= 300: try: msg = await response.json() except ValueError: msg = await response.text() raise DydxAsyncAPIError(response.status, msg) return await response.json()",false,if response . status >= 300 :,=response.text(),37.5,12.87
"def semanticTags(self, semanticTags): if semanticTags is None: self.__semanticTags = OrderedDict() # check for key, value in list(semanticTags.items()): if not isinstance(key, int): raise TypeError(""At least one key is not a valid int position"") if not isinstance(value, list): raise TypeError( ""At least one value of the provided dict is not a list of string"" ) for x in value: if not isinstance(x, str): raise TypeError( ""At least one value of the provided dict is not a list of string"" ) self.__semanticTags = semanticTags",false,"if not isinstance ( x , str ) :","=self.__semanticTags.set(key, value) #self.__semanticTags.set(key, value) #",33.33,1.83
"def start_cutting_tool(self, event, axis, direction): toggle = event.EventObject self.cutting = toggle.Value if toggle.Value: # Disable the other toggles for child in self.cutsizer.Children: child = child.Window if child != toggle: child.Value = False self.cutting_axis = axis self.cutting_direction = direction else: self.cutting_axis = None self.cutting_direction = None self.cutting_dist = None",false,if child != toggle :,self.cutsizer.Children:self.cutsizer.Children:self.cutsizer.Children:self.,16.67,1.91
"def decoration_helper(self, patched, args, keywargs): extra_args = [] with contextlib.ExitStack() as exit_stack: for patching in patched.patchings: arg = exit_stack.enter_context(patching) if patching.attribute_name is not None: keywargs.update(arg) elif patching.new is DEFAULT: extra_args.append(arg) args += tuple(extra_args) yield (args, keywargs)",false,elif patching . new is DEFAULT :,=keywargs.update(arg),14.29,6.57
def decodeattrs(attrs): names = [] for bit in range(16): mask = 1 << bit if attrs & mask: if attrnames.has_key(mask): names.append(attrnames[mask]) else: names.append(hex(mask)) return names,false,if attrnames . has_key ( mask ) :,=,0.0,0.0
"def pytest_collection_modifyitems(items): for item in items: if item.nodeid.startswith(""tests/params""): if ""stage"" not in item.keywords: item.add_marker(pytest.mark.stage(""unit"")) if ""init"" not in item.keywords: item.add_marker(pytest.mark.init(rng_seed=123))",false,"if ""init"" not in item . keywords :","item.add_marker(pytest.mark.stage(""unit"")))",40.0,5.11
"def handle_socket(self, request): conn = request.connection while True: chunk = conn.recv(4) if len(chunk) < 4: break slen = struct.unpack("">L"", chunk)[0] chunk = conn.recv(slen) while len(chunk) < slen: chunk = chunk + conn.recv(slen - len(chunk)) obj = pickle.loads(chunk) record = logging.makeLogRecord(obj) self.log_output += record.msg + ""\n"" self.handled.release()",false,if len ( chunk ) < 4 :,=self.handled.acquire()self.handled.release()self.handled.release()self.handled.release,25.0,1.65
"def on_source_foreach(self, model, path, iter, id): m_id = model.get_value(iter, self.COLUMN_ID) if m_id == id: if self._foreach_mode == ""get"": self._foreach_take = model.get_value(iter, self.COLUMN_ENABLED) elif self._foreach_mode == ""set"": self._foreach_take = iter",false,"elif self . _foreach_mode == ""set"" :","model.get_value(iter, self.COLUMN_ENABLED)",20.0,6.75
"def parts(): for l in lists.leaves: head_name = l.get_head_name() if head_name == ""System`List"": yield l.leaves elif head_name != ""System`Missing"": raise MessageException(""Catenate"", ""invrp"", l)",false,"elif head_name != ""System`Missing"" :",list,0.0,0.0
"def __fill_counter_values(self, command: str): result = [] regex = r""(item[0-9]+\.counter_value)"" for token in re.split(regex, command): if re.match(regex, token) is not None: try: result.append(str(self.simulator_config.item_dict[token].value)) except (KeyError, ValueError, AttributeError): logger.error(""Could not get counter value for "" + token) else: result.append(token) return """".join(result)",false,"if re . match ( regex , token ) is not None :",=self.simulator_config.item_dict[token].value)=self.simulator_config.item_,23.08,1.98
"def IMPORTFROM(self, node): if node.module == ""__future__"": if not self.futuresAllowed: self.report(messages.LateFutureImport, node, [n.name for n in node.names]) else: self.futuresAllowed = False for alias in node.names: if alias.name == ""*"": self.scope.importStarred = True self.report(messages.ImportStarUsed, node, node.module) continue name = alias.asname or alias.name importation = Importation(name, node) if node.module == ""__future__"": importation.used = (self.scope, node) self.addBinding(node, importation)",false,"if node . module == ""__future__"" :",importation.importStarred =importation.importStarred = Trueimportation.importStarred = Trueimportation.,30.0,3.74
"def _split_batch_list(args, batch_list): new_list = [] for batch in batch_list.batches: new_list.append(batch) if len(new_list) == args.batch_size_limit: yield batch_pb2.BatchList(batches=new_list) new_list = [] if new_list: yield batch_pb2.BatchList(batches=new_list)",false,if len ( new_list ) == args . batch_size_limit :,=,9.09,0.0
"def get_branch_or_use_upstream(branch_name, arg, repo): if not branch_name: # use upstream branch current_b = repo.current_branch upstream_b = current_b.upstream if not upstream_b: raise ValueError( ""No {0} branch specified and the current branch has no upstream "" ""branch set"".format(arg) ) ret = current_b.upstream else: ret = get_branch(branch_name, repo) return ret",false,if not upstream_b :,repo.current_branch,0.0,8.75
"def __init__(self, **settings): default_settings = self.get_default_settings() for name, value in default_settings.items(): if not hasattr(self, name): setattr(self, name, value) for name, value in settings.items(): if name not in default_settings: raise ImproperlyConfigured( ""Invalid setting '{}' for {}"".format( name, self.__class__.__name__, ) ) setattr(self, name, value)",false,"if not hasattr ( self , name ) :","self.__class__.__name__=name,self.__class__.__name__=name,",33.33,1.53
"def _declare(self, name, obj, included=False, quals=0): if name in self._declarations: prevobj, prevquals = self._declarations[name] if prevobj is obj and prevquals == quals: return if not self._override: raise api.FFIError( ""multiple declarations of %s (for interactive usage, "" ""try cdef(xx, override=True))"" % (name,) ) assert ""__dotdotdot__"" not in name.split() self._declarations[name] = (obj, quals) if included: self._included_declarations.add(obj)",false,if not self . _override :,=self._declarations[name] ==self._declarations[name]=self._declarations[name,33.33,5.86
"def include_file(name, fdir=tmp_dir, b64=False): try: if fdir is None: fdir = """" if b64: with io.open(os.path.join(fdir, name), ""rb"") as f: return base64.b64encode(f.read()).decode(""utf-8"") else: with io.open(os.path.join(fdir, name), ""r"", encoding=""utf-8"") as f: return f.read() except (OSError, IOError) as e: logger.error(""Could not include file '{}': {}"".format(name, e))",false,if b64 :,"filename,fdir,fdir,fdir, name,fdir,fdir,fdir",0.0,0.0
"def to_raw_json(self): parts = {} for p in self.parts: if p[0] not in parts: parts[p[0]] = [] parts[p[0]].append({""value"": p[2], ""parameters"": p[1]}) children = [x.to_raw_json() for x in self.children] return { ""type"": self.__class__.__name__, ""children"": children, ""parts"": parts, }",false,if p [ 0 ] not in parts :,=,0.0,0.0
"def process_output( output: str, filename: str, start_line: int ) -> Tuple[Optional[str], bool]: error_found = False for line in output.splitlines(): t = get_revealed_type(line, filename, start_line) if t: return t, error_found elif ""error:"" in line: error_found = True return None, True # finding no reveal_type is an error",false,if t :,:line:line:line:line::line::::,33.33,2.63
"def __init__( self, resize_keyboard=None, one_time_keyboard=None, selective=None, row_width=3 ): if row_width > self.max_row_keys: # Todo: Will be replaced with Exception in future releases if not DISABLE_KEYLEN_ERROR: logger.error( ""Telegram does not support reply keyboard row width over %d."" % self.max_row_keys ) row_width = self.max_row_keys self.resize_keyboard = resize_keyboard self.one_time_keyboard = one_time_keyboard self.selective = selective self.row_width = row_width self.keyboard = []",false,if not DISABLE_KEYLEN_ERROR :,selective = selectiveselective = selectiveself . selective = selectiveself .= selective selfself,0.0,0.0
"def realizeElementExpressions(innerElement): elementHasBeenRealized = False for exp in innerElement.expressions: if not hasattr(exp, ""realize""): continue # else: before, during, after = exp.realize(innerElement) elementHasBeenRealized = True for n in before: newStream.append(n) if during is not None: newStream.append(during) for n in after: newStream.append(n) if elementHasBeenRealized is False: newStream.append(innerElement)",false,if during is not None :,newStream.append(innerElement)newStream.append(innerElement)newStream.append(innerElement) ifis,0.0,0.0
"def lex_number(self, pos): # numeric literal start = pos found_dot = False while pos < len(self.string) and ( self.string[pos].isdigit() or self.string[pos] == ""."" ): if self.string[pos] == ""."": if found_dot is True: raise ValueError(""Invalid number. Found multiple '.'"") found_dot = True # technically we allow more than one ""."" and let float()'s parsing # complain later pos += 1 val = self.string[start:pos] return Token(TokenType.LNUM, val, len(val))",false,"if self . string [ pos ] == ""."" :",pos=pos=pos=pos=pos=pos====,23.08,5.82
"def rename(src, dst): # Try atomic or pseudo-atomic rename if _rename(src, dst): return # Fall back to ""move away and replace"" try: os.rename(src, dst) except OSError as e: if e.errno != errno.EEXIST: raise old = ""%s-%08x"" % (dst, random.randint(0, sys.maxsize)) os.rename(dst, old) os.rename(src, dst) try: os.unlink(old) except Exception: pass",false,if e . errno != errno . EEXIST :,"os.rename(src, dst)os.rename(dst,os.rename(src,os.unlink(dst",20.0,1.79
"def _the_callback(widget, event_id): point = widget.GetCenter() index = widget.WIDGET_INDEX if hasattr(callback, ""__call__""): if num > 1: args = [point, index] else: args = [point] if pass_widget: args.append(widget) try_callback(callback, *args) return",false,if pass_widget :,event_id,0.0,0.0
"def run(self): for _ in range(self.n): error = True try: self.collection.insert_one({""test"": ""insert""}) error = False except: if not self.expect_exception: raise if self.expect_exception: assert error",false,if not self . expect_exception :,True,0.0,0.0
"def handle(self, *args: Any, **options: Any) -> None: realm = self.get_realm(options) if options[""all""]: if realm is None: raise CommandError( ""You must specify a realm if you choose the --all option."" ) self.fix_all_users(realm) return self.fix_emails(realm, options[""emails""])",false,if realm is None :,"options[""emails""]",0.0,0.0
"def recv_tdi(self, nbits, pos): bits = 0 for n in range(nbits * 2): yield from self._wait_for_tck() if (yield self.tck.o) == pos: bits = (bits << 1) | (yield self.tdi.o) return bits",false,if ( yield self . tck . o ) == pos :,bits,0.0,0.0
"def _split_head(self): if not hasattr(self, ""_severed_head""): if self._tree: tree = self._tree.copy() head = tree.get_heading_text() tree.remove_heading() self._severed_head = (head, tree) else: self._severed_head = (None, None) return self._severed_head",false,if self . _tree :,self._severed_head,40.0,30.21
"def buildSearchTrie(self, choices): searchtrie = trie.Trie() for choice in choices: for token in self.tokenizeChoice(choice): if not searchtrie.has_key(token): searchtrie[token] = [] searchtrie[token].append(choice) return searchtrie",false,if not searchtrie . has_key ( token ) :,=,0.0,0.0
"def format_sql(sql, params): rv = [] if isinstance(params, dict): # convert sql with named parameters to sql with unnamed parameters conv = _FormatConverter(params) if params: sql = sql_to_string(sql) sql = sql % conv params = conv.params else: params = () for param in params or (): if param is None: rv.append(""NULL"") param = safe_repr(param) rv.append(param) return sql, rv",false,if param is None :,"sql,params============",0.0,0.0
def on_completed2(): doner[0] = True if not qr: if len(ql) > 0: observer.on_next(False) observer.on_completed() elif donel[0]: observer.on_next(True) observer.on_completed(),false,elif donel [ 0 ] :,else:,16.67,0.0
"def notify_digest(self, frequency, changes): notifications = defaultdict(list) users = {} for change in changes: for user in self.get_users(frequency, change): if change.project is None or user.can_access_project(change.project): notifications[user.pk].append(change) users[user.pk] = user for user in users.values(): self.send_digest( user.profile.language, user.email, notifications[user.pk], subscription=user.current_subscription, )",false,if change . project is None or user . can_access_project ( change . project ) :,notifications[user.pk],12.5,1.57
"def _any_listener_using(self, target_group_arn): for load_balancer in self.load_balancers.values(): for listener in load_balancer.listeners.values(): for rule in listener.rules: for action in rule.actions: if action.data.get(""target_group_arn"") == target_group_arn: return True return False",false,"if action . data . get ( ""target_group_arn"" ) == target_group_arn :",rule,0.0,0.0
"def train_dict(self, triples): """"""Train a dict lemmatizer given training (word, pos, lemma) triples."""""" # accumulate counter ctr = Counter() ctr.update([(p[0], p[1], p[2]) for p in triples]) # find the most frequent mappings for p, _ in ctr.most_common(): w, pos, l = p if (w, pos) not in self.composite_dict: self.composite_dict[(w, pos)] = l if w not in self.word_dict: self.word_dict[w] = l return",false,if w not in self . word_dict :,"=self.word_dict[(w, pos)]=self.word_dict[(w, pos)]=",50.0,13.6
"def parse_git_config(path): """"""Parse git config file."""""" config = dict() section = None with open(os.path.join(path, ""config""), ""r"") as f: for line in f: line = line.strip() if line.startswith(""[""): section = line[1:-1].strip() config[section] = dict() elif section: key, value = line.replace("" "", """").split(""="") config[section][key] = value return config",false,"if line . startswith ( ""["" ) :",config,0.0,0.0
"def send_signal(self, pid, signum): if pid in self.processes: process = self.processes[pid] hook_result = self.call_hook(""before_signal"", pid=pid, signum=signum) if signum != signal.SIGKILL and not hook_result: logger.debug( ""before_signal hook didn't return True "" ""=> signal %i is not sent to %i"" % (signum, pid) ) else: process.send_signal(signum) self.call_hook(""after_signal"", pid=pid, signum=signum) else: logger.debug(""process %s does not exist"" % pid)",false,if signum != signal . SIGKILL and not hook_result :,"signal %i isnotsignal %i is%i"" % ())))))",9.09,2.15
"def validate_pos_return(self): if self.is_pos and self.is_return: total_amount_in_payments = 0 for payment in self.payments: total_amount_in_payments += payment.amount invoice_total = self.rounded_total or self.grand_total if total_amount_in_payments < invoice_total: frappe.throw( _(""Total payments amount can't be greater than {}"").format( -invoice_total ) )",false,if total_amount_in_payments < invoice_total :,self.is_pos:self.is_return:,20.0,4.1
"def delete(key, inner_key=None): if inner_key is not None: try: del cache[key][inner_key] del use_count[key][inner_key] if not cache[key]: del cache[key] del use_count[key] wrapper.cache_size -= 1 except KeyError: return False else: return True else: try: wrapper.cache_size -= len(cache[key]) del cache[key] del use_count[key] except KeyError: return False else: return True",false,if not cache [ key ] :,del cache[key],57.14,44.83
"def insertionsort(array): size = array.getsize() array.reset(""Insertion sort"") for i in range(1, size): j = i - 1 while j >= 0: if array.compare(j, j + 1) <= 0: break array.swap(j, j + 1) j = j - 1 array.message(""Sorted"")",false,"if array . compare ( j , j + 1 ) <= 0 :",=array.sort(i)=array.sort(i)=array.sort(i)=,33.33,4.34
"def publish_state(cls, payload, state): try: if isinstance(payload, LiveActionDB): if state == action_constants.LIVEACTION_STATUS_REQUESTED: cls.process(payload) else: worker.get_worker().process(payload) except Exception: traceback.print_exc() print(payload)",false,if state == action_constants . LIVEACTION_STATUS_REQUESTED :,state:,25.0,0.0
"def change_opacity_function(self, new_f): self.opacity_function = new_f dr = self.radius / self.num_levels sectors = [] for submob in self.submobjects: if type(submob) == AnnularSector: sectors.append(submob) for (r, submob) in zip(np.arange(0, self.radius, dr), sectors): if type(submob) != AnnularSector: # it's the shadow, don't dim it continue alpha = self.opacity_function(r) submob.set_fill(opacity=alpha)",false,if type ( submob ) != AnnularSector :,"self.opacity_function=( r , submob)=( r , submob)=",44.44,5.11
"def is_suppressed_warning( type: str, subtype: str, suppress_warnings: List[str] ) -> bool: """"""Check the warning is suppressed or not."""""" if type is None: return False for warning_type in suppress_warnings: if ""."" in warning_type: target, subtarget = warning_type.split(""."", 1) else: target, subtarget = warning_type, None if target == type: if ( subtype is None or subtarget is None or subtarget == subtype or subtarget == ""*"" ): return True return False",false,"if ""."" in warning_type :","TrueTrueTrueTrueTrueTrueTrue ifsubtarget ==subtarget == ""*""subtarget == ""*""",28.57,2.91
"def set_many(self, mapping, timeout=None): timeout = self._normalize_timeout(timeout) # Use transaction=False to batch without calling redis MULTI # which is not supported by twemproxy pipe = self._client.pipeline(transaction=False) for key, value in _items(mapping): dump = self.dump_object(value) if timeout == -1: pipe.set(name=self.key_prefix + key, value=dump) else: pipe.setex(name=self.key_prefix + key, value=dump, time=timeout) return pipe.execute()",false,if timeout == - 1 :,=pipe,14.29,0.0
"def maybe_relative_path(path): if not os.path.isabs(path): return path # already relative dir = path names = [] while True: prevdir = dir dir, name = os.path.split(prevdir) if dir == prevdir or not dir: return path # failed to make it relative names.append(name) try: if samefile(dir, os.curdir): names.reverse() return os.path.join(*names) except OSError: pass",false,"if samefile ( dir , os . curdir ) :",dir==============,10.0,2.63
"def word_range(word): for ind in range(len(word)): temp = word[ind] for c in [chr(x) for x in range(ord(""a""), ord(""z"") + 1)]: if c != temp: yield word[:ind] + c + word[ind + 1 :]",false,if c != temp :,:,16.67,0.0
"def validate(self): self.update_soil_edit(""sand_composition"") for soil_type in self.soil_types: if self.get(soil_type) > 100 or self.get(soil_type) < 0: frappe.throw(_(""{0} should be a value between 0 and 100"").format(soil_type)) if sum(self.get(soil_type) for soil_type in self.soil_types) != 100: frappe.throw(_(""Soil compositions do not add up to 100""))",false,if self . get ( soil_type ) > 100 or self . get ( soil_type ) < 0 :,soil_type)notsoil_type)notself.get(soil_type)notself,36.84,34.87
"def on_click(self, event): run = self._is_running() if event[""button""] == self.button_activate: self.py3.command_run([""xscreensaver-command"", ""-activate""]) if event[""button""] == self.button_toggle: if run: self.py3.command_run([""xscreensaver-command"", ""-exit""]) else: # Because we want xscreensaver to continue running after # exit, we instead use preexec_fn=setpgrp here. Popen( [""xscreensaver"", ""-no-splash"", ""-no-capture-stderr""], stdout=PIPE, stderr=PIPE, preexec_fn=setpgrp, )",false,if run :,###############,0.0,0.0
"def maybe_relative_path(path): if not os.path.isabs(path): return path # already relative dir = path names = [] while True: prevdir = dir dir, name = os.path.split(prevdir) if dir == prevdir or not dir: return path # failed to make it relative names.append(name) try: if samefile(dir, os.curdir): names.reverse() return os.path.join(*names) except OSError: pass",false,if dir == prevdir or not dir :,dir==============,33.33,9.78
"def _format_micros(self, datestring): parts = datestring[:-1].split(""."") if len(parts) == 1: if datestring.endswith(""Z""): return datestring[:-1] + "".000000Z"" else: return datestring + "".000000Z"" else: micros = parts[-1][:6] if len(parts[-1]) > 6 else parts[-1] return ""."".join(parts[:-1] + [""{:06d}"".format(int(micros))]) + ""Z""",false,"if datestring . endswith ( ""Z"" ) :",+++++,0.0,0.0
"def preprocess_raw_enwik9(input_filename, output_filename): with open(input_filename, ""r"") as f1: with open(output_filename, ""w"") as f2: while True: line = f1.readline() if not line: break line = list(enwik9_norm_transform([line]))[0] if line != "" "" and line != """": if line[0] == "" "": line = line[1:] f2.writelines(line + ""\n"")",false,"if line != "" "" and line != """" :",line:line:line:line:line:line:line:line:line:line:,23.08,2.52
"def set(self, item, data): if not type(item) is slice: item = slice(item, item + len(data), None) virt_item = self.item2virtitem(item) if not virt_item: return off = 0 for s, n_item in virt_item: if isinstance(s, ProgBits): i = slice(off, n_item.stop + off - n_item.start, n_item.step) data_slice = data.__getitem__(i) s.content.__setitem__(n_item, data_slice) off = i.stop else: raise ValueError(""TODO XXX"") return",false,"if isinstance ( s , ProgBits ) :","s, data_slice):s.content.__setitem__(n_item, data_slice) off +=s.content",62.5,3.59
"def walk(msg, callback, data): partnum = 0 for part in msg.walk(): # multipart/* are just containers if part.get_content_maintype() == ""multipart"": continue ctype = part.get_content_type() if ctype is None: ctype = OCTET_TYPE filename = part.get_filename() if not filename: filename = PART_FN_TPL % (partnum) headers = dict(part) LOG.debug(headers) headers[""Content-Type""] = ctype payload = util.fully_decoded_payload(part) callback(data, filename, payload, headers) partnum = partnum + 1",false,if not filename :,partnum += 1partnum=partnum + 1partnum + 1partnum + 1partnum=,0.0,0.0
"def _run_wes(args): """"""Run CWL using a Workflow Execution Service (WES) endpoint"""""" main_file, json_file, project_name = _get_main_and_json(args.directory) main_file = _pack_cwl(main_file) if args.host and ""stratus"" in args.host: _run_wes_stratus(args, main_file, json_file) else: opts = [""--no-wait""] if args.host: opts += [""--host"", args.host] if args.auth: opts += [""--auth"", args.auth] cmd = [""wes-client""] + opts + [main_file, json_file] _run_tool(cmd)",false,if args . host :,args.host: opts +=args.host: opts +=args.auth: opts +=args.host: opts +=args,80.0,8.06
"def insertTestData(self, rows): for row in rows: if isinstance(row, Worker): self.workers[row.id] = dict( id=row.id, name=row.name, paused=0, graceful=0, info=row.info ) elif isinstance(row, ConfiguredWorker): row.id = row.buildermasterid * 10000 + row.workerid self.configured[row.id] = dict( buildermasterid=row.buildermasterid, workerid=row.workerid ) elif isinstance(row, ConnectedWorker): self.connected[row.id] = dict(masterid=row.masterid, workerid=row.workerid)",false,"elif isinstance ( row , ConfiguredWorker ) :",= row.buildermasterid * 10000 +row.buildermasteridrow.buildermasteridrow.buildermasteridrow,12.5,2.84
"def local_shape_to_shape_i(node): if node.op == T.shape: # This optimization needs ShapeOpt and fgraph.shape_feature if not hasattr(node.fgraph, ""shape_feature""): return shape_feature = node.fgraph.shape_feature ret = shape_feature.make_vector_shape(node.inputs[0]) # We need to copy over stack trace from input to output copy_stack_trace(node.outputs[0], ret) return [ret]",false,"if not hasattr ( node . fgraph , ""shape_feature"" ) :","shape_feature.make_vector_shape(node.inputs[0],shape_feature.make_vector_shape(node.",38.46,8.37
"def get_config(): """"""Get INI parser with version.ini data."""""" # TODO(hanuszczak): See comment in `setup.py` for `grr-response-proto`. ini_path = os.path.join(THIS_DIRECTORY, ""version.ini"") if not os.path.exists(ini_path): ini_path = os.path.join(THIS_DIRECTORY, ""../../version.ini"") if not os.path.exists(ini_path): raise RuntimeError(""Couldn't find version.ini"") config = configparser.ConfigParser() config.read(ini_path) return config",false,if not os . path . exists ( ini_path ) :,configparser.ConfigParser()configparser.ConfigParser()configparser.ConfigParser()configparser.ConfigParser()config,36.36,2.57
"def init_weights(self, pretrained=None): if isinstance(pretrained, str): logger = logging.getLogger() load_checkpoint(self, pretrained, strict=False, logger=logger) elif pretrained is None: for m in self.modules(): if isinstance(m, nn.Conv2d): kaiming_init(m) elif isinstance(m, (_BatchNorm, nn.GroupNorm)): constant_init(m, 1) else: raise TypeError(""pretrained must be a str or None"")",false,"if isinstance ( m , nn . Conv2d ) :",:::::::::::::::,10.0,2.63
"def isValidDateString(config_param_name, value, valid_value): try: if value == ""DD-MM-YYYY"": return value day, month, year = value.split(""-"") if int(day) < 1 or int(day) > 31: raise DateStringValueError(config_param_name, value) if int(month) < 1 or int(month) > 12: raise DateStringValueError(config_param_name, value) if int(year) < 1900 or int(year) > 2013: raise DateStringValueError(config_param_name, value) return value except Exception: raise DateStringValueError(config_param_name, value)",false,"if value == ""DD-MM-YYYY"" :",if< 1 or int(month) >1 or int(month) >12: raise DateStringValueError(config_param,16.67,1.87
"def from_obj(cls, py_obj): if not isinstance(py_obj, Image): raise TypeError(""py_obj must be a wandb.Image"") else: if hasattr(py_obj, ""_boxes"") and py_obj._boxes: box_keys = list(py_obj._boxes.keys()) else: box_keys = [] if hasattr(py_obj, ""masks"") and py_obj.masks: mask_keys = list(py_obj.masks.keys()) else: mask_keys = [] return cls(box_keys, mask_keys)",false,"if hasattr ( py_obj , ""_boxes"" ) and py_obj . _boxes :",py_objpy_obj.image)py_obj.image),21.43,16.12
"def _path_type(st, lst): parts = [] if st: if stat.S_ISREG(st.st_mode): parts.append(""file"") elif stat.S_ISDIR(st.st_mode): parts.append(""dir"") else: parts.append(""other"") if lst: if stat.S_ISLNK(lst.st_mode): parts.append(""link"") return "" "".join(parts)",false,elif stat . S_ISDIR ( st . st_mode ) :,lst:,10.0,0.0
"def is_destructive(queries): """"""Returns if any of the queries in *queries* is destructive."""""" keywords = (""drop"", ""shutdown"", ""delete"", ""truncate"", ""alter"") for query in sqlparse.split(queries): if query: if query_starts_with(query, keywords) is True: return True elif query_starts_with( query, [""update""] ) is True and not query_has_where_clause(query): return True return False",false,"if query_starts_with ( query , keywords ) is True :",TrueTrue,0.0,0.0
"def _store_gsuite_membership_post(self): """"""Flush storing gsuite memberships."""""" if not self.member_cache: return self.session.flush() # session.execute automatically flushes if self.membership_items: if get_sql_dialect(self.session) == ""sqlite"": # SQLite doesn't support bulk insert for item in self.membership_items: stmt = self.dao.TBL_MEMBERSHIP.insert(item) self.session.execute(stmt) else: stmt = self.dao.TBL_MEMBERSHIP.insert(self.membership_items) self.session.execute(stmt)",false,"if get_sql_dialect ( self . session ) == ""sqlite"" :",self.dao.TBL_MEMBERSHIP.insert(self.membership_items) self.session.execute(stmt)self,38.46,9.12
"def forward(self, inputs: paddle.Tensor): outputs = [] blocks = self.block(inputs) route = None for i, block in enumerate(blocks): if i > 0: block = paddle.concat([route, block], axis=1) route, tip = self.yolo_blocks[i](block) block_out = self.block_outputs[i](tip) outputs.append(block_out) if i < 2: route = self.route_blocks_2[i](route) route = self.upsample(route) return outputs",false,if i > 0 :,outputs====,0.0,0.0
"def deep_dict(self, root=None): if root is None: root = self result = {} for key, value in root.items(): if isinstance(value, dict): result[key] = self.deep_dict(root=self.__class__._get_next(key, root)) else: result[key] = value return result",false,"if isinstance ( value , dict ) :",self,0.0,0.0
"def _parse_param_list(self, content): r = Reader(content) params = [] while not r.eof(): header = r.read().strip() if "" : "" in header: arg_name, arg_type = header.split("" : "")[:2] else: arg_name, arg_type = header, """" desc = r.read_to_next_unindented_line() desc = dedent_lines(desc) params.append((arg_name, arg_type, desc)) return params",false,"if "" : "" in header :",=r.read()))),0.0,0.0
"def _ungroup(sequence, groups=None): for v in sequence: if isinstance(v, (list, tuple)): if groups is not None: groups.append(list(_ungroup(v, groups=None))) for v in _ungroup(v, groups): yield v else: yield v",false,"if isinstance ( v , ( list , tuple ) ) :",=,0.0,0.0
"def _add_resource_group(obj): if isinstance(obj, list): for array_item in obj: _add_resource_group(array_item) elif isinstance(obj, dict): try: if ""resourcegroup"" not in [x.lower() for x in obj.keys()]: if obj[""id""]: obj[""resourceGroup""] = _parse_id(obj[""id""])[""resource-group""] except (KeyError, IndexError, TypeError): pass for item_key in obj: if item_key != ""sourceVault"": _add_resource_group(obj[item_key])",false,"if item_key != ""sourceVault"" :","item_key= _parse_id(obj[item_key][""resource-group""]):if isinstance(obj,",75.0,6.6
"def haslayer(self, cls): """"""true if self has a layer that is an instance of cls. Superseded by ""cls in self"" syntax."""""" if self.__class__ == cls or self.__class__.__name__ == cls: return 1 for f in self.packetfields: fvalue_gen = self.getfieldval(f.name) if fvalue_gen is None: continue if not f.islist: fvalue_gen = SetGen(fvalue_gen, _iterpacket=0) for fvalue in fvalue_gen: if isinstance(fvalue, Packet): ret = fvalue.haslayer(cls) if ret: return ret return self.payload.haslayer(cls)",false,"if isinstance ( fvalue , Packet ) :","isinstance(fvalue, SetGen):fvalue_gen.islist: fvalue_gen =fvalue_gen.",75.0,13.52
"def _post_attachment(self, message, channel, color, sub_fields=None): if channel is None: message_channels = self.channels else: message_channels = [channel] for message_channel in message_channels: attachment = { ""fallback"": message, ""text"": message, ""color"": color, } if sub_fields is not None: attachment[""fields""] = sub_fields self.slack_client.api_call( ""chat.postMessage"", channel=message_channel, attachments=[attachment], as_user=True, )",false,if sub_fields is not None :,"=self.slack_client.api_call(""chat.postMessage"", channel=message_channel, attachments=[attachment], as",0.0,1.24
"def create(cls, repository, args): key = cls() passphrase = os.environ.get(""ATTIC_PASSPHRASE"") if passphrase is not None: passphrase2 = passphrase else: passphrase, passphrase2 = 1, 2 while passphrase != passphrase2: passphrase = getpass(""Enter passphrase: "") if not passphrase: print(""Passphrase must not be blank"") continue passphrase2 = getpass(""Enter same passphrase again: "") if passphrase != passphrase2: print(""Passphrases do not match"") key.init(repository, passphrase) if passphrase: print(""Remember your passphrase. Your data will be inaccessible without it."") return key",false,if not passphrase :,"passphrase2 =passphrase2: print(""Passphrasespassphrase2:passphrase2:passphrase2:",25.0,3.09
"def _generate_create_date(self): if self.timezone is not None: # First, assume correct capitalization tzinfo = tz.gettz(self.timezone) if tzinfo is None: # Fall back to uppercase tzinfo = tz.gettz(self.timezone.upper()) if tzinfo is None: raise util.CommandError(""Can't locate timezone: %s"" % self.timezone) create_date = ( datetime.datetime.utcnow().replace(tzinfo=tz.tzutc()).astimezone(tzinfo) ) else: create_date = datetime.datetime.now() return create_date",false,if tzinfo is None :,timezonetimezonetimezonetimezonetimezonetimezone.lower()) create_date =timezone.lower() create_datetimezone.,0.0,0.0
"def _read_header_lines(fp): """"""Read lines with headers until the start of body"""""" lines = deque() for line in fp: if is_empty(line): break # tricky case if it's not a header and not an empty line # usually means that user forgot to separate the body and newlines # so ""unread"" this line here, what means to treat it like a body if not _RE_HEADER.match(line): fp.seek(fp.tell() - len(line)) break lines.append(line) return lines",false,if not _RE_HEADER . match ( line ) :,lineslines=== fp.tell()= fp.tell()====,33.33,2.66
"def _media_files_drag_received(widget, context, x, y, data, info, timestamp): uris = data.get_uris() files = [] for uri in uris: try: uri_tuple = GLib.filename_from_uri(uri) except: continue uri, unused = uri_tuple if os.path.exists(uri) == True: if utils.is_media_file(uri) == True: files.append(uri) if len(files) == 0: return open_dropped_files(files)",false,if os . path . exists ( uri ) == True :,===============,15.38,5.26
"def remove_importlib(frame, options): if frame is None: return None for child in frame.children: remove_importlib(child, options=options) if ""<frozen importlib._bootstrap"" in child.file_path: # remove this node, moving the self_time and children up to the parent frame.self_time += child.self_time frame.add_children(child.children, after=child) child.remove_from_parent() return frame",false,"if ""<frozen importlib._bootstrap"" in child . file_path :",child,7.69,0.0
"def __call__(self, graph): for layer_name, data in self.params: if layer_name in graph: node = graph.get_node(layer_name) node.data = self.adjust_parameters(node, data) else: print_stderr(""Ignoring parameters for non-existent layer: %s"" % layer_name) return graph",false,if layer_name in graph :,graph:,40.0,0.0
"def test_with_three_points(self): cba = ia.Polygon([(1, 2), (3, 4), (5, 5)]) for i, xy in enumerate(cba): assert i in [0, 1, 2] if i == 0: assert np.allclose(xy, (1, 2)) elif i == 1: assert np.allclose(xy, (3, 4)) elif i == 2: assert np.allclose(xy, (5, 5)) assert i == 2",false,elif i == 1 :,"assert np.allclose(xy, (3,2) elif== 3: assert np.allclose(xy,(",66.67,3.92
"def _serve(self): self._conn = self.manager.request(REQUEST_DNS_LISTENER, self.domain) conn = MsgPackMessages(self._conn) while self.active: request = conn.recv() if not request: logger.warning(""DNS: Recieved empty request. Shutdown"") self.stop() break now = time.time() response = self.handler.process(request) if not response: response = [] used = time.time() - now if used > 1: logger.warning(""DNS: Slow processing speed (%s)s"", used) conn.send(response)",false,if used > 1 :,"self.manager.request(REQUEST_DNS_LISTENER, self.domain)self.handler.process(response)self.manager",0.0,0.0
"def read(cls, fp, **kwargs): major_version, minor_version, count = read_fmt(""2HI"", fp) items = [] for _ in range(count): length = read_fmt(""I"", fp)[0] - 4 if length > 0: with io.BytesIO(fp.read(length)) as f: items.append(Annotation.read(f)) return cls(major_version=major_version, minor_version=minor_version, items=items)",false,if length > 0 :,**kwargs):,20.0,10.68
"def save_uploaded_files(): files = [] unzip = bool(request.form.get(""unzip"") in [""true"", ""on""]) for uploaded_file in request.files.getlist(""files""): if unzip and zipfile.is_zipfile(uploaded_file): with zipfile.ZipFile(uploaded_file, ""r"") as zf: for info in zf.infolist(): name = info.filename size = info.file_size data = zf.read(name) if size > 0: files.append(save_file(data, filename=name.split(""/"")[-1])) else: files.append(save_file(uploaded_file)) return files",false,if unzip and zipfile . is_zipfile ( uploaded_file ) :,===============,0.0,0.0
"def analyze_string_content(self, string, line_num, filename): output = {} if self.keyword_exclude and self.keyword_exclude.search(string): return output for identifier in self.secret_generator( string, filetype=determine_file_type(filename), ): if self.is_secret_false_positive(identifier): continue secret = PotentialSecret( self.secret_type, filename, identifier, line_num, ) output[secret] = secret return output",false,if self . is_secret_false_positive ( identifier ) :,=filename,0.0,0.0
"def _validate_and_set_default_hyperparameters(self): """"""Placeholder docstring"""""" # Check if all the required hyperparameters are set. If there is a default value # for one, set it. for name, definition in self.hyperparameter_definitions.items(): if name not in self.hyperparam_dict: spec = definition[""spec""] if ""DefaultValue"" in spec: self.hyperparam_dict[name] = spec[""DefaultValue""] elif ""IsRequired"" in spec and spec[""IsRequired""]: raise ValueError(""Required hyperparameter: %s is not set"" % name)",false,"elif ""IsRequired"" in spec and spec [ ""IsRequired"" ] :",ifis set.None:None:None:None:None:None:None: self.hyper,7.14,1.91
"def get_code(self, fullname=None): fullname = self._fix_name(fullname) if self.code is None: mod_type = self.etc[2] if mod_type == imp.PY_SOURCE: source = self.get_source(fullname) self.code = compile(source, self.filename, ""exec"") elif mod_type == imp.PY_COMPILED: self._reopen() try: self.code = read_code(self.file) finally: self.file.close() elif mod_type == imp.PKG_DIRECTORY: self.code = self._get_delegate().get_code() return self.code",false,elif mod_type == imp . PKG_DIRECTORY :,self.file.open()self.file.close()self.file.close()self.file.close()self,12.5,1.29
"def eigh_abstract_eval(operand, lower): if isinstance(operand, ShapedArray): if operand.ndim < 2 or operand.shape[-2] != operand.shape[-1]: raise ValueError( ""Argument to symmetric eigendecomposition must have shape [..., n, n],"" ""got shape {}"".format(operand.shape) ) batch_dims = operand.shape[:-2] n = operand.shape[-1] v = ShapedArray(batch_dims + (n, n), operand.dtype) w = ShapedArray(batch_dims + (n,), lax.lax._complex_basetype(operand.dtype)) else: v, w = operand, operand return v, w",false,if operand . ndim < 2 or operand . shape [ - 2 ] != operand . shape [ - 1 ] :,===============,4.17,1.44
"def conninfo_parse(dsn): ret = {} length = len(dsn) i = 0 while i < length: if dsn[i].isspace(): i += 1 continue param_match = PARAMETER_RE.match(dsn[i:]) if not param_match: return param = param_match.group(1) i += param_match.end() if i >= length: return value, end = read_param_value(dsn[i:]) if value is None: return i += end ret[param] = value return ret",false,if not param_match :,dsn==============,0.0,0.0
"def load_weights_from_unsupervised(self, unsupervised_model): update_state_dict = copy.deepcopy(self.network.state_dict()) for param, weights in unsupervised_model.network.state_dict().items(): if param.startswith(""encoder""): # Convert encoder's layers name to match new_param = ""tabnet."" + param else: new_param = param if self.network.state_dict().get(new_param) is not None: # update only common layers update_state_dict[new_param] = weights self.network.load_state_dict(update_state_dict)",false,if self . network . state_dict ( ) . get ( new_param ) is not None :,weights= weightsupdate_state_dict[param] = weights ifis not None:update_state_dict[new,17.65,11.41
"def viewer_setup(self): for key, value in DEFAULT_CAMERA_CONFIG.items(): if isinstance(value, np.ndarray): getattr(self.viewer.cam, key)[:] = value else: setattr(self.viewer.cam, key, value)",false,"if isinstance ( value , np . ndarray ) :",self.viewer.cam,10.0,3.93
"def colormap_changed(change): if change[""new""]: cmap_colors = [ color[1:] for color in cmap.step.__dict__[""_schemes""][colormap.value] ] palette.value = "", "".join(cmap_colors) colorbar = getattr(cmap.step, colormap.value) colorbar_output = self.colorbar_widget with colorbar_output: colorbar_output.clear_output() display(colorbar) if len(palette.value) > 0 and "","" in palette.value: labels = [f""Class {i+1}"" for i in range(len(palette.value.split("","")))] legend_labels.value = "", "".join(labels)",false,"if len ( palette . value ) > 0 and "","" in palette . value :",[] labels =labels[ i + 1] labels[ i + 1] labels[ i + 1,0.0,0.0
"def invalidate(self, layers=None): if layers is None: layers = Layer.AllLayers if layers: layers = set(layers) self.invalidLayers.update(layers) blockRenderers = [ br for br in self.blockRenderers if br.layer is Layer.Blocks or br.layer not in layers ] if len(blockRenderers) < len(self.blockRenderers): self.forgetDisplayLists() self.blockRenderers = blockRenderers if self.renderer.showRedraw and Layer.Blocks in layers: self.needsRedisplay = True",false,if len ( blockRenderers ) < len ( self . blockRenderers ) :,self.renderer.showRedraw and Layer.Blocks inlayers: self.needsRedisplay =self.renderer.showRedraw,23.08,4.24
"def fromstring(cls, input): productions = [] for linenum, line in enumerate(input.split(""\n"")): line = line.strip() if line.startswith(""#"") or line == """": continue try: productions += _read_dependency_production(line) except ValueError: raise ValueError(""Unable to parse line %s: %s"" % (linenum, line)) if len(productions) == 0: raise ValueError(""No productions found!"") return DependencyGrammar(productions)",false,"if line . startswith ( ""#"" ) or line == """" :",class:))))))))))))),12.5,2.92
"def repl(m, base_path, rel_path=None): if m.group(""comments""): tag = m.group(""comments"") else: tag = m.group(""open"") if rel_path is None: tag += RE_TAG_LINK_ATTR.sub( lambda m2: repl_absolute(m2, base_path), m.group(""attr"") ) else: tag += RE_TAG_LINK_ATTR.sub( lambda m2: repl_relative(m2, base_path, rel_path), m.group(""attr"") ) tag += m.group(""close"") return tag",false,if rel_path is None :,"re_tag_link_attr.sub( lambda m2: repl_absolute(m2,re_tag_link_attr",20.0,1.72
"def encode(path): if isinstance(path, str_cls): try: path = path.encode(fs_encoding, ""strict"") except UnicodeEncodeError: if not platform.is_linux(): raise path = path.encode(fs_fallback_encoding, ""strict"") return path",false,if not platform . is_linux ( ) :,path,0.0,0.0
"def __iter__(self): base_iterator = super(ProcessIterable, self).__iter__() if getattr(self.queryset, ""_coerced"", False): for process in base_iterator: if isinstance(process, self.queryset.model): process = coerce_to_related_instance( process, process.flow_class.process_class ) yield process else: for process in base_iterator: yield process",false,"if isinstance ( process , self . queryset . model ) :",self.queryset.model):,58.33,48.95
"def footnotes_under(n: Element) -> Iterator[nodes.footnote]: if isinstance(n, nodes.footnote): yield n else: for c in n.children: if isinstance(c, addnodes.start_of_file): continue elif isinstance(c, nodes.Element): yield from footnotes_under(c)",false,"if isinstance ( c , addnodes . start_of_file ) :",n:,10.0,0.0
"def _process_submissions(self) -> None: """"""Process all submissions which have not been processed yet."""""" while self._to_be_processed: job = self._to_be_processed[0] job.process() # trigger computation if not self.batch_mode: heapq.heappush( self._steady_priority_queue, OrderedJobs(job.release_time, self._order, job), ) self._to_be_processed.popleft() # remove right after it is added to the heap queue self._order += 1",false,if not self . batch_mode :,self._to_be_processed.popleft()self._orderself._orderself._orderself._orderself,33.33,3.33
"def valid_localparts(strip_delimiters=False): for line in ABRIDGED_LOCALPART_VALID_TESTS.split(""\n""): # strip line, skip over empty lines line = line.strip() if line == """": continue # skip over comments or empty lines match = COMMENT.match(line) if match: continue # skip over localparts with delimiters if strip_delimiters: if "","" in line or "";"" in line: continue yield line",false,if match :,line:line:line:line:line:line:line:line:line:,33.33,2.15
"def _get_payload_hash(self, method, data=None): if method in (""POST"", ""PUT""): if data: if hasattr(data, ""next"") or hasattr(data, ""__next__""): # File upload; don't try to read the entire payload return UNSIGNED_PAYLOAD return _hash(data) else: return UNSIGNED_PAYLOAD else: return _hash("""")",false,"if hasattr ( data , ""next"" ) or hasattr ( data , ""__next__"" ) :",_get_payload_hash(data),15.79,2.65
"def get_download_info(self): try: download_info = self.api.get_download_info(self.game) result = True except NoDownloadLinkFound as e: print(e) if Config.get(""current_download"") == self.game.id: Config.unset(""current_download"") GLib.idle_add( self.parent.parent.show_error, _(""Download error""), _( ""There was an error when trying to fetch the download link!\n{}"".format( e ) ), ) download_info = False result = False return result, download_info",false,"if Config . get ( ""current_download"" ) == self . game . id :",())(),11.76,0.77
"def find_id(self, doc_id): self._lock.acquire() try: doc = self._docs.get(doc_id) if doc: doc = copy.deepcopy(doc) doc[""id""] = doc_id return doc finally: self._lock.release()",false,if doc :,=,0.0,0.0
"def assign_art(self, session, task): """"""Place the discovered art in the filesystem."""""" if task in self.art_candidates: candidate = self.art_candidates.pop(task) self._set_art(task.album, candidate, not self.src_removed) if self.src_removed: task.prune(candidate.path)",false,if self . src_removed :,candidate.path),20.0,7.55
"def _replace_named(self, named, replace_scalar): for item in named: for name, value in self._get_replaced_named(item, replace_scalar): if not is_string(name): raise DataError(""Argument names must be strings."") yield name, value",false,if not is_string ( name ) :,:,14.29,0.0
"def qtTypeIdent(conn, *args): # We're not using the conn object at the moment, but - we will # modify the # logic to use the server version specific keywords later. res = None value = None for val in args: # DataType doesn't have len function then convert it to string if not hasattr(val, ""__len__""): val = str(val) if len(val) == 0: continue value = val if Driver.needsQuoting(val, True): value = value.replace('""', '""""') value = '""' + value + '""' res = ((res and res + ""."") or """") + value return res",false,"if Driver . needsQuoting ( val , True ) :",===============,0.0,0.0
"def _update_tileable_and_chunk_shape(self, tileable_graph, chunk_result, failed_ops): for n in tileable_graph: if n.op in failed_ops: continue tiled_n = get_tiled(n) if has_unknown_shape(tiled_n): if any(c.key not in chunk_result for c in tiled_n.chunks): # some of the chunks has been fused continue new_nsplits = self.get_tileable_nsplits(n, chunk_result=chunk_result) for node in (n, tiled_n): node._update_shape(tuple(sum(nsplit) for nsplit in new_nsplits)) tiled_n._nsplits = new_nsplits",false,if n . op in failed_ops :,"n.key, chunk_result=chunk_result)n.chunks):n.chunks):n.chunks):n",42.86,3.18
"def _read_filter(self, data): if data: if self.expected_inner_sha256: self.inner_sha.update(data) if self.expected_inner_md5sum: self.inner_md5.update(data) return data",false,if self . expected_inner_sha256 :,if,20.0,0.0
"def find_previous_editable(self, *args): if self.editw == 0: if self._active_page > 0: self.switch_page(self._active_page - 1) if not self.editw == 0: # remember that xrange does not return the 'last' value, # so go to -1, not 0! (fence post error in reverse) for n in range(self.editw - 1, -1, -1): if self._widgets__[n].editable and not self._widgets__[n].hidden: self.editw = n break",false,if self . _widgets__ [ n ] . editable and not self . _widgets__ [ n ] . hidden :,"self.editw- 1,- 1,- 1,- 1,- 1, -1,-",10.0,2.84
"def _get_event_for_message(self, message_id): with self.event_lock: if message_id not in self._events: raise RuntimeError( ""Event for message[{}] should have been created before accessing"".format( message_id ) ) return self._events[message_id]",false,if message_id not in self . _events :,message_id,12.5,0.0
"def _get_deepest(self, t): if isinstance(t, list): if len(t) == 1: return t[0] else: for part in t: res = self._get_deepest(part) if res: return res return None return None",false,if res :,t,0.0,0.0
"def _get_notify(self, action_node): if action_node.name not in self._skip_notify_tasks: if action_node.notify: task_notify = NotificationsHelper.to_model(action_node.notify) return task_notify elif self._chain_notify: return self._chain_notify return None",false,if action_node . notify :,action_node,20.0,0.0
"def __init__(self, centered=None, shape_params=()): assert centered is None or isinstance(centered, (float, torch.Tensor)) assert isinstance(shape_params, (tuple, list)) assert all(isinstance(name, str) for name in shape_params) if is_validation_enabled(): if isinstance(centered, float): assert 0 <= centered and centered <= 1 elif isinstance(centered, torch.Tensor): assert (0 <= centered).all() assert (centered <= 1).all() else: assert centered is None self.centered = centered self.shape_params = shape_params",false,"elif isinstance ( centered , torch . Tensor ) :",shape_params= shape_params= shape_params self .= shape_params self .= shape_params self .,10.0,1.51
"def collect(self): for nickname in self.squid_hosts.keys(): squid_host = self.squid_hosts[nickname] fulldata = self._getData(squid_host[""host""], squid_host[""port""]) if fulldata is not None: fulldata = fulldata.splitlines() for data in fulldata: matches = self.stat_pattern.match(data) if matches: self.publish_counter( ""%s.%s"" % (nickname, matches.group(1)), float(matches.group(2)) )",false,if matches :,"self.publish_counter( ""%s.%s"" % (nickname,))self.publish_counter( ""%s",0.0,0.0
"def test_len(self): eq = self.assertEqual eq(base64MIME.base64_len(""hello""), len(base64MIME.encode(""hello"", eol=""""))) for size in range(15): if size == 0: bsize = 0 elif size <= 3: bsize = 4 elif size <= 6: bsize = 8 elif size <= 9: bsize = 12 elif size <= 12: bsize = 16 else: bsize = 20 eq(base64MIME.base64_len(""x"" * size), bsize)",false,elif size <= 9 :,bsize)bsize)bsize)bsize)bsize)bsize)bsize)b,0.0,0.0
"def wait_for_initial_conf(self, timeout=1.0): logger.info(""Waiting for initial configuration"") cur_timeout = timeout # Arbiter do not already set our have_conf param while not self.new_conf and not self.interrupted: elapsed, _, _ = self.handleRequests(cur_timeout) if elapsed: cur_timeout -= elapsed if cur_timeout > 0: continue cur_timeout = timeout sys.stdout.write(""."") sys.stdout.flush()",false,if cur_timeout > 0 :,=self.new_conf===========,0.0,2.29
"def __init__(self, querylist=None): self.query_id = -1 if querylist is None: self.querylist = [] else: self.querylist = querylist for query in self.querylist: if self.query_id == -1: self.query_id = query.query_id else: if self.query_id != query.query_id: raise ValueError(""query in list must be same query_id"")",false,if self . query_id != query . query_id :,:self.query_id,40.0,20.96
"def candidates() -> Generator[""Symbol"", None, None]: s = self if Symbol.debug_lookup: Symbol.debug_print(""searching in self:"") print(s.to_string(Symbol.debug_indent + 1), end="""") while True: if matchSelf: yield s if recurseInAnon: yield from s.children_recurse_anon else: yield from s._children if s.siblingAbove is None: break s = s.siblingAbove if Symbol.debug_lookup: Symbol.debug_print(""searching in sibling:"") print(s.to_string(Symbol.debug_indent + 1), end="""")",false,if recurseInAnon :,"end="""")yield from s.children_recurse_anonifs.siblingAbove is None:yield from s.",33.33,1.65
"def get_default_params(problem_type: str, penalty: str): # TODO: get seed from seeds provider if problem_type == REGRESSION: default_params = {""C"": None, ""random_state"": 0, ""fit_intercept"": True} if penalty == L2: default_params[""solver""] = ""auto"" else: default_params = { ""C"": None, ""random_state"": 0, ""solver"": _get_solver(problem_type), ""n_jobs"": -1, ""fit_intercept"": True, } model_params = list(default_params.keys()) return model_params, default_params",false,if penalty == L2 :,model_params[problem_type] =model_params[problem_type] = model_params[problem_type]model,33.33,1.65
"def _UploadDirectory(local_dir: str, gcs_bucket: storage.Bucket, gcs_dir: str): """"""Upload the contents of a local directory to a GCS Bucket."""""" for file_name in os.listdir(local_dir): path = os.path.join(local_dir, file_name) if not os.path.isfile(path): logging.info(""Skipping %s as it's not a file."", path) continue logging.info(""Uploading: %s"", path) gcs_blob = gcs_bucket.blob(f""{gcs_dir}/{file_name}"") gcs_blob.upload_from_filename(path)",false,if not os . path . isfile ( path ) :,"gcs_bucket.blob(f""{gcs_dir}/{file_name}"")gcs_blob.upload(f""{",36.36,1.76
"def decode_query_ids(self, trans, conditional): if conditional.operator == ""and"": self.decode_query_ids(trans, conditional.left) self.decode_query_ids(trans, conditional.right) else: left_base = conditional.left.split(""."")[0] if left_base in self.FIELDS: field = self.FIELDS[left_base] if field.id_decode: conditional.right = trans.security.decode_id(conditional.right)",false,if field . id_decode :,=field.id_decode:= trans.security.decode_id(conditional.right),80.0,23.59
"def data_dir(self) -> Path: try: from appdirs import user_data_dir except ImportError: # linux path = Path.home() / "".local"" / ""share"" if path.exists(): return path / ""dephell"" # mac os path = Path.home() / ""Library"" / ""Application Support"" if path.exists(): return path / ""dephell"" self.pip_main([""install"", ""appdirs""]) from appdirs import user_data_dir return Path(user_data_dir(""dephell""))",false,if path . exists ( ) :,"path =path = Path(user_data_dir(""dephell"")))",42.86,2.83
"def setGameCard(self, isGameCard=False): if isGameCard: targetValue = 1 else: targetValue = 0 for nca in self: if isinstance(nca, Nca): if nca.header.getIsGameCard() == targetValue: continue Print.info(""writing isGameCard for %s, %d"" % (str(nca._path), targetValue)) nca.header.setIsGameCard(targetValue)",false,if nca . header . getIsGameCard ( ) == targetValue :,=nca.header.getIsGameCard()nca.header.setIsGameCard()nca.header.,66.67,31.87
"def check_apns_certificate(ss): mode = ""start"" for s in ss.split(""\n""): if mode == ""start"": if ""BEGIN RSA PRIVATE KEY"" in s or ""BEGIN PRIVATE KEY"" in s: mode = ""key"" elif mode == ""key"": if ""END RSA PRIVATE KEY"" in s or ""END PRIVATE KEY"" in s: mode = ""end"" break elif s.startswith(""Proc-Type"") and ""ENCRYPTED"" in s: raise ImproperlyConfigured( ""Encrypted APNS private keys are not supported"" ) if mode != ""end"": raise ImproperlyConfigured(""The APNS certificate doesn't contain a private key"")",false,"elif mode == ""key"" :","""end""""end""""end""""end""""end""""end""""end""""",25.0,2.05
"def register_aggregate_groups(conn, *groups): seen = set() for group in groups: klasses = AGGREGATE_COLLECTION[group] for klass in klasses: name = getattr(klass, ""name"", klass.__name__) if name not in seen: seen.add(name) conn.create_aggregate(name, -1, klass)",false,if name not in seen :,=,0.0,0.0
"def _impl(inputs, input_types): data = inputs[0] axis = None keepdims = False if len(inputs) > 2: # default, torch have only data, axis=None, keepdims=False if isinstance(inputs[1], int): axis = int(inputs[1]) elif _is_int_seq(inputs[1]): axis = inputs[1] else: axis = list(_infer_shape(inputs[1])) keepdims = bool(inputs[2]) return get_relay_op(name)(data, axis=axis, keepdims=keepdims)",false,elif _is_int_seq ( inputs [ 1 ] ) :,)())))()))))) return get_rel,22.22,3.01
"def walks_generator(): if filelist is not None: bucket = [] for filename in filelist: with io.open(filename) as inf: for line in inf: walk = [int(x) for x in line.strip(""\n"").split("" "")] bucket.append(walk) if len(bucket) == batch_size: yield bucket bucket = [] if len(bucket): yield bucket else: for _ in range(epoch): for nodes in graph.node_batch_iter(batch_size): walks = graph.random_walk(nodes, walk_len) yield walks",false,if len ( bucket ) :,:::::::::::::::,16.67,2.63
"def _calculate_runtimes(states): results = {""runtime"": 0.00, ""num_failed_states"": 0, ""num_passed_states"": 0} for state, resultset in states.items(): if isinstance(resultset, dict) and ""duration"" in resultset: # Count the pass vs failures if resultset[""result""]: results[""num_passed_states""] += 1 else: results[""num_failed_states""] += 1 # Count durations results[""runtime""] += resultset[""duration""] log.debug(""Parsed state metrics: {}"".format(results)) return results",false,"if isinstance ( resultset , dict ) and ""duration"" in resultset :",stateresultset=resultset=resultset,14.29,2.1
"def _replicator_primary_device() -> snt_replicator.Replicator: # NOTE: The explicit device list is required since currently Replicator # only considers CPU and GPU devices. This means on TPU by default we only # mirror on the local CPU. for device_type in (""TPU"", ""GPU"", ""CPU""): devices = tf.config.experimental.list_logical_devices(device_type=device_type) if devices: devices = [d.name for d in devices] logging.info(""Replicating over %s"", devices) return snt_replicator.Replicator(devices=devices) assert False, ""No TPU/GPU or CPU found""",false,if devices :,devices)= tf.config.experimental.list_logical_devices(device_type=device_type) ifdevices:,66.67,1.79
"def get_tag_values(self, event): http = event.interfaces.get(""sentry.interfaces.Http"") if not http: return [] if not http.headers: return [] headers = http.headers # XXX: transitional support for workers if isinstance(headers, dict): headers = headers.items() output = [] for key, value in headers: if key != ""User-Agent"": continue ua = Parse(value) if not ua: continue result = self.get_tag_from_ua(ua) if result: output.append(result) return output",false,"if key != ""User-Agent"" :",===============,10.0,2.63
"def general(metadata, value): if metadata.get(""commands"") and value: if not metadata.get(""nargs""): v = quote(value) else: v = value return u""{0} {1}"".format(metadata[""commands""][0], v) else: if not value: return None elif not metadata.get(""nargs""): return quote(value) else: return value",false,"if not metadata . get ( ""nargs"" ) :",value:,9.09,0.0
"def _actions_read(self, c): self.action_input.handle_read(c) if c in [curses.KEY_ENTER, util.KEY_ENTER2]: # take action if self.action_input.selected_index == 0: # Cancel self.back_to_parent() elif self.action_input.selected_index == 1: # Apply self._apply_prefs() client.core.get_config().addCallback(self._update_preferences) elif self.action_input.selected_index == 2: # OK self._apply_prefs() self.back_to_parent()",false,elif self . action_input . selected_index == 2 :,self.back_to_parent()self.back_to_parent()self.back_to_parent()self.,30.0,3.24
def logic(): if reset == 1: lfsr.next = 1 else: if enable: # lfsr.next[24:1] = lfsr[23:0] lfsr.next = lfsr << 1 lfsr.next[0] = lfsr[23] ^ lfsr[22] ^ lfsr[21] ^ lfsr[16],false,if enable :,lfsr[0]lfsr[0]lfsr[1]lfsr[0]l,0.0,0.0
"def action_delete(self, request, attachments): deleted_attachments = [] desynced_posts = [] for attachment in attachments: if attachment.post: deleted_attachments.append(attachment.pk) desynced_posts.append(attachment.post_id) if desynced_posts: with transaction.atomic(): for post in Post.objects.filter(id__in=desynced_posts): self.delete_from_cache(post, deleted_attachments) for attachment in attachments: attachment.delete() message = _(""Selected attachments have been deleted."") messages.success(request, message)",false,if attachment . post :,attachments.delete() for attachment inattachments.delete() for attachment inattachments.delete() for attachment inattachments.delete() for,40.0,1.65
"def __getitem__(self, index): if self._check(): if isinstance(index, int): if index < 0 or index >= len(self.features): raise IndexError(index) if self.features[index] is None: feature = self.device.feature_request(FEATURE.FEATURE_SET, 0x10, index) if feature: (feature,) = _unpack(""!H"", feature[:2]) self.features[index] = FEATURE[feature] return self.features[index] elif isinstance(index, slice): indices = index.indices(len(self.features)) return [self.__getitem__(i) for i in range(*indices)]",false,"elif isinstance ( index , slice ) :","=self.device.feature_request(FEATURE.FEATURE_SET, 0x10, index):self.device.feature_",62.5,3.63
"def _skip_start(self): start, stop = self.start, self.stop for chunk in self.app_iter: self._pos += len(chunk) if self._pos < start: continue elif self._pos == start: return b"""" else: chunk = chunk[start - self._pos :] if stop is not None and self._pos > stop: chunk = chunk[: stop - self._pos] assert len(chunk) == stop - start return chunk else: raise StopIteration()",false,if self . _pos < start :,=chunk:self._pos,57.14,37.68
"def get_files(d): f = [] for root, dirs, files in os.walk(d): for name in files: if ""meta-environment"" in root or ""cross-canadian"" in root: continue if ""qemux86copy-"" in root or ""qemux86-"" in root: continue if ""do_build"" not in name and ""do_populate_sdk"" not in name: f.append(os.path.join(root, name)) return f",false,"if ""meta-environment"" in root or ""cross-canadian"" in root :",f:name):f:f:f:,5.88,3.12
"def _load_windows_store_certs(self, storename, purpose): certs = bytearray() try: for cert, encoding, trust in enum_certificates(storename): # CA certs are never PKCS#7 encoded if encoding == ""x509_asn"": if trust is True or purpose.oid in trust: certs.extend(cert) except PermissionError: warnings.warn(""unable to enumerate Windows certificate store"") if certs: self.load_verify_locations(cadata=certs) return certs",false,"if encoding == ""x509_asn"" :",certs:cert,12.5,0.0
"def test_tokenizer_identifier_with_correct_config(self): for tokenizer_class in [BertTokenizer, BertTokenizerFast, AutoTokenizer]: tokenizer = tokenizer_class.from_pretrained(""wietsedv/bert-base-dutch-cased"") self.assertIsInstance(tokenizer, (BertTokenizer, BertTokenizerFast)) if isinstance(tokenizer, BertTokenizer): self.assertEqual(tokenizer.basic_tokenizer.do_lower_case, False) else: self.assertEqual(tokenizer.do_lower_case, False) self.assertEqual(tokenizer.model_max_length, 512)",false,"if isinstance ( tokenizer , BertTokenizer ) :","self.assertEqual(tokenizer.basic_tokenizer.do_lower_case, True)self.assertEqual(tokenizer",50.0,3.92
"def run(self): global WAITING_BEFORE_START time.sleep(WAITING_BEFORE_START) while self.keep_alive: path_id, module, resolve = self.queue_receive.get() if path_id is None: continue self.lock.acquire() self.modules[path_id] = module self.lock.release() if resolve: resolution = self._resolve_with_other_modules(resolve) self._relations[path_id] = [] for package in resolution: self._relations[path_id].append(resolution[package]) self.queue_send.put((path_id, module, False, resolution))",false,if resolve :,=self.queue_receive.get()self.queue_receive.get()self.queue_receive.get(),0.0,0.0
"def __new__(mcs, name, bases, attrs): include_profile = include_trace = include_garbage = True bases = list(bases) if name == ""SaltLoggingClass"": for base in bases: if hasattr(base, ""trace""): include_trace = False if hasattr(base, ""garbage""): include_garbage = False if include_profile: bases.append(LoggingProfileMixin) if include_trace: bases.append(LoggingTraceMixin) if include_garbage: bases.append(LoggingGarbageMixin) return super(LoggingMixinMeta, mcs).__new__(mcs, name, tuple(bases), attrs)",false,"if hasattr ( base , ""garbage"" ) :","name, bases, attrs):bases.append(LoggingMixin) ifbases.append(LoggingMixin) ifbases.append(",40.0,3.92
"def __str__(self, prefix="""", printElemNumber=0): res = """" if self.has_owner_: res += prefix + (""owner: %s\n"" % self.DebugFormatString(self.owner_)) cnt = 0 for e in self.entries_: elm = """" if printElemNumber: elm = ""(%d)"" % cnt res += prefix + (""entries%s <\n"" % elm) res += e.__str__(prefix + "" "", printElemNumber) res += prefix + "">\n"" cnt += 1 return res",false,if printElemNumber :,"res +=prefix+""""""""""""""""""""""",0.0,0.0
"def parse_tag(self): buf = [] escaped = False for c in self.get_next_chars(): if escaped: buf.append(c) elif c == ""\\"": escaped = True elif c == "">"": return """".join(buf) else: buf.append(c) raise Exception(""Unclosed tag "" + """".join(buf))",false,"elif c == "">"" :",buf.append(c),12.5,5.82
"def get_batches(train_nodes, train_labels, batch_size=64, shuffle=True): if shuffle: random.shuffle(train_nodes) total = train_nodes.shape[0] for i in range(0, total, batch_size): if i + batch_size <= total: cur_nodes = train_nodes[i : i + batch_size] cur_labels = train_labels[cur_nodes] yield cur_nodes, cur_labels",false,if i + batch_size <= total :,==,12.5,0.0
"def _get_all_info_lines(data): infos = [] for row in data: splitrow = row.split() if len(splitrow) > 0: if splitrow[0] == ""INFO:"": infos.append("" "".join(splitrow[1:])) return infos",false,"if splitrow [ 0 ] == ""INFO:"" :",=,8.33,0.0
